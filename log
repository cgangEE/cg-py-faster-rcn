Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 104, in append_flipped_images
    widths = self._get_widths()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 100, in _get_widths
    for i in xrange(self.num_images)]
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/tattoo.py", line 112, in image_path_at
    return self.image_path_from_index(self._image_index[i])
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/tattoo.py", line 119, in image_path_from_index
    'Path does not exist: {}'.format(image_path)
AssertionError: Path does not exist: /home/cgangee/code/cg-py-faster-rcnn/data/tattoo/image/./tattoo/1009019541_ad75c5b35f.jpg.jpg
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 106, in append_flipped_images
    boxes = self.roidb[i]['boxes'].copy()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 67, in roidb
    self._roidb = self.roidb_handler()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/tattoo.py", line 49, in gt_roidb
    for index in self.image_index]
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/tattoo.py", line 68, in _load_tattoo_annotation
    label = open(filename)
IOError: [Errno 2] No such file or directory: '/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/label/./tattoo/1009019541_ad75c5b35f.jpg.txt'
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 106, in append_flipped_images
    boxes = self.roidb[i]['boxes'].copy()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 67, in roidb
    self._roidb = self.roidb_handler()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/tattoo.py", line 53, in gt_roidb
    'tattoo_annotations.txt'))
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/getBoundingBox.py", line 50, in getBoundingBox
    im = im[:, :, (2, 1, 0)]
TypeError: 'NoneType' object has no attribute '__getitem__'
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 106, in append_flipped_images
    boxes = self.roidb[i]['boxes'].copy()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 67, in roidb
    self._roidb = self.roidb_handler()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/tattoo.py", line 53, in gt_roidb
    'tattoo_annotations.txt'))
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/getBoundingBox.py", line 50, in getBoundingBox
    im = im[:, :, (2, 1, 0)]
TypeError: 'NoneType' object has no attribute '__getitem__'
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

160.569106 191.295547 67.073171 57.692308
94.512195 49.595142 215.447154 451.417004
208.595388 164.167770 18.867925 19.788079
26.142056 294.549266 150.005607 131.027254
147.827103 378.406709 117.328037 56.603774
252.084112 309.224319 38.590654 42.976939
19.548583 224.318658 62.690283 132.075472
250.761134 235.849057 43.141700 55.555556
225.819838 299.790356 39.097166 39.832285
256.153846 246.331237 72.801619 89.098532
0.000000 166.129496 494.776828 108.546763
95.677570 60.796646 163.937815 392.033543
138.917004 225.366876 204.959514 117.400419
131.325911 0.000000 201.923077 193.920335
160.172065 122.641509 101.720648 109.014675
36.437247 83.857442 271.002024 370.020964
0.674089 17.819706 177.285425 459.119497
140.884615 198.113208 48.534413 202.306080
159.085020 49.266247 142.906883 169.811321
10.785425 78.616352 301.317814 386.792453
100.439271 88.050314 184.700405 337.526205
0.000000 111.111111 295.251012 278.825996
49.882591 0.000000 235.931174 500.000000
265.591093 0.000000 68.082996 136.268344
24.267206 32.494759 293.228745 439.203354
57.297571 145.702306 252.109312 271.488470
50.556680 28.301887 227.168016 442.348008
254.510921 88.437160 51.756885 160.142965
167.174089 378.406709 101.787449 75.471698
270.983806 211.740042 16.852227 33.542977
104.483806 209.643606 168.522267 83.857442
76.172065 361.635220 26.289474 30.398323
77.520243 405.660377 14.829960 27.253669
294.576923 199.161426 18.874494 36.687631
142.232794 153.039832 111.224696 123.689727
256.153846 212.788260 9.437247 14.675052
250.761134 202.306080 51.904858 84.905660
283.117409 228.511530 24.267206 50.314465
192.115385 136.268344 174.589069 145.702306
133.469636 0.000000 97.742915 64.989518
278.398785 0.000000 36.400810 76.519916
304.014170 37.735849 66.734818 29.350105
330.303644 64.989518 41.119433 25.157233
250.761134 98.532495 122.010121 171.907757
128.751012 50.314465 104.483806 193.920335
184.700405 98.532495 122.684211 129.979036
203.574899 74.423480 74.149798 74.423480
197.508097 274.633124 126.728745 192.872117
103.135628 196.016771 31.682186 59.748428
146.277328 218.029350 43.815789 54.507338
198.856275 231.656184 18.874494 27.253669
212.338057 221.174004 31.008097 48.218029
240.649798 147.798742 44.489879 160.377358
0.000000 234.800839 155.714575 97.484277
141.558704 122.641509 70.105263 88.050314
99.091093 332.285115 82.912955 165.618449
175.937247 245.283019 84.935223 58.700210
248.064777 162.473795 72.801619 52.410901
280.421053 210.691824 58.645749 62.893082
49.882591 71.278826 29.659919 45.073375
67.408907 113.207547 26.963563 39.832285
49.208502 206.498952 23.593117 23.060797
73.475709 208.595388 17.526316 16.771488
122.684211 294.549266 12.807692 20.964361
101.787449 314.465409 15.504049 18.867925
299.969636 66.037736 20.896761 14.675052
299.295547 94.339623 16.178138 9.433962
377.489879 139.412998 18.874494 11.530398
405.127530 122.641509 18.874494 20.964361
434.787449 206.498952 18.200405 19.916143
463.773279 198.113208 22.244939 19.916143
99.765182 28.301887 330.303644 316.561845
76.172065 302.935010 173.914980 168.763103
53.253036 156.184486 94.372470 107.966457
141.558704 179.245283 52.578947 83.857442
193.463563 180.293501 55.275304 89.098532
254.805668 172.955975 49.208502 77.568134
312.103239 172.955975 59.319838 75.471698
361.311741 167.714885 66.734818 77.568134
409.846154 165.618449 76.846154 81.761006
88.979757 91.194969 235.257085 181.341719
17.094017 25.585132 441.595442 182.362110
80.712788 92.086093 290.356394 285.198675
97.165992 70.230608 195.091093 428.721174
5.313765 10.482180 273.279352 485.324948
156.310273 148.073394 46.289308 35.229358
0.000000 0.827815 495.807128 369.205298
34.591195 28.145695 250.524109 406.456954
99.580713 21.523179 263.102725 323.675497
374.213836 122.516556 57.651992 81.125828
50.314465 128.311258 59.748428 72.019868
51.619433 164.570231 264.929150 315.513627
273.279352 330.188679 24.291498 40.880503
278.593117 376.310273 25.809717 75.471698
180.968661 78.400000 227.008547 36.800000
176.866097 135.200000 261.652422 186.400000
0.821862 41.928721 404.356275 458.071279
216.971660 59.748428 186.562753 207.547170
88.761134 193.920335 69.858300 159.329140
166.016194 193.920335 68.214575 157.232704
159.441296 46.121593 178.344130 302.935010
155.331984 7.337526 250.668016 323.899371
0.000000 0.000000 373.947368 500.000000
221.902834 117.400419 128.210526 165.618449
9.040486 205.450734 108.485830 51.362683
123.279352 180.293501 136.429150 52.410901
264.639676 170.859539 36.161943 37.735849
309.842105 166.666667 55.886640 37.735849
369.016194 160.377358 92.870445 31.446541
250.668016 62.893082 119.991903 131.027254
36.983806 61.844864 387.919028 226.415094
123.279352 235.849057 126.566802 63.941300
71.502024 181.341719 100.267206 91.194969
55.886640 186.582809 142.182186 148.846960
27.121457 122.641509 41.914980 46.121593
181.631579 24.109015 50.955466 90.146751
230.943320 52.410901 28.765182 61.844864
258.064777 53.459119 40.271255 84.905660
306.554656 34.591195 13.149798 87.002096
327.923077 59.748428 28.765182 61.844864
362.441296 57.651992 30.408907 61.844864
401.068826 45.073375 23.012146 72.327044
198.890688 135.220126 204.643725 92.243187
198.890688 219.077568 209.574899 94.339623
208.753036 308.176101 189.850202 79.664570
82.238866 202.306080 160.433198 220.125786
1.348178 40.880503 323.562753 422.431866
142.906883 171.907757 18.200405 15.723270
107.854251 150.943396 56.623482 80.712788
93.698381 177.148847 9.437247 14.675052
179.981781 175.052411 141.558704 71.278826
163.129555 215.932914 12.807692 18.867925
151.670040 192.872117 15.504049 20.964361
154.366397 163.522013 15.504049 15.723270
127.402834 174.004193 16.178138 13.626834
122.684211 137.316562 22.244939 23.060797
146.951417 113.207547 12.807692 20.964361
173.914980 74.423480 139.536437 112.159329
157.736842 88.050314 16.178138 16.771488
183.352227 74.423480 20.896761 22.012579
158.410931 115.303983 16.852227 29.350105
98.417004 42.976939 148.299595 166.666667
59.319838 103.773585 32.356275 161.425577
276.376518 79.664570 54.601215 145.702306
17.526316 101.677149 22.919028 39.832285
125.380567 297.693920 100.439271 133.123690
86.957490 50.314465 43.815789 68.134172
268.961538 24.109015 38.423077 132.075472
72.801619 140.461216 161.107287 140.461216
217.056680 123.689727 61.342105 70.230608
45.163968 99.580713 150.321862 298.742138
194.811741 178.197065 140.210526 234.800839
146.951417 34.591195 153.692308 269.392034
59.993927 30.398323 150.321862 406.708595
40.445344 40.880503 277.724696 424.528302
6.066802 2.096436 328.955466 483.228512
156.388664 379.454927 74.149798 118.448637
113.921053 262.054507 51.904858 66.037736
115.943320 151.991614 72.127530 87.002096
13.481781 52.410901 227.842105 441.299790
70.779352 0.000000 230.538462 461.215933
25.615385 246.331237 75.497976 113.207547
207.619433 106.918239 124.032389 135.220126
62.016194 14.675052 270.983806 323.899371
83.587045 88.050314 128.076923 313.417191
114.595142 232.704403 86.957490 23.060797
125.380567 199.161426 74.823887 41.928721
68.082996 82.809224 208.967611 358.490566
33.030364 53.459119 59.319838 340.670860
138.188259 49.266247 137.514170 399.371069
264.242915 451.781971 16.178138 20.964361
282.443320 445.492662 21.570850 26.205451
20.222672 31.446541 163.129555 191.823899
178.633603 116.352201 120.661943 172.955975
192.789474 79.664570 33.704453 36.687631
229.190283 69.182390 41.119433 41.928721
275.028340 76.519916 36.400810 44.025157
176.100629 189.569536 66.037736 52.152318
204.402516 142.384106 98.532495 95.198675
381.551363 100.993377 67.085954 139.900662
1.048218 43.874172 496.855346 288.079470
28.396761 5.241090 269.093117 479.035639
87.218623 220.125786 108.178138 191.823899
65.582996 91.194969 80.457490 206.498952
64.906883 174.004193 181.874494 140.461216
243.400810 156.184486 76.400810 94.339623
31.101215 13.626834 191.340081 463.312369
5.408907 194.968553 189.311741 301.886792
77.076923 137.316562 160.914980 362.683438
96.008097 55.555556 119.672065 83.857442
0.000000 0.000000 275.178138 325.995807
16.226721 4.192872 285.319838 485.324948
10.141700 10.482180 126.433198 469.601677
208.242915 154.088050 80.457490 57.651992
142.659919 133.123690 57.469636 56.603774
5.408907 1.048218 210.271255 445.492662
22.987854 71.278826 242.048583 305.031447
101.417004 51.362683 174.437247 269.392034
4.056680 0.000000 171.732794 486.373166
54.765182 24.109015 223.793522 440.251572
187.175043 162.145749 101.906412 87.449393
176.776430 0.000000 123.743501 225.000000
206.666667 110.691824 31.111111 45.077407
76.666667 112.368973 31.666667 45.283019
49.605517 35.348643 114.368276 139.315240
26.180690 0.519833 232.411034 499.559499
136.956522 347.800000 73.913043 75.345455
191.847826 227.381818 34.782609 22.200000
222.826087 218.636364 9.782609 14.127273
14.948747 337.526205 82.004556 90.146751
155.466970 215.932914 35.876993 56.603774
36.269685 42.976939 191.092520 422.431866
129.017341 76.764706 126.936416 141.882353
148.786127 47.647059 229.421965 207.000000
253.872832 133.941176 121.734104 139.235294
213.815029 64.588235 106.127168 102.705882
46.300578 113.823529 273.641618 282.705882
101.965318 119.647059 161.271676 272.117647
119.653179 138.705882 128.497110 149.294118
170.115607 130.235294 11.445087 15.352941
182.080925 141.352941 12.485549 18.529412
196.127168 136.588235 11.445087 15.352941
202.369942 137.117647 35.895954 29.117647
243.468208 126.529412 56.184971 42.352941
304.855491 123.882353 20.289017 22.235294
323.063584 138.705882 8.323699 13.764706
331.387283 117.529412 11.445087 27.529412
359.479769 112.235294 15.606936 22.764706
376.127168 125.470588 17.167630 20.117647
215.375723 243.000000 18.208092 21.176471
108.728324 20.117647 182.080925 478.588235
126.936416 76.235294 142.023121 107.470588
14.566474 3.705882 214.855491 490.764706
197.148476 37.532374 209.439528 186.863309
278.761062 195.647482 12.782694 44.719424
201.885880 252.620545 9.044487 23.060797
313.667650 283.273381 86.529007 60.251799
148.475910 160.971223 225.172075 142.086331
212.881023 166.366906 51.622419 57.553957
125.483559 112.159329 133.462282 244.234801
75.221239 89.928058 30.481809 33.273381
109.144543 174.460432 147.492625 44.064748
330.875123 221.223022 99.803343 58.453237
318.092429 164.568345 56.047198 27.877698
379.547689 131.294964 7.866273 16.187050
139.626352 159.172662 11.799410 22.482014
142.084562 213.129496 45.722714 36.870504
8.196325 9.433962 111.797872 159.329140
29.178917 93.291405 160.647969 299.790356
30.317073 119.496855 37.666667 129.979036
45.934959 104.821803 254.479675 352.201258
107.966457 46.357616 292.452830 309.602649
127.882600 48.013245 286.163522 288.907285
145.702306 10.761589 257.861635 310.430464
175.052411 51.324503 188.679245 208.609272
5.044534 20.964361 333.659919 458.071279
103.052632 162.473795 153.497976 192.872117
70.623482 36.687631 245.740891 439.203354
0.000000 340.670860 43.238866 28.301887
51.886640 68.134172 407.165992 245.283019
0.000000 24.109015 169.352227 180.293501
56.210526 129.979036 100.890688 120.545073
155.659919 126.834382 182.323887 155.136268
341.587045 122.641509 98.008097 120.545073
464.097166 94.339623 34.591093 74.423480
0.000000 323.899371 226.283401 52.410901
28.105263 50.314465 423.020243 258.909853
237.813765 2.096436 82.153846 263.102725
113.141700 84.905660 185.206478 140.461216
148.453441 89.098532 152.777328 175.052411
27.384615 74.423480 472.024291 236.897275
71.344130 1.048218 272.404858 30.398323
413.651822 0.000000 85.036437 45.073375
20.178138 0.000000 464.817814 479.035639
38.714575 65.391990 293.016194 351.218267
182.186235 75.939085 147.267206 121.291594
154.858300 117.072756 245.951417 157.151717
89.574899 124.455722 324.898785 37.969542
105.516194 20.039481 235.323887 282.662149
187.500000 356.491814 85.020243 15.820643
5.313765 6.328257 369.686235 487.275793
12.904858 98.087984 57.692308 137.112236
74.392713 219.379578 126.012146 75.939085
146.508097 111.799208 45.546559 32.695995
135.880567 168.753522 60.728745 36.914833
207.995951 90.705018 43.269231 61.173152
10.627530 75.939085 63.006073 55.899604
0.000000 251.020863 69.078947 137.112236
18.977733 91.759727 312.753036 393.406647
30.364372 390.242519 18.977733 52.735475
239.119433 287.935696 65.283401 52.735475
64.524291 206.723064 207.995951 267.896215
160.931174 302.701629 15.941296 27.422447
38.714575 294.263953 56.933198 191.957131
61.487854 190.902421 226.973684 271.060344
96.406883 124.455722 141.194332 198.285388
155.617409 202.504226 41.751012 40.078961
162.449393 284.771568 9.109312 16.875352
181.427126 286.880987 15.182186 14.765933
201.923077 282.662149 12.145749 17.930062
222.419028 271.060344 12.145749 23.203609
299.089069 310.084596 15.941296 14.765933
319.585020 328.014657 12.145749 13.711224
334.767206 330.124076 14.423077 18.984771
356.022267 337.507043 12.904858 17.930062
16.700405 17.930062 141.953441 141.331074
346.153846 266.841506 34.159919 36.914833
393.218623 22.148900 94.888664 179.300617
53.896761 64.337280 307.439271 326.959948
74.392713 18.984771 220.141700 446.142123
7.591093 0.000000 212.550607 478.838117
214.068826 0.000000 207.236842 473.564570
59.210526 3.164129 207.995951 490.439922
56.933198 238.364349 28.846154 45.352509
146.508097 175.081779 66.042510 46.407218
220.900810 215.160740 17.459514 49.571347
87.297571 215.160740 133.603239 102.306822
72.115385 120.236884 177.631579 260.513249
26.568826 239.419059 472.165992 137.112236
142.712551 125.510432 180.668016 112.853918
34.919028 68.556118 219.382591 364.929490
52.378543 65.391990 282.388664 433.485608
24.291498 318.522272 336.285425 55.899604
313.512146 126.565141 185.222672 243.637897
45.546559 0.000000 336.285425 239.419059
128.289474 85.431470 245.951417 252.075573
256.578947 304.811048 118.421053 189.847712
12.904858 7.382967 284.665992 491.494631
66.042510 287.935696 81.224696 78.048504
88.815789 0.000000 329.453441 348.054138
112.348178 245.747316 88.056680 106.525660
74.392713 130.783979 167.004049 228.871964
95.647773 8.437676 271.002024 223.598416
99.443320 164.534683 181.427126 274.224472
116.902834 244.692606 47.064777 54.844894
199.645749 300.592210 27.327935 32.695995
41.751012 47.461928 327.176113 210.941902
57.692308 51.680766 207.995951 433.485608
103.238866 140.276365 201.923077 296.373372
113.866397 185.628874 21.255061 28.477157
359.058704 194.066550 15.182186 26.367738
137.398785 146.604622 212.550607 89.650308
112.348178 36.914833 189.018219 445.087413
283.906883 66.446699 12.145749 21.094190
61.487854 73.829666 201.163968 265.786796
0.000000 34.805414 150.303644 323.795819
152.580972 22.148900 175.354251 332.233496
330.971660 15.820643 168.522267 342.780591
48.582996 46.407218 205.718623 303.756339
302.125506 123.401013 35.678138 27.422447
55.555556 112.264901 231.656184 209.417219
85.953878 78.441501 362.683438 145.368653
113.993711 35.639413 155.660377 118.448637
253.144654 58.700210 39.308176 26.205451
151.729560 103.773585 106.918239 116.352201
98.270440 171.907757 187.893082 310.272537
169.025157 10.482180 50.314465 39.832285
228.773585 20.964361 88.050314 135.220126
92.767296 175.052411 209.119497 107.966457
92.767296 38.784067 65.251572 99.580713
156.446541 52.410901 125.786164 182.389937
132.075472 196.016771 119.496855 55.555556
76.257862 4.192872 117.924528 487.421384
24.371069 49.266247 441.823899 211.740042
108.490566 77.568134 73.113208 57.651992
158.805031 47.169811 76.257862 47.169811
262.578616 39.832285 40.880503 24.109015
312.893082 41.928721 18.867925 19.916143
120.283019 163.522013 20.440252 33.542977
239.779874 224.318658 45.597484 46.121593
377.358491 259.958071 39.308176 39.832285
272.012579 277.777778 18.867925 16.771488
290.880503 299.790356 18.867925 16.771488
80.974843 278.825996 78.616352 109.014675
106.132075 192.872117 23.584906 28.301887
125.786164 89.098532 150.943396 215.932914
175.314465 161.425577 14.150943 36.687631
186.320755 162.473795 21.226415 33.542977
211.477987 139.412998 50.314465 38.784067
308.962264 295.597484 25.943396 60.796646
246.069182 285.115304 29.088050 104.821803
125.786164 262.054507 123.427673 137.316562
121.069182 1.048218 200.471698 361.635220
175.314465 72.327044 32.232704 23.060797
136.006289 93.291405 85.691824 372.117400
141.509434 38.784067 105.345912 338.574423
202.830189 64.989518 36.163522 39.832285
145.440252 211.740042 80.188679 78.616352
225.123808 79.664570 147.712815 189.727463
0.789908 386.792453 104.267869 46.121593
76.621086 394.129979 148.502723 104.821803
176.149506 90.146751 86.099983 176.100629
22.117427 29.350105 184.838495 314.465409
56.083475 132.075472 18.957794 28.301887
78.990810 157.232704 22.907335 28.301887
160.351344 147.798742 23.697243 36.687631
297.005445 47.169811 186.418312 277.777778
319.122872 164.570231 31.596324 27.253669
436.029271 116.352201 18.957794 27.253669
432.079730 162.473795 25.277059 26.205451
348.349472 315.513627 24.487151 32.494759
406.012763 325.995807 18.957794 39.832285
33.966048 0.000000 463.676054 331.236897
36.335773 0.000000 172.199966 500.000000
284.366916 44.025157 125.595388 206.498952
123.225664 9.433962 309.643975 371.069182
262.249489 49.266247 92.419248 95.387841
125.595388 211.740042 34.755956 51.362683
159.561436 207.547170 82.940350 75.471698
0.000000 40.880503 88.469707 194.968553
108.217410 60.796646 68.722005 56.603774
303.324710 73.375262 69.511913 60.796646
176.149506 120.545073 113.746766 139.412998
52.133935 28.301887 340.450391 419.287212
253.560500 70.230608 180.099047 212.788260
101.898145 123.689727 238.552246 179.245283
357.828369 219.077568 59.243107 94.339623
268.568754 137.316562 41.075221 28.301887
54.503659 0.000000 186.418312 498.951782
169.040333 205.450734 82.940350 59.748428
58.453199 47.169811 208.535738 356.394130
45.024762 318.658281 18.957794 80.712788
221.174268 113.207547 60.822924 90.146751
198.266933 253.668763 78.200902 219.077568
51.344026 188.679245 40.285313 27.253669
62.402740 248.427673 37.125681 31.446541
59.243107 288.259958 94.788972 126.834382
120.066031 29.350105 255.140316 283.018868
151.662355 72.327044 218.804544 183.438155
149.292631 88.050314 18.957794 64.989518
218.804544 89.098532 32.386232 32.494759
168.250425 188.679245 36.335773 34.591195
164.300885 183.438155 12.638530 20.964361
240.132062 196.016771 8.688989 18.867925
46.604578 1.048218 270.148570 497.903564
119.276123 39.832285 252.770592 281.970650
80.570626 10.482180 290.686181 460.167715
281.446541 201.257862 18.867925 32.494759
278.301887 237.945493 58.176101 53.459119
243.710692 280.922432 32.232704 44.025157
259.433962 312.368973 31.446541 41.928721
264.937107 353.249476 26.729560 38.784067
102.201258 29.350105 185.534591 438.155136
99.056604 93.291405 171.383648 377.358491
261.006289 138.364780 55.817610 25.157233
152.515723 122.641509 246.855346 89.098532
301.886792 196.016771 71.540881 23.060797
0.822584 0.000000 419.517612 499.949686
93.774525 96.628931 174.387713 260.477987
208.113639 75.622642 37.838843 30.459119
78.145438 0.000000 157.113459 81.924528
69.097018 32.559748 127.500451 195.358491
305.178498 13.654088 122.564949 214.264151
27.145257 0.000000 239.371814 302.490566
83.080939 363.408805 60.048599 133.389937
113.516530 391.767296 32.903342 26.257862
108.581029 466.339623 23.854923 23.106918
224.565310 290.937107 149.710207 206.911950
16.224189 24.618705 212.389381 114.604317
31.956735 162.992806 177.482793 89.136691
274.336283 22.071942 195.181908 140.920863
296.951819 166.388489 176.991150 79.798561
301.376598 196.100719 174.041298 67.913669
11.262055 48.218029 276.914046 444.444444
104.670860 184.486373 67.572327 64.989518
14.574423 11.530398 225.903564 483.228512
27.823899 202.306080 18.549266 20.964361
2.649895 211.740042 82.809224 113.207547
277.576520 331.236897 72.209644 35.639413
407.421384 177.148847 54.322851 119.496855
316.662474 214.884696 9.274633 14.675052
374.297694 203.354298 6.624738 18.867925
369.660377 240.041929 51.010482 47.169811
389.534591 350.104822 23.849057 22.012579
39.823009 68.345324 430.678466 208.633094
66.371681 61.151079 409.046214 141.187050
176.007866 34.172662 211.406096 334.532374
47.689282 134.892086 418.387414 213.129496
126.352016 26.978417 287.610619 285.071942
201.081613 110.611511 10.324484 14.388489
0.000000 94.339623 284.591195 295.597484
7.861635 185.534591 28.301887 33.542977
135.220126 98.532495 32.232704 24.109015
102.987421 6.289308 261.792453 353.249476
73.899371 0.000000 204.402516 481.132075
316.823899 401.467505 56.603774 99.580713
103.773585 75.471698 80.188679 107.966457
192.610063 119.496855 21.226415 36.687631
146.226415 157.232704 41.666667 55.555556
73.899371 132.075472 252.358491 127.882600
117.138365 19.916143 172.955975 341.719078
76.257862 5.241090 147.012579 462.264151
58.176101 0.000000 211.477987 395.178197
33.805031 292.452830 18.867925 71.278826
100.628931 438.155136 40.880503 46.121593
161.163522 437.106918 34.591195 42.976939
209.905660 418.238994 26.729560 37.735849
245.283019 378.406709 25.943396 33.542977
68.396226 23.060797 336.477987 273.584906
44.811321 298.742138 36.163522 59.748428
72.327044 319.706499 49.528302 37.735849
176.100629 341.719078 30.660377 15.723270
215.408805 341.719078 32.232704 15.723270
251.572327 342.767296 43.238994 14.675052
298.742138 329.140461 62.106918 29.350105
362.421384 317.610063 54.245283 41.928721
401.729560 292.452830 49.528302 52.410901
442.610063 182.389937 35.377358 88.050314
88.050314 0.000000 208.333333 411.949686
82.547170 271.488470 13.364780 31.446541
195.754717 430.817610 25.157233 18.867925
237.421384 331.236897 26.729560 53.459119
283.805031 353.249476 11.792453 22.012579
241.352201 397.274633 21.226415 23.060797
84.905660 32.494759 263.364780 270.440252
134.433962 48.218029 130.503145 377.358491
179.323215 165.949686 96.242276 82.974843
269.392034 95.198675 80.712788 66.225166
79.664570 24.834437 348.008386 123.344371
145.702306 87.748344 72.327044 55.463576
297.693920 91.887417 70.230608 51.324503
115.303983 117.549669 288.259958 177.152318
136.268344 189.569536 92.243187 48.013245
278.825996 187.913907 93.291405 46.357616
154.088050 72.847682 210.691824 202.814570
127.882600 133.278146 220.125786 67.052980
133.123690 192.880795 44.025157 43.046358
177.148847 201.986755 47.169811 47.185430
226.415094 216.059603 41.928721 43.874172
80.712788 74.503311 60.796646 59.602649
100.628931 105.960265 81.761006 51.324503
190.775681 119.205298 182.389937 232.615894
178.390688 58.700210 158.653846 392.033543
59.969636 158.280922 221.659919 290.356394
63.765182 79.664570 412.196356 170.859539
52.378543 44.025157 136.639676 145.702306
130.566802 193.920335 51.619433 64.989518
147.267206 162.473795 54.655870 44.025157
356.022267 181.341719 25.050607 27.253669
368.927126 207.547170 17.459514 19.916143
384.109312 225.366876 12.145749 16.771488
0.000000 0.000000 251.265182 497.903564
53.896761 28.301887 284.665992 468.553459
25.050607 17.819706 217.864372 467.505241
50.860324 118.448637 345.394737 114.255765
256.578947 214.884696 59.210526 55.555556
132.844130 32.494759 223.937247 276.729560
97.484277 59.138833 233.967187 165.409029
194.968553 355.606925 16.771488 19.221996
209.643606 359.975560 12.578616 20.969450
225.366876 344.248473 36.687631 22.716904
264.150943 335.511202 55.555556 27.085540
59.969636 60.796646 179.908907 400.419287
214.068826 384.696017 20.495951 37.735849
182.945344 289.308176 48.582996 80.712788
153.340081 74.423480 54.655870 105.870021
205.718623 202.306080 15.182186 31.446541
40.232794 134.171908 213.309717 157.232704
119.385733 35.639413 152.375400 355.345912
82.185039 38.153558 218.996063 292.093633
170.283019 172.752809 109.905660 96.910112
198.584906 266.853933 34.905660 51.966292
105.660377 231.741573 26.886792 141.151685
130.188679 149.578652 158.962264 280.196629
72.641509 0.000000 330.188679 373.595506
189.622642 242.977528 91.509434 53.370787
200.471698 311.095506 145.283019 33.707865
244.811321 131.320225 194.811321 191.011236
62.735849 73.033708 296.226415 219.803371
166.981132 115.870787 36.320755 22.471910
71.789687 14.940239 20.728008 41.085657
73.306370 60.507968 47.522750 61.254980
97.573306 126.245020 41.456016 58.266932
118.301314 192.729084 43.983822 54.531873
127.906977 253.984064 50.050556 55.278884
135.995956 312.998008 53.083923 44.073705
148.129424 361.553785 23.761375 18.675299
379.170880 187.500000 17.694641 20.916335
384.732053 201.693227 29.322548 32.868526
86.450961 147.161355 29.828109 44.820717
195.652174 20.916335 178.463094 451.195219
0.505561 23.157371 180.990900 463.147410
34.883721 0.000000 464.610718 373.505976
76.845298 0.000000 69.261881 322.709163
158.746208 0.000000 182.507583 183.017928
65.722952 102.340637 11.627907 45.567729
371.587462 0.000000 77.350859 330.179283
8.088979 123.256972 251.769464 375.000000
38.422649 67.978088 292.214358 363.047809
187.057634 64.243028 158.746208 174.800797
178.968655 9.711155 147.118301 47.808765
127.906977 103.834661 252.275025 186.752988
161.274014 157.619522 66.734075 71.713147
229.019211 184.511952 27.805865 35.109562
183.518706 290.587649 36.905966 28.386454
270.475228 296.563745 22.244692 24.651394
309.908999 345.866534 41.456016 23.904382
310.414560 262.948207 69.767442 56.025896
356.926188 177.041833 22.750253 61.254980
29.828109 0.747012 117.290192 121.015936
110.212336 107.569721 107.178969 106.822709
0.000000 184.511952 77.856421 178.535857
8.594540 293.575697 312.436805 134.462151
161.779575 411.603586 9.605662 11.205179
107.178969 418.326693 10.111223 14.193227
213.852376 416.085657 12.639029 12.699203
300.303337 388.446215 9.605662 13.446215
271.991911 326.444223 10.111223 7.470120
39.433771 318.974104 12.133468 11.205179
22.244692 388.446215 9.605662 11.952191
104.651163 188.994024 196.663296 225.597610
176.946411 430.278884 82.406471 30.627490
257.330637 428.037849 37.917088 24.651394
31.344793 53.037849 225.985844 355.577689
0.000000 6.723108 323.053589 492.280876
5.055612 77.689243 169.868554 298.804781
0.000000 0.000000 374.240891 498.951782
26.568826 179.245283 63.765182 40.880503
0.000000 213.836478 191.295547 131.027254
211.032389 156.184486 79.706478 78.616352
124.493927 96.436059 154.099190 373.165618
159.412955 206.498952 38.714575 50.314465
117.661943 115.303983 115.384615 234.800839
37.196356 190.775681 54.655870 83.857442
321.862348 171.907757 70.597166 83.857442
173.836032 125.786164 85.779352 85.953878
59.210526 191.823899 47.064777 104.821803
73.633603 312.368973 65.283401 105.870021
173.836032 208.595388 88.056680 113.207547
63.765182 75.471698 94.129555 132.075472
113.107287 68.134172 315.030364 115.303983
230.769231 127.882600 92.611336 159.329140
274.038462 75.471698 18.977733 39.832285
221.659919 109.014675 207.236842 167.714885
92.611336 46.121593 154.858300 431.865828
63.006073 57.651992 182.186235 359.538784
23.532389 68.134172 121.457490 255.765199
122.975709 101.677149 225.455466 255.765199
200.404858 38.784067 63.765182 39.832285
338.562753 72.327044 128.289474 262.054507
327.935223 350.104822 10.627530 11.530398
331.730769 364.779874 16.700405 14.675052
343.117409 377.358491 14.423077 12.578616
355.263158 382.599581 8.350202 9.433962
143.471660 127.882600 129.807692 216.981132
56.174089 258.909853 93.370445 62.893082
217.105263 294.549266 58.451417 18.867925
161.690283 240.041929 92.611336 59.748428
262.651822 241.090147 24.291498 17.819706
146.508097 192.872117 16.700405 60.796646
169.281377 209.643606 31.123482 102.725367
211.032389 213.836478 46.305668 116.352201
274.797571 210.691824 53.896761 117.400419
248.228745 269.392034 40.232794 36.687631
334.008097 198.113208 35.678138 106.918239
369.686235 190.775681 15.941296 67.085954
78.947368 256.813417 168.522267 237.945493
40.991903 27.253669 238.360324 444.444444
71.356275 133.123690 29.605263 45.073375
219.382591 131.027254 19.736842 30.398323
243.674089 131.027254 70.597166 157.232704
254.301619 308.176101 44.787449 187.631027
137.398785 189.727463 89.574899 159.329140
356.022267 140.461216 16.700405 18.867925
349.949393 155.136268 22.014170 9.433962
113.866397 207.547170 37.955466 117.400419
158.653846 268.343816 43.269231 46.121593
192.813765 192.872117 16.700405 19.916143
197.368421 182.389937 62.246964 70.230608
22.014170 0.000000 236.082996 468.553459
167.763158 74.423480 205.718623 214.884696
131.325911 42.976939 188.259109 154.088050
8.385744 5.377483 484.276730 338.013245
199.101215 1.048218 150.068826 358.490566
75.777328 81.761006 52.004049 72.327044
37.888664 1.048218 335.797571 496.855346
77.263158 230.607966 182.014170 256.813417
31.202429 30.398323 253.334008 409.853249
115.151822 432.914046 19.315789 23.060797
17.829960 6.289308 442.034413 298.742138
173.099190 135.220126 132.981781 131.027254
78.006073 2.096436 218.417004 483.228512
78.748988 35.639413 343.969636 166.666667
130.753036 232.704403 71.319838 17.819706
318.710526 238.993711 37.145749 11.530398
187.957490 221.174004 54.232794 61.844864
56.461538 40.880503 208.759109 457.023061
130.010121 44.025157 200.587045 249.475891
123.323887 17.819706 265.220648 336.477987
33.431174 0.000000 178.299595 357.442348
280.821862 20.964361 165.670040 352.201258
18.572874 0.000000 200.587045 497.903564
56.461538 334.381551 27.487854 70.230608
245.904858 5.241090 163.441296 472.746331
311.281377 18.867925 90.635628 122.641509
365.514170 223.270440 38.631579 133.123690
24.516194 244.234801 146.354251 112.159329
164.927126 266.247379 21.544534 23.060797
190.929150 296.645702 22.287449 23.060797
226.589069 318.658281 17.829960 27.253669
139.668016 181.341719 16.344130 13.626834
125.552632 222.222222 27.487854 24.109015
60.919028 218.029350 16.344130 17.819706
0.742915 288.259958 26.744939 27.253669
3.714575 242.138365 19.315789 22.012579
92.121457 193.920335 20.801619 17.819706
174.585020 228.511530 17.087045 20.964361
25.259109 206.498952 18.572874 13.626834
78.006073 64.989518 43.831984 24.109015
57.947368 8.385744 263.734818 487.421384
158.240891 131.027254 101.036437 172.955975
0.000000 116.352201 17.087045 26.205451
121.838057 104.821803 50.518219 48.218029
147.840081 190.775681 112.923077 133.123690
222.874494 122.641509 23.773279 16.771488
96.578947 35.639413 228.074899 296.645702
37.888664 0.000000 270.421053 494.758910
95.093117 8.385744 178.299595 349.056604
245.161943 156.184486 7.429150 10.482180
69.091093 104.821803 89.892713 143.605870
167.898785 60.796646 228.817814 177.148847
41.603239 51.362683 300.880567 411.949686
245.161943 0.000000 69.091093 73.375262
43.831984 12.578616 179.785425 474.842767
141.153846 82.809224 190.929150 172.955975
97.321862 148.846960 166.412955 218.029350
100.293522 10.482180 131.495951 481.132075
83.206478 205.450734 72.062753 93.291405
179.042510 189.727463 34.917004 53.459119
322.425101 264.150943 44.574899 107.966457
86.178138 356.394130 72.062753 51.362683
182.757085 377.358491 53.489879 54.507338
245.904858 364.779874 40.860324 44.025157
31.202429 6.289308 187.214575 470.649895
66.119433 1.048218 271.906883 429.769392
60.176113 84.905660 222.874494 297.693920
194.643725 103.773585 34.174089 38.784067
284.536437 297.693920 17.829960 52.410901
208.759109 76.519916 126.295547 164.570231
155.269231 269.392034 81.720648 100.628931
15.601215 140.461216 54.232794 190.775681
68.348178 140.461216 14.858300 37.735849
102.522267 114.255765 101.036437 122.641509
169.384615 141.509434 124.809717 129.979036
314.995951 204.402516 21.544534 44.025157
344.712551 230.607966 6.686235 9.433962
353.627530 237.945493 6.686235 12.578616
344.712551 168.763103 11.886640 17.819706
80.234818 161.425577 8.172065 35.639413
93.607287 184.486373 34.174089 24.109015
0.000000 287.211740 79.491903 158.280922
83.949393 224.318658 149.325911 127.882600
165.670040 127.882600 37.888664 35.639413
208.759109 136.268344 23.030364 26.205451
193.157895 238.993711 13.372470 13.626834
195.386640 276.729560 9.657895 9.433962
204.301619 301.886792 9.657895 8.385744
227.331984 303.983229 27.487854 24.109015
228.074899 321.802935 104.751012 177.148847
141.896761 298.742138 42.346154 44.025157
182.757085 356.394130 58.690283 66.037736
33.431174 246.331237 197.615385 106.918239
193.157895 140.461216 43.089069 104.821803
0.742915 132.075472 94.350202 117.400419
105.493927 71.278826 59.433198 101.677149
119.609312 468.553459 54.232794 30.398323
18.572874 327.044025 86.178138 14.675052
31.945344 357.442348 80.234818 18.867925
40.117409 372.117400 75.777328 22.012579
23.773279 343.815514 84.692308 11.530398
228.817814 288.259958 44.574899 13.626834
229.560729 300.838574 45.317814 13.626834
231.046559 310.272537 47.546559 12.578616
236.246964 323.899371 44.574899 14.675052
248.133603 368.972746 43.089069 5.241090
243.676113 355.345912 45.317814 5.241090
57.947368 394.129979 63.147773 39.832285
63.890688 409.853249 60.919028 39.832285
222.874494 485.324948 40.860324 13.626834
140.410931 234.800839 71.319838 66.037736
253.334008 235.849057 10.400810 28.301887
240.704453 266.247379 24.516194 36.687631
107.722672 189.727463 158.240891 84.905660
288.251012 427.672956 28.973684 31.446541
77.263158 237.945493 16.344130 20.964361
75.777328 275.681342 9.657895 15.723270
98.807692 268.343816 22.287449 24.109015
75.777328 303.983229 20.058704 19.916143
69.834008 331.236897 8.914980 15.723270
87.663968 355.345912 16.344130 12.578616
80.234818 411.949686 12.629555 20.964361
85.435223 445.492662 9.657895 17.819706
101.036437 455.974843 23.030364 23.060797
97.321862 481.132075 10.400810 15.723270
121.838057 225.366876 15.601215 19.916143
159.726721 213.836478 9.657895 19.916143
164.184211 244.234801 14.858300 19.916143
176.813765 242.138365 11.886640 12.578616
193.900810 255.765199 6.686235 13.626834
190.186235 298.742138 7.429150 15.723270
201.329960 372.117400 11.886640 12.578616
178.299595 361.635220 11.143725 14.675052
177.556680 327.044025 11.886640 13.626834
167.155870 302.935010 16.344130 18.867925
130.010121 286.163522 21.544534 15.723270
138.925101 315.513627 22.287449 22.012579
155.269231 339.622642 18.572874 23.060797
126.295547 345.911950 14.858300 15.723270
150.811741 367.924528 16.344130 18.867925
47.546559 287.211740 23.773279 47.169811
14.115385 51.362683 232.532389 385.744235
23.030364 134.171908 143.382591 315.513627
155.269231 327.044025 32.688259 19.916143
124.066802 114.255765 63.890688 63.941300
130.010121 138.364780 8.172065 7.337526
164.927126 214.884696 22.287449 27.253669
175.327935 250.524109 22.287449 35.639413
141.896761 240.041929 14.115385 18.867925
230.303644 77.568134 46.060729 26.205451
243.676113 64.989518 38.631579 20.964361
62.404858 266.247379 42.346154 68.134172
107.722672 297.693920 7.429150 28.301887
153.783401 297.693920 7.429150 22.012579
20.058704 285.115304 40.860324 47.169811
64.633603 263.102725 20.801619 17.819706
89.892713 119.496855 243.676113 129.979036
70.576923 244.234801 10.400810 34.591195
220.645749 110.062893 14.858300 24.109015
338.769231 102.725367 3.714575 8.385744
36.402834 182.389937 59.433198 211.740042
89.149798 295.597484 54.975709 118.448637
101.779352 144.654088 199.844130 295.597484
145.611336 19.916143 201.329960 308.176101
5.943320 168.763103 309.795547 111.111111
205.787449 110.062893 144.125506 39.832285
80.977733 157.232704 150.811741 250.524109
246.647773 186.582809 46.803644 51.362683
129.267206 197.064990 116.637652 85.953878
298.651822 199.161426 86.921053 63.941300
75.034413 148.846960 159.726721 59.748428
5.943320 4.192872 210.987854 488.469602
3.714575 10.482180 202.072874 475.890985
80.234818 0.000000 146.354251 496.855346
129.267206 243.186583 167.155870 72.327044
0.000000 0.000000 36.402834 203.354298
2.228745 0.000000 268.192308 495.807128
238.475709 136.268344 124.066802 361.635220
336.540486 406.708595 20.058704 88.050314
123.323887 241.090147 40.117409 62.893082
143.382591 301.886792 31.945344 19.916143
173.099190 274.633124 17.087045 19.916143
167.898785 242.138365 34.174089 39.832285
169.384615 233.752621 11.143725 16.771488
170.127530 75.471698 43.089069 80.712788
214.702429 133.123690 11.886640 13.626834
157.497976 137.316562 13.372470 34.591195
155.269231 178.197065 13.372470 14.675052
146.354251 204.402516 23.030364 24.109015
177.556680 162.473795 52.004049 34.591195
182.757085 207.547170 28.973684 27.253669
180.528340 194.968553 15.601215 16.771488
164.184211 206.498952 9.657895 9.433962
143.382591 30.398323 100.293522 156.184486
249.619433 29.350105 95.093117 166.666667
160.469636 99.580713 144.868421 295.597484
98.064777 1.048218 179.785425 496.855346
40.117409 106.918239 150.068826 164.570231
120.545073 16.556291 245.283019 314.569536
197.064990 73.675497 107.966457 152.317881
118.026316 33.542977 81.012146 444.444444
113.137652 12.578616 6.983806 7.337526
97.773279 32.494759 3.491903 7.337526
90.789474 66.037736 6.983806 10.482180
106.852227 56.603774 2.793522 8.385744
111.740891 70.230608 6.285425 10.482180
115.232794 79.664570 4.888664 9.433962
120.121457 30.398323 6.285425 9.433962
131.993927 30.398323 6.285425 9.433962
112.439271 46.121593 6.285425 11.530398
125.010121 38.784067 6.983806 10.482180
136.882591 23.060797 4.888664 4.192872
140.374494 35.639413 6.285425 4.192872
94.281377 47.169811 4.888664 6.289308
115.931174 30.398323 4.888664 4.192872
69.838057 40.880503 6.983806 25.157233
47.489879 59.748428 10.475709 22.012579
51.680162 115.303983 11.174089 26.205451
71.234818 174.004193 3.491903 9.433962
78.218623 94.339623 4.190283 18.867925
90.091093 103.773585 4.888664 24.109015
92.884615 203.354298 6.285425 22.012579
112.439271 140.461216 18.157895 30.398323
103.360324 112.159329 9.777328 30.398323
120.121457 59.748428 14.665992 16.771488
146.659919 80.712788 23.744939 20.964361
175.293522 126.834382 33.522267 16.771488
168.309717 40.880503 16.062753 17.819706
187.165992 180.293501 32.125506 26.205451
224.878543 84.905660 18.157895 19.916143
242.338057 155.136268 34.220648 28.301887
260.495951 95.387841 19.554656 26.205451
261.194332 132.075472 33.522267 31.446541
254.908907 70.230608 23.046559 9.433962
292.621457 96.436059 26.538462 15.723270
298.208502 122.641509 38.410931 24.109015
302.398785 167.714885 30.030364 25.157233
282.844130 205.450734 25.141700 22.012579
305.890688 230.607966 33.522267 23.060797
312.874494 124.737945 143.168016 218.029350
333.825911 104.821803 34.220648 18.867925
366.649798 88.050314 23.744939 16.771488
396.680162 107.966457 14.665992 23.060797
27.236842 73.375262 342.206478 158.280922
33.696356 169.811321 37.805668 39.832285
87.117409 219.077568 180.809717 241.090147
124.923077 445.492662 179.987854 53.459119
266.283401 410.901468 24.655870 22.012579
138.072874 36.687631 235.052632 315.513627
92.048583 10.482180 239.983806 269.392034
102.732794 47.169811 155.331984 451.781971
105.198381 296.645702 16.437247 20.964361
36.161943 28.301887 114.238866 437.106918
198.068826 28.301887 101.910931 426.624738
96.979757 292.452830 34.518219 40.880503
90.404858 383.647799 46.846154 97.484277
69.036437 264.150943 39.449393 29.350105
216.149798 198.113208 135.607287 174.004193
72.323887 219.077568 94.514170 131.027254
149.578947 136.268344 182.453441 85.953878
124.101215 268.343816 73.145749 205.450734
24.655870 30.398323 299.979757 417.190776
93.692308 190.775681 121.635628 307.127883
144.647773 481.132075 19.724696 17.819706
200.534413 448.637317 10.684211 26.205451
212.862348 403.563941 10.684211 30.398323
134.785425 205.450734 83.829960 111.111111
122.457490 111.111111 152.866397 306.079665
0.000000 106.918239 64.105263 70.230608
64.105263 128.930818 198.890688 119.496855
272.858300 85.953878 110.951417 285.115304
88.761134 128.930818 161.085020 295.597484
148.757085 178.197065 50.133603 52.410901
205.465587 42.976939 46.846154 95.387841
262.174089 40.880503 44.380567 99.580713
179.165992 245.283019 76.433198 44.025157
303.267206 246.331237 35.340081 38.784067
194.781377 251.572327 50.133603 80.712788
23.834008 3.144654 170.125506 393.081761
46.846154 363.731656 8.218623 32.494759
198.890688 5.241090 164.372470 387.840671
366.550607 1.048218 124.923077 385.744235
64.105263 253.668763 306.554656 120.545073
259.708502 324.947589 64.927126 25.157233
293.404858 321.802935 29.587045 12.578616
324.635628 299.790356 40.271255 7.337526
0.000000 403.563941 295.048583 95.387841
143.825911 207.547170 262.174089 206.498952
156.153846 59.748428 206.287449 184.486373
95.336032 55.555556 37.805668 22.012579
232.587045 136.268344 109.307692 94.339623
170.125506 109.014675 131.497976 34.591195
231.765182 136.268344 24.655870 53.459119
262.995951 134.171908 5.753036 5.241090
202.178138 138.364780 11.506073 7.337526
221.902834 102.725367 5.753036 9.433962
42.736842 78.616352 302.445344 373.165618
83.008097 72.327044 262.174089 365.828092
177.678048 205.861635 231.145978 127.088050
415.824916 128.201658 83.052750 204.497279
436.588103 198.243540 12.906846 9.380609
431.537598 178.231574 12.345679 10.631357
423.681257 168.225591 8.978676 10.631357
53.310887 1.250748 147.025814 310.810850
138.608305 145.712129 12.345679 33.144819
28.058361 76.295621 215.488215 240.768968
358.585859 130.703154 84.736251 170.727086
0.000000 0.000000 81.930415 188.237557
77.441077 0.625374 210.437710 227.636115
298.540965 86.301604 146.464646 287.672014
130.751964 16.885096 204.826038 454.021483
52.749719 51.906037 321.548822 390.858714
33.670034 109.440440 144.781145 124.449415
209.876543 155.718112 57.239057 58.159777
312.570146 106.938944 143.097643 126.950910
78.002245 66.915012 340.628507 75.044873
54.433221 66.915012 204.826038 250.774951
57.239057 136.331520 121.773288 128.201658
0.000000 85.676230 83.052750 177.606200
72.951740 76.920995 167.227834 145.712129
237.373737 65.664264 171.156004 143.210633
88.103255 234.515229 160.493827 163.847973
259.820426 219.506254 151.515152 167.600217
52.188552 52.531411 305.836139 417.124420
170.594837 88.803100 64.534231 99.434457
122.334456 253.901821 185.746352 151.965868
222.222222 301.430241 21.324355 18.135844
67.901235 50.029915 242.985410 444.640873
0.561167 6.179775 498.877666 258.353403
288.384110 216.853933 63.523206 84.831461
239.476864 200.000000 185.510246 25.280899
169.207831 346.067416 79.263469 87.078652
131.543629 311.235955 89.944362 62.921348
135.478695 227.528090 123.111345 107.865169
96.128037 68.539326 118.051975 212.359551
200.126205 138.202247 26.983309 75.842697
100.625255 0.000000 122.549193 143.258427
42.161420 23.595506 220.363687 468.539326
306.935135 137.640449 56.215226 62.359551
87.695753 0.000000 206.872032 293.820225
87.133601 107.865169 216.428621 145.505618
267.584477 110.674157 28.107613 24.157303
298.502851 118.539326 34.291288 25.842697
319.302485 150.561798 20.237481 28.089888
365.961123 142.134831 48.345095 149.438202
454.781180 106.741573 39.350658 136.516854
178.202267 158.426966 127.608564 90.449438
77.577012 117.415730 190.007465 116.853933
253.530670 158.988764 36.539897 25.842697
207.996337 165.168539 32.604831 27.528090
427.235719 266.853933 20.237481 23.595506
5.621523 67.415730 80.949926 200.000000
133.230086 318.539326 88.820057 90.449438
149.532502 216.853933 43.847876 46.629213
156.278329 270.224719 38.226354 38.202247
101.187407 107.865169 96.128037 96.629213
1.124305 143.258427 395.755193 151.685393
0.000000 1.123596 395.755193 143.820225
404.187476 5.056180 94.441580 286.516854
30.356222 229.213483 311.994505 138.764045
14.615959 50.000000 465.462073 271.348315
166.959222 125.842697 84.322839 105.056180
245.098386 185.955056 115.241214 62.359551
57.901683 58.988764 234.979646 144.382022
28.107613 74.719101 280.513979 258.426966
278.265370 160.112360 35.977745 20.786517
14.615959 33.146067 248.471300 428.651685
66.333967 152.247191 175.953658 277.528090
132.105782 160.112360 204.623423 214.606742
397.441649 254.494382 22.486090 35.955056
123.111345 200.561798 28.107613 56.179775
159.089090 210.674157 17.426720 29.213483
184.948094 212.359551 29.794070 28.651685
136.603000 270.224719 76.452708 50.561798
222.612296 306.741573 9.556588 7.865169
338.415662 91.573034 87.133601 181.460674
64.647510 138.764045 109.619691 158.426966
41.599267 128.089888 290.632720 205.056180
156.278329 394.382022 17.426720 23.033708
64.647510 23.595506 211.369251 476.404494
30.356222 206.741573 247.909148 117.415730
0.000000 2.247191 499.753361 480.898876
51.155856 287.078652 286.697654 153.370787
0.000000 284.269663 48.907247 155.056180
40.474963 164.044944 334.480596 200.000000
76.452708 114.044944 158.526938 162.359551
245.098386 103.932584 148.408197 160.674157
28.058361 64.918534 171.156004 245.926680
280.583614 35.896130 173.961841 274.949084
2.805836 22.912424 133.557800 374.236253
171.717172 12.219959 148.709315 396.384929
342.873176 25.203666 149.270483 384.164969
101.010101 409.368635 43.209877 24.439919
154.320988 414.714868 143.658810 25.967413
65.095398 69.501018 253.086420 349.796334
113.916947 71.028513 50.505051 31.313646
115.039282 147.403259 148.148148 172.606925
96.520763 71.028513 221.099888 368.890020
173.961841 174.898167 113.355780 154.276986
265.993266 135.947047 66.778900 263.492872
102.693603 146.639511 79.124579 238.289206
39.842873 152.749491 104.377104 199.338086
50.505051 21.384929 360.269360 272.657841
86.419753 58.808554 207.070707 352.087576
69.584736 0.000000 151.515152 425.407332
79.685746 420.061100 58.361392 64.154786
220.538721 334.521385 52.188552 146.639511
230.078563 491.089613 14.029181 9.164969
291.806958 336.048880 65.095398 151.985743
373.737374 250.509165 17.396184 32.077393
322.109989 103.105906 44.332211 83.248473
240.740741 100.814664 68.462402 45.824847
185.185185 109.979633 43.771044 86.303462
84.736251 69.501018 14.029181 18.329939
283.389450 85.539715 173.400673 193.228106
300.785634 310.845214 164.983165 51.171079
0.000000 0.000000 186.868687 181.771894
186.868687 0.000000 76.318743 107.688391
131.313131 104.633401 125.701459 187.118126
157.126824 293.279022 88.664422 58.808554
145.342312 262.729124 129.629630 44.297352
212.121212 294.806517 80.808081 52.698574
305.274972 431.517312 74.635241 54.226069
83.052750 102.342159 116.722783 347.505092
68.462402 68.737271 262.065095 400.967413
113.355780 389.511202 121.212121 110.743381
145.903479 165.733198 93.153760 155.804481
65.095398 129.837067 66.778900 79.429735
30.303030 122.963340 295.735129 343.686354
276.655443 235.997963 17.957351 29.786151
274.971942 167.260692 16.273850 19.093686
92.031425 98.523422 310.325477 104.633401
227.272727 215.376782 81.930415 98.523422
314.814815 214.613035 74.635241 76.374745
389.450056 184.826884 71.829405 60.336049
232.884400 119.144603 128.507295 153.513238
86.419753 122.963340 201.459035 262.729124
304.713805 170.315682 23.569024 40.478615
297.418631 268.075356 36.475870 39.714868
280.583614 241.344196 52.188552 21.384929
62.289562 276.476578 38.159371 43.533605
314.253648 139.002037 107.744108 92.413442
331.649832 29.786151 67.340067 103.869654
18.518519 41.242363 137.485971 49.643585
171.717172 218.431772 29.741863 29.786151
175.645342 249.745418 20.202020 32.841141
131.313131 133.655804 169.472503 279.531568
82.491582 123.727088 291.806958 376.527495
46.576880 122.963340 257.575758 373.472505
39.842873 148.167006 71.268238 57.281059
0.000000 127.545825 43.771044 44.297352
177.328844 64.918534 26.936027 41.242363
173.961841 35.132383 11.784512 17.566191
242.424242 128.309572 8.978676 9.928717
270.482604 42.006110 30.864198 45.824847
0.000000 279.531568 373.176207 189.409369
22.446689 30.549898 102.693603 141.293279
98.204265 0.763747 224.466891 195.519348
121.773288 223.014257 122.895623 75.610998
43.771044 48.879837 100.448934 292.515275
187.429854 49.643585 127.946128 290.987780
364.197531 50.407332 98.204265 287.169043
27.497194 19.093686 451.739618 375.763747
7.856341 67.973523 320.426487 345.977597
11.223345 38.951120 305.274972 390.274949
46.015713 25.967413 191.919192 451.374745
19.640853 141.293279 100.448934 357.433809
99.326599 25.203666 111.672278 450.610998
207.070707 0.000000 42.648709 118.380855
89.225589 10.692464 213.804714 475.814664
29.180696 23.676171 283.950617 469.704684
80.246914 5.346232 192.480359 488.798371
82.491582 19.093686 160.493827 457.484725
40.404040 407.077393 42.087542 92.413442
19.640853 29.786151 290.684624 454.429735
200.336700 212.321792 89.225589 105.397149
230.078563 126.018330 114.478114 67.973523
208.754209 195.519348 69.584736 80.957230
17.957351 40.478615 201.459035 184.826884
283.389450 53.462322 152.076319 106.160896
202.020202 212.321792 86.980920 92.413442
14.029181 0.000000 352.974186 458.248473
166.105499 0.000000 43.209877 61.099796
232.323232 3.054990 40.965208 37.423625
404.601571 313.136456 60.606061 62.627291
439.955107 328.411405 31.986532 25.203666
442.199776 312.372709 19.640853 11.456212
51.066218 148.167006 79.124579 56.517312
0.000000 43.533605 184.062851 113.034623
260.381594 277.240326 47.699214 106.160896
90.347924 323.065173 12.906846 9.928717
86.980920 305.498982 43.771044 58.808554
87.542088 297.097760 12.345679 6.873727
23.007856 322.301426 12.906846 12.983707
19.640853 297.861507 15.712682 9.164969
14.029181 219.959267 28.619529 33.604888
104.377104 294.042770 10.101010 13.747454
83.052750 274.185336 20.202020 12.983707
181.257015 223.778004 25.813692 27.494908
186.868687 86.303462 48.821549 60.336049
0.000000 88.594705 22.446689 17.566191
80.246914 129.837067 10.662177 8.401222
115.600449 127.545825 12.906846 9.928717
126.012146 151.991614 88.815789 187.631027
179.908907 98.532495 110.829960 302.935010
0.000000 96.436059 28.087045 31.446541
39.473684 198.113208 27.327935 33.542977
73.633603 154.088050 39.473684 72.327044
113.866397 101.677149 50.101215 60.796646
261.133603 19.916143 116.143725 274.633124
54.655870 35.639413 107.793522 70.230608
199.645749 180.293501 29.605263 26.205451
222.419028 133.123690 104.757085 33.542977
233.805668 72.327044 128.289474 46.121593
226.214575 14.675052 150.303644 45.073375
232.287449 140.461216 144.230769 59.748428
174.595142 137.316562 54.655870 73.375262
156.376518 267.295597 144.230769 95.387841
48.582996 168.763103 328.694332 111.111111
87.297571 84.905660 164.726721 259.958071
56.174089 92.243187 321.103239 153.039832
227.732794 115.303983 148.026316 168.763103
0.759109 7.337526 303.643725 421.383648
306.680162 5.241090 70.597166 410.901468
193.572874 188.679245 80.465587 95.387841
205.718623 184.486373 47.064777 62.893082
178.390688 183.438155 31.123482 75.471698
0.000000 290.356394 333.248988 209.643606
31.882591 103.773585 188.259109 112.159329
9.868421 14.675052 335.526316 451.781971
203.441296 244.234801 119.939271 213.836478
330.971660 186.582809 13.663968 26.205451
244.433198 185.534591 38.714575 66.037736
55.414980 16.771488 267.206478 468.553459
163.208502 5.241090 173.836032 370.020964
244.433198 160.377358 103.997976 111.111111
91.852227 0.000000 128.289474 497.903564
338.562753 221.174004 26.568826 48.218029
224.696356 153.039832 132.844130 180.293501
120.698381 36.687631 205.718623 298.742138
132.085020 134.171908 189.777328 331.236897
138.157895 98.532495 178.390688 180.293501
220.141700 66.037736 15.182186 10.482180
22.773279 34.591195 354.504049 237.945493
35.678138 76.519916 340.840081 197.064990
81.224696 31.446541 245.192308 423.480084
59.210526 81.761006 165.485830 319.706499
34.919028 11.530398 342.358300 293.501048
16.700405 67.085954 360.576923 172.955975
48.582996 11.530398 328.694332 199.161426
44.787449 54.507338 332.489879 222.222222
79.706478 0.000000 226.214575 467.505241
179.908907 29.350105 197.368421 330.188679
60.728745 151.991614 88.815789 215.932914
9.868421 2.096436 367.408907 455.974843
44.787449 0.000000 277.834008 480.083857
0.000000 68.134172 377.277328 179.245283
247.469636 134.171908 27.327935 124.737945
31.882591 234.800839 53.137652 92.243187
40.991903 293.501048 138.917004 183.438155
197.368421 139.412998 120.698381 53.459119
122.975709 151.991614 42.510121 42.976939
34.159919 159.329140 74.392713 93.291405
127.530364 212.788260 75.910931 74.423480
32.641700 284.067086 93.370445 87.002096
34.919028 334.381551 100.961538 132.075472
56.174089 256.813417 19.736842 69.182390
21.255061 257.861635 29.605263 25.157233
22.014170 159.329140 48.582996 62.893082
198.127530 229.559748 113.866397 129.979036
179.908907 182.389937 47.823887 46.121593
140.435223 208.595388 44.028340 33.542977
25.809717 20.964361 246.710526 477.987421
250.506073 442.348008 80.465587 54.507338
87.297571 28.301887 176.872470 410.901468
64.524291 13.626834 202.682186 466.457023
163.208502 192.872117 214.068826 46.121593
15.182186 1.048218 362.095142 495.807128
93.370445 133.123690 271.761134 197.064990
31.123482 51.362683 346.153846 280.922432
88.056680 132.075472 196.609312 202.306080
189.777328 266.247379 96.406883 106.918239
250.506073 77.568134 33.400810 39.832285
27.327935 17.819706 223.937247 479.035639
50.101215 204.402516 202.682186 241.090147
54.655870 64.989518 170.799595 424.528302
92.611336 30.398323 284.665992 283.018868
183.704453 301.886792 66.801619 45.073375
179.149798 214.884696 102.479757 59.748428
104.757085 255.765199 66.042510 46.121593
281.629555 137.316562 72.874494 61.844864
21.255061 222.222222 93.370445 50.314465
77.429150 148.846960 278.593117 67.085954
148.785425 243.186583 97.925101 104.821803
52.378543 33.542977 324.898785 372.117400
28.846154 19.916143 116.902834 423.480084
169.281377 17.819706 138.917004 422.431866
326.417004 27.253669 50.860324 409.853249
25.050607 74.423480 189.018219 342.767296
259.615385 78.616352 117.661943 317.610063
92.611336 428.721174 54.655870 14.675052
175.354251 390.985325 18.977733 25.157233
19.736842 174.004193 170.040486 83.857442
282.388664 131.027254 94.129555 135.220126
60.728745 136.268344 138.917004 138.364780
29.605263 19.916143 347.672065 349.056604
22.773279 259.958071 141.194332 237.945493
176.872470 313.417191 84.261134 185.534591
85.779352 0.000000 246.710526 244.234801
280.922432 139.900662 98.532495 125.000000
175.052411 72.847682 58.700210 157.284768
109.014675 132.450331 70.230608 129.966887
104.170040 280.922432 97.449393 38.784067
104.842105 185.534591 98.121457 117.400419
67.206478 9.433962 254.040486 488.469602
165.327935 193.920335 103.497976 64.989518
187.506073 128.930818 104.170040 80.712788
157.263158 71.278826 87.368421 160.377358
1.344130 126.834382 96.777328 174.004193
122.987854 6.289308 209.684211 346.960168
139.789474 149.895178 148.526316 61.844864
86.696356 253.668763 47.716599 48.218029
96.777328 112.159329 227.157895 259.958071
107.530364 140.461216 134.412955 137.316562
103.497976 180.293501 229.174089 99.580713
81.319838 221.174004 250.008097 113.207547
29.570850 15.723270 200.947368 443.396226
237.238866 272.536688 55.109312 58.700210
209.012146 312.368973 30.242915 26.205451
138.445344 322.851153 63.846154 52.410901
120.299595 333.333333 36.291498 24.109015
76.615385 295.597484 48.388664 37.735849
69.222672 31.446541 263.449393 415.094340
57.797571 38.784067 274.874494 278.825996
28.898785 0.000000 303.773279 301.886792
42.340081 117.400419 202.963563 271.488470
0.000000 17.819706 332.672065 480.083857
274.202429 138.364780 58.469636 193.920335
245.975709 183.438155 69.894737 44.025157
148.526316 230.607966 151.214575 102.725367
82.663968 7.337526 250.008097 485.324948
1.344130 271.488470 77.287449 106.918239
0.000000 220.125786 66.534413 40.880503
82.663968 78.616352 159.279352 356.394130
24.194332 264.150943 98.121457 60.796646
93.417004 99.580713 174.064777 324.947589
78.631579 288.259958 74.599190 79.664570
30.914980 67.085954 225.813765 116.352201
247.319838 126.834382 85.352227 81.761006
28.898785 14.675052 227.157895 114.255765
72.582996 39.832285 260.089069 453.878407
135.085020 89.098532 122.315789 118.448637
254.712551 141.509434 77.287449 78.616352
1.344130 1.048218 248.663968 497.903564
100.809717 398.322851 86.024291 46.121593
0.000000 1.048218 322.591093 98.532495
3.360324 112.159329 319.902834 113.207547
20.161943 246.331237 306.461538 96.436059
3.360324 366.876310 326.623482 128.930818
48.388664 0.000000 260.761134 496.855346
63.846154 36.687631 223.797571 359.538784
76.615385 45.073375 204.979757 394.129979
24.866397 144.654088 250.680162 291.404612
19.489879 162.473795 283.611336 292.452830
34.275304 170.859539 225.141700 283.018868
60.485830 118.448637 176.080972 319.706499
69.894737 47.169811 179.441296 319.706499
67.878543 67.085954 192.210526 367.924528
39.651822 50.314465 243.287449 392.033543
25.538462 33.542977 305.789474 400.419287
38.307692 2.096436 265.465587 436.058700
2.016194 372.117400 71.238866 92.243187
80.647773 92.243187 250.680162 237.945493
161.967611 3.144654 124.331984 327.044025
50.404858 345.911950 235.222672 102.725367
57.125506 165.618449 16.129555 25.157233
116.939271 233.752621 81.991903 47.169811
247.991903 264.150943 42.340081 17.819706
289.659919 154.088050 42.340081 177.148847
11.425101 7.337526 231.862348 489.517820
51.076923 169.811321 107.530364 253.668763
122.315789 161.425577 106.186235 94.339623
112.234818 207.547170 217.748988 286.163522
60.485830 236.897275 147.182186 176.100629
202.291498 233.752621 27.554656 49.266247
231.862348 203.354298 10.080972 34.591195
201.619433 265.199161 56.453441 100.628931
259.239130 92.163636 85.869565 203.163636
298.913043 24.890909 70.108696 40.363636
341.847826 18.163636 34.782609 16.145455
184.817814 120.545073 68.550607 172.955975
149.870445 24.109015 76.615385 53.459119
30.914980 28.301887 193.554656 344.863732
174.736842 72.327044 157.263158 143.605870
202.291498 36.687631 129.708502 259.958071
79.303644 118.448637 151.214575 234.800839
182.129555 46.121593 132.396761 285.115304
118.283401 158.280922 88.712551 243.186583
19.489879 7.337526 304.445344 487.421384
111.562753 114.255765 219.765182 171.907757
79.303644 44.025157 129.036437 305.031447
6.048583 39.832285 301.757085 322.851153
174.064777 336.477987 72.582996 133.123690
7.856341 16.802444 148.148148 384.928717
176.206510 7.637475 156.004489 404.786151
354.096521 7.637475 143.658810 396.384929
411.949686 6.558704 67.085954 51.649798
71.514423 6.235955 375.026457 314.320725
95.685841 80.712788 133.296460 386.792453
304.203540 142.557652 50.331858 67.085954
368.915929 324.947589 32.079646 48.218029
132.743363 299.790356 129.977876 146.750524
255.530973 321.802935 12.721239 11.530398
276.548673 314.465409 11.615044 22.012579
300.331858 323.899371 11.061947 11.530398
315.265487 312.368973 17.699115 11.530398
284.292035 299.790356 10.508850 12.578616
297.566372 303.983229 11.615044 14.675052
299.778761 284.067086 11.061947 12.578616
319.690265 288.259958 11.615044 10.482180
0.000000 76.271186 498.075570 296.912833
419.171053 342.767296 65.723684 28.301887
394.342105 234.800839 74.486842 106.918239
7.414980 143.605870 231.886640 126.834382
248.064777 156.184486 86.283401 115.303983
334.460526 156.184486 83.250000 67.085954
278.292587 151.969981 39.925079 43.151970
287.756309 421.200750 34.601735 37.523452
170.394737 122.641509 24.342105 109.014675
191.491228 92.243187 113.055556 178.197065
88.713450 38.784067 186.081871 424.528302
44.356725 156.184486 179.590643 288.259958
181.754386 178.197065 101.695906 190.775681
55.716374 24.109015 248.830409 284.067086
57.339181 291.404612 0.000000 0.000000
343.494152 76.519916 30.292398 111.111111
181.754386 199.161426 42.733918 34.591195
221.242690 288.259958 31.374269 49.266247
170.394737 142.557652 46.520468 38.784067
5.409357 15.723270 149.298246 343.815514
158.494152 15.723270 166.067251 344.863732
320.774854 15.723270 145.511696 338.574423
465.204678 121.593291 31.915205 133.123690
8.654971 76.519916 40.570175 95.387841
34.078947 16.771488 81.140351 53.459119
24.342105 177.148847 94.122807 167.714885
116.842105 88.050314 15.146199 73.375262
121.710526 26.205451 72.485380 39.832285
190.950292 67.085954 93.040936 50.314465
114.678363 166.666667 78.976608 151.991614
298.596491 144.654088 84.926901 92.243187
392.719298 170.859539 77.353801 163.522013
401.374269 80.712788 14.605263 84.905660
408.406433 19.916143 84.385965 59.748428
19.473684 78.616352 82.222222 69.182390
100.614035 82.809224 77.353801 89.098532
198.523392 53.459119 16.769006 15.723270
178.508772 75.471698 69.780702 102.725367
250.994152 96.436059 197.441520 89.098532
333.216374 81.761006 17.850877 18.867925
456.008772 69.182390 17.850877 78.616352
142.266082 72.327044 194.736842 146.750524
27.817525 42.100479 242.409861 385.219387
25.546707 0.000000 121.488783 61.045695
28.385230 1.052512 464.382357 245.235293
0.567705 3.157536 111.837805 219.975005
113.540919 1.052512 135.681398 223.132541
251.493135 11.577632 122.624192 211.554909
374.685031 8.420096 122.056487 217.869981
123.759601 96.831103 279.878364 102.093663
17.598842 119.986367 473.465630 207.344862
246.951498 146.299166 189.045629 88.411007
74.369302 99.988639 165.769741 116.828831
177.123833 502.048218 0.000000 0.000000
179.394651 126.301438 93.671258 97.883615
44.280958 17.892704 211.186109 475.735418
20.437365 26.312800 181.097765 207.344862
262.847226 2.105024 175.988424 225.237565
113.540919 150.509214 113.540919 316.806108
40.874731 72.623327 345.732097 251.550365
14.760319 2.105024 342.893574 493.628122
37.468503 147.351678 243.545270 332.593788
13.624910 43.152991 277.039841 187.347134
74.937006 12.630144 194.154971 477.840442
270.795091 11.577632 30.656048 246.287805
259.440999 230.500125 26.114411 75.780863
287.258524 322.068668 14.192615 39.995456
117.514851 28.417824 230.488065 191.557182
157.254172 1.052512 109.566986 312.596060
74.369302 48.415551 221.972496 411.532187
300.883434 154.719262 68.124551 125.248927
102.186827 229.447613 98.212895 83.148447
204.373653 182.084574 51.093413 58.940671
153.847945 61.045695 102.754531 95.778591
274.769023 114.723807 51.093413 42.100479
296.909502 70.518303 80.614052 85.253471
110.134691 165.244382 117.514851 92.621055
250.925430 87.358495 148.738603 88.411007
260.576408 52.625599 65.853733 26.312800
344.596688 68.413279 94.238962 39.995456
349.138325 43.152991 97.077485 39.995456
265.118045 28.417824 64.150619 22.102752
178.500631 124.987421 115.984281 80.874214
127.500451 159.647799 159.581209 85.075472
111.871363 35.710692 123.387533 414.874214
18.096838 11.553459 162.048960 151.245283
233.613729 0.000000 175.210297 324.547170
74.855103 130.238994 66.629268 57.767296
105.290695 242.622642 126.677867 55.666667
63.338934 272.031447 106.113278 169.100629
7.403252 0.000000 146.419872 217.415094
135.726286 235.270440 41.951761 50.415094
74.032520 191.157233 160.403793 280.433962
117.629448 236.320755 102.822944 94.528302
289.549411 186.955975 106.935862 112.383648
246.775066 19.955975 129.968201 293.037736
37.016260 148.094340 266.517071 314.044025
115.161697 314.044025 55.935682 45.163522
123.387533 56.716981 173.565130 367.610063
94.597109 197.459119 308.468832 126.037736
182.613549 128.138365 115.161697 116.584906
80.613188 36.761006 251.710567 437.981132
103.645528 17.855346 304.355915 264.679245
54.290515 11.553459 266.517071 419.075472
53.467931 9.452830 304.355915 484.194969
189.194217 298.289308 143.129538 200.610063
74.855103 91.377358 199.887803 318.245283
62.516350 0.000000 139.016620 419.075472
140.661788 111.333333 213.871724 188.006289
199.065220 287.786164 105.290695 86.125786
101.177777 285.685535 40.306594 42.012579
88.016440 103.981132 311.759167 142.842767
79.790605 160.698113 231.145978 327.698113
95.419692 37.811321 117.629448 323.496855
48.532430 67.220126 113.516530 108.182390
11.516170 36.761006 397.307856 408.572327
27.145257 202.710692 181.790965 291.987421
68.274435 1.050314 203.178138 173.301887
123.387533 130.238994 86.371273 251.025157
18.919422 25.207547 331.501172 340.301887
380.033602 233.169811 106.935862 49.364780
361.936763 28.358491 117.629448 170.150943
141.484371 39.911950 227.033061 401.220126
73.254671 125.664269 269.419862 102.700240
279.744346 23.601918 155.850541 190.729017
152.409046 154.369305 62.930187 122.474820
72.763029 257.069544 124.385447 142.249400
191.248771 323.410072 41.297935 38.911271
170.108161 269.189448 49.655851 36.997602
209.931170 199.659472 67.846608 70.167866
459.193707 347.649880 39.823009 51.031175
26.232094 57.623967 420.762791 258.864590
48.425101 2.096436 335.388664 449.685535
69.050607 213.836478 207.151822 105.870021
84.295547 38.784067 127.340081 166.666667
102.230769 207.547170 134.514170 174.004193
94.159919 56.603774 277.099190 297.693920
82.502024 204.402516 203.564777 200.209644
114.041096 150.263620 136.643836 211.775044
113.805970 127.882600 16.169154 29.350105
159.203980 2.096436 125.621891 475.890985
65.298507 0.000000 208.955224 498.951782
36.069652 42.976939 300.373134 429.769392
233.208955 316.561845 230.099502 134.171908
83.955224 79.664570 309.079602 243.186583
144.900498 104.821803 59.079602 55.555556
243.781095 62.893082 202.114428 176.100629
189.054726 0.000000 32.960199 23.060797
34.203980 227.463312 217.039801 244.234801
226.990050 283.018868 36.691542 24.109015
87.064677 29.350105 350.746269 332.285115
54.726368 211.740042 138.681592 141.509434
47.263682 252.620545 124.378109 230.607966
161.069652 269.392034 81.467662 80.712788
111.940299 12.578616 292.910448 320.754717
65.920398 17.819706 89.552239 75.471698
41.666667 88.050314 431.592040 186.582809
107.587065 78.616352 127.487562 315.513627
172.885572 143.605870 86.442786 186.582809
280.472637 154.088050 82.089552 183.438155
132.462687 178.197065 93.283582 114.255765
10.375494 95.989761 468.379447 165.742321
57.312253 55.034130 364.624506 197.098976
135.375494 268.131399 17.292490 22.397611
156.126482 229.735495 28.656126 39.675768
176.383399 174.701365 63.735178 74.872014
206.521739 250.853242 22.233202 26.237201
248.517787 206.058020 25.691700 25.597270
238.636364 145.264505 40.019763 44.155290
287.055336 181.740614 35.079051 28.796928
334.980237 142.704778 25.691700 26.877133
41.007905 33.276451 363.142292 130.546075
212.944664 110.068259 137.351779 149.744027
156.620553 168.302048 175.395257 101.109215
114.522822 115.827645 264.730290 105.588737
180.082988 240.614334 51.867220 90.870307
181.327801 128.626280 146.887967 216.296928
302.074689 95.989761 31.950207 25.597270
46.887967 26.877133 419.917012 338.523891
143.153527 122.226962 255.601660 134.385666
234.024896 74.232082 139.004149 242.534130
80.497925 74.232082 41.493776 28.796928
164.315353 190.059727 219.502075 101.749147
137.344398 49.914676 92.116183 159.343003
249.377593 73.592150 165.975104 191.979522
79.253112 55.674061 229.875519 323.805461
57.676349 158.063140 278.008299 149.744027
70.539419 123.506826 299.585062 353.882253
59.751037 67.192833 223.651452 301.407850
8.713693 49.914676 344.398340 176.621160
121.161826 22.397611 26.970954 21.757679
141.078838 211.177474 78.838174 152.943686
222.406639 148.464164 60.580913 165.102389
60.165975 173.421502 41.908714 124.786689
73.029046 213.737201 136.099585 62.713311
149.377593 67.832765 167.634855 214.377133
216.597510 173.421502 116.597510 324.445392
201.244813 143.984642 66.390041 46.715017
74.855103 90.327044 176.032880 393.867925
70.742186 126.037736 189.194217 254.176101
196.597469 252.075472 35.371093 236.320755
0.000000 105.031447 331.501172 389.666667
93.774525 22.056604 170.274796 98.729560
31.258175 54.616352 32.080759 22.056604
0.000000 107.132075 20.564589 31.509434
51.822764 76.672956 274.742907 168.050314
242.662148 201.660377 29.613008 14.704403
307.646249 157.547170 23.032339 38.861635
341.372175 120.786164 42.774345 32.559748
61.693766 19.955975 250.887984 421.176101
1.645167 18.905660 423.630530 476.842767
157.113459 40.962264 23.032339 21.006289
154.645708 72.471698 119.274615 267.830189
38.661427 221.616352 102.822944 256.276730
96.242276 394.918239 69.919602 102.930818
69.919602 282.534591 117.629448 96.628931
49.355013 56.716981 346.307676 152.295597
78.968021 67.220126 173.565130 365.509434
23.032339 42.012579 265.694488 426.427673
56.758265 115.534591 202.355554 308.792453
230.088496 146.294964 29.006883 19.904077
148.733075 230.607966 178.655706 198.113208
0.000000 286.163522 180.882591 211.740042
103.052632 97.484277 126.834008 219.077568
42.518219 30.398323 64.858300 112.159329
101.611336 40.880503 181.603239 381.551363
0.720648 0.000000 251.506073 466.457023
101.611336 98.532495 45.400810 22.012579
0.720648 0.000000 337.983806 487.421384
0.000000 1.048218 492.202429 315.513627
294.024291 211.740042 62.696356 58.700210
31.708502 162.473795 324.291498 337.526205
15.133603 321.802935 46.121457 32.494759
0.000000 409.853249 10.089069 53.459119
82.153846 244.234801 79.991903 88.050314
81.433198 319.706499 54.048583 178.197065
143.408907 307.127883 162.866397 146.750524
157.821862 419.287212 110.979757 76.519916
296.906883 425.576520 59.813765 36.687631
253.668016 375.262055 98.008097 36.687631
300.510121 373.165618 24.502024 13.626834
182.323887 208.595388 97.287449 91.194969
250.785425 309.224319 33.149798 22.012579
157.821862 196.016771 38.914980 23.060797
206.105263 87.558753 75.668016 58.143553
34.319838 139.412998 271.056680 358.490566
72.141700 109.014675 184.206478 229.559748
215.724696 129.979036 42.724696 33.542977
80.546559 3.144654 142.882591 492.662474
180.704453 118.448637 9.105263 6.289308
189.109312 133.123690 6.303644 15.723270
198.914980 149.895178 3.502024 9.433962
198.914980 126.834382 4.902834 8.385744
165.995951 15.723270 154.789474 212.788260
87.550607 110.062893 142.182186 308.176101
139.380567 444.444444 30.817814 24.109015
0.000000 0.000000 330.591093 381.551363
26.615385 345.911950 252.846154 153.039832
285.064777 142.557652 184.906883 182.389937
184.906883 256.813417 118.368421 124.737945
84.048583 138.364780 203.817814 207.547170
132.376518 271.488470 126.773279 204.402516
93.774525 25.207547 260.758986 277.283019
28.790424 165.949686 85.548690 132.339623
73.209936 322.446541 168.629628 161.748428
324.097920 248.924528 125.855284 96.628931
103.645528 39.911950 209.758806 345.553459
97.887443 127.088050 108.581029 151.245283
83.903522 26.257862 77.322854 88.226415
321.630169 116.584906 61.693766 65.119497
150.532790 94.528302 70.742186 117.635220
326.565671 161.748428 101.177777 143.893082
347.952843 47.264151 79.790605 119.735849
56.758265 23.106918 106.935862 174.352201
7.403252 57.767296 417.872445 210.062893
432.678949 89.276730 34.548509 129.188679
23.854923 23.106918 435.969283 234.220126
64.161517 1.050314 389.904604 384.415094
29.613008 64.069182 369.340015 234.220126
13.983920 111.333333 182.613549 191.157233
306.823665 92.427673 175.210297 206.911950
21.387172 47.264151 259.936403 420.125786
1.645167 173.301887 254.178318 227.918239
0.822584 397.018868 200.710387 72.471698
64.984101 253.125786 70.742186 76.672956
100.355193 232.119497 139.839204 175.402516
141.484371 185.905660 38.661427 46.213836
84.726106 131.289308 92.129358 25.207547
25.500090 236.320755 41.129178 74.572327
62.516350 47.264151 50.177597 38.861635
0.822584 47.264151 47.709846 120.786164
190.839384 32.559748 90.484191 74.572327
80.613188 5.251572 111.048780 205.861635
290.371994 32.559748 112.693947 194.308176
7.403252 1.050314 361.114180 497.849057
215.516891 143.893082 155.468292 355.006289
86.371273 299.339623 204.000721 186.955975
18.096838 12.603774 402.243358 267.830189
158.758626 298.289308 28.790424 36.761006
4.112918 114.484277 170.274796 368.660377
137.371453 118.685535 86.371273 92.427673
72.387353 17.855346 141.484371 293.037736
185.903883 264.679245 74.855103 115.534591
142.306955 141.792453 73.209936 150.194969
207.291055 183.805031 48.532430 177.503145
180.968382 431.679245 37.838843 66.169811
11.516170 455.836478 40.306594 40.962264
127.500451 31.509434 184.258716 258.377358
54.290515 217.415094 417.872445 234.220126
319.162419 200.610063 159.581209 34.660377
372.630350 156.496855 53.467931 43.062893
93.774525 127.088050 224.565310 209.012579
281.323575 426.427673 118.452032 51.465409
27.145257 40.962264 125.855284 50.415094
72.387353 88.226415 263.226737 365.509434
143.129538 15.754717 205.645888 376.012579
275.565490 446.383648 34.548509 29.408805
220.452392 440.081761 41.951761 37.811321
169.452212 423.276730 41.951761 46.213836
102.822944 364.459119 35.371093 46.213836
215.516891 194.308176 107.758445 39.911950
93.774525 116.584906 200.710387 255.226415
55.935682 7.352201 319.985002 344.503145
361.114180 116.584906 83.080939 164.899371
372.630350 25.207547 31.258175 79.823899
83.903522 117.635220 122.564949 216.364780
166.161878 199.559748 199.887803 148.094340
204.823305 126.037736 41.951761 68.270440
266.517071 5.251572 107.758445 168.050314
119.274615 15.754717 281.323575 318.245283
399.775607 115.534591 85.548690 155.446541
405.533692 35.710692 31.258175 66.169811
64.984101 160.698113 84.726106 117.635220
95.419692 279.383648 61.693766 65.119497
281.323575 277.283019 26.322674 72.471698
287.081660 178.553459 19.742005 29.408805
30.435591 19.955975 147.242456 469.490566
132.435952 55.666667 211.403973 227.918239
115.161697 165.949686 88.839024 207.962264
89.661607 1.050314 204.823305 462.138365
161.226376 119.735849 245.952482 169.100629
139.016620 296.188679 51.822764 54.616352
186.726467 341.352201 17.274255 11.553459
124.210117 335.050314 13.161337 11.553459
190.839384 293.037736 14.806504 11.553459
117.629448 295.138365 16.451671 7.352201
125.855284 280.433962 55.113098 11.553459
125.032700 120.786164 91.306774 140.742138
72.387353 120.786164 51.822764 68.270440
11.516170 174.352201 45.242095 38.861635
241.839565 186.955975 44.419512 33.610063
245.129899 139.691824 23.032339 23.106918
1.348178 5.241090 205.597166 345.911950
208.967611 2.096436 289.184211 168.763103
211.663968 179.245283 148.973684 172.955975
361.311741 179.245283 136.840081 170.859539
39.097166 26.205451 234.582996 441.299790
0.000000 23.060797 333.674089 468.553459
49.882591 0.000000 123.358300 476.939203
252.109312 0.000000 152.344130 498.951782
80.216599 52.410901 354.570850 247.379455
0.674089 0.000000 498.825911 333.333333
195.485830 394.129979 93.024291 55.555556
165.825911 394.129979 28.985830 36.687631
119.987854 384.696017 62.690283 52.410901
44.489879 429.769392 49.208502 46.121593
20.222672 411.949686 76.846154 51.362683
9.437247 402.515723 89.653846 51.362683
4.044534 393.081761 84.261134 45.073375
62.016194 376.310273 44.489879 19.916143
210.315789 202.306080 53.253036 40.880503
276.376518 259.958071 58.645749 171.907757
151.670040 151.991614 93.698381 102.725367
335.022267 250.524109 51.230769 145.702306
372.771255 200.209644 43.141700 36.687631
318.170040 202.306080 70.105263 62.893082
197.508097 245.283019 62.690283 125.786164
228.516194 269.392034 18.874494 35.639413
191.441296 208.595388 52.578947 59.748428
241.323887 148.846960 34.378543 29.350105
298.621457 156.184486 36.400810 140.461216
332.325911 149.895178 27.637652 115.303983
231.886640 186.582809 18.200405 30.398323
57.971660 218.029350 22.919028 23.060797
342.437247 79.664570 16.852227 25.157233
381.534413 69.182390 21.570850 11.530398
428.046559 72.327044 20.896761 14.675052
108.528340 88.050314 300.643725 238.993711
56.623482 61.844864 373.445344 309.224319
36.400810 226.415094 362.659919 48.218029
139.536437 88.050314 253.457490 63.941300
124.706478 1.048218 318.844130 337.526205
290.532389 166.666667 126.054656 120.545073
113.246964 37.735849 239.301619 234.800839
0.000000 6.289308 216.382591 241.090147
263.568826 57.651992 71.453441 137.316562
334.348178 10.482180 160.433198 224.318658
469.840081 1.048218 28.311741 39.832285
6.740891 1.048218 119.987854 229.559748
145.603239 1.048218 101.787449 228.511530
284.465587 0.000000 50.556680 220.125786
21.570850 248.427673 103.135628 248.427673
140.210526 254.716981 119.987854 238.993711
272.331984 246.331237 62.690283 238.993711
334.348178 2.096436 52.578947 219.077568
335.696356 262.054507 51.230769 224.318658
60.668016 0.000000 274.354251 315.513627
0.000000 0.000000 62.016194 374.213836
335.022267 1.048218 130.773279 372.117400
101.787449 109.014675 232.560729 186.582809
320.866397 91.194969 176.611336 157.232704
345.807692 96.436059 28.311741 34.591195
30.425101 57.651992 235.963563 80.712788
54.089069 207.54717Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 106, in append_flipped_images
    boxes = self.roidb[i]['boxes'].copy()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 67, in roidb
    self._roidb = self.roidb_handler()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/tattoo.py", line 55, in gt_roidb
    for bb in boxes:
TypeError: 'NoneType' object is not iterable
0 280.587045 53.459119
332.325911 177.148847 113.921053 226.415094
14.874494 38.784067 319.801619 411.949686
334.348178 48.218029 163.129555 392.033543
0.000000 35.639413 334.676113 462.264151
334.348178 96.436059 39.097166 30.398323
4.056680 1.048218 329.943320 492.662474
335.696356 19.916143 21.570850 102.725367
142.232794 89.098532 306.036437 190.775681
14.874494 59.748428 319.801619 259.958071
334.348178 192.872117 14.155870 49.266247
364.682186 238.993711 84.935223 75.471698
22.987854 0.000000 311.688259 273.584906
320.192308 96.436059 137.514170 262.054507
3.370445 19.916143 117.965587 305.031447
121.336032 7.337526 142.232794 320.754717
263.568826 6.289308 138.862348 303.983229
401.082996 37.735849 98.417004 272.536688
103.445344 109.014675 196.072874 201.257862
243.346154 30.398323 107.180162 112.159329
284.465587 185.534591 28.985830 78.616352
0.676113 0.000000 239.344130 331.236897
260.198381 0.000000 53.253036 101.677149
279.072874 0.000000 161.107287 166.666667
230.538462 149.895178 163.803644 128.930818
285.813765 96.436059 128.076923 196.016771
151.670040 159.329140 74.823887 56.603774
284.465587 145.702306 192.789474 114.255765
463.099190 289.308176 16.178138 40.880503
28.311741 2.096436 452.987854 465.408805
174.437247 190.775681 25.692308 91.194969
157.534413 342.767296 29.748988 38.784067
300.194332 212.788260 34.481781 89.098532
335.022267 213.836478 18.874494 57.651992
39.214575 99.580713 101.417004 143.605870
364.682186 107.966457 105.157895 125.786164
299.790356 161.423841 46.121593 110.927152
352.201258 158.940397 96.436059 96.026490
91.275304 177.148847 242.724696 114.255765
334.381551 175.496689 34.591195 20.695364
Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 106, in append_flipped_images
    boxes = self.roidb[i]['boxes'].copy()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 67, in roidb
    self._roidb = self.roidb_handler()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/tattoo.py", line 68, in gt_roidb
    bxes[ix, :] = [x1, y1, x2, y2]
NameError: global name 'ix' is not defined
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 106, in append_flipped_images
    boxes = self.roidb[i]['boxes'].copy()
AttributeError: 'list' object has no attribute 'copy'
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 111, in append_flipped_images
    assert (boxes[:, 2] >= boxes[:, 0]).all()
AssertionError
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 111, in append_flipped_images
    assert (boxes[:, 2] >= boxes[:, 0]).all()
AssertionError
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 111, in append_flipped_images
    assert (boxes[:, 2] >= boxes[:, 0]).all()
AssertionError
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 113, in append_flipped_images
    assert (boxes[:, 2] >= boxes[:, 0]).all()
AssertionError
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

[214]
[147]
[405]
[190]
[124]
[105]
[306 185  80]
[156  67  42]
[355 124 149 118]
[292  81 110  46]
[499]
[5]
[279]
[115]
[236 243]
[31 41]
[172]
[71]
[296]
[25]
[375 235 216]
[198 186  74]
[322]
[20]
[232]
[47]
[375]
[80]
[266  50]
[   30 65518]
Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 116, in append_flipped_images
    assert (boxes[:, 2] >= boxes[:, 0]).all()
AssertionError
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

[227]
[214]
[160]
[147]
[309]
[405]
[94]
[190]
[227]
[124]
[208]
[105]
[176 265 290]
[306 185  80]
[ 26 147 252]
[156  67  42]
[ 82 293 264 328]
[355 124 149 118]
[ 19 250 225 256]
[292  81 110  46]
[494]
[499]
[0]
[5]
[259]
[279]
[95]
[115]
[343 333]
[236 243]
[138 131]
[31 41]
[261]
[172]
[160]
[71]
[307]
[296]
[36]
[25]
[177 189 301]
[375 235 216]
[  0 140 159]
[198 186  74]
[312]
[322]
[10]
[20]
[285]
[232]
[100]
[47]
[295]
[375]
[0]
[80]
[285 333]
[266  50]
[ 49 265]
[   30 65518]
Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 117, in append_flipped_images
    assert (boxes[:, 2] >= boxes[:, 0]).all()
AssertionError
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

375
[227]
[214]
[160]
[147]
500
[309]
[405]
[94]
[190]
333
[227]
[124]
[208]
[105]
333
[176 265 290]
[306 185  80]
[ 26 147 252]
[156  67  42]
375
[ 82 293 264 328]
[355 124 149 118]
[ 19 250 225 256]
[292  81 110  46]
500
[494]
[499]
[0]
[5]
375
[259]
[279]
[95]
[115]
375
[343 333]
[236 243]
[138 131]
[31 41]
333
[261]
[172]
[160]
[71]
333
[307]
[296]
[36]
[25]
376
[177 189 301]
[375 235 216]
[  0 140 159]
[198 186  74]
333
[312]
[322]
[10]
[20]
333
[285]
[232]
[100]
[47]
376
[295]
[375]
[0]
[80]
316
[285 333]
[266  50]
[ 49 265]
[   30 65518]
Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 117, in append_flipped_images
    assert (boxes[:, 2] >= boxes[:, 0]).all()
AssertionError
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

375
[227]
[147]
[160]
[214]
500
[309]
[190]
[94]
[405]
333
[227]
[105]
[208]
[124]
333
[176 265 290]
[156  67  42]
[ 26 147 252]
[306 185  80]
375
[ 82 293 264 328]
[292  81 110  46]
[ 19 250 225 256]
[355 124 149 118]
500
[494]
[5]
[0]
[499]
375
[259]
[115]
[95]
[279]
375
[343 333]
[31 41]
[138 131]
[236 243]
333
[261]
[71]
[160]
[172]
333
[307]
[25]
[36]
[296]
376
[177 189 301]
[198 186  74]
[  0 140 159]
[375 235 216]
333
[312]
[20]
[10]
[322]
333
[285]
[47]
[100]
[232]
376
[295]
[80]
[0]
[375]
316
[285 333]
[   30 65518]
[ 49 265]
[266  50]
Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 118, in append_flipped_images
    assert (boxes[:, 2] >= boxes[:, 0]).all()
AssertionError
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1009019541_ad75c5b35f.jpg
375
[227]
[147]
[160]
[214]
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/101946336_2141d72b57.jpg
500
[309]
[190]
[94]
[405]
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1023796755_9be6020d8f.jpg
333
[227]
[105]
[208]
[124]
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1023796845_ba71b87a3c.jpg
333
[176 265 290]
[156  67  42]
[ 26 147 252]
[306 185  80]
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1046908494_dba8abe16a.jpg
375
[ 82 293 264 328]
[292  81 110  46]
[ 19 250 225 256]
[355 124 149 118]
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/105674462_58c7e87d39.jpg
500
[494]
[5]
[0]
[499]
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1089200564_b048a0f43a.jpg
375
[259]
[115]
[95]
[279]
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1093885445_4962fc3fe4.jpg
375
[343 333]
[31 41]
[138 131]
[236 243]
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094192991_a46ca53e50.jpg
333
[261]
[71]
[160]
[172]
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094201835_e77ae97d80.jpg
333
[307]
[25]
[36]
[296]
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094262579_2632d6d8c2.jpg
376
[177 189 301]
[198 186  74]
[  0 140 159]
[375 235 216]
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094952484_c9260976a7.jpg
333
[312]
[20]
[10]
[322]
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1095018976_844da9c8ff.jpg
333
[285]
[47]
[100]
[232]
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1095020932_ae2f343883.jpg
376
[295]
[80]
[0]
[375]
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1095024602_20919049ac.jpg
316
[285 333]
[   30 65518]
[ 49 265]
[266  50]
Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 115, in append_flipped_images
    assert (boxes[:, 2] >= boxes[:, 0]).all()
AssertionError
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1009019541_ad75c5b35f.jpg
[[160 191 227 248]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/101946336_2141d72b57.jpg
[[ 94  49 309 501]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1023796755_9be6020d8f.jpg
[[208 164 227 183]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1023796845_ba71b87a3c.jpg
[[ 26 294 176 425]
 [147 378 265 435]
 [252 309 290 352]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1046908494_dba8abe16a.jpg
[[ 19 224  82 356]
 [250 235 293 291]
 [225 299 264 339]
 [256 246 328 335]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/105674462_58c7e87d39.jpg
[[  0 166 494 274]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1089200564_b048a0f43a.jpg
[[ 95  60 259 452]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1093885445_4962fc3fe4.jpg
[[138 225 343 342]
 [131   0 333 193]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094192991_a46ca53e50.jpg
[[160 122 261 231]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094201835_e77ae97d80.jpg
[[ 36  83 307 453]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094262579_2632d6d8c2.jpg
[[  0  17 177 476]
 [140 198 189 400]
 [159  49 301 219]]
376
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094952484_c9260976a7.jpg
[[ 10  78 312 465]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1095018976_844da9c8ff.jpg
[[100  88 285 425]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1095020932_ae2f343883.jpg
[[  0 111 295 389]]
376
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1095024602_20919049ac.jpg
[[ 49   0 285 500]
 [265   0 333 136]]
316
Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 106, in append_flipped_images
    boxes = self.roidb[i]['boxes'].copy()
IndexError: list index out of range
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

0
Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

('1009019541_ad75c5b35f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1011255694_9062554e64.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('101946336_2141d72b57.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1023796755_9be6020d8f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1023796845_ba71b87a3c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1024011331_d4d147e3bb.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1046908494_dba8abe16a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('105674462_58c7e87d39.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1089200564_b048a0f43a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1093885445_4962fc3fe4.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1094192991_a46ca53e50.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1094201835_e77ae97d80.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1094262579_2632d6d8c2.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1094282721_139792966c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1094952484_c9260976a7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1095018976_844da9c8ff.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1095020932_ae2f343883.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1095024602_20919049ac.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('10971006892551804891.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1101137366_8926b4ab79.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1108566904_6384a3bad5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('117120812_2b30638a7b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1185523215_80ca94ea21.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1197720849_adc3d46046.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('120852263_60faacb01e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1219663380_f0e2d08238.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('122144380_e11e7bd9af.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1225224232_c2388c99a7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1227427019_adb2dcd507.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1229369901_8ef5d43dcf.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('123646178_909a0c8cec.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('125353686_f2d1ef48da.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1254149851_22924a919f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1268964755_c2b24e007a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('127661510_dfbdf75ee8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('12778417_289408de33.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('128154647_396167d57e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1286505016_9bf7419ea1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1298913018_636d2e95a2.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1307239144_f00e11c8a7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1307891690_4664054657.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1316709779_55a9a01110.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('132169808_f6c948117a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1329579952_0f0194eb7e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1331659508_6464b516c6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1333499525_86a8601795.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1335778648_c4a50ad9bb.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1341107995_88b5ab74ae.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1347076459_4f75e5000f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1351911665_3f8a69aba6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1369137244_208c7d61e2.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1388188597_e96e5cbb90.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1392301470_cf4bda8245.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1397359339_82f782caf3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('140048873_3b3ab43b47.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1409458119_2bde733d41.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1410887081_cfeb936e19.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1411461937_2bbc6c51be.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1424737978_d6600b3175.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('143791136_5df31d241e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('143791137_4ee0f455e2.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1448804866_ad4f63def7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('145394295_eb36e1d6a4.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1465368001_61a182b8a8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1468219029_e373c580cc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('148633656_b49a3ceb36.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1497589545_a44d754759.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('151993266_90a427dc3c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1534239119_46b55606af.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('153700476_9e25b3c5d3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('153700477_96ab762a8d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('153863291_6881620ecf.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('157650572_d4eea92767.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1638793379_170ad48222.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1640950771_ef2b64282e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1716564733_8ef43d8af5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('172421670_d8d81af630.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('176056555_9b34c8c91d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('176457819_3883b3ed6f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('179511654_10e6ef5468.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1811186173_9f437912d7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1823784033_b669756d8f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1828809749_4f1338f30f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1861082589_5ecb77c2cc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1862359906_0cf4229ed8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('188228561_6de2532cc9.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('190456763_bb57178ab6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1918130045_5149be9781.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1929852214_150ac0c7eb.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('193256934_4141b8a5d0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1933839482_3413575684.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1938816398_d93d49d5ef.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('197228604_4e25e86a52.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('199614442_973fbb5dfc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('20081212082635755.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2036332475_9988491472.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2048824986_2e5cc16964.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2049487802_23d896ab46.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2051782359_e906f380e0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('205184480_66a0613063.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2055320128_f74a3145b1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2057746553_33fd4ad2be.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2065982006_6be37ab23e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('4b7c4413d5c12.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3916538_18f603bc2a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('4278416_a83470e5a8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('19353801_6b70df7364.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('397940_b6a5311483.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3287424_e9321fb99d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('6737303_f787c716fa.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('4878768.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('26582111_c97963a0da.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('41286067_b212435218.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('26766632_4eefbfe9c1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('49802444_0f88429a25.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('51473994_db87d5e670.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('62669891_afe916b2c6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('26277926_d8a7a6fce8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('28760576_18e8300388.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('61057968_7c9f0c7e1f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('66066190_c1625bcd7b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('96877322_65c1592f17.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('45682058_a7d712a236.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('74604323_4f36c8ab26.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('172895556_2e5e2211b4.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2087711425_9679d5a5dc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2087958343_f4096284e1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2088270167_7a07eac6fe.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2094257603_cc30c66f6f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2101532320_ca0e5de1fc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2106421677_0e0f7e3492.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2114439401_9ac571e1cf.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2114441515_c2e4b3e22f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2120534859_f57fcdefc0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2120592245_3bcbb1b4e0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2120592617_7185fd8dce.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2120616549_7065e26bb0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2120638387_3f40eda925.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2120638641_514bd5a307.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2120742010_b4cf2905fc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2121308266_0feb6bd3c1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2121308652_23d5939cc4.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2121396114_2f86117579.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2127275961_cd93d081d1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2134074375_149d862357.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2149838267_94ea9891b6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2150033646_8f8fd675de.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2150994100_9bb5ec0108.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('215379990_3ed625de38.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2154127392_168e063905.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2156120334_48bcc44148.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2157489719_691ec8a89b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2164381309_9d386ecefe.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2170248258_1fb6548e14.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2172674705_2201095c22.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2173791112_5689226d9a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('217500624_50610f717f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2183176820_1e4982707d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2183360185_6b5e46e631.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2183783930_6e8231b4ef.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2184177597_ceb78fc248.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2184777112_21af1cd3dc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2193202899_6c9f61624f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2198164545_dd260b5c4e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2203733278_e1e3c3a38b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2205190538_f47a2a1f95.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2205580880_14687d983f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2205826561_604b392aab.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2208595992_384b46fb36.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2216818357_53aa50a475.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2217612504_c4cb0294c7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2217615696_eaf2d81b4a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('221946827_4d40b23bd1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2221481634_edc1370f54.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2223294366_1b5d5330b7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2226764359_62eeb96b01.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2229191027_ee9dd338e6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2230402473_191892b401.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2232372587_6b33fb53ed.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2238516020_e6f5091200.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2241663874_5a80893e7a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2244641611_0b7498dc80.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2246080353_cb576db276.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2246765886_893d89de80.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2247200137_eff32df6f2.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2247994418_546afbe2d1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2247995416_213426728c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2248083004_6d51969940.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2251779379_1881808098.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2253151891_5b8d1ed18f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2253152505_99921ee047.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2257300988_e8ef2dd9de.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2261674986_2b2e751ece.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2262961683_850e2b91a8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2263621763_c5149a4c33.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2265309279_3081c778eb.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2265425499_104cfa2ba5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2265486209_b701ded4ba.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2266858419_7ae85fa372.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2266858673_a7f88e502d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2268124830_b999bb8115.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2270812904_5e8ab887e6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2270910038_620d74d459.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('227163082_9ab0c827f4.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2272959478_96dd2e78ea.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2274460565_df97fdab04.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2275292017_381c4c9b72.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2280688074_2a88704eb5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2285292754_a70cd79725.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2286966085_02f6bea7a6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('228904477_aef825456b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('228916074_d742dbb7fd.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2292851476_d192d5d8d3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2296993846_e889d4dcb7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2300139058_b8455ccfb9.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2300724768_d6e361a50f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2304094592_ecbd20e01e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2305043830_f363c4ffe0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2306401854_c83aae519e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('230866069_d612f315fc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('23094975_e930e82792.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2310656629_4156b2dd8f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2315610704_7310b88e99.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2319693821_7a48984aa6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2327103136_bae2dd7bd6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2327660291_165109979b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2328487406_2031338149.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2329559458_bb162e048a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2344460596_65e4873629.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2344952778_c8bc92cf78.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2345817057_ffcb173b94.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2346522858_bd46c19afd.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2346715500_63dd4438df.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2351851997_094dc2c3a5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2358441681_e0a1ef1faa.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2359268818_8f2176c076.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2364662936_b8111a72d7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2367246785_c3dfd0daba.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2368216381_18cccec1dc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2368364743_91b8a85716.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2369032418_7384f6d1e3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2369037302_151819236e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2369074132_d3ceb5ee15.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2369137778_67740f6025.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2369148272_2148eb1f20.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('23694472_465d821473.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2369487067_02091a5c3a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2370322658_3b475d76de.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2372584000_7ea95282ed.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('237581278_f234d9b737.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2378083529_b52e68c002.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2380815105_4da3aff255.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2380818535_618f43e38b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2381513336_7222e6ef19.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2381513374_2dbd39bc78.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2381513772_cf97eb6ec7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2381513994_644e247411.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2384218313_829d92d498.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2384436283_ea13f06b6b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2385048032_360b78b42a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2385871211_193cd454bb.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2387184984_9676d3b438.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2396340080_cbb9298dd0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2397473121_63f063d88b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2397874870_ce5b260756.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2398591727_6673c0e1d0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('239861827_6d75db1fba.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2398853821_99b061c81d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2401641247_08f79f93cd.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2402272922_32ec903489.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2404388242_d54a5b13da.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2405182936_228e117ebc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2405679969_33efa874f0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2413843006_3d08633a75.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2414178223_3ce9da7f97.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2418055284_a449a57d5f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2418832514_cf6fcff0b7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2430004725_37ac7dff9c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2430032387_5c4a75d66c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2430099765_0c1be9ffa7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2430596505_c908f6ee63.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2430904006_43dc28c2eb.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2432002864_20cd8b9235.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2436242400_5b6a0f1afc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2437301757_e8664b36ff.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2447365206_5f0079baab.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2447497468_d5fdde9a93.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2447640796_3850b84c30.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2454096425_5427a16f57.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2454694772_72e758aff4.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2456693920_bf7125b4ff.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2457459044_3e6f613b32.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2461709782_b5817c2ac6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2468000564_4af4798092.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2473592777_e8644cfaef.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2473913448_b2ec902625.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2474255555_8f214a55af.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2474256101_71e84191ff.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2474259291_4605cd92a5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2476916812_e0f31ae6a7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2476979013_bfc2970635.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2481597791_d8005610ce.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2481865703_313c80a58f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2491124514_b175a14ddd.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2496086056_a7ecf23077.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2498133478_ed8911fa9e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2505192902_eed63ee978.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2507220019_1f7d0e1bde.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2517968190_f038717e3a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2520995415_3263300bea.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2523371065_08b7d48350.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2525271488_2a83c60a06.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2526912310_d2ef7f4c2a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2527677679_9840cd71c9.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2527692165_411a36c0e6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2528527842_7de26e95ef.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('252987414_611a166eca.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2529952056_82876c3d84.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('253023518_14afed3d55.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2532506091_a44b8716cb.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2533475023_ac9dc24069.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2537277610_00fe896e40.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2539678251_719861bcfb.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2541835881_dcb02ef43e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2542234049_2a8ae04c6a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('254346095_b58855ba3d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2546654478_052ba75487.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2552987958_17cf7b0305.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2553681929_a9d920340e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2555088242_71824fce38.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2557754615_0d17a06aae.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2558617913_a98c13e4fc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2559142688_2ce0a5716a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2560134492_4ebb9c9ea5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2563203017_7dd3d881c9.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2568272651_7e3ddae456.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2568636143_b7a3c6a4ed.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2573319771_2d08776df5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2574309342_50638b320b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2575453808_2fd56f5412.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2578266787_1aaec0da8d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2582566200_6e100670ba.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2582657304_cacf00d438.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2584088276_80711571da.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('258723907_06bfac4a09.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2588381721_677d8ef225.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2588666751_cc5bf0d4c2.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2588700867_9060705df9.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2590095503_cf477c3567.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2596375186_9bf3a1a3ab.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2600793104_8af0cace7a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2608446760_9c2394d056.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2609103510_b6f6e53b0c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2612226557_b8fe251992.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('262146475_984e254ab5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2628577697_e448737748.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2629570606_f5fa5f08aa.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2632984162_843945a512.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('264274250_0c291d90c5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2643601867_2b568c6a25.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2645842785_a54eeeb731.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2649519850_79fd66af16.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2652426535_28663e7267.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2652433127_bf6b390115.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2652618186_7eb8f07572.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2652664363_15d0ce0b93.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2652690958_35aca49304.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2652711562_76a9b3e011.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2652810541_8fa69f5fe8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2652977647_93a3292ee5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2653044466_f9e83d8985.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2653337168_8e05f70e28.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2653809650_706dbb20f1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2654045098_2a3e5c6abd.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2654087592_9d66cb9eb4.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2654093602_2fe31819e1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2654160152_9aa08c7a98.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2655498704_f2cb7f46e4.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('265659902_bd8e4516d1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('265853920_0ada58df98.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2662720918_27579552f4.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2666882541_0129165308.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2666882621_bf0dd2d6cb.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2669768096_ab6a1e3edb.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('26740486_a4fb2be2f8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2679894177_d3608a1eaa.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2681740404_5d3850274a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('268518057_741c32fba1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2690421177_e272c7860b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2690926752_dffa328bb5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2695932735_1f6b9cbdd9.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2698992095_7565eb70e0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2700790089_3b0d75de67.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2703148949_0ef318393e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2703152933_e884025ec6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2703961842_da62ef3b82.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2705476086_be05a013d4.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2711164445_6287425776.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2714461756_e16c1319c7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2714574533_2407359a66.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2714580907_911d650a24.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2716147072_3bef9e454c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2718443045_d70f5ff176.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2718757883_435816ee93.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2720158038_28c049e68b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2720900320_67fe25b22a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2730261820_3f3b1ee7a9.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2733325661_fc7dc37b4f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2733807671_ff6de6fc9f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2745005957_73ce6c0a64.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2745151002_46fb8e2ac8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2747412378_037360f78a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2747558829_ed6edc592f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2749603745_9f42c2d98a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2753936643_00949a0265.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2754245827_0d5f878722.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('275584567_7121aeebff.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2761414328_e403e3ed60.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2763157565_3f4aee6696.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2766377454_1f55bbfd1e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2768618367_1da87afcb9.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2771030169_39f85caa71.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2475188798_a1b46f71cc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2772618853_a2cee485c6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('277291396_e260ad2ea7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('277565292_54f8beb23f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2776091620_fdf96ec30c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2776427893_58405d2a94.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2778947321_cbe6bc7f44.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('277962214_3adc405693.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2782095902_167159724b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2784769634_27dcdc7193.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2791868333_13858f2719.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2793025208_e3861fe15a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2793785631_c690e96241.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2794870729_06e4e7eb02.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2798753295_b66b809701.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2800958317_890c163a29.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2807171070_02351c23a0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2807454896_225b9256d4.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2807809131_db19397186.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2809597385_db57bcc9ca.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2813856361_a11c753d47.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2815789263_80bea49fc1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2818176783_a397db9455.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2819955649_791bac378f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2820836141_fe1ee4ea32.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2826324415_641acac488.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2832176048_555ffbfb8b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2838997888_2d03221f99.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2839225554_8cc4495b30.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2845202225_b0a659f3d1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2848306494_a002b051de.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2849547636_033f228b41.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2851098138_59bf014293.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2852267364_7454d8f763.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2852658836_d833345c46.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2854214078_dfcd48344d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2855459432_88ec45b32c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('285618288_1c918a2ac1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2863451255_433b60fdcc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2863453803_91ea19a817.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2864282572_dc376f0aa0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2864287502_af55034acd.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2864774198_0edb174895.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2868751900_2980673817.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2868885829_a0269d92fd.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2873081985_753080e6f3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2874970563_0e72963fbe.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('287777501_1e2cc91ce9.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('288589725_fbbc0e3d04.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2888968522_90838d9e2e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2888970540_852ff8f6ca.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2898954235_88d087f0bc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2907253251_805115d676.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2909688874_69f101d45d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2909745583_31e55f811a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('291036830_65baded7cb.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2912250169_c29e41b212.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2914127688_82b217b565.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2914329245_395bcf61b9.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2918786855_41beb0f736.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('291912618_3308241c12.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('291912619_47848e29dd.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('291912622_3fc5909dc9.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2919513446_43321f17eb.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2922973367_accb4bd3a1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2923258307_d6f72dd804.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2924816257_a2e49e1df9.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2925448737_9840b6ac5a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('292748328_bede44b488.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2929480706_a83fa542ca.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2930098935_75260f5646.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('294370578_dc185ab904.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2948752327_b311736e12.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2948947051_f183b72900.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2953256282_f3f74b62d5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2956473595_07c6a3ae30.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('295902445_5e8e3fd448.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2961238151_61be2079fb.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2961582497_cf7ec72db2.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2961750515_0a93366092.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2961751317_d6123fb4e8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2961751449_b63ccf2e0f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2961751765_16f7364caf.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2961751831_8cf55dd059.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2961752621_81ddc41652.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2962531198_6ceae5f46e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2962531278_a57e4721a3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2962531460_e4e55f406d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2962597736_9e5ffd2c00.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2962597966_82e6881ff3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2968940175_5fed5916e8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2972125287_4e58ab5099.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2972446329_29b0d2b972.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2973880350_a0cfd6febd.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2976152958_d241a381c8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2979465142_53286886bf.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2980217144_f234ab467c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2982109896_3974877001.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2982971987_f363055ef3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2984733160_c175ea3d94.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2987061733_370583cb4f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2996085409_567f3470ce.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2998020005_3b0e9a51e7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3000979475_3ce2cba61b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3004209138_27ddba22c6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('300437668_be563d90c3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3005141153_7b602f3d8a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3010857777_70ea03ef7f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3011695116_bdd9bcb7de.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3011695472_12fdc37bcd.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3019089911_bb86cb6858.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('301997624_62c79e3592.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3025321199_9537df1d5d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3036139461_63071eaf11.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3037608589_c90605ee5a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3037609921_76206e25c4.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3037610539_74de7feb2e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3038446742_b1142f2c06.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3038447028_5a315984d1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3038447174_92d815ba9c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3041501172_8245ebdef9.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3043391765_5572d4d240.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3043392473_6ec394f999.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3044226950_3a21c92fba.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3044227020_9a249997ec.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3044229386_32593cebd3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3044229526_30387a748a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3044229596_19e7ee9607.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3044230214_d59ce23fc3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3044231234_11c545d2e9.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3045646890_89333d5567.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3046486969_8fd88a26c5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3046926875_1e4be38bdc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3047321228_4399e0940a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3060824051_15a5cdf62c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3062848663_02e02baaff.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3063777501_9db8419e98.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3068376505_4a500de84e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3069342606_06776de4a8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3070508435_f7c0c50acd.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3072815499_f627d3210c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3075027356_a9249e79c8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3080379161_32b047453d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3081137466_d2cccecde2.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3082667838_ce5dd034d0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3086953272_0cd2eea3c3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3091097438_e353dc7964.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3095329523_79bd47f0bd.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3096158214_fa63ceb894.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3096316718_ec940bc2bc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3097760585_5de850ff27.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3098919253_65fddd46d9.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3104341096_928e36c49a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3106036279_7e45664b5b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3107065952_a377d0c01a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3108498252_6acdc05593.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3115996073_40850ba4c7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3115997297_9dcc260ea5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3116079303_152fc185ea.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3116086075_95c747e8ce.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3124463487_8bc4e47ab3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3127479926_1a55a648c8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('312904843_73c5967ca7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('31324344_323b180249.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3141500829_602eb4ecfc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3144185252_2e442eeeab.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3146123887_2fa65cb02e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3146862214_80007dc750.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3160809701_93e4698087.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('316143428_738c499392.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3166662876_016e8fe64a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3167010230_d79d6c1aeb.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3169421788_1732d9d253.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3170745032_0c07ef139a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3170930443_8efb68aa7a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3174707172_c0514fec4c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3176116455_dc77796c97.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3177517156_bbb6c2e1ba.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3177641560_a90f8d3250.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3179871568_2149d1cd54.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3180156147_2f6c5f0871.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3180412766_b96bdafbee.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3182235303_d49941bdbe.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3183077960_43561c03c5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3183181850_003f5232c5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3185550957_c5e7b0b0ab.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('318797088_5970a18d2b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3193691598_4f29a2a8b6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3193894049_cdde75e62f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3195382665_38005a1f94.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3196231386_6048239cd3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3196231698_13819e101a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3197473569_cf378b0350.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3198234302_597217858f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3201090752_6ff618dfa0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3206819822_68ba090901.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('320809001_ec6e930d22.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3213024203_353ed6aecf.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3214677412_594e2e547f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3220282737_4d1b082563.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3220282929_2b6467ea0d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3220284789_effce039c0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3220285133_99a96f9810.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3220285637_231e815326.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3220285811_ee9399a6a7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3220286717_e2d181d8a7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3221130346_035c6b7587.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3221135676_17c64d24fe.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3221225563_d257a9eb8f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3225708734_88352809b1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3225754413_8c30b34711.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3227189872_6744f06790.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3227189894_589e351f3f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3237023034_cab218688b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3240217703_d466f5a712.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3241353935_19e596d478.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3244644333_98382328ed.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3245333421_7f70625e97.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('324887249_b43353be8d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3250383_ce86c9399a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3250580693_1cfca51f31.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3251417046_27d988f35e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3254423786_f5c9cbda2b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3258604458_443732434f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3265691901_2d2ca785df.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3266787242_ba233105e7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3270101941_09a7575c5f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3274997344_5e4161acac.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3275353122_b718dfbdf6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3275512766_4cffbc6a86.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2936701358_18fac3831b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3213713199_80ef146f88.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3276353259_88d9237164.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3276968640_b32ba72ee7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3281570026_119ccd6e22.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3284890059_871432ae3d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1442132437_0b419abef4.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('148445831_95978a00ea.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('149409472_3a32381a07.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3286963378_2e7987c323.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3289753936_8fbb29c76b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3292896712_24ba702200.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3293584558_f2c4a06b1a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3293984494_ffbb9996a1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3294544224_f37e894a2b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3297401_c1e06821fe.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3300629947_6690083942.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3300691285_8e718f301c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3300732975_474ca4715b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3300766777_2295f67a35.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3300766925_118a20a772.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3300770835_9d3202fe45.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3300771609_7d9f5f23ea.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3300772023_d18e699a7d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3301523636_26db494d6c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3301547396_288b615172.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3301570230_d2c5525157.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3301570414_fcc88d571c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3301602512_1b9a4d2737.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3301605808_246c213d4c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3301993624_df872428b0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3304788110_ce00183705.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3316529018_107ce2043d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3317638565_7bd3bec768.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3317638953_26317b6181.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3319813235_e54b5bfd4e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3333025455_59520f8ac3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3334254215_c87a73a142.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3334256871_9bfff5868f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3334260711_29aa8543e5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3334261279_65a8236b5c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3334265083_e20ff05aa5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3335095922_d2e18403bf.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3335096106_b6a64dc731.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3335096784_a9fb832f87.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3335097282_6a0cef1dac.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3335938138_4eb2a2225d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3336987098_7f977b4b3e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3340337009_fd70613910.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3344056821_f1ffa4f32a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3345278421_3535b01501.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3345706488_0f47bf07b2.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3347003223_61bf31fd2c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3347276019_b414ab7d24.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3348521427_3055d0f12d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3354329990_df03c94a94.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3358769063_fabe91c0b3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3359187245_947fa0a896.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3360721895_2ced667ac2.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3362787168_4fcfd1f400.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3363549441_206c91a3a3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3363554549_6f2a690ea5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3363620445_6af6b1516e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3364230459_dd1c6edd46.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3364442552_d90662fa53.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3364497658_911f07438d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3364529634_91812a8126.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3366970399_a62f14b6b5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3372215017_87466f8ee0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3380538877_cd102010e8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3381695306_7b60b67b48.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3373223735_ac45d1ba69.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3387302973_bd05b2c589.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3388962335_440d65c57a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3396028537_db73cf320d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3400122192_6c615b3e17.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3401570447_ab1d4f71e2.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3402602198_7a230f8413.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3408617087_0080ec7fe7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3413098783_2d0225d542.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3426218716_cf745b990a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3426477598_8b682696a5.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3432506510_77f164c6c0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3438566827_175622694b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3440610098_a7b1892ebb.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3442564013_26334ee43e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3444499700_100071dfc6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3445037917_dea6905eb4.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3451511530_91920d76f9.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3452335629_58f309d10c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3452439438_254c4793b9.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3460968820_0b011cdbcd.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3460991491_e9b52e8bdd.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3464942261_1697e43c7e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3468719708_cc8725bb6c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3469683372_376bd29b98.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3476198590_6c8680012a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3478851089_864a2d84a1.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3478865855_070e05b920.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3478879327_82cb46f398.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3478881749_699380e3a0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3478893381_a927efa03f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3478893837_eb77e41c99.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3478902807_f736398a3d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3478903289_c9b61cf369.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3478905439_5c6258b293.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3478905527_0fe6000d1e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3478907053_b93b8d391a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3478908661_4fcd6d44ea.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3479661338_5fc154af5f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3479666184_1408ffa6dc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3479682148_469e871357.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3479701072_2eb4621873.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3479702998_cde0204de3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3479706490_74557fea0e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3479709288_7eb4d38434.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3479709980_d45cfc3306.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3479710846_0464526c79.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3480713612_d71516d4a7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3481967156_05e3b1db3a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3482620017_7a74c211f2.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3491606676_113b86832c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3497360691_9f78fb63df.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3500080887_04cf304cae.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3500891042_74b627144f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3501428711_9b5cb2822e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3503185963_f66d4cd2be.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3503912825_d00e85afb0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3507129285_d9b717357d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3511251262_eb18f1f0ab.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3518522218_cd34657afc.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3520042076_bf840c3ea6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3522587470_81d82698ea.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3523868436_9bdd8a3b32.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3523868836_b4c088855c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3523927642_d826c289eb.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3524812043_e29046c271.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3525624142_6007a2552f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3528284686_bc6f8191e6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3529916041_19f5568d89.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3530561522_c9ec019bc8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3530732462_dd25f0d337.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3532681885_a921e64e36.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3537328926_4124b10c48.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3537483962_b76fbd8733.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3540115975_f6d2d79a55.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3546397165_8d7b7d2dbd.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3547238244_e58134f30d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3547436126_2b0282f29a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3548018635_0acc2ac6a0.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3548018837_ea79da5984.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3551832851_980532d738.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3552947245_0d9f925589.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3555838394_33a6769bd6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3556357515_b6ef02cb78.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3557824569_97566c4420.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3563236055_3d8a5f317a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3568696784_1b635e313b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3569860109_4feb270c20.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3570440523_170e8dfe1d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('357995884_4d8907cacb.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3582113075_63baea1709.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3585929687_ed48ac181b.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3586738238_f73a4ca4ec.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('359060088_235e9f34bb.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3591255956_446aa1a6d8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3591256256_7f1d6f2708.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3593172849_30be0787c6.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3601048369_5509d763df.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3605416329_e580fa309f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3606770457_8785d7f461.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3607710630_66828b9c3d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3608027022_94468c16ec.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3609287948_f7dd30648a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3610252372_0c0f0b2053.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3610445320_e80eddf3ea.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3611486287_a9bfddc6d8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3620191653_10568e0311.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3621163640_a372309a3a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3621640554_fd112d3d31.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3622989574_a5228528c3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3622989880_4028440523.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('362300915_1dec631120.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3627661153_bbc2793627.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3630653945_2258c249b8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3633487203_f4ed731607.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3633488591_7d7b75dab3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3635095435_3372db8b54.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3641612129_3f4eca1467.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3641619697_99a4103a33.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('364630499_487d6e10ac.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('364630556_961f320197.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('364630583_a1d651bcd9.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3647780943_b6ddb85b4f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3651372267_1159d92028.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3651376577_a75ea6f906.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3651394653_a2a5242f48.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3652343954_6122375b4a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3653146639_38c85e3159.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('3655064345_2483cdca94.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1034001005_e20b14be84.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1094999966_d05db85c4d.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1095126246_7082ff536a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1095201316_61667cb508.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1426333050_65c8552bbd.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1429770136_ac4802270c.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1432675543_fbd62b34d8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('154242826_4042377052.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1552068329_536e23e1a7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1580204857_7fa62a6265.jpg 'Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 106, in append_flipped_images
    boxes = self.roidb[i]['boxes'].copy()
IndexError: list index out of range
, './tattoo/1009019541_ad75c5b35f.jpg')
('163207_83e306de09.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1641813600_860cb7299f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1779088892_d0dddb5f7f.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1800665414_0edcfdb162.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1800822292_35e093473e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1802791887_5b5498a748.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('182640703_846d348394.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('183291642_f2aa331a43.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('183535687_7b15156636.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('193502106_fa3a3a2f68.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1938567922_469c4523d3.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1938668464_c383ad46a2.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('194533031_d927c41b27.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1961336703_c7139a2a7a.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('1993680184_6499ddccd4.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('200680082_b4339415fd.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2016720185_320c59e5a8.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2022460424_2f262dbf64.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2042679874_f4f72ecfc2.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2044682046_580b6ebf06.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2058671357_0247eb3f1e.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2059818330_bbd646e623.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('206422238_6031b099b9.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('207022191_000db3ea29.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2074432432_f0f84b5fa7.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2078682352_84c409afdb.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2079251541_f79b57cc12.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
('2081392868_46d57cd549.jpg ', './tattoo/1009019541_ad75c5b35f.jpg')
Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 106, in append_flipped_images
    boxes = self.roidb[i]['boxes'].copy()
IndexError: list index out of range
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

0
Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 106, in append_flipped_images
    boxes = self.roidb[i]['boxes'].copy()
IndexError: list index out of range

-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
0
Logging output to ./log
Traceback (most recent call last):
  File "./tools/train_faster_rcnn_alt_opt.py", line 19, in <module>
    from datasets.factory import get_imdb
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/factory.py", line 17, in <module>
    from datasets.tattoo import tattoo
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/tattoo.py", line 61
    if (len(bb)==2) break
                        ^
SyntaxError: invalid syntax
Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 106, in append_flipped_images
    boxes = self.roidb[i]['boxes'].copy()
IndexError: list index out of range
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

-1
('./tattoo/1009019541_ad75c5b35f.jpg', '1009019541_ad75c5b35f.jpg ')
-1
('./tattoo/1009019541_ad75c5b35f.jpg', '1011255694_9062554e64.jpg ')
-1
('./tattoo/1009019541_ad75c5b35f.jpg', '101946336_2141d72b57.jpg ')
-1
('./tattoo/1009019541_ad75c5b35f.jpg', '1023796755_9be6020d8f.jpg ')
-1
('./tattoo/1009019541_ad75c5b35f.jpg', '1023796845_ba71b87a3c.jpg ')
-1
('./tattoo/1009019541_ad75c5b35f.jpg', '1024011331_d4d147e3bb.jpg ')
-1
('./tattoo/1009019541_ad75c5b35f.jpg', '1046908494_dba8abe16a.jpg ')
-1
('./tattoo/1009019541_ad75c5b35f.jpg', '105674462_58c7e87d39.jpg ')
0
Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ftProcess Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 115, in append_flipped_images
    assert (boxes[:, 2] >= boxes[:, 0]).all()
AssertionError

ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
ft
712
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1009019541_ad75c5b35f.jpg
[[160 191 227 248]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/101946336_2141d72b57.jpg
[[208 164 227 183]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1023796755_9be6020d8f.jpg
[[ 26 294 176 425]
 [147 378 265 435]
 [252 309 290 352]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1023796845_ba71b87a3c.jpg
[[ 19 224  82 356]
 [250 235 293 291]
 [225 299 264 339]
 [256 246 328 335]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1046908494_dba8abe16a.jpg
[[ 95  60 259 452]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/105674462_58c7e87d39.jpg
[[138 225 343 342]
 [131   0 333 193]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1089200564_b048a0f43a.jpg
[[160 122 261 231]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1093885445_4962fc3fe4.jpg
[[ 36  83 307 453]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094192991_a46ca53e50.jpg
[[  0  17 177 476]
 [140 198 189 400]
 [159  49 301 219]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094201835_e77ae97d80.jpg
[[ 10  78 312 465]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094262579_2632d6d8c2.jpg
[[100  88 285 425]]
376
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094952484_c9260976a7.jpg
[[ 49   0 285 500]
 [265   0 333 136]]
333
Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 115, in append_flipped_images
    assert (boxes[:, 2] >= boxes[:, 0]).all()
AssertionError
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1009019541_ad75c5b35f.jpg
[[160 191 227 248]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/101946336_2141d72b57.jpg
[[208 164 227 183]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1023796755_9be6020d8f.jpg
[[ 26 294 176 425]
 [147 378 265 435]
 [252 309 290 352]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1023796845_ba71b87a3c.jpg
[[ 19 224  82 356]
 [250 235 293 291]
 [225 299 264 339]
 [256 246 328 335]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1046908494_dba8abe16a.jpg
[[ 95  60 259 452]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/105674462_58c7e87d39.jpg
[[138 225 343 342]
 [131   0 333 193]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1089200564_b048a0f43a.jpg
[[160 122 261 231]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1093885445_4962fc3fe4.jpg
[[ 36  83 307 453]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094192991_a46ca53e50.jpg
[[  0  17 177 476]
 [140 198 189 400]
 [159  49 301 219]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094201835_e77ae97d80.jpg
[[ 10  78 312 465]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094262579_2632d6d8c2.jpg
[[100  88 285 425]]
376
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094952484_c9260976a7.jpg
[[ 49   0 285 500]
 [265   0 333 136]]
333
Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1009019541_ad75c5b35f.jpg
[[160 191 226 247]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/101946336_2141d72b57.jpg
[[208 164 226 182]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1023796755_9be6020d8f.jpg
[[ 26 294 175 424]
 [147 378 264 434]
 [252 309 289 351]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1023796845_ba71b87a3c.jpg
[[ 19 224  81 355]
 [250 235 292 290]
 [225 299 263 338]
 [256 246 327 334]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1046908494_dba8abe16a.jpg
[[ 95  60 258 451]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/105674462_58c7e87d39.jpg
[[138 225 342 341]
 [131   0 332 192]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1089200564_b048a0f43a.jpg
[[160 122 260 230]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1093885445_4962fc3fe4.jpg
[[ 36  83 306 452]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094192991_a46ca53e50.jpg
[[  0  17 176 475]
 [140 198 188 399]
 [159  49 300 218]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094201835_e77ae97d80.jpg
[[ 10  78 311 464]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094262579_2632d6d8c2.jpg
[[100  88 284 424]]
376
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1094952484_c9260976a7.jpg
[[ 49   0 284 499]
 [265   0 332 135]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1095018976_844da9c8ff.jpg
[[ 24  32 316 470]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1095020932_ae2f343883.jpg
[[ 57 145 308 416]]
376
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1095024602_20919049ac.jpg
[[ 50  28 276 469]]
316
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/10971006892551804891.jpg
[[254  88 305 247]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1101137366_8926b4ab79.jpg
[[167 378 267 452]
 [270 211 286 244]
 [104 209 272 292]
 [ 76 361 101 391]
 [ 77 405  91 431]
 [294 199 312 234]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1108566904_6384a3bad5.jpg
[[142 153 252 275]
 [256 212 264 226]
 [250 202 301 286]
 [283 228 306 277]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/117120812_2b30638a7b.jpg
[[192 136 365 280]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1185523215_80ca94ea21.jpg
[[133   0 230  63]
 [278   0 313  75]
 [304  37 369  66]
 [330  64 370  89]
 [250  98 371 269]
 [128  50 232 243]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1197720849_adc3d46046.jpg
[[184  98 306 227]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/120852263_60faacb01e.jpg
[[203  74 276 147]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1219663380_f0e2d08238.jpg
[[197 274 323 466]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1225224232_c2388c99a7.jpg
[[240 147 284 307]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1227427019_adb2dcd507.jpg
[[  0 234 154 331]
 [141 122 210 209]
 [ 99 332 181 496]
 [175 245 259 302]
 [248 162 319 213]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1229369901_8ef5d43dcf.jpg
[[280 210 338 272]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/123646178_909a0c8cec.jpg
[[ 49  71  78 115]
 [ 67 113  93 152]
 [ 49 206  71 228]
 [ 73 208  90 224]
 [122 294 134 314]
 [101 314 116 332]
 [299  66 319  79]
 [299  94 314 102]
 [377 139 395 149]
 [405 122 423 142]
 [434 206 451 225]
 [463 198 485 217]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/125353686_f2d1ef48da.jpg
[[ 99  28 429 343]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1254149851_22924a919f.jpg
[[ 76 302 249 470]]
413
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1268964755_c2b24e007a.jpg
[[ 53 156 146 263]
 [141 179 193 262]
 [193 180 247 268]
 [254 172 303 249]
 [312 172 370 247]
 [361 167 427 244]
 [409 165 485 246]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/12778417_289408de33.jpg
[[ 17  25 457 206]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/128154647_396167d57e.jpg
[[ 80  92 370 376]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1286505016_9bf7419ea1.jpg
[[ 97  70 291 497]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1307239144_f00e11c8a7.jpg
[[156 148 201 182]]
320
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/132169808_f6c948117a.jpg
[[ 99  21 361 344]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1329579952_0f0194eb7e.jpg
[[374 122 430 202]
 [ 50 128 109 199]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1331659508_6464b516c6.jpg
[[ 51 164 315 479]
 [273 330 296 370]
 [278 376 303 450]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1333499525_86a8601795.jpg
[[180  78 406 114]
 [176 135 437 320]]
480
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1335778648_c4a50ad9bb.jpg
[[  0  41 404 499]]
406
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1341107995_88b5ab74ae.jpg
[[216  59 402 266]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1347076459_4f75e5000f.jpg
[[ 88 193 157 352]
 [166 193 233 350]]
337
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1351911665_3f8a69aba6.jpg
[[159  46 336 348]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1369137244_208c7d61e2.jpg
[[155   7 405 330]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1388188597_e96e5cbb90.jpg
[[  0   0 372 499]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1392301470_cf4bda8245.jpg
[[221 117 349 282]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1397359339_82f782caf3.jpg
[[  9 205 116 255]
 [123 180 258 231]
 [264 170 299 207]
 [309 166 364 203]
 [369 160 460 190]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/140048873_3b3ab43b47.jpg
[[250  62 369 192]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1409458119_2bde733d41.jpg
[[ 36  61 423 287]
 [123 235 248 298]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1411461937_2bbc6c51be.jpg
[[ 55 186 197 334]]
294
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1424737978_d6600b3175.jpg
[[ 27 122  68 167]
 [181  24 231 113]
 [230  52 258 113]
 [258  53 297 137]
 [306  34 318 120]
 [327  59 355 120]
 [362  57 391 118]
 [401  45 423 116]
 [198 135 402 226]
 [198 219 407 312]
 [208 308 397 386]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/143791137_4ee0f455e2.jpg
[[  1  40 323 462]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1448804866_ad4f63def7.jpg
[[142 171 160 186]
 [107 150 163 230]
 [ 93 177 102 190]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/145394295_eb36e1d6a4.jpg
[[179 175 320 245]
 [163 215 174 233]
 [151 192 166 212]
 [154 163 168 178]
 [127 174 142 186]
 [122 137 143 159]
 [146 113 158 133]
 [173  74 312 185]
 [157  88 172 103]
 [183  74 203  95]
 [158 115 174 143]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1465368001_61a182b8a8.jpg
[[ 98  42 245 208]
 [ 59 103  90 264]
 [276  79 329 224]
 [ 17 101  39 140]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1468219029_e373c580cc.jpg
[[125 297 224 429]
 [ 86  50 129 117]
 [268  24 306 155]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/148633656_b49a3ceb36.jpg
[[ 72 140 232 279]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1497589545_a44d754759.jpg
[[217 123 277 192]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/151993266_90a427dc3c.jpg
[[ 45  99 194 397]
 [194 178 334 411]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1534239119_46b55606af.jpg
[[146  34 299 302]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/153700477_96ab762a8d.jpg
[[ 40  40 317 464]]
373
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/153863291_6881620ecf.jpg
[[  6   2 334 484]]
353
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/157650572_d4eea92767.jpg
[[156 379 229 496]
 [113 262 164 327]
 [115 151 187 237]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1638793379_170ad48222.jpg
[[ 13  52 240 492]]
271
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1640950771_ef2b64282e.jpg
[[ 70   0 300 460]
 [ 25 246 100 358]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1716564733_8ef43d8af5.jpg
[[207 106 330 241]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/172421670_d8d81af630.jpg
[[ 62  14 332 337]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/176056555_9b34c8c91d.jpg
[[ 83  88 210 400]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/176457819_3883b3ed6f.jpg
[[114 232 200 254]
 [125 199 199 240]]
316
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/179511654_10e6ef5468.jpg
[[ 68  82 276 440]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1811186173_9f437912d7.jpg
[[ 33  53  91 393]
 [138  49 274 447]
 [264 451 279 471]
 [282 445 303 470]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1828809749_4f1338f30f.jpg
[[178 116 298 288]
 [192  79 225 115]
 [229  69 269 110]
 [275  76 310 119]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1861082589_5ecb77c2cc.jpg
[[176 189 241 240]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1862359906_0cf4229ed8.jpg
[[204 142 301 236]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/188228561_6de2532cc9.jpg
[[381 100 447 239]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1918130045_5149be9781.jpg
[[ 28   5 296 483]]
334
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1929852214_150ac0c7eb.jpg
[[ 87 220 194 410]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/1938816398_d93d49d5ef.jpg
[[ 31  13 221 475]]
250
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/197228604_4e25e86a52.jpg
[[  5 194 193 495]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2036332475_9988491472.jpg
[[ 16   4 300 488]]
305
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2048824986_2e5cc16964.jpg
[[ 10  10 135 479]]
143
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2049487802_23d896ab46.jpg
[[208 154 287 210]
 [142 133 199 188]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2051782359_e906f380e0.jpg
[[  5   1 214 445]]
217
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/205184480_66a0613063.jpg
[[ 22  71 264 375]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2057746553_33fd4ad2be.jpg
[[  4   0 174 485]]
182
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2065982006_6be37ab23e.jpg
[[ 54  24 277 463]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/4b7c4413d5c12.jpg
[[187 162 288 248]]
600
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/3916538_18f603bc2a.jpg
[[176   0 299 224]]
338
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/19353801_6b70df7364.jpg
[[ 49  35 162 173]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/397940_b6a5311483.jpg
[[ 26   0 257 499]]
291
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/3287424_e9321fb99d.jpg
[[136 347 209 422]
 [191 227 225 248]
 [222 218 231 231]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/6737303_f787c716fa.jpg
[[ 14 337  95 426]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/4878768.jpg
[[155 215 190 271]]
297
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/26582111_c97963a0da.jpg
[[ 36  42 226 464]]
275
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/41286067_b212435218.jpg
[[129  76 254 217]]
360
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/26766632_4eefbfe9c1.jpg
[[148  47 377 253]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/49802444_0f88429a25.jpg
[[253 133 374 272]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/51473994_db87d5e670.jpg
[[213  64 318 166]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/62669891_afe916b2c6.jpg
[[ 46 113 318 395]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/28760576_18e8300388.jpg
[[119 138 247 287]]
352
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/61057968_7c9f0c7e1f.jpg
[[170 130 180 144]
 [182 141 193 158]
 [196 136 206 150]
 [202 137 237 165]
 [243 126 298 167]
 [304 123 324 145]
 [323 138 330 151]
 [331 117 341 144]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/66066190_c1625bcd7b.jpg
[[359 112 374 134]
 [376 125 392 144]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/96877322_65c1592f17.jpg
[[215 243 232 263]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/45682058_a7d712a236.jpg
[[108  20 289 497]]
332
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/74604323_4f36c8ab26.jpg
[[126  76 267 182]]
378
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/172895556_2e5e2211b4.jpg
[[ 14   3 228 493]]
232
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2087711425_9679d5a5dc.jpg
[[197  37 405 223]
 [278 195 290 239]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2087958343_f4096284e1.jpg
[[201 252 209 274]]
334
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2088270167_7a07eac6fe.jpg
[[313 283 399 342]
 [148 160 372 302]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2094257603_cc30c66f6f.jpg
[[212 166 263 222]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2101532320_ca0e5de1fc.jpg
[[125 112 257 355]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2106421677_0e0f7e3492.jpg
[[ 75  89 104 122]
 [109 174 255 217]
 [330 221 429 278]
 [318 164 373 191]
 [379 131 386 146]
 [139 159 150 180]
 [142 213 186 249]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2114439401_9ac571e1cf.jpg
[[  8   9 118 167]
 [ 29  93 188 392]]
339
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2114441515_c2e4b3e22f.jpg
[[ 30 119  66 248]
 [ 45 104 299 456]]
339
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2120534859_f57fcdefc0.jpg
[[107  46 399 354]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2120592245_3bcbb1b4e0.jpg
[[127  48 413 335]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2120592617_7185fd8dce.jpg
[[145  10 402 320]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2120616549_7065e26bb0.jpg
[[175  51 362 258]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2120638387_3f40eda925.jpg
[[  5  20 337 478]]
356
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2120638641_514bd5a307.jpg
[[103 162 255 354]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2121308652_23d5939cc4.jpg
[[  0  24 168 203]
 [ 56 129 156 249]
 [155 126 336 280]
 [341 122 438 242]
 [464  94 497 167]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2121396114_2f86117579.jpg
[[  0 323 225 375]
 [ 28  50 450 308]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2134074375_149d862357.jpg
[[113  84 297 224]]
448
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2150033646_8f8fd675de.jpg
[[ 27  74 498 310]
 [ 71   1 342  30]
 [413   0 497  44]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2150994100_9bb5ec0108.jpg
[[ 20   0 483 478]]
487
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/215379990_3ed625de38.jpg
[[ 38  65 330 415]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2154127392_168e063905.jpg
[[182  75 328 196]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2156120334_48bcc44148.jpg
[[154 117 399 273]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2157489719_691ec8a89b.jpg
[[ 89 124 413 161]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2164381309_9d386ecefe.jpg
[[105  20 339 301]
 [187 356 271 371]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2170248258_1fb6548e14.jpg
[[  5   6 374 492]
 [ 12  98  69 234]]
398
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2172674705_2201095c22.jpg
[[ 74 219 199 294]
 [146 111 191 143]
 [135 168 195 204]
 [207  90 250 150]
 [ 10  75  72 130]]
310
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2173791112_5689226d9a.jpg
[[  0 251  68 387]
 [ 18  91 330 484]]
334
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/217500624_50610f717f.jpg
[[ 30 390  48 441]
 [239 287 303 339]
 [ 64 206 271 473]
 [160 302 175 329]]
352
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2183176820_1e4982707d.jpg
[[ 38 294  94 485]
 [ 61 190 287 460]]
334
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2184177597_ceb78fc248.jpg
[[ 53  64 360 390]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2193202899_6c9f61624f.jpg
[[  7   0 219 477]
 [214   0 420 472]]
439
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2198164545_dd260b5c4e.jpg
[[ 59   3 266 492]]
286
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2203733278_e1e3c3a38b.jpg
[[ 56 238  84 282]
 [146 175 211 220]
 [220 215 237 263]
 [ 87 215 219 316]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2205190538_f47a2a1f95.jpg
[[ 72 120 248 379]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2205826561_604b392aab.jpg
[[142 125 322 237]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2216818357_53aa50a475.jpg
[[ 52  65 333 497]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2217612504_c4cb0294c7.jpg
[[ 24 318 359 373]
 [313 126 497 369]
 [ 45   0 380 238]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2217615696_eaf2d81b4a.jpg
[[128  85 373 336]
 [256 304 374 493]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/221946827_4d40b23bd1.jpg
[[ 12   7 296 497]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2226764359_62eeb96b01.jpg
[[ 74 130 240 358]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2229191027_ee9dd338e6.jpg
[[ 95   8 365 231]]
474
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2230402473_191892b401.jpg
[[ 99 164 279 437]]
398
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2232372587_6b33fb53ed.jpg
[[116 244 162 298]
 [199 300 225 332]]
345
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2241663874_5a80893e7a.jpg
[[ 57  51 264 484]]
335
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2244641611_0b7498dc80.jpg
[[103 140 304 435]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2246080353_cb576db276.jpg
[[113 185 134 213]
 [359 194 373 219]
 [137 146 348 235]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2246765886_893d89de80.jpg
[[112  36 300 481]
 [283  66 295  86]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2247200137_eff32df6f2.jpg
[[ 61  73 261 338]]
300
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2248083004_6d51969940.jpg
[[302 123 336 149]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2251779379_1881808098.jpg
[[ 55 112 286 320]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2253151891_5b8d1ed18f.jpg
[[ 85  78 447 222]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2262961683_850e2b91a8.jpg
[[ 76   4 193 490]]
289
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2263621763_c5149a4c33.jpg
[[ 24  49 465 260]
 [108  77 180 134]
 [158  47 234  93]
 [262  39 302  62]
 [312  41 330  60]
 [120 163 139 196]
 [239 224 284 269]
 [377 259 415 298]
 [272 277 289 293]
 [290 299 308 315]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2265309279_3081c778eb.jpg
[[ 80 278 158 386]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2265425499_104cfa2ba5.jpg
[[106 192 128 220]
 [125  89 275 304]
 [175 161 188 197]
 [186 162 206 195]
 [211 139 260 177]
 [308 295 333 355]
 [246 285 274 388]
 [125 262 248 398]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2265486209_b701ded4ba.jpg
[[121   1 320 361]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2266858419_7ae85fa372.jpg
[[175  72 206  94]
 [136  93 220 464]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2266858673_a7f88e502d.jpg
[[141  38 245 376]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2268124830_b999bb8115.jpg
[[202  64 237 103]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2270812904_5e8ab887e6.jpg
[[145 211 224 289]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2270910038_620d74d459.jpg
[[225  79 371 268]
 [  0 386 104 431]
 [ 76 394 224 497]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/227163082_9ab0c827f4.jpg
[[176  90 261 265]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2272959478_96dd2e78ea.jpg
[[ 22  29 205 342]
 [ 56 132  74 159]
 [ 78 157 100 184]
 [160 147 183 183]
 [297  47 482 323]
 [319 164 349 190]
 [436 116 453 142]
 [432 162 456 187]
 [348 315 371 347]
 [406 325 423 364]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2274460565_df97fdab04.jpg
[[ 33   0 496 330]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2275292017_381c4c9b72.jpg
[[ 36   0 207 499]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2280688074_2a88704eb5.jpg
[[284  44 408 249]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2286966085_02f6bea7a6.jpg
[[262  49 353 143]
 [125 211 159 262]
 [159 207 241 282]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/228904477_aef825456b.jpg
[[  0  40  87 234]
 [108  60 175 116]
 [303  73 371 133]
 [176 120 288 258]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/228916074_d742dbb7fd.jpg
[[ 52  28 391 446]]
466
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2292851476_d192d5d8d3.jpg
[[253  70 432 282]
 [101 123 339 301]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2296993846_e889d4dcb7.jpg
[[357 219 416 312]
 [268 137 308 164]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2300139058_b8455ccfb9.jpg
[[ 54   0 239 497]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2304094592_ecbd20e01e.jpg
[[ 58  47 265 402]
 [ 45 318  62 398]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2305043830_f363c4ffe0.jpg
[[221 113 280 202]
 [198 253 275 471]
 [ 51 188  90 214]
 [ 62 248  98 278]
 [ 59 288 153 414]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2306401854_c83aae519e.jpg
[[120  29 374 311]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/230866069_d612f315fc.jpg
[[151  72 369 254]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/23094975_e930e82792.jpg
[[149  88 167 152]
 [218  89 250 120]
 [168 188 203 222]
 [164 183 175 203]
 [240 196 247 213]]
334
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2310656629_4156b2dd8f.jpg
[[ 46   1 315 497]]
334
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2315610704_7310b88e99.jpg
[[119  39 371 320]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2319693821_7a48984aa6.jpg
[[ 80  10 370 469]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2327660291_165109979b.jpg
[[102  29 286 466]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2328487406_2031338149.jpg
[[ 99  93 269 469]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2329559458_bb162e048a.jpg
[[261 138 315 162]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2344460596_65e4873629.jpg
[[152 122 398 210]
 [301 196 372 218]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2344952778_c8bc92cf78.jpg
[[  0   0 419 498]]
453
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2345817057_ffcb173b94.jpg
[[ 93  96 267 356]
 [208  75 244 105]
 [ 78   0 234  80]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2358441681_e0a1ef1faa.jpg
[[ 16  24 227 138]
 [ 31 162 208 251]
 [274  22 468 161]
 [296 166 472 245]
 [301 196 474 263]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2359268818_8f2176c076.jpg
[[ 11  48 287 491]]
316
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2364662936_b8111a72d7.jpg
[[104 184 171 248]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2367246785_c3dfd0daba.jpg
[[ 14  11 239 493]]
254
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2368216381_18cccec1dc.jpg
[[ 27 202  45 222]
 [  2 211  84 323]
 [277 331 348 365]
 [407 177 460 295]
 [316 214 324 228]
 [374 203 379 221]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2368364743_91b8a85716.jpg
[[369 240 419 286]
 [389 350 412 371]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2369032418_7384f6d1e3.jpg
[[ 39  68 469 275]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2369037302_151819236e.jpg
[[ 66  61 474 201]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2369074132_d3ceb5ee15.jpg
[[176  34 386 367]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2369137778_67740f6025.jpg
[[ 47 134 465 347]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2369148272_2148eb1f20.jpg
[[126  26 412 311]
 [201 110 210 124]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/23694472_465d821473.jpg
[[  0  94 283 388]
 [  7 185  35 218]
 [135  98 166 121]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2369487067_02091a5c3a.jpg
[[102   6 363 358]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/237581278_f234d9b737.jpg
[[ 73 132 325 258]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2378083529_b52e68c002.jpg
[[117  19 289 360]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2380818535_618f43e38b.jpg
[[ 58   0 268 394]
 [ 33 292  51 362]
 [100 438 140 483]
 [161 437 194 479]
 [209 418 235 454]
 [245 378 270 410]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2381513336_7222e6ef19.jpg
[[ 68  23 403 295]
 [ 44 298  79 357]
 [ 72 319 120 356]
 [176 341 205 356]
 [215 341 246 356]
 [251 342 293 356]
 [298 329 359 357]
 [362 317 415 358]
 [401 292 450 343]
 [442 182 476 269]]
480
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2381513374_2dbd39bc78.jpg
[[ 88   0 295 410]
 [ 82 271  94 301]
 [195 430 219 448]
 [237 331 263 383]
 [283 353 294 374]
 [241 397 261 419]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2381513772_cf97eb6ec7.jpg
[[ 84  32 347 301]]
480
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2381513994_644e247411.jpg
[[134  48 263 424]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2384218313_829d92d498.jpg
[[179 165 274 247]
 [269  95 349 160]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2385048032_360b78b42a.jpg
[[154  72 363 274]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2385871211_193cd454bb.jpg
[[127 133 347 199]
 [133 192 176 234]
 [177 201 223 248]
 [226 216 267 258]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2396340080_cbb9298dd0.jpg
[[178  58 336 449]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2397874870_ce5b260756.jpg
[[ 63  79 474 249]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/239861827_6d75db1fba.jpg
[[356 181 380 207]
 [368 207 385 226]
 [384 225 395 241]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2401641247_08f79f93cd.jpg
[[ 53  28 337 495]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2402272922_32ec903489.jpg
[[ 25  17 241 484]]
256
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2405679969_33efa874f0.jpg
[[ 97  59 330 223]
 [194 355 210 373]
 [209 359 221 379]
 [225 344 261 365]
 [264 335 318 361]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2413843006_3d08633a75.jpg
[[ 59  60 238 460]
 [214 384 233 421]
 [182 289 230 369]
 [153  74 206 179]
 [205 202 219 232]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2418832514_cf6fcff0b7.jpg
[[ 82  38 300 329]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2430004725_37ac7dff9c.jpg
[[170 172 279 268]
 [198 266 232 317]
 [105 231 131 371]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2430032387_5c4a75d66c.jpg
[[130 149 288 428]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2430099765_0c1be9ffa7.jpg
[[ 72   0 401 372]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2430596505_c908f6ee63.jpg
[[189 242 280 295]
 [200 311 344 343]
 [244 131 438 321]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2430904006_43dc28c2eb.jpg
[[ 62  73 357 291]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2432002864_20cd8b9235.jpg
[[166 115 202 137]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2436242400_5b6a0f1afc.jpg
[[ 71  14  91  55]
 [ 73  60 119 120]
 [ 97 126 138 183]
 [118 192 161 246]
 [127 253 176 308]
 [135 312 188 356]
 [148 361 170 379]]
281
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2437301757_e8664b36ff.jpg
[[379 187 395 207]
 [384 201 413 233]
 [ 86 147 115 190]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2447365206_5f0079baab.jpg
[[195  20 373 471]
 [  0  23 180 485]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2447497468_d5fdde9a93.jpg
[[ 34   0 498 372]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2454096425_5427a16f57.jpg
[[  8 123 258 497]]
269
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2454694772_72e758aff4.jpg
[[ 38  67 329 430]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2468000564_4af4798092.jpg
[[  0 184  76 362]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2473592777_e8644cfaef.jpg
[[  8 293 320 427]
 [161 411 170 421]
 [107 418 116 431]
 [213 416 225 427]
 [300 388 308 400]
 [271 326 281 332]
 [ 39 318  50 329]
 [ 22 388  30 399]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2473913448_b2ec902625.jpg
[[104 188 300 413]
 [176 430 258 459]
 [257 428 294 451]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2474259291_4605cd92a5.jpg
[[  5  77 173 375]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2476916812_e0f31ae6a7.jpg
[[  0   0 373 497]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2481597791_d8005610ce.jpg
[[124  96 277 468]
 [159 206 197 255]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2481865703_313c80a58f.jpg
[[117 115 232 349]]
334
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2491124514_b175a14ddd.jpg
[[ 37 190  90 273]
 [321 171 391 254]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2496086056_a7ecf23077.jpg
[[173 125 258 210]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2507220019_1f7d0e1bde.jpg
[[230 127 322 286]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2517968190_f038717e3a.jpg
[[274  75 292 114]
 [221 109 427 275]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2525271488_2a83c60a06.jpg
[[ 23  68 143 322]
 [122 101 347 356]
 [200  38 263  77]
 [338  72 465 333]
 [327 350 337 360]
 [331 364 347 378]
 [343 377 356 388]
 [355 382 362 391]]
498
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2527677679_9840cd71c9.jpg
[[ 56 258 148 320]]
332
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2527692165_411a36c0e6.jpg
[[217 294 274 312]]
332
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/252987414_611a166eca.jpg
[[146 192 162 252]
 [169 209 199 311]
 [211 213 256 329]
 [274 210 327 327]
 [248 269 287 305]
 [334 198 368 304]
 [369 190 384 256]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2529952056_82876c3d84.jpg
[[ 78 256 246 493]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/253023518_14afed3d55.jpg
[[ 40  27 278 470]]
300
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2532506091_a44b8716cb.jpg
[[ 71 133  99 177]
 [219 131 238 160]
 [243 131 313 287]
 [254 308 298 494]]
334
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2533475023_ac9dc24069.jpg
[[137 189 225 348]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2537277610_00fe896e40.jpg
[[356 140 371 158]
 [349 155 370 163]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2541835881_dcb02ef43e.jpg
[[ 22   0 257 467]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/254346095_b58855ba3d.jpg
[[131  42 318 196]]
442
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2546654478_052ba75487.jpg
[[  8   5 491 342]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2552987958_17cf7b0305.jpg
[[199   1 348 358]
 [ 75  81 126 153]]
367
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2553681929_a9d920340e.jpg
[[ 37   1 372 496]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2555088242_71824fce38.jpg
[[ 77 230 258 486]]
303
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2557754615_0d17a06aae.jpg
[[ 31  30 283 439]
 [115 432 133 454]]
305
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2558617913_a98c13e4fc.jpg
[[ 17   6 458 304]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2559142688_2ce0a5716a.jpg
[[173 135 305 265]]
400
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2560134492_4ebb9c9ea5.jpg
[[ 78   2 295 484]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2563203017_7dd3d881c9.jpg
[[ 78  35 421 201]
 [130 232 201 249]
 [318 238 354 249]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2568272651_7e3ddae456.jpg
[[187 221 241 282]]
411
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2573319771_2d08776df5.jpg
[[130  44 329 292]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2578266787_1aaec0da8d.jpg
[[ 18   0 218 496]
 [ 56 334  82 403]
 [245   5 408 476]
 [311  18 400 140]]
415
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2582566200_6e100670ba.jpg
[[365 223 403 355]
 [ 24 244 169 355]
 [164 266 185 288]
 [190 296 212 318]
 [226 318 243 344]
 [139 181 155 193]
 [125 222 152 245]
 [ 60 218  76 234]
 [  0 288  26 314]
 [  3 242  22 263]
 [ 92 193 111 210]
 [174 228 190 248]
 [ 25 206  42 219]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2582657304_cacf00d438.jpg
[[ 78  64 120  88]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2584088276_80711571da.jpg
[[ 57   8 320 494]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/258723907_06bfac4a09.jpg
[[158 131 258 302]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2588381721_677d8ef225.jpg
[[  0 116  16 141]
 [121 104 171 152]
 [147 190 259 322]
 [222 122 245 138]]
334
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2588700867_9060705df9.jpg
[[ 37   0 307 493]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2600793104_8af0cace7a.jpg
[[ 41  51 341 462]
 [245   0 313  72]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2608446760_9c2394d056.jpg
[[ 43  12 222 486]]
262
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2609103510_b6f6e53b0c.jpg
[[141  82 331 254]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2612226557_b8fe251992.jpg
[[ 97 148 262 365]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/262146475_984e254ab5.jpg
[[100  10 230 490]]
331
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2628577697_e448737748.jpg
[[ 83 205 154 297]
 [179 189 212 242]
 [322 264 366 371]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2629570606_f5fa5f08aa.jpg
[[ 86 356 157 406]
 [182 377 235 430]
 [245 364 285 407]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2632984162_843945a512.jpg
[[ 31   6 217 475]]
246
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/264274250_0c291d90c5.jpg
[[ 66   1 337 429]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2643601867_2b568c6a25.jpg
[[ 60  84 282 381]
 [194 103 227 141]
 [284 297 301 349]]
392
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2645842785_a54eeeb731.jpg
[[208  76 334 240]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2652433127_bf6b390115.jpg
[[  0 287  78 444]
 [ 83 224 232 351]
 [165 127 202 162]
 [208 136 230 161]
 [193 238 205 251]
 [195 276 204 285]
 [204 301 212 309]
 [227 303 253 327]
 [228 321 331 497]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2652618186_7eb8f07572.jpg
[[141 298 183 341]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2652664363_15d0ce0b93.jpg
[[182 356 240 421]
 [ 33 246 230 352]
 [193 140 235 244]
 [  0 132  94 248]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2652690958_35aca49304.jpg
[[105  71 163 171]
 [119 468 172 497]
 [ 18 327 103 340]
 [ 31 357 111 375]
 [ 40 372 114 393]
 [ 23 343 107 354]
 [228 288 272 300]
 [229 300 273 313]
 [231 310 277 321]
 [236 323 279 337]
 [248 368 290 373]
 [243 355 287 359]
 [ 57 394 120 432]
 [ 63 409 123 448]
 [222 485 262 497]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2652711562_76a9b3e011.jpg
[[140 234 210 299]
 [253 235 262 263]
 [240 266 264 301]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2652810541_8fa69f5fe8.jpg
[[107 189 264 273]
 [288 427 316 458]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2652977647_93a3292ee5.jpg
[[ 77 237  92 257]
 [ 75 275  84 290]
 [ 98 268 120 291]
 [ 75 303  94 322]
 [ 69 331  77 345]
 [ 87 355 103 366]
 [ 80 411  91 431]
 [ 85 445  94 462]
 [101 455 123 478]
 [ 97 481 106 495]
 [121 225 136 244]
 [159 213 168 232]
 [164 244 178 263]
 [176 242 187 253]
 [193 255 199 268]
 [190 298 196 313]
 [201 372 212 383]
 [178 361 188 375]
 [177 327 188 339]
 [167 302 182 320]
 [130 286 150 300]
 [138 315 160 336]
 [155 339 172 361]
 [126 345 140 360]
 [150 367 166 385]
 [ 47 287  70 333]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2653044466_f9e83d8985.jpg
[[ 14  51 245 436]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2653337168_8e05f70e28.jpg
[[ 23 134 165 448]
 [155 327 186 345]
 [124 114 186 177]
 [130 138 137 144]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2653809650_706dbb20f1.jpg
[[164 214 186 241]
 [175 250 196 285]
 [141 240 155 257]
 [230  77 275 102]
 [243  64 281  84]
 [ 62 266 103 333]
 [107 297 114 324]
 [153 297 160 318]
 [ 20 285  59 331]
 [ 64 263  84 279]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2654045098_2a3e5c6abd.jpg
[[ 89 119 332 248]
 [ 70 244  79 277]
 [220 110 234 133]
 [338 102 341 110]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2654087592_9d66cb9eb4.jpg
[[ 36 182  94 393]
 [ 89 295 143 413]
 [101 144 300 439]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2654093602_2fe31819e1.jpg
[[145  19 345 327]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2654160152_9aa08c7a98.jpg
[[  5 168 314 278]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/265659902_bd8e4516d1.jpg
[[ 80 157 230 406]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2662720918_27579552f4.jpg
[[ 75 148 233 207]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2666882541_0129165308.jpg
[[  5   4 215 491]]
222
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2669768096_ab6a1e3edb.jpg
[[ 80   0 225 495]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/26740486_a4fb2be2f8.jpg
[[129 243 295 314]]
402
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2679894177_d3608a1eaa.jpg
[[  0   0  35 202]
 [  2   0 269 494]
 [238 136 361 496]
 [336 406 355 493]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2681740404_5d3850274a.jpg
[[123 241 162 302]
 [143 301 174 320]
 [173 274 189 293]
 [167 242 201 280]
 [169 233 179 249]
 [170  75 212 155]
 [214 133 225 145]
 [157 137 169 170]
 [155 178 167 191]
 [146 204 168 227]
 [177 162 228 196]
 [182 207 210 233]
 [180 194 195 210]
 [164 206 172 214]]
338
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2690421177_e272c7860b.jpg
[[160  99 304 394]]
336
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2690926752_dffa328bb5.jpg
[[ 98   1 276 496]]
353
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2695932735_1f6b9cbdd9.jpg
[[ 40 106 189 270]]
253
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2698992095_7565eb70e0.jpg
[[120  16 364 330]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2703148949_0ef318393e.jpg
[[118  33 198 476]
 [113  12 119  18]
 [ 97  32 100  38]
 [ 90  66  96  75]
 [106  56 108  63]
 [111  70 117  79]
 [115  79 119  88]
 [120  30 125  38]
 [131  30 137  38]
 [112  46 117  56]
 [125  38 130  48]
 [136  23 140  26]
 [140  35 145  38]
 [ 94  47  98  52]
 [115  30 119  33]]
345
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2703152933_e884025ec6.jpg
[[ 69  40  75  65]
 [ 47  59  56  80]
 [ 51 115  61 140]
 [ 71 174  73 182]
 [ 78  94  81 112]
 [ 90 103  93 126]
 [ 92 203  98 224]
 [112 140 129 169]
 [103 112 112 141]
 [120  59 133  75]
 [146  80 169 100]
 [175 126 207 142]
 [168  40 183  57]
 [187 180 218 205]
 [224  84 242 103]
 [242 155 275 182]
 [260  95 279 120]
 [261 132 293 162]
 [254  70 276  78]
 [292  96 318 111]
 [298 122 335 145]
 [302 167 331 191]
 [282 205 306 226]
 [305 230 338 252]
 [312 124 455 341]
 [333 104 367 122]
 [366  88 389 103]
 [396 107 410 130]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2711164445_6287425776.jpg
[[138  36 372 351]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2714461756_e16c1319c7.jpg
[[ 92  10 331 278]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2714574533_2407359a66.jpg
[[102  47 257 497]
 [105 296 120 316]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2714580907_911d650a24.jpg
[[ 36  28 149 464]
 [198  28 298 453]]
353
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2718443045_d70f5ff176.jpg
[[ 69 264 107 292]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2718757883_435816ee93.jpg
[[216 198 350 371]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2720158038_28c049e68b.jpg
[[ 72 219 165 349]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2720900320_67fe25b22a.jpg
[[149 136 331 221]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2733325661_fc7dc37b4f.jpg
[[ 93 190 214 496]
 [144 481 163 497]
 [200 448 210 473]
 [212 403 222 432]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2733807671_ff6de6fc9f.jpg
[[134 205 217 315]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2745005957_73ce6c0a64.jpg
[[122 111 274 416]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2745151002_46fb8e2ac8.jpg
[[  0 106  63 176]
 [ 64 128 261 247]
 [272  85 382 370]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2747412378_037360f78a.jpg
[[ 88 128 248 423]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2747558829_ed6edc592f.jpg
[[148 178 197 229]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2753936643_00949a0265.jpg
[[303 246 337 284]
 [194 251 243 331]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2754245827_0d5f878722.jpg
[[ 23   3 192 395]
 [ 46 363  54 395]
 [198   5 362 392]
 [366   1 490 385]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/275584567_7121aeebff.jpg
[[ 64 253 369 373]
 [259 324 323 349]
 [293 321 321 333]
 [324 299 363 306]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2761414328_e403e3ed60.jpg
[[  0 403 294 497]
 [143 207 405 413]
 [156  59 361 243]
 [ 95  55 132  76]]
439
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2763157565_3f4aee6696.jpg
[[232 136 340 229]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2766377454_1f55bbfd1e.jpg
[[170 109 300 142]
 [231 136 255 188]
 [262 134 267 138]
 [202 138 212 144]
 [221 102 226 111]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2768618367_1da87afcb9.jpg
[[ 42  78 344 450]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2771030169_39f85caa71.jpg
[[ 83  72 344 437]]
400
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2475188798_a1b46f71cc.jpg
[[177 205 407 331]
 [415 128 497 331]
 [436 198 448 206]
 [431 178 442 187]
 [423 168 431 177]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/277291396_e260ad2ea7.jpg
[[138 145 149 177]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/277565292_54f8beb23f.jpg
[[ 28  76 242 316]
 [358 130 442 300]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2776091620_fdf96ec30c.jpg
[[  0   0  80 187]
 [ 77   0 286 227]
 [298  86 444 372]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2778947321_cbe6bc7f44.jpg
[[ 52  51 373 441]]
447
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/277962214_3adc405693.jpg
[[ 33 109 177 232]
 [209 155 266 212]
 [312 106 454 232]
 [ 78  66 417 140]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2782095902_167159724b.jpg
[[ 54  66 258 316]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2784769634_27dcdc7193.jpg
[[ 57 136 178 263]
 [  0  85  82 262]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2791868333_13858f2719.jpg
[[ 72  76 239 221]
 [237  65 407 207]
 [ 88 234 247 397]
 [259 219 410 386]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2793025208_e3861fe15a.jpg
[[ 52  52 357 468]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2793785631_c690e96241.jpg
[[170  88 234 187]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2794870729_06e4e7eb02.jpg
[[122 253 307 404]
 [222 301 242 318]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2798753295_b66b809701.jpg
[[ 67  50 309 493]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2807171070_02351c23a0.jpg
[[288 216 350 300]
 [239 200 423 224]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2807809131_db19397186.jpg
[[ 42  23 261 491]]
307
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2809597385_db57bcc9ca.jpg
[[306 137 362 199]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2813856361_a11c753d47.jpg
[[ 87   0 293 292]]
403
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2815789263_80bea49fc1.jpg
[[ 87 107 302 252]
 [267 110 294 133]
 [298 118 331 143]
 [319 150 338 177]
 [365 142 413 290]
 [454 106 493 242]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2818176783_a397db9455.jpg
[[178 158 304 247]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2820836141_fe1ee4ea32.jpg
[[427 266 446 289]
 [  5  67  85 266]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2832176048_555ffbfb8b.jpg
[[101 107 196 203]]
362
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2838997888_2d03221f99.jpg
[[  1 143 395 293]
 [  0   1 394 143]
 [404   5 497 290]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2839225554_8cc4495b30.jpg
[[ 30 229 341 366]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2845202225_b0a659f3d1.jpg
[[ 14  50 479 320]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2851098138_59bf014293.jpg
[[ 28  74 307 332]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2852267364_7454d8f763.jpg
[[278 160 313 179]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2852658836_d833345c46.jpg
[[ 14  33 262 460]]
291
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2854214078_dfcd48344d.jpg
[[ 66 152 241 428]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2855459432_88ec45b32c.jpg
[[132 160 335 373]
 [397 254 418 289]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/285618288_1c918a2ac1.jpg
[[123 200 150 255]
 [159 210 175 238]
 [184 212 213 240]
 [136 270 212 319]
 [222 306 231 313]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2863451255_433b60fdcc.jpg
[[338  91 424 272]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2863453803_91ea19a817.jpg
[[ 64 138 173 296]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2864282572_dc376f0aa0.jpg
[[ 41 128 331 332]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2864287502_af55034acd.jpg
[[156 394 172 416]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2868751900_2980673817.jpg
[[ 30 206 277 323]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2868885829_a0269d92fd.jpg
[[  0   2 498 482]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2873081985_753080e6f3.jpg
[[ 51 287 336 439]
 [  0 284  47 438]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2874970563_0e72963fbe.jpg
[[ 40 164 373 363]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/288589725_fbbc0e3d04.jpg
[[ 28  64 198 309]
 [280  35 453 309]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2888968522_90838d9e2e.jpg
[[  2  22 135 396]
 [171  12 319 407]
 [342  25 491 408]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2888970540_852ff8f6ca.jpg
[[101 409 143 432]
 [154 414 296 439]
 [ 65  69 317 418]
 [113  71 163 101]]
393
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2898954235_88d087f0bc.jpg
[[115 147 262 319]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2907253251_805115d676.jpg
[[ 96  71 316 438]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2909688874_69f101d45d.jpg
[[173 174 286 328]
 [265 135 331 398]
 [102 146 180 383]
 [ 39 152 143 351]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2909745583_31e55f811a.jpg
[[ 50  21 409 293]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/291036830_65baded7cb.jpg
[[ 86  58 292 409]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2912250169_c29e41b212.jpg
[[ 69   0 220 424]]
335
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2914127688_82b217b565.jpg
[[ 79 420 137 483]
 [220 334 271 480]
 [230 491 243 499]
 [291 336 355 487]
 [373 250 390 281]
 [322 103 365 185]
 [240 100 308 145]
 [185 109 227 195]
 [ 84  69  97  86]]
394
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2914329245_395bcf61b9.jpg
[[283  85 455 277]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2918786855_41beb0f736.jpg
[[300 310 464 361]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/291912618_3308241c12.jpg
[[  0   0 185 180]
 [186   0 262 106]
 [131 104 256 290]
 [157 293 244 351]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/291Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 115, in append_flipped_images
    assert (boxes[:, 2] >= boxes[:, 0]).all()
AssertionError
912619_47848e29dd.jpg
[[145 262 273 306]
 [212 294 291 346]
 [305 431 378 484]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/291912622_3fc5909dc9.jpg
[[ 83 102 198 448]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2919513446_43321f17eb.jpg
[[ 68  68 329 468]]
389
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2922973367_accb4bd3a1.jpg
[[113 389 233 499]
 [145 165 238 320]
 [ 65 129 130 208]]
333
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2923258307_d6f72dd804.jpg
[[ 30 122 325 465]
 [276 235 293 264]
 [274 167 290 185]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2924816257_a2e49e1df9.jpg
[[ 92  98 401 202]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2925448737_9840b6ac5a.jpg
[[227 215 308 312]
 [314 214 388 289]
 [389 184 460 244]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/292748328_bede44b488.jpg
[[232 119 360 271]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2929480706_a83fa542ca.jpg
[[ 86 122 286 384]
 [304 170 327 209]
 [297 268 332 306]
 [280 241 331 261]
 [ 62 276  99 319]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2930098935_75260f5646.jpg
[[314 139 420 230]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/294370578_dc185ab904.jpg
[[331  29 397 132]
 [ 18  41 155  89]]
500
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2948752327_b311736e12.jpg
[[171 218 200 247]
 [175 249 194 281]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2953256282_f3f74b62d5.jpg
[[ 82 123 373 499]]
375
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/2956473595_07c6a3ae30.jpg
[[ 46 122 303 495]]
304
/home/cgangee/code/cg-py-faster-rcnn/data/tattoo/./tattoo/295902445_5e8e3fd448.jpg
[[ 39 148 110 204]
 [  0 127  42 170]
 [177  64 203 105]
 [173  35 184  51]
 [242 128 250 137]
 [270  42 300  86]]
300
Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 114, in append_flipped_images
    assert (boxes[:, 2] >= boxes[:, 0]).all()
AssertionError
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 116, in append_flipped_images
    assert (boxes[:, 2] >= boxes[:, 0]).all()
AssertionError
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

[[160 191 227 248]]
[[208 164 227 183]]
[[ 26 294 176 425]
 [147 378 265 435]
 [252 309 290 352]]
[[ 19 224  82 356]
 [250 235 293 291]
 [225 299 264 339]
 [256 246 328 335]]
[[ 95  60 259 452]]
[[138 225 343 342]
 [131   0 333 193]]
[[160 122 261 231]]
[[ 36  83 307 453]]
[[  0  17 177 476]
 [140 198 189 400]
 [159  49 301 219]]
[[ 10  78 312 465]]
[[100  88 285 425]]
[[ 49   0 285 500]
 [265   0 333 136]]
Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 117, in append_flipped_images
    assert (boxes[:, 2] >= boxes[:, 0]).all()
AssertionError
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

375
[[160 191 227 248]]
500
[[208 164 227 183]]
333
[[ 26 294 176 425]
 [147 378 265 435]
 [252 309 290 352]]
333
[[ 19 224  82 356]
 [250 235 293 291]
 [225 299 264 339]
 [256 246 328 335]]
375
[[ 95  60 259 452]]
500
[[138 225 343 342]
 [131   0 333 193]]
375
[[160 122 261 231]]
375
[[ 36  83 307 453]]
333
[[  0  17 177 476]
 [140 198 189 400]
 [159  49 301 219]]
333
[[ 10  78 312 465]]
376
[[100  88 285 425]]
333
[[ 49   0 285 500]
 [265   0 333 136]]
Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

375
[[159 190 226 247]]
500
[[207 163 226 182]]
333
[[ 25 293 175 424]
 [146 377 264 434]
 [251 308 289 351]]
333
[[ 18 223  81 355]
 [249 234 292 290]
 [224 298 263 338]
 [255 245 327 334]]
375
[[ 94  59 258 451]]
500
[[137 224 342 341]
 [130   0 332 192]]
375
[[159 121 260 230]]
375
[[ 35  82 306 452]]
333
[[  0  16 176 475]
 [139 197 188 399]
 [158  48 300 218]]
333
[[  9  77 311 464]]
376
[[ 99  87 284 424]]
333
[[ 48   0 284 499]
 [264   0 332 135]]
333
[[ 23  31 316 470]]
376
[[ 56 144 308 416]]
316
[[ 49  27 276 469]]
500
[[253  87 305 247]]
333
[[166 377 267 452]
 [269 210 286 244]
 [103 208 272 292]
 [ 75 360 101 391]
 [ 76 404  91 431]
 [293 198 312 234]]
500
[[141 152 252 275]
 [255 211 264 226]
 [249 201 301 286]
 [282 227 306 277]]
500
[[191 135 365 280]]
500
[[132   0 230  63]
 [277   0 313  75]
 [303  36 369  66]
 [329  63 370  89]
 [249  97 371 269]
 [127  49 232 243]]
500
[[183  97 306 227]]
500
[[202  73 276 147]]
375
[[196 273 323 466]]
500
[[239 146 284 307]]
375
[[  0 233 154 331]
 [140 121 210 209]
 [ 98 331 181 496]
 [174 244 259 302]
 [247 161 319 213]]
500
[[279 209 338 272]]
500
[[ 48  70  78 115]
 [ 66 112  93 152]
 [ 48 205  71 228]
 [ 72 207  90 224]
 [121 293 134 314]
 [100 313 116 332]
 [298  65 319  79]
 [298  93 314 102]
 [376 138 395 149]
 [404 121 423 142]
 [433 205 451 225]
 [462 197 485 217]]
500
[[ 98  27 429 343]]
413
[[ 75 301 249 470]]
500
[[ 52 155 146 263]
 [140 178 193 262]
 [192 179 247 268]
 [253 171 303 249]
 [311 171 370 247]
 [360 166 427 244]
 [408 164 485 246]]
500
[[ 16  24 457 206]]
500
[[ 79  91 370 376]]
375
[[ 96  69 291 497]]
320
[[155 147 201 182]]
500
[[ 98  20 361 344]]
500
[[373 121 430 202]
 [ 49 127 109 199]]
375
[[ 50 163 315 479]
 [272 329 296 370]
 [277 375 303 450]]
480
[[179  77 406 114]
 [175 134 437 320]]
406
[[  0  40 404 499]]
500
[[215  58 402 266]]
337
[[ 87 192 157 352]
 [165 192 233 350]]
500
[[158  45 336 348]]
500
[[154   6 405 330]]
375
[[  0   0 372 499]]
500
[[220 116 349 282]]
500
[[  8 204 116 255]
 [122 179 258 231]
 [263 169 299 207]
 [308 165 364 203]
 [368 159 460 190]]
500
[[249  61 369 192]]
500
[[ 35  60 423 287]
 [122 234 248 298]]
294
[[ 54 185 197 334]]
500
[[ 26 121  68 167]
 [180  23 231 113]
 [229  51 258 113]
 [257  52 297 137]
 [305  33 318 120]
 [326  58 355 120]
 [361  56 391 118]
 [400  44 423 116]
 [197 134 402 226]
 [197 218 407 312]
 [207 307 397 386]]
333
[[  0  39 323 462]]
333
[[141 170 160 186]
 [106 149 163 230]
 [ 92 176 102 190]]
500
[[178 174 320 245]
 [162 214 174 233]
 [150 191 166 212]
 [153 162 168 178]
 [126 173 142 186]
 [121 136 143 159]
 [145 112 158 133]
 [172  73 312 185]
 [156  87 172 103]
 [182  73 203  95]
 [157 114 174 143]]
375
[[ 97  41 245 208]
 [ 58 102  90 264]
 [275  78 329 224]
 [ 16 100  39 140]]
375
[[124 296 224 429]
 [ 85  49 129 117]
 [267  23 306 155]]
375
[[ 71 139 232 279]]
500
[[216 122 277 192]]
375
[[ 44  98 194 397]
 [193 177 334 411]]
500
[[145  33 299 302]]
373
[[ 39  39 317 464]]
353
[[  5   1 334 484]]
333
[[155 378 229 496]
 [112 261 164 327]
 [114 150 187 237]]
271
[[ 12  51 240 492]]
375
[[ 69   0 300 460]
 [ 24 245 100 358]]
500
[[206 105 330 241]]
500
[[ 61  13 332 337]]
333
[[ 82  87 210 400]]
316
[[113 231 200 254]
 [124 198 199 240]]
375
[[ 67  81 276 440]]
333
[[ 32  52  91 393]
 [137  48 274 447]
 [263 450 279 471]
 [281 444 303 470]]
500
[[177 115 298 288]
 [191  78 225 115]
 [228  68 269 110]
 [274  75 310 119]]
500
[[175 188 241 240]]
500
[[203 141 301 236]]
500
[[380  99 447 239]]
334
[[ 27   4 296 483]]
375
[[ 86 219 194 410]]
250
[[ 30  12 221 475]]
333
[[  4 193 193 495]]
305
[[ 15   3 300 488]]
143
[[  9   9 135 479]]
500
[[207 153 287 210]
 [141 132 199 188]]
217
[[  4   0 214 445]]
333
[[ 21  70 264 375]]
182
[[  3   0 174 485]]
375
[[ 53  23 277 463]]
600
[[186 161 288 248]]
338
[[175   0 299 224]]
333
[[ 48  34 162 173]]
291
[[ 25   0 257 499]]
375
[[135 346 209 422]
 [190 226 225 248]
 [221 217 231 231]]
333
[[ 13 336  95 426]]
297
[[154 214 190 271]]
275
[[ 35  41 226 464]]
360
[[128  75 254 217]]
500
[[147  46 377 253]]
500
[[252 132 374 272]]
375
[[212  63 318 166]]
333
[[ 45 112 318 395]]
352
[[118 137 247 287]]
500
[[169 129 180 144]
 [181 140 193 158]
 [195 135 206 150]
 [201 136 237 165]
 [242 125 298 167]
 [303 122 324 145]
 [322 137 330 151]
 [330 116 341 144]]
500
[[358 111 374 134]
 [375 124 392 144]]
375
[[214 242 232 263]]
332
[[107  19 289 497]]
378
[[125  75 267 182]]
232
[[ 13   2 228 493]]
500
[[196  36 405 223]
 [277 194 290 239]]
334
[[200 251 209 274]]
500
[[312 282 399 342]
 [147 159 372 302]]
500
[[211 165 263 222]]
375
[[124 111 257 355]]
500
[[ 74  88 104 122]
 [108 173 255 217]
 [329 220 429 278]
 [317 163 373 191]
 [378 130 386 146]
 [138 158 150 180]
 [141 212 186 249]]
339
[[  7   8 118 167]
 [ 28  92 188 392]]
339
[[ 29 118  66 248]
 [ 44 103 299 456]]
500
[[106  45 399 354]]
500
[[126  47 413 335]]
500
[[144   9 402 320]]
500
[[174  50 362 258]]
356
[[  4  19 337 478]]
375
[[102 161 255 354]]
500
[[  0  23 168 203]
 [ 55 128 156 249]
 [154 125 336 280]
 [340 121 438 242]
 [463  93 497 167]]
500
[[  0 322 225 375]
 [ 27  49 450 308]]
448
[[112  83 297 224]]
500
[[ 26  73 498 310]
 [ 70   0 342  30]
 [412   0 497  44]]
487
[[ 19   0 483 478]]
375
[[ 37  64 330 415]]
500
[[181  74 328 196]]
500
[[153 116 399 273]]
500
[[ 88 123 413 161]]
500
[[104  19 339 301]
 [186 355 271 371]]
398
[[  4   5 374 492]
 [ 11  97  69 234]]
310
[[ 73 218 199 294]
 [145 110 191 143]
 [134 167 195 204]
 [206  89 250 150]
 [  9  74  72 130]]
334
[[  0 250  68 387]
 [ 17  90 330 484]]
352
[[ 29 389  48 441]
 [238 286 303 339]
 [ 63 205 271 473]
 [159 301 175 329]]
334
[[ 37 293  94 485]
 [ 60 189 287 460]]
375
[[ 52  63 360 390]]
439
[[  6   0 219 477]
 [213   0 420 472]]
286
[[ 58   2 266 492]]
500
[[ 55 237  84 282]
 [145 174 211 220]
 [219 214 237 263]
 [ 86 214 219 316]]
333
[[ 71 119 248 379]]
500
[[141 124 322 237]]
375
[[ 51  64 333 497]]
500
[[ 23 317 359 373]
 [312 125 497 369]
 [ 44   0 380 238]]
375
[[127  84 373 336]
 [255 303 374 493]]
375
[[ 11   6 296 497]]
333
[[ 73 129 240 358]]
474
[[ 94   7 365 231]]
398
[[ 98 163 279 437]]
345
[[115 243 162 298]
 [198 299 225 332]]
335
[[ 56  50 264 484]]
375
[[102 139 304 435]]
500
[[112 184 134 213]
 [358 193 373 219]
 [136 145 348 235]]
375
[[111  35 300 481]
 [282  65 295  86]]
300
[[ 60  72 261 338]]
500
[[301 122 336 149]]
500
[[ 54 111 286 320]]
500
[[ 84  77 447 222]]
289
[[ 75   3 193 490]]
500
[[ 23  48 465 260]
 [107  76 180 134]
 [157  46 234  93]
 [261  38 302  62]
 [311  40 330  60]
 [119 162 139 196]
 [238 223 284 269]
 [376 258 415 298]
 [271 276 289 293]
 [289 298 308 315]]
375
[[ 79 277 158 386]]
375
[[105 191 128 220]
 [124  88 275 304]
 [174 160 188 197]
 [185 161 206 195]
 [210 138 260 177]
 [307 294 333 355]
 [245 284 274 388]
 [124 261 248 398]]
500
[[120   0 320 361]]
375
[[174  71 206  94]
 [135  92 220 464]]
375
[[140  37 245 376]]
500
[[201  63 237 103]]
500
[[144 210 224 289]]
375
[[224  78 371 268]
 [  0 385 104 431]
 [ 75 393 224 497]]
375
[[175  89 261 265]]
500
[[ 21  28 205 342]
 [ 55 131  74 159]
 [ 77 156 100 184]
 [159 146 183 183]
 [296  46 482 323]
 [318 163 349 190]
 [435 115 453 142]
 [431 161 456 187]
 [347 314 371 347]
 [405 324 423 364]]
500
[[ 32   0 496 330]]
333
[[ 35   0 207 499]]
500
[[283  43 408 249]]
500
[[261  48 353 143]
 [124 210 159 262]
 [158 206 241 282]]
500
[[  0  39  87 234]
 [107  59 175 116]
 [302  72 371 133]
 [175 119 288 258]]
466
[[ 51  27 391 446]]
500
[[252  69 432 282]
 [100 122 339 301]]
500
[[356 218 416 312]
 [267 136 308 164]]
375
[[ 53   0 239 497]]
333
[[ 57  46 265 402]
 [ 44 317  62 398]]
333
[[220 112 280 202]
 [197 252 275 471]
 [ 50 187  90 214]
 [ 61 247  98 278]
 [ 58 287 153 414]]
500
[[119  28 374 311]]
500
[[150  71 369 254]]
334
[[148  87 167 152]
 [217  88 250 120]
 [167 187 203 222]
 [163 182 175 203]
 [239 195 247 213]]
334
[[ 45   0 315 497]]
500
[[118  38 371 320]]
375
[[ 79   9 370 469]]
375
[[101  28 286 466]]
375
[[ 98  92 269 469]]
500
[[260 137 315 162]]
500
[[151 121 398 210]
 [300 195 372 218]]
453
[[  0   0 419 498]]
375
[[ 92  95 267 356]
 [207  74 244 105]
 [ 77   0 234  80]]
500
[[ 15  23 227 138]
 [ 30 161 208 251]
 [273  21 468 161]
 [295 165 472 245]
 [300 195 474 263]]
316
[[ 10  47 287 491]]
333
[[103 183 171 248]]
254
[[ 13  10 239 493]]
500
[[ 26 201  45 222]
 [  1 210  84 323]
 [276 330 348 365]
 [406 176 460 295]
 [315 213 324 228]
 [373 202 379 221]]
500
[[368 239 419 286]
 [388 349 412 371]]
500
[[ 38  67 469 275]]
500
[[ 65  60 474 201]]
500
[[175  33 386 367]]
500
[[ 46 133 465 347]]
500
[[125  25 412 311]
 [200 109 210 124]]
375
[[  0  93 283 388]
 [  6 184  35 218]
 [134  97 166 121]]
500
[[101   5 363 358]]
500
[[ 72 131 325 258]]
375
[[116  18 289 360]]
375
[[ 57   0 268 394]
 [ 32 291  51 362]
 [ 99 437 140 483]
 [160 436 194 479]
 [208 417 235 454]
 [244 377 270 410]]
480
[[ 67  22 403 295]
 [ 43 297  79 357]
 [ 71 318 120 356]
 [175 340 205 356]
 [214 340 246 356]
 [250 341 293 356]
 [297 328 359 357]
 [361 316 415 358]
 [400 291 450 343]
 [441 181 476 269]]
375
[[ 87   0 295 410]
 [ 81 270  94 301]
 [194 429 219 448]
 [236 330 263 383]
 [282 352 294 374]
 [240 396 261 419]]
480
[[ 83  31 347 301]]
375
[[133  47 263 424]]
500
[[178 164 274 247]
 [268  94 349 160]]
500
[[153  71 363 274]]
500
[[126 132 347 199]
 [132 191 176 234]
 [176 200 223 248]
 [225 215 267 258]]
375
[[177  57 336 449]]
500
[[ 62  78 474 249]]
500
[[355 180 380 207]
 [367 206 385 226]
 [383 224 395 241]]
375
[[ 52  27 337 495]]
256
[[ 24  16 241 484]]
500
[[ 96  58 330 223]
 [193 354 210 373]
 [208 358 221 379]
 [224 343 261 365]
 [263 334 318 361]]
375
[[ 58  59 238 460]
 [213 383 233 421]
 [181 288 230 369]
 [152  73 206 179]
 [204 201 219 232]]
500
[[ 81  37 300 329]]
500
[[169 171 279 268]
 [197 265 232 317]
 [104 230 131 371]]
375
[[129 148 288 428]]
500
[[ 71   0 401 372]]
500
[[188 241 280 295]
 [199 310 344 343]
 [243 130 438 321]]
500
[[ 61  72 357 291]]
500
[[165 114 202 137]]
281
[[ 70  13  91  55]
 [ 72  59 119 120]
 [ 96 125 138 183]
 [117 191 161 246]
 [126 252 176 308]
 [134 311 188 356]
 [147 360 170 379]]
500
[[378 186 395 207]
 [383 200 413 233]
 [ 85 146 115 190]]
375
[[194  19 373 471]
 [  0  22 180 485]]
500
[[ 33   0 498 372]]
269
[[  7 122 258 497]]
375
[[ 37  66 329 430]]
500
[[  0 183  76 362]]
333
[[  7 292 320 427]
 [160 410 170 421]
 [106 417 116 431]
 [212 415 225 427]
 [299 387 308 400]
 [270 325 281 332]
 [ 38 317  50 329]
 [ 21 387  30 399]]
375
[[103 187 300 413]
 [175 429 258 459]
 [256 427 294 451]]
375
[[  4  76 173 375]]
375
[[  0   0 373 497]]
375
[[123  95 277 468]
 [158 205 197 255]]
334
[[116 114 232 349]]
500
[[ 36 189  90 273]
 [320 170 391 254]]
500
[[172 124 258 210]]
500
[[229 126 322 286]]
500
[[273  74 292 114]
 [220 108 427 275]]
498
[[ 22  67 143 322]
 [121 100 347 356]
 [199  37 263  77]
 [337  71 465 333]
 [326 349 337 360]
 [330 363 347 378]
 [342 376 356 388]
 [354 381 362 391]]
332
[[ 55 257 148 320]]
332
[[216 293 274 312]]
500
[[145 191 162 252]
 [168 208 199 311]
 [210 212 256 329]
 [273 209 327 327]
 [247 268 287 305]
 [333 197 368 304]
 [368 189 384 256]]
375
[[ 77 255 246 493]]
300
[[ 39  26 278 470]]
334
[[ 70 132  99 177]
 [218 130 238 160]
 [242 130 313 287]
 [253 307 298 494]]
375
[[136 188 225 348]]
500
[[355 139 371 158]
 [348 154 370 163]]
333
[[ 21   0 257 467]]
442
[[130  41 318 196]]
500
[[  7   4 491 342]]
367
[[198   0 348 358]
 [ 74  80 126 153]]
375
[[ 36   0 372 496]]
303
[[ 76 229 258 486]]
305
[[ 30  29 283 439]
 [114 431 133 454]]
500
[[ 16   5 458 304]]
400
[[172 134 305 265]]
375
[[ 77   1 295 484]]
500
[[ 77  34 421 201]
 [129 231 201 249]
 [317 237 354 249]]
411
[[186 220 241 282]]
333
[[129  43 329 292]]
415
[[ 17   0 218 496]
 [ 55 333  82 403]
 [244   4 408 476]
 [310  17 400 140]]
500
[[364 222 403 355]
 [ 23 243 169 355]
 [163 265 185 288]
 [189 295 212 318]
 [225 317 243 344]
 [138 180 155 193]
 [124 221 152 245]
 [ 59 217  76 234]
 [  0 287  26 314]
 [  2 241  22 263]
 [ 91 192 111 210]
 [173 227 190 248]
 [ 24 205  42 219]]
500
[[ 77  63 120  88]]
375
[[ 56   7 320 494]]
333
[[157 130 258 302]]
334
[[  0 115  16 141]
 [120 103 171 152]
 [146 189 259 322]
 [221 121 245 138]]
375
[[ 36   0 307 493]]
375
[[ 40  50 341 462]
 [244   0 313  72]]
262
[[ 42  11 222 486]]
500
[[140  81 331 254]]
375
[[ 96 147 262 365]]
331
[[ 99   9 230 490]]
375
[[ 82 204 154 297]
 [178 188 212 242]
 [321 263 366 371]]
375
[[ 85 355 157 406]
 [181 376 235 430]
 [244 363 285 407]]
246
[[ 30   5 217 475]]
375
[[ 65   0 337 429]]
392
[[ 59  83 282 381]
 [193 102 227 141]
 [283 296 301 349]]
500
[[207  75 334 240]]
333
[[  0 286  78 444]
 [ 82 223 232 351]
 [164 126 202 162]
 [207 135 230 161]
 [192 237 205 251]
 [194 275 204 285]
 [203 300 212 309]
 [226 302 253 327]
 [227 320 331 497]]
333
[[140 297 183 341]]
333
[[181 355 240 421]
 [ 32 245 230 352]
 [192 139 235 244]
 [  0 131  94 248]]
333
[[104  70 163 171]
 [118 467 172 497]
 [ 17 326 103 340]
 [ 30 356 111 375]
 [ 39 371 114 393]
 [ 22 342 107 354]
 [227 287 272 300]
 [228 299 273 313]
 [230 309 277 321]
 [235 322 279 337]
 [247 367 290 373]
 [242 354 287 359]
 [ 56 393 120 432]
 [ 62 408 123 448]
 [221 484 262 497]]
333
[[139 233 210 299]
 [252 234 262 263]
 [239 265 264 301]]
333
[[106 188 264 273]
 [287 426 316 458]]
333
[[ 76 236  92 257]
 [ 74 274  84 290]
 [ 97 267 120 291]
 [ 74 302  94 322]
 [ 68 330  77 345]
 [ 86 354 103 366]
 [ 79 410  91 431]
 [ 84 444  94 462]
 [100 454 123 478]
 [ 96 480 106 495]
 [120 224 136 244]
 [158 212 168 232]
 [163 243 178 263]
 [175 241 187 253]
 [192 254 199 268]
 [189 297 196 313]
 [200 371 212 383]
 [177 360 188 375]
 [176 326 188 339]
 [166 301 182 320]
 [129 285 150 300]
 [137 314 160 336]
 [154 338 172 361]
 [125 344 140 360]
 [149 366 166 385]
 [ 46 286  70 333]]
333
[[ 13  50 245 436]]
333
[[ 22 133 165 448]
 [154 326 186 345]
 [123 113 186 177]
 [129 137 137 144]]
500
[[163 213 186 241]
 [174 249 196 285]
 [140 239 155 257]
 [229  76 275 102]
 [242  63 281  84]
 [ 61 265 103 333]
 [106 296 114 324]
 [152 296 160 318]
 [ 19 284  59 331]
 [ 63 262  84 279]]
500
[[ 88 118 332 248]
 [ 69 243  79 277]
 [219 109 234 133]
 [337 101 341 110]]
333
[[ 35 181  94 393]
 [ 88 294 143 413]
 [100 143 300 439]]
500
[[144  18 345 327]]
333
[[  4 167 314 278]]
333
[[ 79 156 230 406]]
500
[[ 74 147 233 207]]
222
[[  4   3 215 491]]
333
[[ 79   0 225 495]]
402
[[128 242 295 314]]
375
[[  0   0  35 202]
 [  1   0 269 494]
 [237 135 361 496]
 [335 405 355 493]]
338
[[122 240 162 302]
 [142 300 174 320]
 [172 273 189 293]
 [166 241 201 280]
 [168 232 179 249]
 [169  74 212 155]
 [213 132 225 145]
 [156 136 169 170]
 [154 177 167 191]
 [145 203 168 227]
 [176 161 228 196]
 [181 206 210 233]
 [179 193 195 210]
 [163 205 172 214]]
336
[[159  98 304 394]]
353
[[ 97   0 276 496]]
253
[[ 39 105 189 270]]
500
[[119  15 364 330]]
345
[[117  32 198 476]
 [112  11 119  18]
 [ 96  31 100  38]
 [ 89  65  96  75]
 [105  55 108  63]
 [110  69 117  79]
 [114  78 119  88]
 [119  29 125  38]
 [130  29 137  38]
 [111  45 117  56]
 [124  37 130  48]
 [135  22 140  26]
 [139  34 145  38]
 [ 93  46  98  52]
 [114  29 119  33]]
500
[[ 68  39  75  65]
 [ 46  58  56  80]
 [ 50 114  61 140]
 [ 70 173  73 182]
 [ 77  93  81 112]
 [ 89 102  93 126]
 [ 91 202  98 224]
 [111 139 129 169]
 [102 111 112 141]
 [119  58 133  75]
 [145  79 169 100]
 [174 125 207 142]
 [167  39 183  57]
 [186 179 218 205]
 [223  83 242 103]
 [241 154 275 182]
 [259  94 279 120]
 [260 131 293 162]
 [253  69 276  78]
 [291  95 318 111]
 [297 121 335 145]
 [301 166 331 191]
 [281 204 306 226]
 [304 229 338 252]
 [311 123 455 341]
 [332 103 367 122]
 [365  87 389 103]
 [395 106 410 130]]
500
[[137  35 372 351]]
500
[[ 91   9 331 278]]
333
[[101  46 257 497]
 [104 295 120 316]]
353
[[ 35  27 149 464]
 [197  27 298 453]]
333
[[ 68 263 107 292]]
500
[[215 197 350 371]]
333
[[ 71 218 165 349]]
375
[[148 135 331 221]]
333
[[ 92 189 214 496]
 [143 480 163 497]
 [199 447 210 473]
 [211 402 222 432]]
500
[[133 204 217 315]]
375
[[121 110 274 416]]
500
[[  0 105  63 176]
 [ 63 127 261 247]
 [271  84 382 370]]
333
[[ 87 127 248 423]]
500
[[147 177 197 229]]
500
[[302 245 337 284]
 [193 250 243 331]]
500
[[ 22   2 192 395]
 [ 45 362  54 395]
 [197   4 362 392]
 [365   0 490 385]]
500
[[ 63 252 369 373]
 [258 323 323 349]
 [292 320 321 333]
 [323 298 363 306]]
439
[[  0 402 294 497]
 [142 206 405 413]
 [155  58 361 243]
 [ 94  54 132  76]]
500
[[231 135 340 229]]
500
[[169 108 300 142]
 [230 135 255 188]
 [261 133 267 138]
 [201 137 212 144]
 [220 101 226 111]]
375
[[ 41  77 344 450]]
400
[[ 82  71 344 437]]
500
[[176 204 407 331]
 [414 127 497 331]
 [435 197 448 206]
 [430 177 442 187]
 [422 167 431 177]]
500
[[137 144 149 177]]
500
[[ 27  75 242 316]
 [357 129 442 300]]
500
[[  0   0  80 187]
 [ 76   0 286 227]
 [297  85 444 372]]
447
[[ 51  50 373 441]]
500
[[ 32 108 177 232]
 [208 154 266 212]
 [311 105 454 232]
 [ 77  65 417 140]]
500
[[ 53  65 258 316]]
333
[[ 56 135 178 263]
 [  0  84  82 262]]
500
[[ 71  75 239 221]
 [236  64 407 207]
 [ 87 233 247 397]
 [258 218 410 386]]
375
[[ 51  51 357 468]]
500
[[169  87 234 187]]
375
[[121 252 307 404]
 [221 300 242 318]]
375
[[ 66  49 309 493]]
500
[[287 215 350 300]
 [238 199 423 224]]
307
[[ 41  22 261 491]]
500
[[305 136 362 199]]
403
[[ 86   0 293 292]]
500
[[ 86 106 302 252]
 [266 109 294 133]
 [297 117 331 143]
 [318 149 338 177]
 [364 141 413 290]
 [453 105 493 242]]
500
[[177 157 304 247]]
500
[[426 265 446 289]
 [  4  66  85 266]]
362
[[100 106 196 203]]
500
[[  0 142 395 293]
 [  0   0 394 143]
 [403   4 497 290]]
375
[[ 29 228 341 366]]
500
[[ 13  49 479 320]]
500
[[ 27  73 307 332]]
500
[[277 159 313 179]]
291
[[ 13  32 262 460]]
500
[[ 65 151 241 428]]
500
[[131 159 335 373]
 [396 253 418 289]]
375
[[122 199 150 255]
 [158 209 175 238]
 [183 211 213 240]
 [135 269 212 319]
 [221 305 231 313]]
500
[[337  90 424 272]]
333
[[ 63 137 173 296]]
500
[[ 40 127 331 332]]
333
[[155 393 172 416]]
333
[[ 29 205 277 323]]
500
[[  0   1 498 482]]
375
[[ 50 286 336 439]
 [  0 283  47 438]]
500
[[ 39 163 373 363]]
500
[[ 27  63 198 309]
 [279  34 453 309]]
500
[[  1  21 135 396]
 [170  11 319 407]
 [341  24 491 408]]
393
[[100 408 143 432]
 [153 413 296 439]
 [ 64  68 317 418]
 [112  70 163 101]]
375
[[114 146 262 319Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 117, in append_flipped_images
    assert (boxes[:, 2] >= boxes[:, 0]).all()
AssertionError
]]
375
[[ 95  70 316 438]]
333
[[172 173 286 328]
 [264 134 331 398]
 [101 145 180 383]
 [ 38 151 143 351]]
500
[[ 49  20 409 293]]
375
[[ 85  57 292 409]]
335
[[ 68   0 220 424]]
394
[[ 78 419 137 483]
 [219 333 271 480]
 [229 490 243 499]
 [290 335 355 487]
 [372 249 390 281]
 [321 102 365 185]
 [239  99 308 145]
 [184 108 227 195]
 [ 83  68  97  86]]
500
[[282  84 455 277]]
500
[[299 309 464 361]]
500
[[  0   0 185 180]
 [185   0 262 106]
 [130 103 256 290]
 [156 292 244 351]]
500
[[144 261 273 306]
 [211 293 291 346]
 [304 430 378 484]]
500
[[ 82 101 198 448]]
389
[[ 67  67 329 468]]
333
[[112 388 233 499]
 [144 164 238 320]
 [ 64 128 130 208]]
375
[[ 29 121 325 465]
 [275 234 293 264]
 [273 166 290 185]]
500
[[ 91  97 401 202]]
500
[[226 214 308 312]
 [313 213 388 289]
 [388 183 460 244]]
500
[[231 118 360 271]]
375
[[ 85 121 286 384]
 [303 169 327 209]
 [296 267 332 306]
 [279 240 331 261]
 [ 61 275  99 319]]
500
[[313 138 420 230]]
500
[[330  28 397 132]
 [ 17  40 155  89]]
375
[[170 217 200 247]
 [174 248 194 281]]
375
[[ 81 122 373 499]]
304
[[ 45 121 303 495]]
300
[[ 38 147 110 204]
 [  0 126  42 170]
 [176  63 203 105]
 [172  34 184  51]
 [241 127 250 137]
 [269  41 300  86]]
Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 106, in append_flipped_images
    boxes = self.roidb[i]['boxes'].copy()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 67, in roidb
    self._roidb = self.roidb_handler()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/tattoo.py", line 68, in gt_roidb
    w, h = PIL.Image.open(image_index[idx])
  File "/home/cgangee/.local/lib/python2.7/site-packages/PIL/Image.py", line 2280, in open
    fp = builtins.open(filename, "rb")
IOError: [Errno 2] No such file or directory: './tattoo/1009019541_ad75c5b35f.jpg'
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 106, in append_flipped_images
    boxes = self.roidb[i]['boxes'].copy()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 67, in roidb
    self._roidb = self.roidb_handler()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/tattoo.py", line 68, in gt_roidb
    w, h = PIL.Image.open(self.image_path_at(idx))
TypeError: 'JpegImageFile' object is not iterable
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 106, in append_flipped_images
    boxes = self.roidb[i]['boxes'].copy()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 67, in roidb
    self._roidb = self.roidb_handler()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/tattoo.py", line 81, in gt_roidb
    x1 = update(x1, w)
NameError: global name 'update' is not defined
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run
    self._target(*self._args, **self._kwargs)
  File "./tools/train_faster_rcnn_alt_opt.py", line 122, in train_rpn
    roidb, imdb = get_roidb(imdb_name)
  File "./tools/train_faster_rcnn_alt_opt.py", line 67, in get_roidb
    roidb = get_training_roidb(imdb)
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/fast_rcnn/train.py", line 119, in get_training_roidb
    imdb.append_flipped_images()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 106, in append_flipped_images
    boxes = self.roidb[i]['boxes'].copy()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/imdb.py", line 67, in roidb
    self._roidb = self.roidb_handler()
  File "/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/datasets/tattoo.py", line 81, in gt_roidb
    x1 = update(x1, w)
NameError: global name 'update' is not defined
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

Logging output to ./log
Called with args:
Namespace(cfg_file='experiments/cfgs/faster_rcnn_alt_opt.yml', gpu_id=0, imdb_name='tattoo_train', net_name='ZF', pretrained_model='data/imagenet_models/ZF.v2.caffemodel', set_cfgs=None)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Init model: data/imagenet_models/ZF.v2.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

375
[[160 191 227 248]]
500
[[208 164 227 183]]
333
[[ 26 294 176 425]
 [147 378 265 435]
 [252 309 290 352]]
333
[[ 19 224  82 356]
 [250 235 293 291]
 [225 299 264 339]
 [256 246 328 335]]
375
[[ 95  60 259 452]]
500
[[138 225 343 342]
 [131   0 333 193]]
375
[[160 122 261 231]]
375
[[ 36  83 307 453]]
333
[[  0  17 177 476]
 [140 198 189 400]
 [159  49 301 219]]
333
[[ 10  78 312 465]]
376
[[100  88 285 425]]
333
[[ 49   0 285 499]
 [265   0 332 136]]
333
[[ 24  32 317 471]]
376
[[ 57 145 309 417]]
316
[[ 50  28 277 470]]
500
[[254  88 306 248]]
333
[[167 378 268 453]
 [270 211 287 245]
 [104 209 273 293]
 [ 76 361 102 392]
 [ 77 405  92 432]
 [294 199 313 235]]
500
[[142 153 253 276]
 [256 212 265 227]
 [250 202 302 287]
 [283 228 307 278]]
500
[[192 136 366 281]]
500
[[133   0 231  64]
 [278   0 314  76]
 [304  37 370  67]
 [330  64 371  90]
 [250  98 372 270]
 [128  50 233 244]]
500
[[184  98 307 228]]
500
[[203  74 277 148]]
375
[[197 274 324 467]]
500
[[240 147 285 308]]
375
[[  0 234 155 332]
 [141 122 211 210]
 [ 99 332 182 497]
 [175 245 260 303]
 [248 162 320 214]]
500
[[280 210 339 273]]
500
[[ 49  71  79 116]
 [ 67 113  94 153]
 [ 49 206  72 229]
 [ 73 208  91 225]
 [122 294 135 315]
 [101 314 117 332]
 [299  66 320  80]
 [299  94 315 103]
 [377 139 396 150]
 [405 122 424 143]
 [434 206 452 226]
 [463 198 486 218]]
500
[[ 99  28 430 344]]
413
[[ 76 302 250 471]]
500
[[ 53 156 147 264]
 [141 179 194 263]
 [193 180 248 269]
 [254 172 304 250]
 [312 172 371 248]
 [361 167 428 245]
 [409 165 486 247]]
500
[[ 17  25 458 207]]
500
[[ 80  92 371 377]]
375
[[ 97  70 292 498]]
320
[[156 148 202 183]]
500
[[ 99  21 362 345]]
500
[[374 122 431 203]
 [ 50 128 110 200]]
375
[[ 51 164 316 480]
 [273 330 297 371]
 [278 376 304 451]]
480
[[180  78 407 115]
 [176 135 438 319]]
406
[[  0  41 405 499]]
500
[[216  59 403 267]]
337
[[ 88 193 158 353]
 [166 193 234 351]]
500
[[159  46 337 349]]
500
[[155   7 406 331]]
375
[[  0   0 373 499]]
500
[[221 117 350 283]]
500
[[  9 205 117 256]
 [123 180 259 232]
 [264 170 300 208]
 [309 166 365 204]
 [369 160 461 191]]
500
[[250  62 370 193]]
500
[[ 36  61 424 288]
 [123 235 249 299]]
294
[[ 55 186 198 335]]
500
[[ 27 122  69 168]
 [181  24 232 114]
 [230  52 259 114]
 [258  53 298 138]
 [306  34 319 121]
 [327  59 356 121]
 [362  57 392 119]
 [401  45 424 117]
 [198 135 403 227]
 [198 219 408 313]
 [208 308 398 387]]
333
[[  1  40 324 463]]
333
[[142 171 161 187]
 [107 150 164 231]
 [ 93 177 103 191]]
500
[[179 175 321 246]
 [163 215 175 234]
 [151 192 167 213]
 [154 163 169 179]
 [127 174 143 187]
 [122 137 144 160]
 [146 113 159 134]
 [173  74 313 186]
 [157  88 173 104]
 [183  74 204  96]
 [158 115 175 144]]
375
[[ 98  42 246 209]
 [ 59 103  91 265]
 [276  79 330 225]
 [ 17 101  40 141]]
375
[[125 297 225 430]
 [ 86  50 130 118]
 [268  24 307 156]]
375
[[ 72 140 233 280]]
500
[[217 123 278 193]]
375
[[ 45  99 195 398]
 [194 178 335 412]]
500
[[146  34 300 303]]
373
[[ 40  40 318 465]]
353
[[  6   2 335 485]]
333
[[156 379 230 497]
 [113 262 165 328]
 [115 151 188 238]]
271
[[ 13  52 241 493]]
375
[[ 70   0 301 461]
 [ 25 246 101 359]]
500
[[207 106 331 242]]
500
[[ 62  14 333 338]]
333
[[ 83  88 211 401]]
316
[[114 232 201 255]
 [125 199 200 241]]
375
[[ 68  82 277 441]]
333
[[ 33  53  92 394]
 [138  49 275 448]
 [264 451 280 472]
 [282 445 304 471]]
500
[[178 116 299 289]
 [192  79 226 116]
 [229  69 270 111]
 [275  76 311 120]]
500
[[176 189 242 241]]
500
[[204 142 302 237]]
500
[[381 100 448 240]]
334
[[ 28   5 297 484]]
375
[[ 87 220 195 411]]
250
[[ 31  13 222 476]]
333
[[  5 194 194 496]]
305
[[ 16   4 301 489]]
143
[[ 10  10 136 480]]
500
[[208 154 288 211]
 [142 133 200 189]]
217
[[  5   1 215 446]]
333
[[ 22  71 265 376]]
182
[[  4   0 175 486]]
375
[[ 54  24 278 464]]
600
[[187 162 289 249]]
338
[[176   0 300 225]]
333
[[ 49  35 163 174]]
291
[[ 26   0 258 499]]
375
[[136 347 210 423]
 [191 227 226 249]
 [222 218 232 232]]
333
[[ 14 337  96 427]]
297
[[155 215 191 272]]
275
[[ 36  42 227 465]]
360
[[129  76 255 218]]
500
[[148  47 378 254]]
500
[[253 133 375 273]]
375
[[213  64 319 167]]
333
[[ 46 113 319 396]]
352
[[119 138 248 287]]
500
[[170 130 181 145]
 [182 141 194 159]
 [196 136 207 151]
 [202 137 238 166]
 [243 126 299 168]
 [304 123 325 146]
 [323 138 331 152]
 [331 117 342 145]]
500
[[359 112 375 135]
 [376 125 393 145]]
375
[[215 243 233 264]]
332
[[108  20 290 498]]
378
[[126  76 268 183]]
232
[[ 14   3 229 494]]
500
[[197  37 406 224]
 [278 195 291 240]]
334
[[201 252 210 275]]
500
[[313 283 400 343]
 [148 160 373 303]]
500
[[212 166 264 223]]
375
[[125 112 258 356]]
500
[[ 75  89 105 123]
 [109 174 256 218]
 [330 221 430 279]
 [318 164 374 192]
 [379 131 387 147]
 [139 159 151 181]
 [142 213 187 250]]
339
[[  8   9 119 168]
 [ 29  93 189 393]]
339
[[ 30 119  67 249]
 [ 45 104 300 457]]
500
[[107  46 400 355]]
500
[[127  48 414 336]]
500
[[145  10 403 321]]
500
[[175  51 363 259]]
356
[[  5  20 338 479]]
375
[[103 162 256 355]]
500
[[  0  24 169 204]
 [ 56 129 157 250]
 [155 126 337 281]
 [341 122 439 243]
 [464  94 498 168]]
500
[[  0 323 226 374]
 [ 28  50 451 309]]
448
[[113  84 298 225]]
500
[[ 27  74 499 311]
 [ 71   1 343  31]
 [413   0 498  45]]
487
[[ 20   0 484 479]]
375
[[ 38  65 331 416]]
500
[[182  75 329 197]]
500
[[154 117 400 274]]
500
[[ 89 124 414 162]]
500
[[105  20 340 302]
 [187 356 272 372]]
398
[[  5   6 375 493]
 [ 12  98  70 235]]
310
[[ 74 219 200 295]
 [146 111 192 144]
 [135 168 196 205]
 [207  90 251 151]
 [ 10  75  73 131]]
334
[[  0 251  69 388]
 [ 18  91 331 485]]
352
[[ 30 390  49 442]
 [239 287 304 340]
 [ 64 206 272 474]
 [160 302 176 330]]
334
[[ 38 294  95 486]
 [ 61 190 288 461]]
375
[[ 53  64 361 391]]
439
[[  7   0 220 478]
 [214   0 421 473]]
286
[[ 59   3 267 493]]
500
[[ 56 238  85 283]
 [146 175 212 221]
 [220 215 238 264]
 [ 87 215 220 317]]
333
[[ 72 120 249 380]]
500
[[142 125 323 238]]
375
[[ 52  65 334 498]]
500
[[ 24 318 360 374]
 [313 126 498 370]
 [ 45   0 381 239]]
375
[[128  85 374 337]
 [256 304 374 494]]
375
[[ 12   7 297 498]]
333
[[ 74 130 241 359]]
474
[[ 95   8 366 232]]
398
[[ 99 164 280 438]]
345
[[116 244 163 299]
 [199 300 226 333]]
335
[[ 57  51 265 485]]
375
[[103 140 305 436]]
500
[[113 185 135 214]
 [359 194 374 220]
 [137 146 349 236]]
375
[[112  36 301 482]
 [283  66 296  87]]
300
[[ 61  73 262 339]]
500
[[302 123 337 150]]
500
[[ 55 112 287 321]]
500
[[ 85  78 448 223]]
289
[[ 76   4 194 491]]
500
[[ 24  49 466 261]
 [108  77 181 135]
 [158  47 235  94]
 [262  39 303  63]
 [312  41 331  61]
 [120 163 140 197]
 [239 224 285 270]
 [377 259 416 299]
 [272 277 290 294]
 [290 299 309 316]]
375
[[ 80 278 159 387]]
375
[[106 192 129 221]
 [125  89 276 305]
 [175 161 189 198]
 [186 162 207 196]
 [211 139 261 178]
 [308 295 334 356]
 [246 285 275 389]
 [125 262 249 399]]
500
[[121   1 321 362]]
375
[[175  72 207  95]
 [136  93 221 465]]
375
[[141  38 246 377]]
500
[[202  64 238 104]]
500
[[145 211 225 290]]
375
[[225  79 372 269]
 [  0 386 105 432]
 [ 76 394 225 498]]
375
[[176  90 262 266]]
500
[[ 22  29 206 343]
 [ 56 132  75 160]
 [ 78 157 101 185]
 [160 147 184 184]
 [297  47 483 324]
 [319 164 350 191]
 [436 116 454 143]
 [432 162 457 188]
 [348 315 372 348]
 [406 325 424 365]]
500
[[ 33   0 497 331]]
333
[[ 36   0 208 499]]
500
[[284  44 409 250]]
500
[[262  49 354 144]
 [125 211 160 263]
 [159 207 242 283]]
500
[[  0  40  88 235]
 [108  60 176 117]
 [303  73 372 134]
 [176 120 289 259]]
466
[[ 52  28 392 447]]
500
[[253  70 433 283]
 [101 123 340 302]]
500
[[357 219 417 313]
 [268 137 309 165]]
375
[[ 54   0 240 498]]
333
[[ 58  47 266 403]
 [ 45 318  63 399]]
333
[[221 113 281 203]
 [198 253 276 472]
 [ 51 188  91 215]
 [ 62 248  99 279]
 [ 59 288 154 415]]
500
[[120  29 375 312]]
500
[[151  72 370 255]]
334
[[149  88 168 153]
 [218  89 251 121]
 [168 188 204 223]
 [164 183 176 204]
 [240 196 248 214]]
334
[[ 46   1 316 498]]
500
[[119  39 372 321]]
375
[[ 80  10 371 470]]
375
[[102  29 287 467]]
375
[[ 99  93 270 470]]
500
[[261 138 316 163]]
500
[[152 122 399 211]
 [301 196 373 219]]
453
[[  0   0 420 499]]
375
[[ 93  96 268 357]
 [208  75 245 106]
 [ 78   0 235  81]]
500
[[ 16  24 228 139]
 [ 31 162 209 252]
 [274  22 469 162]
 [296 166 473 246]
 [301 196 475 264]]
316
[[ 11  48 288 492]]
333
[[104 184 172 249]]
254
[[ 14  11 240 494]]
500
[[ 27 202  46 223]
 [  2 211  85 324]
 [277 331 349 366]
 [407 177 461 296]
 [316 214 325 229]
 [374 203 380 222]]
500
[[369 240 420 287]
 [389 350 413 372]]
500
[[ 39  68 470 276]]
500
[[ 66  61 475 202]]
500
[[176  34 387 368]]
500
[[ 47 134 466 348]]
500
[[126  26 413 312]
 [201 110 211 125]]
375
[[  0  94 284 389]
 [  7 185  36 219]
 [135  98 167 122]]
500
[[102   6 364 359]]
500
[[ 73 132 326 259]]
375
[[117  19 290 361]]
375
[[ 58   0 269 395]
 [ 33 292  52 363]
 [100 438 141 484]
 [161 437 195 480]
 [209 418 236 455]
 [245 378 271 411]]
480
[[ 68  23 404 296]
 [ 44 298  80 358]
 [ 72 319 121 357]
 [176 341 206 357]
 [215 341 247 357]
 [251 342 294 357]
 [298 329 360 358]
 [362 317 416 359]
 [401 292 451 344]
 [442 182 477 270]]
375
[[ 88   0 296 411]
 [ 82 271  95 302]
 [195 430 220 449]
 [237 331 264 384]
 [283 353 295 375]
 [241 397 262 420]]
480
[[ 84  32 348 302]]
375
[[134  48 264 425]]
500
[[179 165 275 248]
 [269  95 350 161]]
500
[[154  72 364 275]]
500
[[127 133 348 200]
 [133 192 177 235]
 [177 201 224 249]
 [226 216 268 259]]
375
[[178  58 337 450]]
500
[[ 63  79 475 250]]
500
[[356 181 381 208]
 [368 207 386 227]
 [384 225 396 242]]
375
[[ 53  28 338 496]]
256
[[ 25  17 242 485]]
500
[[ 97  59 331 224]
 [194 355 211 374]
 [209 359 222 380]
 [225 344 262 366]
 [264 335 319 362]]
375
[[ 59  60 239 461]
 [214 384 234 422]
 [182 289 231 370]
 [153  74 207 180]
 [205 202 220 233]]
500
[[ 82  38 301 330]]
500
[[170 172 280 269]
 [198 266 233 318]
 [105 231 132 372]]
375
[[130 149 289 429]]
500
[[ 72   0 402 373]]
500
[[189 242 281 296]
 [200 311 345 344]
 [244 131 439 322]]
500
[[ 62  73 358 292]]
500
[[166 115 203 138]]
281
[[ 71  14  92  56]
 [ 73  60 120 121]
 [ 97 126 139 184]
 [118 192 162 247]
 [127 253 177 309]
 [135 312 189 357]
 [148 361 171 380]]
500
[[379 187 396 208]
 [384 201 414 234]
 [ 86 147 116 191]]
375
[[195  20 374 472]
 [  0  23 181 486]]
500
[[ 34   0 499 373]]
269
[[  8 123 259 498]]
375
[[ 38  67 330 431]]
500
[[  0 184  77 363]]
333
[[  8 293 321 428]
 [161 411 171 422]
 [107 418 117 432]
 [213 416 226 428]
 [300 388 309 401]
 [271 326 282 333]
 [ 39 318  51 330]
 [ 22 388  31 400]]
375
[[104 188 301 414]
 [176 430 259 460]
 [257 428 295 452]]
375
[[  5  77 174 376]]
375
[[  0   0 374 498]]
375
[[124  96 278 469]
 [159 206 198 256]]
334
[[117 115 233 350]]
500
[[ 37 190  91 274]
 [321 171 392 255]]
500
[[173 125 259 211]]
500
[[230 127 323 287]]
500
[[274  75 293 115]
 [221 109 428 276]]
498
[[ 23  68 144 323]
 [122 101 348 357]
 [200  38 264  78]
 [338  72 466 334]
 [327 350 338 361]
 [331 364 348 379]
 [343 377 357 389]
 [355 382 363 392]]
332
[[ 56 258 149 321]]
332
[[217 294 275 313]]
500
[[146 192 163 253]
 [169 209 200 312]
 [211 213 257 330]
 [274 210 328 328]
 [248 269 288 306]
 [334 198 369 305]
 [369 190 385 257]]
375
[[ 78 256 247 494]]
300
[[ 40  27 279 471]]
334
[[ 71 133 100 178]
 [219 131 239 161]
 [243 131 314 288]
 [254 308 299 495]]
375
[[137 189 226 349]]
500
[[356 140 372 159]
 [349 155 371 164]]
333
[[ 22   0 258 468]]
442
[[131  42 319 197]]
500
[[  8   5 492 343]]
367
[[199   1 349 359]
 [ 75  81 127 154]]
375
[[ 37   1 373 497]]
303
[[ 77 230 259 487]]
305
[[ 31  30 284 440]
 [115 432 134 455]]
500
[[ 17   6 459 305]]
400
[[173 135 306 266]]
375
[[ 78   2 296 485]]
500
[[ 78  35 422 202]
 [130 232 202 250]
 [318 238 355 250]]
411
[[187 221 242 283]]
333
[[130  44 330 293]]
415
[[ 18   0 219 497]
 [ 56 334  83 404]
 [245   5 409 477]
 [311  18 401 141]]
500
[[365 223 404 356]
 [ 24 244 170 356]
 [164 266 186 289]
 [190 296 213 319]
 [226 318 244 345]
 [139 181 156 194]
 [125 222 153 246]
 [ 60 218  77 235]
 [  0 288  27 315]
 [  3 242  23 264]
 [ 92 193 112 211]
 [174 228 191 249]
 [ 25 206  43 220]]
500
[[ 78  64 121  89]]
375
[[ 57   8 321 495]]
333
[[158 131 259 303]]
334
[[  0 116  17 142]
 [121 104 172 153]
 [147 190 260 323]
 [222 122 246 139]]
375
[[ 37   0 308 494]]
375
[[ 41  51 342 463]
 [245   0 314  73]]
262
[[ 43  12 223 487]]
500
[[141  82 332 255]]
375
[[ 97 148 263 366]]
331
[[100  10 231 491]]
375
[[ 83 205 155 298]
 [179 189 213 243]
 [322 264 367 372]]
375
[[ 86 356 158 407]
 [182 377 236 431]
 [245 364 286 408]]
246
[[ 31   6 218 476]]
375
[[ 66   1 338 430]]
392
[[ 60  84 283 382]
 [194 103 228 142]
 [284 297 302 350]]
500
[[208  76 335 241]]
333
[[  0 287  79 445]
 [ 83 224 233 352]
 [165 127 203 163]
 [208 136 231 162]
 [193 238 206 252]
 [195 276 205 286]
 [204 301 213 310]
 [227 303 254 328]
 [228 321 332 498]]
333
[[141 298 184 342]]
333
[[182 356 241 422]
 [ 33 246 231 353]
 [193 140 236 245]
 [  0 132  95 249]]
333
[[105  71 164 172]
 [119 468 173 498]
 [ 18 327 104 341]
 [ 31 357 112 376]
 [ 40 372 115 394]
 [ 23 343 108 355]
 [228 288 273 301]
 [229 300 274 314]
 [231 310 278 322]
 [236 323 280 338]
 [248 368 291 374]
 [243 355 288 360]
 [ 57 394 121 433]
 [ 63 409 124 449]
 [222 485 263 498]]
333
[[140 234 211 300]
 [253 235 263 264]
 [240 266 265 302]]
333
[[107 189 265 274]
 [288 427 317 459]]
333
[[ 77 237  93 258]
 [ 75 275  85 291]
 [ 98 268 121 292]
 [ 75 303  95 323]
 [ 69 331  78 346]
 [ 87 355 104 367]
 [ 80 411  92 432]
 [ 85 445  95 463]
 [101 455 124 479]
 [ 97 481 107 496]
 [121 225 137 245]
 [159 213 169 233]
 [164 244 179 264]
 [176 242 188 254]
 [193 255 200 269]
 [190 298 197 314]
 [201 372 213 384]
 [178 361 189 376]
 [177 327 189 340]
 [167 302 183 321]
 [130 286 151 301]
 [138 315 161 337]
 [155 339 173 362]
 [126 345 141 361]
 [150 367 167 386]
 [ 47 287  71 334]]
333
[[ 14  51 246 437]]
333
[[ 23 134 166 449]
 [155 327 187 346]
 [124 114 187 178]
 [130 138 138 145]]
500
[[164 214 187 242]
 [175 250 197 286]
 [141 240 156 258]
 [230  77 276 103]
 [243  64 282  85]
 [ 62 266 104 332]
 [107 297 115 325]
 [153 297 161 319]
 [ 20 285  60 332]
 [ 64 263  85 280]]
500
[[ 89 119 333 249]
 [ 70 244  80 278]
 [220 110 235 134]
 [338 102 342 111]]
333
[[ 36 182  95 394]
 [ 89 295 144 414]
 [101 144 301 440]]
500
[[145  19 346 328]]
333
[[  5 168 315 279]]
333
[[ 80 157 231 407]]
500
[[ 75 148 234 208]]
222
[[  5   4 216 492]]
333
[[ 80   0 226 496]]
402
[[129 243 296 315]]
375
[[  0   0  36 203]
 [  2   0 270 495]
 [238 136 362 497]
 [336 406 356 494]]
338
[[123 241 163 303]
 [143 301 175 321]
 [173 274 190 294]
 [167 242 202 281]
 [169 233 180 250]
 [170  75 213 156]
 [214 133 226 146]
 [157 137 170 171]
 [155 178 168 192]
 [146 204 169 228]
 [177 162 229 197]
 [182 207 211 234]
 [180 194 196 211]
 [164 206 173 215]]
336
[[160  99 305 395]]
353
[[ 98   1 277 497]]
253
[[ 40 106 190 271]]
500
[[120  16 365 331]]
345
[[118  33 199 477]
 [113  12 120  19]
 [ 97  32 101  39]
 [ 90  66  97  76]
 [106  56 109  64]
 [111  70 118  80]
 [115  79 120  89]
 [120  30 126  39]
 [131  30 138  39]
 [112  46 118  57]
 [125  38 131  49]
 [136  23 141  27]
 [140  35 146  39]
 [ 94  47  99  53]
 [115  30 120  34]]
500
[[ 69  40  76  66]
 [ 47  59  57  81]
 [ 51 115  62 141]
 [ 71 174  74 183]
 [ 78  94  82 113]
 [ 90 103  94 127]
 [ 92 203  99 225]
 [112 140 130 170]
 [103 112 113 142]
 [120  59 134  76]
 [146  80 170 101]
 [175 126 208 143]
 [168  40 184  58]
 [187 180 219 206]
 [224  84 243 104]
 [242 155 276 183]
 [260  95 280 121]
 [261 132 294 163]
 [254  70 277  79]
 [292  96 319 112]
 [298 122 336 146]
 [302 167 332 192]
 [282 205 307 227]
 [305 230 339 253]
 [312 124 456 342]
 [333 104 368 123]
 [366  88 390 104]
 [396 107 411 131]]
500
[[138  36 373 352]]
500
[[ 92  10 332 279]]
333
[[102  47 258 498]
 [105 296 121 317]]
353
[[ 36  28 150 465]
 [198  28 299 454]]
333
[[ 69 264 108 293]]
500
[[216 198 351 372]]
333
[[ 72 219 166 350]]
375
[[149 136 332 222]]
333
[[ 93 190 215 497]
 [144 481 164 498]
 [200 448 211 474]
 [212 403 223 433]]
500
[[134 205 218 316]]
375
[[122 111 275 417]]
500
[[  0 106  64 177]
 [ 64 128 262 248]
 [272  85 383 371]]
333
[[ 88 128 249 424]]
500
[[148 178 198 230]]
500
[[303 246 338 285]
 [194 251 244 332]]
500
[[ 23   3 193 396]
 [ 46 363  55 396]
 [198   5 363 393]
 [366   1 491 386]]
500
[[ 64 253 370 374]
 [259 324 324 350]
 [293 321 322 334]
 [324 299 364 307]]
439
[[  0 403 295 498]
 [143 207 406 414]
 [156  59 362 244]
 [ 95  55 133  77]]
500
[[232 136 341 230]]
500
[[170 109 301 143]
 [231 136 256 189]
 [262 134 268 139]
 [202 138 213 145]
 [221 102 227 112]]
375
[[ 42  78 345 451]]
400
[[ 83  72 345 438]]
500
[[177 205 408 332]
 [415 128 498 332]
 [436 198 449 207]
 [431 178 443 188]
 [423 168 432 178]]
500
[[138 145 150 178]]
500
[[ 28  76 243 317]
 [358 130 443 301]]
500
[[  0   0  81 188]
 [ 77   0 287 228]
 [298  86 445 373]]
447
[[ 52  51 374 442]]
500
[[ 33 109 178 233]
 [209 155 267 213]
 [312 106 455 233]
 [ 78  66 418 141]]
500
[[ 54  66 259 317]]
333
[[ 57 136 179 264]
 [  0  85  83 263]]
500
[[ 72  76 240 222]
 [237  65 408 208]
 [ 88 234 248 398]
 [259 219 411 387]]
375
[[ 52  52 358 469]]
500
[[170  88 235 188]]
375
[[122 253 308 405]
 [222 301 243 319]]
375
[[ 67  50 310 494]]
500
[[288 216 351 301]
 [239 200 424 225]]
307
[[ 42  23 262 492]]
500
[[306 137 363 200]]
403
[[ 87   0 294 293]]
500
[[ 87 107 303 253]
 [267 110 295 134]
 [298 118 332 144]
 [319 150 339 178]
 [365 142 414 291]
 [454 106 494 243]]
500
[[178 158 305 248]]
500
[[427 266 447 290]
 [  5  67  86 267]]
362
[[101 107 197 204]]
500
[[  1 143 396 294]
 [  0   1 395 144]
 [404   5 498 291]]
375
[[ 30 229 342 367]]
500
[[ 14  50 480 321]]
500
[[ 28  74 308 333]]
500
[[278 160 314 180]]
291
[[ 14  33 263 461]]
500
[[ 66 152 242 429]]
500
[[132 160 336 374]
 [397 254 419 290]]
375
[[123 200 151 256]
 [159 210 176 239]
 [184 212 214 241]
 [136 270 213 320]
 [222 306 232 314]]
500
[[338  91 425 273]]
333
[[ 64 138 174 297]]
500
[[ 41 128 332 332]]
333
[[156 394 173 417]]
333
[[ 30 206 278 324]]
500
[[  0   2 499 483]]
375
[[ 51 287 337 440]
 [  0 284  48 439]]
500
[[ 40 164 374 364]]
500
[[ 28  64 199 310]
 [280  35 454 310]]
500
[[  2  22 136 397]
 [171  12 320 408]
 [342  25 492 409]]
393
[[101 409 144 433]
 [154 414 297 440]
 [ 65  69 318 419]
 [113  71 164 102]]
375
[[115 147 263 320]]
375
[[ 96  71 317 439]]
333
[[173 174 287 329]
 [265 135 332 399]
 [102 146 181 384]
 [ 39 152 144 352]]
500
[[ 50  21 410 294]]
375
[[ 86  58 293 410]]
335
[[ 69   0 221 425]]
394
[[ 79 420 138 484]
 [220 334 272 481]
 [230 491 244 499]
 [291 336 356 488]
 [373 250 391 282]
 [322 103 366 186]
 [240 100 309 146]
 [185 109 228 196]
 [ 84  69  98  87]]
500
[[283  85 456 278]]
500
[[300 310 465 362]]
500
[[  0   0 186 181]
 [186   0 263 107]
 [131 104 257 291]
 [157 293 245 352]]
500
[[145 262 274 307]
 [212 294 292 347]
 [305 431 379 485]]
500
[[ 83 102 199 449]]
389
[[ 68  68 330 469]]
333
[[113 389 234 499]
 [145 165 239 321]
 [ 65 129 131 209]]
375
[[ 30 122 326 466]
 [276 235 294 265]
 [274 167 291 186]]
500
[[ 92  98 402 203]]
500
[[227 215 309 313]
 [314 214 389 290]
 [389 184 461 245]]
500
[[232 119 361 272]]
375
[[ 86 122 287 385]
 [304 170 328 210]
 [297 268 333 307]
 [280 241 332 262]
 [ 62 276 100 320]]
500
[[314 139 421 231]]
500
[[331  29 398 133]
 [ 18  41 156  90]]
375
[[171 218 201 248]
 [175 249 195 282]]
375
[[ 82 123 374 499]]
304
[[ 46 122 303 496]]
300
[[ 39 148 111 205]
 [  0 127  43 171]
 [177  64 204 106]
 [173  35 185  52]
 [242 128 251 138]
 [270  42 299  87]]
500
[[ 27  19 479 394]]
333
[[  7  67 328 413]]
334
[[ 46  25 237 477]
 [ 19 141 120 498]]
333
[[ 99  25 210 475]
 [207   0 249 118]]
400
[[ 89  10 303 486]]
333
[[ 29  23 313 493]]
375
[[ 80   5 272 494]]
334
[[ 82  19 242 476]]
334
[[ 40 407  82 499]
 [ 19  29 310 484]]
500
[[208 195 278 276]]
500
[[ 17  40 219 225]
 [283  53 435 159]]
500
[[202 212 289 304]]
375
[[ 14   0 367 458]
 [166   0 209  61]
 [232   3 273  40]]
500
[[404 313 465 374]
 [439 328 471 353]
 [442 312 461 323]
 [ 51 148 130 204]
 [  0  43 184 156]]
336
[[260 277 308 383]
 [ 90 323 103 332]
 [ 86 305 130 364]
 [ 87 297  99 303]
 [ 23 322  35 335]
 [ 19 297  35 307]
 [ 14 219  42 253]
 [104 294 114 307]
 [ 83 274 103 287]
 [181 223 207 251]
 [186  86 235 146]
 [  0  88  22 106]
 [ 80 129  90 138]
 [115 127 128 137]]
375
[[126 151 214 339]
 [179  98 290 401]
 [  0  96  28 127]
 [ 39 198  66 231]
 [ 73 154 113 226]
 [113 101 163 162]]
472
[[ 54  35 162 105]
 [199 180 229 206]
 [222 133 327 166]
 [233  72 362 118]
 [226  14 376  59]]
500
[[232 140 376 200]
 [174 137 229 210]]
342
[[ 87  84 252 344]]
500
[[ 56  92 377 245]]
377
[[193 188 274 284]
 [205 184 252 247]
 [178 183 209 258]]
333
[[  0 290 332 499]]
333
[[ 31 103 220 215]]
364
[[  9  14 345 466]]
500
[[203 244 323 455]
 [330 186 344 212]
 [244 185 283 251]]
375
[[ 55  16 322 485]]
500
[[163   5 337 374]]
500
[[244 160 348 271]]
500
[[338 221 365 269]]
500
[[224 153 357 333]]
375
[[120  36 326 335]]
375
[[132 134 321 465]]
500
[[138  98 316 278]
 [220  66 235  76]]
450
[[ 22  34 377 272]]
500
[[ 35  76 376 273]]
416
[[ 81  31 326 454]]
282
[[ 59  81 224 401]]
450
[[ 34  11 377 305]]
450
[[ 44  54 377 276]]
500
[[179  29 377 359]]
375
[[ 60 151 149 367]]
375
[[ 44   0 322 480]]
500
[[247 134 274 258]]
333
[[ 31 234  85 327]]
298
[[ 34 159 108 252]
 [127 212 203 287]]
321
[[ 32 284 126 371]]
333
[[ 34 334 135 466]
 [ 56 256  75 325]
 [ 21 257  50 283]
 [ 22 159  70 222]
 [198 229 311 359]
 [179 182 227 228]
 [140 208 184 242]]
294
[[ 25  20 272 498]]
340
[[ 64  13 267 480]]
500
[[163 192 377 238]]
405
[[ 15   1 377 496]]
500
[[ 93 133 365 330]]
500
[[ 31  51 377 332]]
500
[[ 88 132 284 334]
 [189 266 286 373]
 [250  77 283 117]]
299
[[ 27  17 251 496]]
344
[[ 50 204 252 445]]
361
[[ 54  64 225 489]]
500
[[ 92  30 377 313]]
500
[[183 301 250 346]
 [179 214 281 274]
 [104 255 170 301]
 [281 137 354 199]
 [ 21 222 114 272]]
500
[[ 77 148 356 215]]
375
[[148 243 246 348]]
500
[[ 52  33 377 405]]
466
[[ 28  19 145 443]
 [169  17 308 440]
 [326  27 377 437]]
500
[[ 25  74 214 417]
 [259  78 377 396]]
319
[[ 92 428 147 443]
 [175 390 194 416]
 [ 19 174 189 257]]
500
[[282 131 376 266]
 [ 60 136 199 274]]
500
[[ 29  19 377 368]]
333
[[ 22 259 163 497]
 [176 313 261 498]
 [ 85   0 332 244]]
500
[[280 139 379 264]
 [175  72 233 230]
 [109 132 179 262]]
332
[[104 280 201 319]
 [104 185 202 302]]
375
[[ 67   9 321 497]]
432
[[165 193 268 258]]
500
[[187 128 291 209]]
500
[[122   6 332 353]]
500
[[139 149 288 211]
 [ 86 253 134 301]]
500
[[ 96 112 323 372]]
500
[[107 140 241 277]]
375
[[103 180 332 279]]
375
[[ 81 221 331 334]]
257
[[ 29  15 230 459]]
375
[[237 272 292 331]
 [209 312 239 338]
 [138 322 202 375]
 [120 333 156 357]
 [ 76 295 125 333]]
375
[[ 69  31 332 446]]
500
[[ 28   0 332 301]]
333
[[ 42 117 245 388]]
334
[[  0  17 332 497]]
500
[[245 183 315 227]
 [148 230 299 333]]
500
[[ 82   7 332 492]
 [  1 271  78 378]
 [  0 220  66 261]]
333
[[ 82  78 241 435]]
500
[[ 24 264 122 324]]
375
[[ 93  99 267 424]]
333
[[ 78 288 153 367]
 [ 30  67 256 183]]
460
[[247 126 332 208]]
403
[[ 72  39 332 493]]
375
[[135  89 257 207]]
500
[[254 141 332 220]]
334
[[100 398 186 444]]
331
[[  0   1 322  99]
 [  3 112 323 225]
 [ 20 246 326 342]
 [  3 366 329 495]]
333
[[ 48   0 309 496]]
333
[[ 76  45 281 439]]
333
[[ 24 144 275 436]]
333
[[ 19 162 303 454]]
333
[[ 34 170 259 453]]
333
[[ 69  47 249 366]]
333
[[ 67  67 260 435]]
333
[[ 39  50 282 442]]
360
[[ 25  33 331 433]]
341
[[ 38   2 303 438]]
333
[[  2 372  73 464]]
500
[[ 80  92 331 330]]
500
[[161   3 286 330]]
333
[[ 50 345 285 448]]
500
[[ 57 165  73 190]
 [116 233 198 280]
 [247 264 290 281]
 [289 154 332 331]]
243
[[ 11   7 242 496]]
375
[[ 51 169 158 423]]
333
[[122 161 228 255]
 [112 207 329 493]]
375
[[ 60 236 207 412]
 [202 233 229 283]
 [231 203 241 237]
 [201 265 258 365]]
500
[[174  72 332 215]]
313
[[ 79 118 230 353]]
500
[[182  46 314 331]]
333
[[118 158 206 401]]
500
[[111 114 331 286]]
364
[[ 79  44 208 349]]
345
[[  6  39 307 362]
 [174 336 246 469]]
500
[[ 71   6 446 320]]
375
[[ 95  80 228 467]]
500
[[304 142 354 209]
 [368 324 400 373]]
484
[[132 299 262 446]]
500
[[255 321 268 333]
 [276 314 288 336]
 [300 323 311 335]
 [315 312 332 323]
 [284 299 294 312]
 [297 303 309 318]
 [299 284 310 296]
 [319 288 331 298]]
500
[[  0  76 498 373]]
500
[[419 342 484 371]
 [394 234 468 341]]
370
[[278 151 318 195]
 [287 421 322 458]]
500
[[170 122 194 231]
 [191  92 304 270]]
375
[[181 178 283 368]]
375
[[ 55  24 304 308]
 [ 57 291  57 291]
 [343  76 373 187]]
500
[[181 199 224 233]
 [221 288 252 337]
 [170 142 216 181]]
500
[[  5  15 154 359]
 [158  15 324 360]
 [320  15 466 354]
 [465 121 497 254]]
500
[[  8  76  49 171]
 [ 34  16 115  70]
 [ 24 177 118 344]
 [116  88 131 161]
 [121  26 194  66]
 [190  67 283 117]
 [114 166 193 318]
 [298 144 383 236]
 [392 170 470 334]
 [401  80 415 165]
 [408  19 492  79]]
500
[[142  72 337 219]]
351
[[ 27  42 270 427]]
500
[[ 25   0 147  61]
 [ 28   1 492 246]]
500
[[123  96 403 198]]
500
[[ 17 119 491 327]]
500
[[246 146 435 234]
 [ 74  99 240 216]
 [177 373 177 373]
 [179 126 273 224]]
284
[[ 44  17 255 493]]
500
[[ 20  26 201 233]
 [262   2 438 227]]
345
[[113 150 227 467]]
375
[[ 14   2 357 495]]
358
[[ 74  12 269 490]
 [270  11 301 257]
 [259 230 285 306]
 [287 322 301 362]]
500
[[157   1 266 313]]
375
[[ 74  48 296 459]]
500
[[300 154 369 279]]
500
[[102 229 200 312]
 [204 182 255 241]]
500
[[153  61 256 156]
 [274 114 325 156]
 [296  70 377 155]]
500
[[110 165 227 257]
 [250  87 399 175]
 [260  52 326  78]
 [344  68 438 108]
 [349  43 446  83]
 [265  28 329  50]]
500
[[178 124 294 205]]
500
[[127 159 287 244]]
333
[[111  35 235 450]]
192
[[ 74 130 141 188]]
333
[[105 242 231 298]]
173
[[ 63 272 169 441]
 [  7   0 153 217]]
500
[[135 235 177 285]]
500
[[117 236 220 330]
 [289 186 396 299]]
500
[[246  19 376 312]]
333
[[ 37 148 303 462]
 [115 314 171 359]]
335
[[123  56 296 424]]
500
[[ 94 197 403 323]]
500
[[182 128 297 244]]
354
[[ 80  36 332 474]]
333
[[189 298 332 498]]
371
[[ 74  91 274 409]]
333
[[ 62   0 201 419]]
500
[[140 111 354 299]]
333
[[199 287 304 373]
 [101 285 141 327]]
334
[[ 79 160 310 488]]
375
[[ 95  37 213 361]]
308
[[ 48  67 162 175]]
494
[[ 11  36 408 445]]
375
[[ 27 202 208 494]
 [ 68   1 271 174]]
500
[[ 18  25 350 365]
 [380 233 486 282]
 [361  28 479 198]]
375
[[141  39 368 441]]
500
[[ 73 125 342 228]]
500
[[279  23 435 214]
 [152 154 215 276]
 [ 72 257 197 399]
 [191 323 232 362]
 [170 269 219 306]
 [209 199 277 269]
 [459 347 499 398]]
500
[[ 26  57 446 316]]
443
[[ 48   2 383 451]]
500
[[ 69 213 276 319]]
333
[[ 84  38 211 205]
 [102 207 236 381]]
500
[[ 94  56 371 354]]
357
[[ 82 204 286 404]]
375
[[114 150 250 362]
 [113 127 129 157]]
375
[[159   2 284 477]]
375
[[ 65   0 274 498]]
375
[[ 36  42 336 472]]
500
[[233 316 463 450]]
500
[[ 83  79 393 322]]
500
[[144 104 203 160]
 [243  62 445 238]
 [189   0 222  23]]
352
[[ 34 227 251 471]
 [226 283 263 307]]
500
[[ 54 211 193 353]]
332
[[ 47 252 171 483]
 [161 269 242 350]]
500
[[111  12 404 333]]
500
[[ 41  88 473 274]]
333
[[107  78 235 394]]
500
[[172 143 259 330]
 [280 154 362 337]]
500
[[132 178 225 292]]
500
[[ 10  95 478 261]]
500
[[135 268 152 290]
 [156 229 184 269]
 [176 174 240 249]
 [206 250 228 277]
 [248 206 274 231]
 [238 145 278 189]
 [287 181 322 210]
 [334 142 360 169]]
500
[[ 41  33 404 163]]
500
[[114 115 379 221]]
500
[[181 128 328 344]
 [302  95 334 121]]
500
[[ 46  26 466 365]]
500
[[143 122 398 256]]
500
[[ 80  74 121 103]
 [164 190 383 291]
 [137  49 229 209]]
489
[[249  73 415 265]]
405
[[ 79  55 309 379]]
500
[[ 57 158 335 307]]
500
[[  8  49 353 226]
 [121  22 148  44]]
333
[[141 211 219 364]]
332
[[222 148 282 313]
 [ 60 173 102 298]]
500
[[ 73 213 209 276]]
334
[[216 173 333 497]]
269
[[ 74  90 250 484]]
344
[[ 70 126 259 380]]
333
[[  0 105 331 494]
 [ 93  22 264 120]]
408
[[ 31  54  63  76]
 [  0 107  20 138]
 [ 51  76 326 244]
 [242 201 272 216]
 [307 157 330 196]
 [341 120 384 153]]
448
[[  1  18 425 495]]
450
[[157  40 180  61]
 [154  72 273 340]]
375
[[ 38 221 141 477]
 [ 96 394 166 497]
 [ 69 282 187 379]]
450
[[ 49  56 395 209]]
333
[[ 78  67 252 432]]
333
[[ 23  42 288 468]]
500
[[230 146 259 166]]
455
[[148 230 327 428]]
334
[[103  97 229 316]]
150
[[ 42  30 107 142]]
332
[[  0   0 252 466]
 [101  98 147 120]]
343
[[  0   0 338 487]]
500
[[  0   1 492 316]]
357
[[ 31 162 356 499]
 [ 15 321  61 354]
 [  0 409  10 463]]
375
[[ 82 244 162 332]
 [ 81 319 135 497]
 [143 307 306 453]
 [157 419 268 495]
 [296 425 356 462]
 [253 375 351 411]
 [300 373 325 386]
 [182 208 279 299]
 [250 309 283 331]
 [157 196 196 219]]
346
[[ 34 139 305 497]]
333
[[ 72 109 256 338]
 [215 129 258 163]]
342
[[ 80   3 223 495]]
284
[[180 118 189 124]
 [189 133 195 148]
 [198 149 202 159]
 [198 126 203 135]]
333
[[165  15 320 228]]
333
[[ 87 110 229 418]
 [139 444 170 468]]
333
[[  0   0 330 381]
 [ 26 345 279 498]]
500
[[285 142 469 324]]
375
[[ 84 138 287 345]]
333
[[132 271 259 475]]
477
[[ 93  25 354 302]
 [ 28 165 114 298]
 [ 73 322 241 484]
 [324 248 449 345]]
500
[[ 97 127 206 278]
 [ 83  26 161 114]
 [321 116 383 181]]
220
[[ 56  23 163 197]]
500
[[  7  57 425 267]
 [432  89 467 218]]
500
[[ 23  23 459 257]]
500
[[ 64   1 454 385]]
500
[[ 13 111 196 302]
 [306  92 482 299]]
333
[[ 21  47 281 467]]
333
[[  1 173 255 401]
 [  0 397 201 469]
 [ 64 253 135 329]]
333
[[100 232 240 407]
 [141 185 180 232]]
500
[[ 84 131 176 156]]
500
[[ 25 236  66 310]
 [ 62  47 112  86]
 [  0  47  48 168]
 [190  32 281 107]]
500
[[ 80   5 191 211]
 [290  32 403 226]]
433
[[ 86 299 290 486]
 [ 18  12 420 280]]
492
[[158 298 187 335]]
375
[[  4 114 174 483]
 [137 118 223 211]]
335
[[ 72  17 213 310]
 [185 264 260 380]]
335
[[142 141 215 291]
 [207 183 255 361]
 [180 431 218 497]
 [ 11 455  51 496]]
500
[[ 54 217 472 451]]
500
[[319 200 478 235]
 [372 156 426 199]]
410
[[143  15 348 391]
 [275 446 310 475]
 [220 440 262 477]
 [169 423 211 469]
 [102 364 138 410]]
500
[[215 194 323 234]]
375
[[ 93 116 294 371]]
500
[[ 83 117 206 334]
 [166 199 366 347]
 [204 126 246 194]
 [266   5 374 173]]
500
[[119  15 400 334]
 [399 115 485 270]
 [405  35 436 101]]
333
[[ 64 160 149 278]
 [ 95 279 157 344]
 [281 277 307 349]
 [287 178 306 207]]
231
[[ 30  19 177 489]]
500
[[132  55 343 283]]
333
[[115 165 204 373]]
375
[[ 89   1 294 463]]
500
[[161 119 407 288]]
500
[[  1   5 206 351]
 [208   2 498 170]
 [211 179 360 352]
 [361 179 498 350]]
356
[[  0  23 333 491]]
416
[[ 49   0 173 476]
 [252   0 404 498]]
500
[[ 80  52 434 299]]
500
[[  0   0 499 333]]
433
[[195 394 288 449]
 [165 394 194 430]
 [119 WARNING: Logging before InitGoogleLogging() is written to STDERR
I1025 22:42:43.565788 16778 solver.cpp:48] Initializing solver from parameters: 
train_net: "models/pascal_voc/ZF/faster_rcnn_alt_opt/stage1_rpn_train.pt"
base_lr: 0.001
display: 100
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 60000
snapshot: 0
snapshot_prefix: "zf_rpn"
average_loss: 100
I1025 22:42:43.565819 16778 solver.cpp:81] Creating training net from train_net file: models/pascal_voc/ZF/faster_rcnn_alt_opt/stage1_rpn_train.pt
I1025 22:42:43.575799 16778 net.cpp:49] Initializing net from parameters: 
name: "ZF"
state {
  phase: TRAIN
}
layer {
  name: "input-data"
  type: "Python"
  top: "data"
  top: "im_info"
  top: "gt_boxes"
  python_param {
    module: "roi_data_layer.layer"
    layer: "RoIDataLayer"
    param_str: "\'num_classes\': 2"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 96
    pad: 3
    kernel_size: 7
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
    engine: CAFFE
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    stride: 2
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
    engine: CAFFE
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "rpn_conv1"
  type: "Convolution"
  bottom: "conv5"
  top: "rpn_conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_relu1"
  type: "ReLU"
  bottom: "rpn_conv1"
  top: "rpn_conv1"
}
layer {
  name: "rpn_cls_score"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_cls_score"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 18
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_bbox_pred"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_bbox_pred"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 36
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_cls_score_reshape"
  type: "Reshape"
  bottom: "rpn_cls_score"
  top: "rpn_cls_score_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 2
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "rpn-data"
  type: "Python"
  bottom: "rpn_cls_score"
  bottom: "gt_boxes"
  bottom: "im_info"
  bottom: "data"
  top: "rpn_labels"
  top: "rpn_bbox_targets"
  top: "rpn_bbox_inside_weights"
  top: "rpn_bbox_outside_weights"
  python_param {
    module: "rpn.anchor_target_layer"
    layer: "AnchorTargetLayer"
    param_str: "\'feat_stride\': 16"
  }
}
layer {
  name: "rpn_loss_cls"
  type: "SoftmaxWithLoss"
  bottom: "rpn_cls_score_reshape"
  bottom: "rpn_labels"
  top: "rpn_cls_loss"
  loss_weight: 1
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: -1
    normalize: true
  }
}
layer {
  name: "rpn_loss_bbox"
  type: "SmoothL1Loss"
  bottom: "rpn_bbox_pred"
  bottom: "rpn_bbox_targets"
  bottom: "rpn_bbox_inside_weights"
  bottom: "rpn_bbox_outside_weights"
  top: "rpn_loss_bbox"
  loss_weight: 1
  smooth_l1_loss_param {
    sigma: 3
  }
}
layer {
  name: "dummy_roi_pool_conv5"
  type: "DummyData"
  top: "dummy_roi_pool_conv5"
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 0.01
    }
    shape {
      dim: 1
      dim: 9216
    }
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "dummy_roi_pool_conv5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "silence_fc7"
  type: "Silence"
  bottom: "fc7"
}
I1025 22:42:43.575933 16778 layer_factory.hpp:77] Creating layer input-data
I1025 22:42:43.584403 16778 net.cpp:106] Creating Layer input-data
I1025 22:42:43.584424 16778 net.cpp:411] input-data -> data
I1025 22:42:43.584431 16778 net.cpp:411] input-data -> im_info
I1025 22:42:43.584436 16778 net.cpp:411] input-data -> gt_boxes
I1025 22:42:43.590970 16778 net.cpp:150] Setting up input-data
I1025 22:42:43.590991 16778 net.cpp:157] Top shape: 1 3 600 1000 (1800000)
I1025 22:42:43.590994 16778 net.cpp:157] Top shape: 1 3 (3)
I1025 22:42:43.590997 16778 net.cpp:157] Top shape: 1 4 (4)
I1025 22:42:43.590998 16778 net.cpp:165] Memory required for data: 7200028
I1025 22:42:43.591002 16778 layer_factory.hpp:77] Creating layer data_input-data_0_split
I1025 22:42:43.591020 16778 net.cpp:106] Creating Layer data_input-data_0_split
I1025 22:42:43.591023 16778 net.cpp:454] data_input-data_0_split <- data
I1025 22:42:43.591039 16778 net.cpp:411] data_input-data_0_split -> data_input-data_0_split_0
I1025 22:42:43.591049 16778 net.cpp:411] data_input-data_0_split -> data_input-data_0_split_1
I1025 22:42:43.591094 16778 net.cpp:150] Setting up data_input-data_0_split
I1025 22:42:43.591102 16778 net.cpp:157] Top shape: 1 3 600 1000 (1800000)
I1025 22:42:43.591116 16778 net.cpp:157] Top shape: 1 3 600 1000 (1800000)
I1025 22:42:43.591120 16778 net.cpp:165] Memory required for data: 21600028
I1025 22:42:43.591125 16778 layer_factory.hpp:77] Creating layer conv1
I1025 22:42:43.591140 16778 net.cpp:106] Creating Layer conv1
I1025 22:42:43.591145 16778 net.cpp:454] conv1 <- data_input-data_0_split_0
I1025 22:42:43.591161 16778 net.cpp:411] conv1 -> conv1
I1025 22:42:43.696056 16778 net.cpp:150] Setting up conv1
I1025 22:42:43.696094 16778 net.cpp:157] Top shape: 1 96 300 500 (14400000)
I1025 22:42:43.696096 16778 net.cpp:165] Memory required for data: 79200028
I1025 22:42:43.696107 16778 layer_factory.hpp:77] Creating layer relu1
I1025 22:42:43.696116 16778 net.cpp:106] Creating Layer relu1
I1025 22:42:43.696120 16778 net.cpp:454] relu1 <- conv1
I1025 22:42:43.696128 16778 net.cpp:397] relu1 -> conv1 (in-place)
I1025 22:42:43.696396 16778 net.cpp:150] Setting up relu1
I1025 22:42:43.696406 16778 net.cpp:157] Top shape: 1 96 300 500 (14400000)
I1025 22:42:43.696409 16778 net.cpp:165] Memory required for data: 136800028
I1025 22:42:43.696413 16778 layer_factory.hpp:77] Creating layer norm1
I1025 22:42:43.696435 16778 net.cpp:106] Creating Layer norm1
I1025 22:42:43.696455 16778 net.cpp:454] norm1 <- conv1
I1025 22:42:43.696460 16778 net.cpp:411] norm1 -> norm1
I1025 22:42:43.696619 16778 net.cpp:150] Setting up norm1
I1025 22:42:43.696626 16778 net.cpp:157] Top shape: 1 96 300 500 (14400000)
I1025 22:42:43.696640 16778 net.cpp:165] Memory required for data: 194400028
I1025 22:42:43.696645 16778 layer_factory.hpp:77] Creating layer pool1
I1025 22:42:43.696650 16778 net.cpp:106] Creating Layer pool1
I1025 22:42:43.696653 16778 net.cpp:454] pool1 <- norm1
I1025 22:42:43.696660 16778 net.cpp:411] pool1 -> pool1
I1025 22:42:43.696697 16778 net.cpp:150] Setting up pool1
I1025 22:42:43.696704 16778 net.cpp:157] Top shape: 1 96 151 251 (3638496)
I1025 22:42:43.696717 16778 net.cpp:165] Memory required for data: 208954012
I1025 22:42:43.696719 16778 layer_factory.hpp:77] Creating layer conv2
I1025 22:42:43.696738 16778 net.cpp:106] Creating Layer conv2
I1025 22:42:43.696743 16778 net.cpp:454] conv2 <- pool1
I1025 22:42:43.696748 16778 net.cpp:411] conv2 -> conv2
I1025 22:42:43.698331 16778 net.cpp:150] Setting up conv2
I1025 22:42:43.698341 16778 net.cpp:157] Top shape: 1 256 76 126 (2451456)
I1025 22:42:43.698354 16778 net.cpp:165] Memory required for data: 218759836
I1025 22:42:43.698359 16778 layer_factory.hpp:77] Creating layer relu2
I1025 22:42:43.698364 16778 net.cpp:106] Creating Layer relu2
I1025 22:42:43.698366 16778 net.cpp:454] relu2 <- conv2
I1025 22:42:43.698372 16778 net.cpp:397] relu2 -> conv2 (in-place)
I1025 22:42:43.698665 16778 net.cpp:150] Setting up relu2
I1025 22:42:43.698675 16778 net.cpp:157] Top shape: 1 256 76 126 (2451456)
I1025 22:42:43.698693 16778 net.cpp:165] Memory required for data: 228565660
I1025 22:42:43.698696 16778 layer_factory.hpp:77] Creating layer norm2
I1025 22:42:43.698704 16778 net.cpp:106] Creating Layer norm2
I1025 22:42:43.698706 16778 net.cpp:454] norm2 <- conv2
I1025 22:42:43.698714 16778 net.cpp:411] norm2 -> norm2
I1025 22:42:43.698807 16778 net.cpp:150] Setting up norm2
I1025 22:42:43.698815 16778 net.cpp:157] Top shape: 1 256 76 126 (2451456)
I1025 22:42:43.698833 16778 net.cpp:165] Memory required for data: 238371484
I1025 22:42:43.698837 16778 layer_factory.hpp:77] Creating layer pool2
I1025 22:42:43.698843 16778 net.cpp:106] Creating Layer pool2
I1025 22:42:43.698848 16778 net.cpp:454] pool2 <- norm2
I1025 22:42:43.698853 16778 net.cpp:411] pool2 -> pool2
I1025 22:42:43.698925 16778 net.cpp:150] Setting up pool2
I1025 22:42:43.698931 16778 net.cpp:157] Top shape: 1 256 39 64 (638976)
I1025 22:42:43.698945 16778 net.cpp:165] Memory required for data: 240927388
I1025 22:42:43.698947 16778 layer_factory.hpp:77] Creating layer conv3
I1025 22:42:43.698956 16778 net.cpp:106] Creating Layer conv3
I1025 22:42:43.698969 16778 net.cpp:454] conv3 <- pool2
I1025 22:42:43.698974 16778 net.cpp:411] conv3 -> conv3
I1025 22:42:43.701529 16778 net.cpp:150] Setting up conv3
I1025 22:42:43.701558 16778 net.cpp:157] Top shape: 1 384 39 64 (958464)
I1025 22:42:43.701560 16778 net.cpp:165] Memory required for data: 244761244
I1025 22:42:43.701570 16778 layer_factory.hpp:77] Creating layer relu3
I1025 22:42:43.701577 16778 net.cpp:106] Creating Layer relu3
I1025 22:42:43.701581 16778 net.cpp:454] relu3 <- conv3
I1025 22:42:43.701589 16778 net.cpp:397] relu3 -> conv3 (in-place)
I1025 22:42:43.701771 16778 net.cpp:150] Setting up relu3
I1025 22:42:43.701781 16778 net.cpp:157] Top shape: 1 384 39 64 (958464)
I1025 22:42:43.701783 16778 net.cpp:165] Memory required for data: 248595100
I1025 22:42:43.701786 16778 layer_factory.hpp:77] Creating layer conv4
I1025 22:42:43.701795 16778 net.cpp:106] Creating Layer conv4
I1025 22:42:43.701809 16778 net.cpp:454] conv4 <- conv3
I1025 22:42:43.701830 16778 net.cpp:411] conv4 -> conv4
I1025 22:42:43.704222 16778 net.cpp:150] Setting up conv4
I1025 22:42:43.704237 16778 net.cpp:157] Top shape: 1 384 39 64 (958464)
I1025 22:42:43.704239 16778 net.cpp:165] Memory required for data: 252428956
I1025 22:42:43.704246 16778 layer_factory.hpp:77] Creating layer relu4
I1025 22:42:43.704251 16778 net.cpp:106] Creating Layer relu4
I1025 22:42:43.704263 16778 net.cpp:454] relu4 <- conv4
I1025 22:42:43.704267 16778 net.cpp:397] relu4 -> conv4 (in-place)
I1025 22:42:43.704543 16778 net.cpp:150] Setting up relu4
I1025 22:42:43.704553 16778 net.cpp:157] Top shape: 1 384 39 64 (958464)
I1025 22:42:43.704556 16778 net.cpp:165] Memory required for data: 256262812
I1025 22:42:43.704560 16778 layer_factory.hpp:77] Creating layer conv5
I1025 22:42:43.704583 16778 net.cpp:106] Creating Layer conv5
I1025 22:42:43.704602 16778 net.cpp:454] conv5 <- conv4
I1025 22:42:43.704608 16778 net.cpp:411] conv5 -> conv5
I1025 22:42:43.706457 16778 net.cpp:150] Setting up conv5
I1025 22:42:43.706467 16778 net.cpp:157] Top shape: 1 256 39 64 (638976)
I1025 22:42:43.706470 16778 net.cpp:165] Memory required for data: 258818716
I1025 22:42:43.706478 16778 layer_factory.hpp:77] Creating layer relu5
I1025 22:42:43.706485 16778 net.cpp:106] Creating Layer relu5
I1025 22:42:43.706487 16778 net.cpp:454] relu5 <- conv5
I1025 22:42:43.706496 16778 net.cpp:397] relu5 -> conv5 (in-place)
I1025 22:42:43.706799 16778 net.cpp:150] Setting up relu5
I1025 22:42:43.706809 16778 net.cpp:157] Top shape: 1 256 39 64 (638976)
I1025 22:42:43.706820 16778 net.cpp:165] Memory required for data: 261374620
I1025 22:42:43.706823 16778 layer_factory.hpp:77] Creating layer rpn_conv1
I1025 22:42:43.706845 16778 net.cpp:106] Creating Layer rpn_conv1
I1025 22:42:43.706850 16778 net.cpp:454] rpn_conv1 <- conv5
I1025 22:42:43.706866 16778 net.cpp:411] rpn_conv1 -> rpn_conv1
I1025 22:42:43.712456 16778 net.cpp:150] Setting up rpn_conv1
I1025 22:42:43.712466 16778 net.cpp:157] Top shape: 1 256 39 64 (638976)
I1025 22:42:43.712468 16778 net.cpp:165] Memory required for data: 263930524
I1025 22:42:43.712482 16778 layer_factory.hpp:77] Creating layer rpn_relu1
I1025 22:42:43.712488 16778 net.cpp:106] Creating Layer rpn_relu1
I1025 22:42:43.712505 16778 net.cpp:454] rpn_relu1 <- rpn_conv1
I1025 22:42:43.712512 16778 net.cpp:397] rpn_relu1 -> rpn_conv1 (in-place)
I1025 22:42:43.712720 16778 net.cpp:150] Setting up rpn_relu1
I1025 22:42:43.712729 16778 net.cpp:157] Top shape: 1 256 39 64 (638976)
I1025 22:42:43.712743 16778 net.cpp:165] Memory required for data: 266486428
I1025 22:42:43.712746 16778 layer_factory.hpp:77] Creating layer rpn_conv1_rpn_relu1_0_split
I1025 22:42:43.712767 16778 net.cpp:106] Creating Layer rpn_conv1_rpn_relu1_0_split
I1025 22:42:43.712771 16778 net.cpp:454] rpn_conv1_rpn_relu1_0_split <- rpn_conv1
I1025 22:42:43.712788 16778 net.cpp:411] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_0
I1025 22:42:43.712807 16778 net.cpp:411] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_1
I1025 22:42:43.712867 16778 net.cpp:150] Setting up rpn_conv1_rpn_relu1_0_split
I1025 22:42:43.712874 16778 net.cpp:157] Top shape: 1 256 39 64 (638976)
I1025 22:42:43.712889 16778 net.cpp:157] Top shape: 1 256 39 64 (638976)
I1025 22:42:43.712893 16778 net.cpp:165] Memory required for data: 271598236
I1025 22:42:43.712905 16778 layer_factory.hpp:77] Creating layer rpn_cls_score
I1025 22:42:43.712916 16778 net.cpp:106] Creating Layer rpn_cls_score
I1025 22:42:43.712930 16778 net.cpp:454] rpn_cls_score <- rpn_conv1_rpn_relu1_0_split_0
I1025 22:42:43.712939 16778 net.cpp:411] rpn_cls_score -> rpn_cls_score
I1025 22:42:43.713958 16778 net.cpp:150] Setting up rpn_cls_score
I1025 22:42:43.713968 16778 net.cpp:157] Top shape: 1 18 39 64 (44928)
I1025 22:42:43.713969 16778 net.cpp:165] Memory required for data: 271777948
I1025 22:42:43.713984 16778 layer_factory.hpp:77] Creating layer rpn_cls_score_rpn_cls_score_0_split
I1025 22:42:43.714001 16778 net.cpp:106] Creating Layer rpn_cls_score_rpn_cls_score_0_split
I1025 22:42:43.714006 16778 net.cpp:454] rpn_cls_score_rpn_cls_score_0_split <- rpn_cls_score
I1025 22:42:43.714025 16778 net.cpp:411] rpn_cls_score_rpn_cls_score_0_split -> rpn_cls_score_rpn_cls_score_0_split_0
I1025 22:42:43.714041 16778 net.cpp:411] rpn_cls_score_rpn_cls_score_0_split -> rpn_cls_score_rpn_cls_score_0_split_1
I1025 22:42:43.714090 16778 net.cpp:150] Setting up rpn_cls_score_rpn_cls_score_0_split
I1025 22:42:43.714097 16778 net.cpp:157] Top shape: 1 18 39 64 (44928)
I1025 22:42:43.714102 16778 net.cpp:157] Top shape: 1 18 39 64 (44928)
I1025 22:42:43.714105 16778 net.cpp:165] Memory required for data: 272137372
I1025 22:42:43.714108 16778 layer_factory.hpp:77] Creating layer rpn_bbox_pred
I1025 22:42:43.714119 16778 net.cpp:106] Creating Layer rpn_bbox_pred
I1025 22:42:43.714123 16778 net.cpp:454] rpn_bbox_pred <- rpn_conv1_rpn_relu1_0_split_1
I1025 22:42:43.714131 16778 net.cpp:411] rpn_bbox_pred -> rpn_bbox_pred
I1025 22:42:43.715101 16778 net.cpp:150] Setting up rpn_bbox_pred
I1025 22:42:43.715111 16778 net.cpp:157] Top shape: 1 36 39 64 (89856)
I1025 22:42:43.715113 16778 net.cpp:165] Memory required for data: 272496796
I1025 22:42:43.715119 16778 layer_factory.hpp:77] Creating layer rpn_cls_score_reshape
I1025 22:42:43.715132 16778 net.cpp:106] Creating Layer rpn_cls_score_reshape
I1025 22:42:43.715137 16778 net.cpp:454] rpn_cls_score_reshape <- rpn_cls_score_rpn_cls_score_0_split_0
I1025 22:42:43.715145 16778 net.cpp:411] rpn_cls_score_reshape -> rpn_cls_score_reshape
I1025 22:42:43.715175 16778 net.cpp:150] Setting up rpn_cls_score_reshape
I1025 22:42:43.715183 16778 net.cpp:157] Top shape: 1 2 351 64 (44928)
I1025 22:42:43.715185 16778 net.cpp:165] Memory required for data: 272676508
I1025 22:42:43.715189 16778 layer_factory.hpp:77] Creating layer rpn-data
I1025 22:42:43.715817 16778 net.cpp:106] Creating Layer rpn-data
I1025 22:42:43.715826 16778 net.cpp:454] rpn-data <- rpn_cls_score_rpn_cls_score_0_split_1
I1025 22:42:43.715829 16778 net.cpp:454] rpn-data <- gt_boxes
I1025 22:42:43.715842 16778 net.cpp:454] rpn-data <- im_info
I1025 22:42:43.715845 16778 net.cpp:454] rpn-data <- data_input-data_0_split_1
I1025 22:42:43.715852 16778 net.cpp:411] rpn-data -> rpn_labels
I1025 22:42:43.715860 16778 net.cpp:411] rpn-data -> rpn_bbox_targets
I1025 22:42:43.715869 16778 net.cpp:411] rpn-data -> rpn_bbox_inside_weights
I1025 22:42:43.715878 16778 net.cpp:411] rpn-data -> rpn_bbox_outside_weights
I1025 22:42:43.716907 16778 net.cpp:150] Setting up rpn-data
I1025 22:42:43.716917 16778 net.cpp:157] Top shape: 1 1 351 64 (22464)
I1025 22:42:43.716920 16778 net.cpp:157] Top shape: 1 36 39 64 (89856)
I1025 22:42:43.716924 16778 net.cpp:157] Top shape: 1 36 39 64 (89856)
I1025 22:42:43.716929 16778 net.cpp:157] Top shape: 1 36 39 64 (89856)
I1025 22:42:43.716933 16778 net.cpp:165] Memory required for data: 273844636
I1025 22:42:43.716936 16778 layer_factory.hpp:77] Creating layer rpn_loss_cls
I1025 22:42:43.716944 16778 net.cpp:106] Creating Layer rpn_loss_cls
I1025 22:42:43.716948 16778 net.cpp:454] rpn_loss_cls <- rpn_cls_score_reshape
I1025 22:42:43.716954 16778 net.cpp:454] rpn_loss_cls <- rpn_labels
I1025 22:42:43.716960 16778 net.cpp:411] rpn_loss_cls -> rpn_cls_loss
I1025 22:42:43.716970 16778 layer_factory.hpp:77] Creating layer rpn_loss_cls
I1025 22:42:43.717262 16778 net.cpp:150] Setting up rpn_loss_cls
I1025 22:42:43.717272 16778 net.cpp:157] Top shape: (1)
I1025 22:42:43.717275 16778 net.cpp:160]     with loss weight 1
I1025 22:42:43.717290 16778 net.cpp:165] Memory required for data: 273844640
I1025 22:42:43.717294 16778 layer_factory.hpp:77] Creating layer rpn_loss_bbox
I1025 22:42:43.717304 16778 net.cpp:106] Creating Layer rpn_loss_bbox
I1025 22:42:43.717308 16778 net.cpp:454] rpn_loss_bbox <- rpn_bbox_pred
I1025 22:42:43.717314 16778 net.cpp:454] rpn_loss_bbox <- rpn_bbox_targets
I1025 22:42:43.717319 16778 net.cpp:454] rpn_loss_bbox <- rpn_bbox_inside_weights
I1025 22:42:43.717322 16778 net.cpp:454] rpn_loss_bbox <- rpn_bbox_outside_weights
I1025 22:42:43.717329 16778 net.cpp:411] rpn_loss_bbox -> rpn_loss_bbox
I1025 22:42:43.718060 16778 net.cpp:150] Setting up rpn_loss_bbox
I1025 22:42:43.718068 16778 net.cpp:157] Top shape: (1)
I1025 22:42:43.718071 16778 net.cpp:160]     with loss weight 1
I1025 22:42:43.718076 16778 net.cpp:165] Memory required for data: 273844644
I1025 22:42:43.718080 16778 layer_factory.hpp:77] Creating layer dummy_roi_pool_conv5
I1025 22:42:43.718092 16778 net.cpp:106] Creating Layer dummy_roi_pool_conv5
I1025 22:42:43.718099 16778 net.cpp:411] dummy_roi_pool_conv5 -> dummy_roi_pool_conv5
I1025 22:42:43.718150 16778 net.cpp:150] Setting up dummy_roi_pool_conv5
I1025 22:42:43.718156 16778 net.cpp:157] Top shape: 1 9216 (9216)
I1025 22:42:43.718169 16778 net.cpp:165] Memory required for data: 273881508
I1025 22:42:43.718173 16778 layer_factory.hpp:77] Creating layer fc6
I1025 22:42:43.718181 16778 net.cpp:106] Creating Layer fc6
I1025 22:42:43.718185 16778 net.cpp:454] fc6 <- dummy_roi_pool_conv5
I1025 22:42:43.718192 16778 net.cpp:411] fc6 -> fc6
I1025 22:42:43.764039 16778 net.cpp:150] Setting up fc6
I1025 22:42:43.764072 16778 net.cpp:157] Top shape: 1 4096 (4096)
I1025 22:42:43.764075 16778 net.cpp:165] Memory required for data: 273897892
I1025 22:42:43.764086 16778 layer_factory.hpp:77] Creating layer relu6
I1025 22:42:43.764106 16778 net.cpp:106] Creating Layer relu6
I1025 22:42:43.764108 16778 net.cpp:454] relu6 <- fc6
I1025 22:42:43.764124 16778 net.cpp:397] relu6 -> fc6 (in-place)
I1025 22:42:43.764552 16778 net.cpp:150] Setting up relu6
I1025 22:42:43.764560 16778 net.cpp:157] Top shape: 1 4096 (4096)
I1025 22:42:43.764561 16778 net.cpp:165] Memory required for data: 273914276
I1025 22:42:43.764564 16778 layer_factory.hpp:77] Creating layer fc7
I1025 22:42:43.764569 16778 net.cpp:106] Creating Layer fc7
I1025 22:42:43.764570 16778 net.cpp:454] fc7 <- fc6
I1025 22:42:43.764575 16778 net.cpp:411] fc7 -> fc7
I1025 22:42:43.784756 16778 net.cpp:150] Setting up fc7
I1025 22:42:43.784781 16778 net.cpp:157] Top shape: 1 4096 (4096)
I1025 22:42:43.784783 16778 net.cpp:165] Memory required for data: 273930660
I1025 22:42:43.784791 16778 layer_factory.hpp:77] Creating layer silence_fc7
I1025 22:42:43.784798 16778 net.cpp:106] Creating Layer silence_fc7
I1025 22:42:43.784801 16778 net.cpp:454] silence_fc7 <- fc7
I1025 22:42:43.784817 16778 net.cpp:150] Setting up silence_fc7
I1025 22:42:43.784818 16778 net.cpp:165] Memory required for data: 273930660
I1025 22:42:43.784821 16778 net.cpp:228] silence_fc7 does not need backward computation.
I1025 22:42:43.784822 16778 net.cpp:228] fc7 does not need backward computation.
I1025 22:42:43.784826 16778 net.cpp:228] relu6 does not need backward computation.
I1025 22:42:43.784827 16778 net.cpp:228] fc6 does not need backward computation.
I1025 22:42:43.784831 16778 net.cpp:228] dummy_roi_pool_conv5 does not need backward computation.
I1025 22:42:43.784833 16778 net.cpp:226] rpn_loss_bbox needs backward computation.
I1025 22:42:43.784847 16778 net.cpp:226] rpn_loss_cls needs backward computation.
I1025 22:42:43.784854 16778 net.cpp:226] rpn-data needs backward computation.
I1025 22:42:43.784859 16778 net.cpp:226] rpn_cls_score_reshape needs backward computation.
I1025 22:42:43.784862 16778 net.cpp:226] rpn_bbox_pred needs backward computation.
I1025 22:42:43.784867 16778 net.cpp:226] rpn_cls_score_rpn_cls_score_0_split needs backward computation.
I1025 22:42:43.784869 16778 net.cpp:226] rpn_cls_score needs backward computation.
I1025 22:42:43.784873 16778 net.cpp:226] rpn_conv1_rpn_relu1_0_split needs backward computation.
I1025 22:42:43.784878 16778 net.cpp:226] rpn_relu1 needs backward computation.
I1025 22:42:43.784880 16778 net.cpp:226] rpn_conv1 needs backward computation.
I1025 22:42:43.784884 16778 net.cpp:226] relu5 needs backward computation.
I1025 22:42:43.784896 16778 net.cpp:226] conv5 needs backward computation.
I1025 22:42:43.784899 16778 net.cpp:226] relu4 needs backward computation.
I1025 22:42:43.784904 16778 net.cpp:226] conv4 needs backward computation.
I1025 22:42:43.784906 16778 net.cpp:226] relu3 needs backward computation.
I1025 22:42:43.784910 16778 net.cpp:226] conv3 needs backward computation.
I1025 22:42:43.784914 16778 net.cpp:226] pool2 needs backward computation.
I1025 22:42:43.784919 16778 net.cpp:226] norm2 needs backward computation.
I1025 22:42:43.784922 16778 net.cpp:226] relu2 needs backward computation.
I1025 22:42:43.784926 16778 net.cpp:226] conv2 needs backward computation.
I1025 22:42:43.784930 16778 net.cpp:226] pool1 needs backward computation.
I1025 22:42:43.784934 16778 net.cpp:226] norm1 needs backward computation.
I1025 22:42:43.784939 16778 net.cpp:226] relu1 needs backward computation.
I1025 22:42:43.784942 16778 net.cpp:226] conv1 needs backward computation.
I1025 22:42:43.784948 16778 net.cpp:228] data_input-data_0_split does not need backward computation.
I1025 22:42:43.784953 16778 net.cpp:228] input-data does not need backward computation.
I1025 22:42:43.784957 16778 net.cpp:270] This network produces output rpn_cls_loss
I1025 22:42:43.784960 16778 net.cpp:270] This network produces output rpn_loss_bbox
I1025 22:42:43.784988 16778 net.cpp:283] Network initialization done.
I1025 22:42:43.785086 16778 solver.cpp:60] Solver scaffolding done.
I1025 22:42:45.395962 16778 net.cpp:816] Ignoring source layer pool5_spm6
I1025 22:42:45.395982 16778 net.cpp:816] Ignoring source layer pool5_spm6_flatten
I1025 22:42:45.418375 16778 net.cpp:816] Ignoring source layer drop6
I1025 22:42:45.428740 16778 net.cpp:816] Ignoring source layer relu7
I1025 22:42:45.428760 16778 net.cpp:816] Ignoring source layer drop7
I1025 22:42:45.428761 16778 net.cpp:816] Ignoring source layer fc8
I1025 22:42:45.428763 16778 net.cpp:816] Ignoring source layer prob
I1025 22:42:45.537374 16778 solver.cpp:229] Iteration 0, loss = 0.885832
I1025 22:42:45.537405 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.697612 (* 1 = 0.697612 loss)
I1025 22:42:45.537410 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.18822 (* 1 = 0.18822 loss)
I1025 22:42:45.537425 16778 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I1025 22:42:54.179874 16778 solver.cpp:229] Iteration 100, loss = 0.298724
I1025 22:42:54.179905 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.192377 (* 1 = 0.192377 loss)
I1025 22:42:54.179910 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.106347 (* 1 = 0.106347 loss)
I1025 22:42:54.179915 16778 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I1025 22:43:02.810535 16778 solver.cpp:229] Iteration 200, loss = 0.619191
I1025 22:43:02.810567 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.452964 (* 1 = 0.452964 loss)
I1025 22:43:02.810573 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.166227 (* 1 = 0.166227 loss)
I1025 22:43:02.810577 16778 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I1025 22:43:11.391657 16778 solver.cpp:229] Iteration 300, loss = 0.255607
I1025 22:43:11.391687 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.208562 (* 1 = 0.208562 loss)
I1025 22:43:11.391692 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0470453 (* 1 = 0.0470453 loss)
I1025 22:43:11.391710 16778 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I1025 22:43:20.054205 16778 solver.cpp:229] Iteration 400, loss = 0.307326
I1025 22:43:20.054250 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.173415 (* 1 = 0.173415 loss)
I1025 22:43:20.054255 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.133911 (* 1 = 0.133911 loss)
I1025 22:43:20.054258 16778 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I1025 22:43:28.724956 16778 solver.cpp:229] Iteration 500, loss = 0.203697
I1025 22:43:28.724987 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.167566 (* 1 = 0.167566 loss)
I1025 22:43:28.724992 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0361305 (* 1 = 0.0361305 loss)
I1025 22:43:28.724997 16778 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I1025 22:43:37.264942 16778 solver.cpp:229] Iteration 600, loss = 0.214408
I1025 22:43:37.264976 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.170336 (* 1 = 0.170336 loss)
I1025 22:43:37.264981 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0440717 (* 1 = 0.0440717 loss)
I1025 22:43:37.264986 16778 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I1025 22:43:45.848443 16778 solver.cpp:229] Iteration 700, loss = 0.212043
I1025 22:43:45.848476 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.204354 (* 1 = 0.204354 loss)
I1025 22:43:45.848482 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00768901 (* 1 = 0.00768901 loss)
I1025 22:43:45.848485 16778 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I1025 22:43:54.648097 16778 solver.cpp:229] Iteration 800, loss = 1.11584
I1025 22:43:54.648128 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.627687 (* 1 = 0.627687 loss)
I1025 22:43:54.648133 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.488148 (* 1 = 0.488148 loss)
I1025 22:43:54.648136 16778 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I1025 22:44:03.613626 16778 solver.cpp:229] Iteration 900, loss = 0.308819
I1025 22:44:03.613658 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.245333 (* 1 = 0.245333 loss)
I1025 22:44:03.613664 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.063486 (* 1 = 0.063486 loss)
I1025 22:44:03.613668 16778 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I1025 22:44:12.396425 16778 solver.cpp:229] Iteration 1000, loss = 0.163519
I1025 22:44:12.396457 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.152355 (* 1 = 0.152355 loss)
I1025 22:44:12.396462 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0111635 (* 1 = 0.0111635 loss)
I1025 22:44:12.396466 16778 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I1025 22:44:21.053326 16778 solver.cpp:229] Iteration 1100, loss = 0.226504
I1025 22:44:21.053357 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.213245 (* 1 = 0.213245 loss)
I1025 22:44:21.053362 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0132588 (* 1 = 0.0132588 loss)
I1025 22:44:21.053366 16778 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I1025 22:44:29.748857 16778 solver.cpp:229] Iteration 1200, loss = 0.293397
I1025 22:44:29.748889 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.283828 (* 1 = 0.283828 loss)
I1025 22:44:29.748894 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0095689 (* 1 = 0.0095689 loss)
I1025 22:44:29.748898 16778 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I1025 22:44:38.925946 16778 solver.cpp:229] Iteration 1300, loss = 0.0945915
I1025 22:44:38.925978 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0429041 (* 1 = 0.0429041 loss)
I1025 22:44:38.925982 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0516874 (* 1 = 0.0516874 loss)
I1025 22:44:38.925987 16778 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I1025 22:44:48.142911 16778 solver.cpp:229] Iteration 1400, loss = 0.1298
I1025 22:44:48.142943 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.105937 (* 1 = 0.105937 loss)
I1025 22:44:48.142948 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0238624 (* 1 = 0.0238624 loss)
I1025 22:44:48.142953 16778 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I1025 22:44:57.142204 16778 solver.cpp:229] Iteration 1500, loss = 0.204488
I1025 22:44:57.142235 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.193233 (* 1 = 0.193233 loss)
I1025 22:44:57.142241 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.011255 (* 1 = 0.011255 loss)
I1025 22:44:57.142244 16778 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I1025 22:45:06.087864 16778 solver.cpp:229] Iteration 1600, loss = 0.147572
I1025 22:45:06.087913 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.137192 (* 1 = 0.137192 loss)
I1025 22:45:06.087918 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0103798 (* 1 = 0.0103798 loss)
I1025 22:45:06.087921 16778 sgd_solver.cpp:106] Iteration 1600, lr = 0.001
I1025 22:45:15.028267 16778 solver.cpp:229] Iteration 1700, loss = 0.277802
I1025 22:45:15.028300 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.22865 (* 1 = 0.22865 loss)
I1025 22:45:15.028304 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0491524 (* 1 = 0.0491524 loss)
I1025 22:45:15.028309 16778 sgd_solver.cpp:106] Iteration 1700, lr = 0.001
I1025 22:45:24.021366 16778 solver.cpp:229] Iteration 1800, loss = 0.752875
I1025 22:45:24.021397 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.4808 (* 1 = 0.4808 loss)
I1025 22:45:24.021401 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.272075 (* 1 = 0.272075 loss)
I1025 22:45:24.021405 16778 sgd_solver.cpp:106] Iteration 1800, lr = 0.001
I1025 22:45:33.135196 16778 solver.cpp:229] Iteration 1900, loss = 0.266824
I1025 22:45:33.135228 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.250462 (* 1 = 0.250462 loss)
I1025 22:45:33.135234 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0163625 (* 1 = 0.0163625 loss)
I1025 22:45:33.135238 16778 sgd_solver.cpp:106] Iteration 1900, lr = 0.001
I1025 22:45:42.127869 16778 solver.cpp:229] Iteration 2000, loss = 0.102345
I1025 22:45:42.127902 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0994564 (* 1 = 0.0994564 loss)
I1025 22:45:42.127907 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00288844 (* 1 = 0.00288844 loss)
I1025 22:45:42.127912 16778 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I1025 22:45:51.121064 16778 solver.cpp:229] Iteration 2100, loss = 0.127041
I1025 22:45:51.121107 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0808215 (* 1 = 0.0808215 loss)
I1025 22:45:51.121112 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.046219 (* 1 = 0.046219 loss)
I1025 22:45:51.121116 16778 sgd_solver.cpp:106] Iteration 2100, lr = 0.001
I1025 22:45:59.968824 16778 solver.cpp:229] Iteration 2200, loss = 0.272024
I1025 22:45:59.968857 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.202743 (* 1 = 0.202743 loss)
I1025 22:45:59.968861 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0692812 (* 1 = 0.0692812 loss)
I1025 22:45:59.968866 16778 sgd_solver.cpp:106] Iteration 2200, lr = 0.001
I1025 22:46:08.989648 16778 solver.cpp:229] Iteration 2300, loss = 0.0785513
I1025 22:46:08.989681 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0764796 (* 1 = 0.0764796 loss)
I1025 22:46:08.989686 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00207176 (* 1 = 0.00207176 loss)
I1025 22:46:08.989689 16778 sgd_solver.cpp:106] Iteration 2300, lr = 0.001
I1025 22:46:17.992489 16778 solver.cpp:229] Iteration 2400, loss = 0.951901
I1025 22:46:17.992521 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.686485 (* 1 = 0.686485 loss)
I1025 22:46:17.992524 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.265417 (* 1 = 0.265417 loss)
I1025 22:46:17.992528 16778 sgd_solver.cpp:106] Iteration 2400, lr = 0.001
I1025 22:46:26.976704 16778 solver.cpp:229] Iteration 2500, loss = 0.582533
I1025 22:46:26.976737 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.477418 (* 1 = 0.477418 loss)
I1025 22:46:26.976742 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.105115 (* 1 = 0.105115 loss)
I1025 22:46:26.976747 16778 sgd_solver.cpp:106] Iteration 2500, lr = 0.001
I1025 22:46:35.988150 16778 solver.cpp:229] Iteration 2600, loss = 0.189782
I1025 22:46:35.988183 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0589019 (* 1 = 0.0589019 loss)
I1025 22:46:35.988188 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.13088 (* 1 = 0.13088 loss)
I1025 22:46:35.988191 16778 sgd_solver.cpp:106] Iteration 2600, lr = 0.001
I1025 22:46:44.984237 16778 solver.cpp:229] Iteration 2700, loss = 0.344848
I1025 22:46:44.984271 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.326135 (* 1 = 0.326135 loss)
I1025 22:46:44.984277 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0187129 (* 1 = 0.0187129 loss)
I1025 22:46:44.984280 16778 sgd_solver.cpp:106] Iteration 2700, lr = 0.001
I1025 22:46:53.987710 16778 solver.cpp:229] Iteration 2800, loss = 0.317578
I1025 22:46:53.987741 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.246405 (* 1 = 0.246405 loss)
I1025 22:46:53.987746 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0711729 (* 1 = 0.0711729 loss)
I1025 22:46:53.987751 16778 sgd_solver.cpp:106] Iteration 2800, lr = 0.001
I1025 22:47:02.959760 16778 solver.cpp:229] Iteration 2900, loss = 0.149239
I1025 22:47:02.959791 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.121032 (* 1 = 0.121032 loss)
I1025 22:47:02.959800 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0282061 (* 1 = 0.0282061 loss)
I1025 22:47:02.959805 16778 sgd_solver.cpp:106] Iteration 2900, lr = 0.001
I1025 22:47:11.938148 16778 solver.cpp:229] Iteration 3000, loss = 0.167833
I1025 22:47:11.938181 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.149772 (* 1 = 0.149772 loss)
I1025 22:47:11.938189 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0180615 (* 1 = 0.0180615 loss)
I1025 22:47:11.938194 16778 sgd_solver.cpp:106] Iteration 3000, lr = 0.001
I1025 22:47:20.792798 16778 solver.cpp:229] Iteration 3100, loss = 0.0817473
I1025 22:47:20.792834 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0793284 (* 1 = 0.0793284 loss)
I1025 22:47:20.792840 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00241889 (* 1 = 0.00241889 loss)
I1025 22:47:20.792845 16778 sgd_solver.cpp:106] Iteration 3100, lr = 0.001
I1025 22:47:29.733382 16778 solver.cpp:229] Iteration 3200, loss = 0.157281
I1025 22:47:29.733417 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0765275 (* 1 = 0.0765275 loss)
I1025 22:47:29.733423 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0807531 (* 1 = 0.0807531 loss)
I1025 22:47:29.733428 16778 sgd_solver.cpp:106] Iteration 3200, lr = 0.001
I1025 22:47:38.657006 16778 solver.cpp:229] Iteration 3300, loss = 0.137166
I1025 22:47:38.657037 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0567905 (* 1 = 0.0567905 loss)
I1025 22:47:38.657043 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0803757 (* 1 = 0.0803757 loss)
I1025 22:47:38.657048 16778 sgd_solver.cpp:106] Iteration 3300, lr = 0.001
I1025 22:47:47.841442 16778 solver.cpp:229] Iteration 3400, loss = 0.165495
I1025 22:47:47.841477 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.114705 (* 1 = 0.114705 loss)
I1025 22:47:47.841485 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0507893 (* 1 = 0.0507893 loss)
I1025 22:47:47.841490 16778 sgd_solver.cpp:106] Iteration 3400, lr = 0.001
I1025 22:47:56.829300 16778 solver.cpp:229] Iteration 3500, loss = 0.129078
I1025 22:47:56.829334 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0863737 (* 1 = 0.0863737 loss)
I1025 22:47:56.829339 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0427045 (* 1 = 0.0427045 loss)
I1025 22:47:56.829342 16778 sgd_solver.cpp:106] Iteration 3500, lr = 0.001
I1025 22:48:05.827390 16778 solver.cpp:229] Iteration 3600, loss = 0.176939
I1025 22:48:05.827438 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.154446 (* 1 = 0.154446 loss)
I1025 22:48:05.827453 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0224925 (* 1 = 0.0224925 loss)
I1025 22:48:05.827457 16778 sgd_solver.cpp:106] Iteration 3600, lr = 0.001
I1025 22:48:14.759791 16778 solver.cpp:229] Iteration 3700, loss = 0.413272
I1025 22:48:14.759824 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.264193 (* 1 = 0.264193 loss)
I1025 22:48:14.759829 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.149079 (* 1 = 0.149079 loss)
I1025 22:48:14.759834 16778 sgd_solver.cpp:106] Iteration 3700, lr = 0.001
I1025 22:48:23.876035 16778 solver.cpp:229] Iteration 3800, loss = 0.165601
I1025 22:48:23.876068 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0929085 (* 1 = 0.0929085 loss)
I1025 22:48:23.876073 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0726921 (* 1 = 0.0726921 loss)
I1025 22:48:23.876078 16778 sgd_solver.cpp:106] Iteration 3800, lr = 0.001
I1025 22:48:32.949563 16778 solver.cpp:229] Iteration 3900, loss = 0.341802
I1025 22:48:32.949594 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.2591 (* 1 = 0.2591 loss)
I1025 22:48:32.949599 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0827017 (* 1 = 0.0827017 loss)
I1025 22:48:32.949602 16778 sgd_solver.cpp:106] Iteration 3900, lr = 0.001
I1025 22:48:41.939218 16778 solver.cpp:229] Iteration 4000, loss = 0.0931918
I1025 22:48:41.939249 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0699718 (* 1 = 0.0699718 loss)
I1025 22:48:41.939254 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.02322 (* 1 = 0.02322 loss)
I1025 22:48:41.939256 16778 sgd_solver.cpp:106] Iteration 4000, lr = 0.001
I1025 22:48:50.861660 16778 solver.cpp:229] Iteration 4100, loss = 0.210702
I1025 22:48:50.861691 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.170304 (* 1 = 0.170304 loss)
I1025 22:48:50.861696 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0403987 (* 1 = 0.0403987 loss)
I1025 22:48:50.861701 16778 sgd_solver.cpp:106] Iteration 4100, lr = 0.001
I1025 22:48:59.853935 16778 solver.cpp:229] Iteration 4200, loss = 0.145336
I1025 22:48:59.853966 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.139769 (* 1 = 0.139769 loss)
I1025 22:48:59.853972 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00556693 (* 1 = 0.00556693 loss)
I1025 22:48:59.853976 16778 sgd_solver.cpp:106] Iteration 4200, lr = 0.001
I1025 22:49:08.896770 16778 solver.cpp:229] Iteration 4300, loss = 1.0411
I1025 22:49:08.896803 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.653761 (* 1 = 0.653761 loss)
I1025 22:49:08.896808 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.387337 (* 1 = 0.387337 loss)
I1025 22:49:08.896812 16778 sgd_solver.cpp:106] Iteration 4300, lr = 0.001
I1025 22:49:17.983232 16778 solver.cpp:229] Iteration 4400, loss = 0.382427
I1025 22:49:17.983261 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.285703 (* 1 = 0.285703 loss)
I1025 22:49:17.983266 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0967241 (* 1 = 0.0967241 loss)
I1025 22:49:17.983270 16778 sgd_solver.cpp:106] Iteration 4400, lr = 0.001
I1025 22:49:27.005861 16778 solver.cpp:229] Iteration 4500, loss = 0.164098
I1025 22:49:27.005900 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.14396 (* 1 = 0.14396 loss)
I1025 22:49:27.005906 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0201382 (* 1 = 0.0201382 loss)
I1025 22:49:27.005910 16778 sgd_solver.cpp:106] Iteration 4500, lr = 0.001
I1025 22:49:36.037654 16778 solver.cpp:229] Iteration 4600, loss = 0.0543845
I1025 22:49:36.037698 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0345059 (* 1 = 0.0345059 loss)
I1025 22:49:36.037703 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0198786 (* 1 = 0.0198786 loss)
I1025 22:49:36.037706 16778 sgd_solver.cpp:106] Iteration 4600, lr = 0.001
I1025 22:49:45.127310 16778 solver.cpp:229] Iteration 4700, loss = 0.13475
I1025 22:49:45.127338 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0973731 (* 1 = 0.0973731 loss)
I1025 22:49:45.127343 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.037377 (* 1 = 0.037377 loss)
I1025 22:49:45.127347 16778 sgd_solver.cpp:106] Iteration 4700, lr = 0.001
I1025 22:49:54.162737 16778 solver.cpp:229] Iteration 4800, loss = 0.0461001
I1025 22:49:54.162770 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0387407 (* 1 = 0.0387407 loss)
I1025 22:49:54.162775 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00735941 (* 1 = 0.00735941 loss)
I1025 22:49:54.162780 16778 sgd_solver.cpp:106] Iteration 4800, lr = 0.001
I1025 22:50:03.044865 16778 solver.cpp:229] Iteration 4900, loss = 0.0889984
I1025 22:50:03.044898 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.076606 (* 1 = 0.076606 loss)
I1025 22:50:03.044903 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0123924 (* 1 = 0.0123924 loss)
I1025 22:50:03.044906 16778 sgd_solver.cpp:106] Iteration 4900, lr = 0.001
I1025 22:50:12.046330 16778 solver.cpp:229] Iteration 5000, loss = 0.113334
I1025 22:50:12.046363 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.107264 (* 1 = 0.107264 loss)
I1025 22:50:12.046368 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00606993 (* 1 = 0.00606993 loss)
I1025 22:50:12.046372 16778 sgd_solver.cpp:106] Iteration 5000, lr = 0.001
I1025 22:50:21.005995 16778 solver.cpp:229] Iteration 5100, loss = 0.111018
I1025 22:50:21.006026 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.054967 (* 1 = 0.054967 loss)
I1025 22:50:21.006031 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.056051 (* 1 = 0.056051 loss)
I1025 22:50:21.006036 16778 sgd_solver.cpp:106] Iteration 5100, lr = 0.001
I1025 22:50:29.961412 16778 solver.cpp:229] Iteration 5200, loss = 0.412818
I1025 22:50:29.961447 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.291399 (* 1 = 0.291399 loss)
I1025 22:50:29.961452 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.121418 (* 1 = 0.121418 loss)
I1025 22:50:29.961455 16778 sgd_solver.cpp:106] Iteration 5200, lr = 0.001
I1025 22:50:38.949759 16778 solver.cpp:229] Iteration 5300, loss = 0.10475
I1025 22:50:38.949792 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0985875 (* 1 = 0.0985875 loss)
I1025 22:50:38.949797 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00616265 (* 1 = 0.00616265 loss)
I1025 22:50:38.949800 16778 sgd_solver.cpp:106] Iteration 5300, lr = 0.001
I1025 22:50:47.936875 16778 solver.cpp:229] Iteration 5400, loss = 0.287029
I1025 22:50:47.936919 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.252153 (* 1 = 0.252153 loss)
I1025 22:50:47.936924 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0348757 (* 1 = 0.0348757 loss)
I1025 22:50:47.936928 16778 sgd_solver.cpp:106] Iteration 5400, lr = 0.001
I1025 22:50:56.911669 16778 solver.cpp:229] Iteration 5500, loss = 0.152285
I1025 22:50:56.911701 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.114216 (* 1 = 0.114216 loss)
I1025 22:50:56.911705 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.038069 (* 1 = 0.038069 loss)
I1025 22:50:56.911710 16778 sgd_solver.cpp:106] Iteration 5500, lr = 0.001
I1025 22:51:05.866966 16778 solver.cpp:229] Iteration 5600, loss = 0.185093
I1025 22:51:05.867007 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.145702 (* 1 = 0.145702 loss)
I1025 22:51:05.867013 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0393916 (* 1 = 0.0393916 loss)
I1025 22:51:05.867017 16778 sgd_solver.cpp:106] Iteration 5600, lr = 0.001
I1025 22:51:14.882098 16778 solver.cpp:229] Iteration 5700, loss = 0.0761642
I1025 22:51:14.882131 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0479267 (* 1 = 0.0479267 loss)
I1025 22:51:14.882136 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0282375 (* 1 = 0.0282375 loss)
I1025 22:51:14.882140 16778 sgd_solver.cpp:106] Iteration 5700, lr = 0.001
I1025 22:51:23.889904 16778 solver.cpp:229] Iteration 5800, loss = 0.0669184
I1025 22:51:23.889935 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0418396 (* 1 = 0.0418396 loss)
I1025 22:51:23.889940 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0250789 (* 1 = 0.0250789 loss)
I1025 22:51:23.889945 16778 sgd_solver.cpp:106] Iteration 5800, lr = 0.001
I1025 22:51:32.886793 16778 solver.cpp:229] Iteration 5900, loss = 0.0962643
I1025 22:51:32.886826 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0290226 (* 1 = 0.0290226 loss)
I1025 22:51:32.886831 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0672417 (* 1 = 0.0672417 loss)
I1025 22:51:32.886836 16778 sgd_solver.cpp:106] Iteration 5900, lr = 0.001
I1025 22:51:41.847978 16778 solver.cpp:229] Iteration 6000, loss = 0.11435
I1025 22:51:41.848011 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.105619 (* 1 = 0.105619 loss)
I1025 22:51:41.848017 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00873154 (* 1 = 0.00873154 loss)
I1025 22:51:41.848021 16778 sgd_solver.cpp:106] Iteration 6000, lr = 0.001
I1025 22:51:50.839382 16778 solver.cpp:229] Iteration 6100, loss = 0.0922026
I1025 22:51:50.839413 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0836237 (* 1 = 0.0836237 loss)
I1025 22:51:50.839418 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00857884 (* 1 = 0.00857884 loss)
I1025 22:51:50.839423 16778 sgd_solver.cpp:106] Iteration 6100, lr = 0.001
I1025 22:51:59.824122 16778 solver.cpp:229] Iteration 6200, loss = 0.197448
I1025 22:51:59.824153 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.134415 (* 1 = 0.134415 loss)
I1025 22:51:59.824158 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0630327 (* 1 = 0.0630327 loss)
I1025 22:51:59.824162 16778 sgd_solver.cpp:106] Iteration 6200, lr = 0.001
I1025 22:52:08.798382 16778 solver.cpp:229] Iteration 6300, loss = 0.29116
I1025 22:52:08.798424 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.211988 (* 1 = 0.211988 loss)
I1025 22:52:08.798429 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0791721 (* 1 = 0.0791721 loss)
I1025 22:52:08.798432 16778 sgd_solver.cpp:106] Iteration 6300, lr = 0.001
I1025 22:52:17.775774 16778 solver.cpp:229] Iteration 6400, loss = 0.320391
I1025 22:52:17.775815 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.258679 (* 1 = 0.258679 loss)
I1025 22:52:17.775820 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0617122 (* 1 = 0.0617122 loss)
I1025 22:52:17.775823 16778 sgd_solver.cpp:106] Iteration 6400, lr = 0.001
I1025 22:52:26.698171 16778 solver.cpp:229] Iteration 6500, loss = 0.289574
I1025 22:52:26.698201 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.194225 (* 1 = 0.194225 loss)
I1025 22:52:26.698223 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0953493 (* 1 = 0.0953493 loss)
I1025 22:52:26.698227 16778 sgd_solver.cpp:106] Iteration 6500, lr = 0.001
I1025 22:52:35.646677 16778 solver.cpp:229] Iteration 6600, loss = 0.206914
I1025 22:52:35.646708 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0743768 (* 1 = 0.0743768 loss)
I1025 22:52:35.646713 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.132537 (* 1 = 0.132537 loss)
I1025 22:52:35.646716 16778 sgd_solver.cpp:106] Iteration 6600, lr = 0.001
I1025 22:52:44.793318 16778 solver.cpp:229] Iteration 6700, loss = 0.0659029
I1025 22:52:44.793350 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.025228 (* 1 = 0.025228 loss)
I1025 22:52:44.793355 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0406749 (* 1 = 0.0406749 loss)
I1025 22:52:44.793359 16778 sgd_solver.cpp:106] Iteration 6700, lr = 0.001
I1025 22:52:53.807867 16778 solver.cpp:229] Iteration 6800, loss = 0.154761
I1025 22:52:53.807907 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.133446 (* 1 = 0.133446 loss)
I1025 22:52:53.807912 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0213149 (* 1 = 0.0213149 loss)
I1025 22:52:53.807916 16778 sgd_solver.cpp:106] Iteration 6800, lr = 0.001
I1025 22:53:02.793004 16778 solver.cpp:229] Iteration 6900, loss = 0.297561
I1025 22:53:02.793036 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.26031 (* 1 = 0.26031 loss)
I1025 22:53:02.793041 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0372515 (* 1 = 0.0372515 loss)
I1025 22:53:02.793045 16778 sgd_solver.cpp:106] Iteration 6900, lr = 0.001
I1025 22:53:11.753027 16778 solver.cpp:229] Iteration 7000, loss = 0.0725903
I1025 22:53:11.753059 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0667159 (* 1 = 0.0667159 loss)
I1025 22:53:11.753063 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00587439 (* 1 = 0.00587439 loss)
I1025 22:53:11.753068 16778 sgd_solver.cpp:106] Iteration 7000, lr = 0.001
I1025 22:53:20.687016 16778 solver.cpp:229] Iteration 7100, loss = 0.0849642
I1025 22:53:20.687043 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0683639 (* 1 = 0.0683639 loss)
I1025 22:53:20.687048 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0166004 (* 1 = 0.0166004 loss)
I1025 22:53:20.687053 16778 sgd_solver.cpp:106] Iteration 7100, lr = 0.001
I1025 22:53:29.583850 16778 solver.cpp:229] Iteration 7200, loss = 0.0846045
I1025 22:53:29.583876 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0795982 (* 1 = 0.0795982 loss)
I1025 22:53:29.583881 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00500635 (* 1 = 0.00500635 loss)
I1025 22:53:29.583885 16778 sgd_solver.cpp:106] Iteration 7200, lr = 0.001
I1025 22:53:38.591388 16778 solver.cpp:229] Iteration 7300, loss = 0.0784536
I1025 22:53:38.591421 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.067334 (* 1 = 0.067334 loss)
I1025 22:53:38.591428 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0111196 (* 1 = 0.0111196 loss)
I1025 22:53:38.591435 16778 sgd_solver.cpp:106] Iteration 7300, lr = 0.001
I1025 22:53:47.552932 16778 solver.cpp:229] Iteration 7400, loss = 0.0442546
I1025 22:53:47.552965 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0261005 (* 1 = 0.0261005 loss)
I1025 22:53:47.552970 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0181541 (* 1 = 0.0181541 loss)
I1025 22:53:47.552974 16778 sgd_solver.cpp:106] Iteration 7400, lr = 0.001
I1025 22:53:56.570092 16778 solver.cpp:229] Iteration 7500, loss = 0.069546
I1025 22:53:56.570122 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0367028 (* 1 = 0.0367028 loss)
I1025 22:53:56.570125 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0328432 (* 1 = 0.0328432 loss)
I1025 22:53:56.570130 16778 sgd_solver.cpp:106] Iteration 7500, lr = 0.001
I1025 22:54:05.480656 16778 solver.cpp:229] Iteration 7600, loss = 0.29398
I1025 22:54:05.480690 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.202665 (* 1 = 0.202665 loss)
I1025 22:54:05.480695 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0913148 (* 1 = 0.0913148 loss)
I1025 22:54:05.480700 16778 sgd_solver.cpp:106] Iteration 7600, lr = 0.001
I1025 22:54:14.523838 16778 solver.cpp:229] Iteration 7700, loss = 0.181462
I1025 22:54:14.523870 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.117658 (* 1 = 0.117658 loss)
I1025 22:54:14.523875 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0638037 (* 1 = 0.0638037 loss)
I1025 22:54:14.523880 16778 sgd_solver.cpp:106] Iteration 7700, lr = 0.001
I1025 22:54:23.522292 16778 solver.cpp:229] Iteration 7800, loss = 0.295264
I1025 22:54:23.522323 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.289383 (* 1 = 0.289383 loss)
I1025 22:54:23.522330 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00588158 (* 1 = 0.00588158 loss)
I1025 22:54:23.522333 16778 sgd_solver.cpp:106] Iteration 7800, lr = 0.001
I1025 22:54:32.427059 16778 solver.cpp:229] Iteration 7900, loss = 0.279465
I1025 22:54:32.427091 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.238031 (* 1 = 0.238031 loss)
I1025 22:54:32.427096 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0414334 (* 1 = 0.0414334 loss)
I1025 22:54:32.427100 16778 sgd_solver.cpp:106] Iteration 7900, lr = 0.001
I1025 22:54:41.522403 16778 solver.cpp:229] Iteration 8000, loss = 0.07385
I1025 22:54:41.522435 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0718488 (* 1 = 0.0718488 loss)
I1025 22:54:41.522450 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00200118 (* 1 = 0.00200118 loss)
I1025 22:54:41.522455 16778 sgd_solver.cpp:106] Iteration 8000, lr = 0.001
I1025 22:54:50.549222 16778 solver.cpp:229] Iteration 8100, loss = 0.12957
I1025 22:54:50.549255 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0601103 (* 1 = 0.0601103 loss)
I1025 22:54:50.549260 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0694592 (* 1 = 0.0694592 loss)
I1025 22:54:50.549264 16778 sgd_solver.cpp:106] Iteration 8100, lr = 0.001
I1025 22:54:59.513674 16778 solver.cpp:229] Iteration 8200, loss = 0.269252
I1025 22:54:59.513706 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.235164 (* 1 = 0.235164 loss)
I1025 22:54:59.513711 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.034088 (* 1 = 0.034088 loss)
I1025 22:54:59.513715 16778 sgd_solver.cpp:106] Iteration 8200, lr = 0.001
I1025 22:55:08.496260 16778 solver.cpp:229] Iteration 8300, loss = 0.093668
I1025 22:55:08.496294 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0873534 (* 1 = 0.0873534 loss)
I1025 22:55:08.496301 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00631466 (* 1 = 0.00631466 loss)
I1025 22:55:08.496307 16778 sgd_solver.cpp:106] Iteration 8300, lr = 0.001
I1025 22:55:17.470520 16778 solver.cpp:229] Iteration 8400, loss = 0.514574
I1025 22:55:17.470551 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.339966 (* 1 = 0.339966 loss)
I1025 22:55:17.470556 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.174608 (* 1 = 0.174608 loss)
I1025 22:55:17.470561 16778 sgd_solver.cpp:106] Iteration 8400, lr = 0.001
I1025 22:55:26.536834 16778 solver.cpp:229] Iteration 8500, loss = 0.337647
I1025 22:55:26.536865 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.318737 (* 1 = 0.318737 loss)
I1025 22:55:26.536871 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0189104 (* 1 = 0.0189104 loss)
I1025 22:55:26.536875 16778 sgd_solver.cpp:106] Iteration 8500, lr = 0.001
I1025 22:55:35.470748 16778 solver.cpp:229] Iteration 8600, loss = 0.0544715
I1025 22:55:35.470782 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0137505 (* 1 = 0.0137505 loss)
I1025 22:55:35.470789 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.040721 (* 1 = 0.040721 loss)
I1025 22:55:35.470795 16778 sgd_solver.cpp:106] Iteration 8600, lr = 0.001
I1025 22:55:44.389251 16778 solver.cpp:229] Iteration 8700, loss = 0.0588288
I1025 22:55:44.389284 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0557941 (* 1 = 0.0557941 loss)
I1025 22:55:44.389292 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00303471 (* 1 = 0.00303471 loss)
I1025 22:55:44.389297 16778 sgd_solver.cpp:106] Iteration 8700, lr = 0.001
I1025 22:55:53.346472 16778 solver.cpp:229] Iteration 8800, loss = 0.126554
I1025 22:55:53.346505 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0587501 (* 1 = 0.0587501 loss)
I1025 22:55:53.346510 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0678042 (* 1 = 0.0678042 loss)
I1025 22:55:53.346514 16778 sgd_solver.cpp:106] Iteration 8800, lr = 0.001
I1025 22:56:02.341311 16778 solver.cpp:229] Iteration 8900, loss = 0.609553
I1025 22:56:02.341344 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.312771 (* 1 = 0.312771 loss)
I1025 22:56:02.341349 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.296782 (* 1 = 0.296782 loss)
I1025 22:56:02.341353 16778 sgd_solver.cpp:106] Iteration 8900, lr = 0.001
I1025 22:56:11.273164 16778 solver.cpp:229] Iteration 9000, loss = 0.120082
I1025 22:56:11.273195 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.103461 (* 1 = 0.103461 loss)
I1025 22:56:11.273200 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0166203 (* 1 = 0.0166203 loss)
I1025 22:56:11.273205 16778 sgd_solver.cpp:106] Iteration 9000, lr = 0.001
I1025 22:56:20.322115 16778 solver.cpp:229] Iteration 9100, loss = 0.219336
I1025 22:56:20.322147 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.187209 (* 1 = 0.187209 loss)
I1025 22:56:20.322152 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0321279 (* 1 = 0.0321279 loss)
I1025 22:56:20.322167 16778 sgd_solver.cpp:106] Iteration 9100, lr = 0.001
I1025 22:56:29.367573 16778 solver.cpp:229] Iteration 9200, loss = 0.165608
I1025 22:56:29.367605 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.136043 (* 1 = 0.136043 loss)
I1025 22:56:29.367611 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0295652 (* 1 = 0.0295652 loss)
I1025 22:56:29.367615 16778 sgd_solver.cpp:106] Iteration 9200, lr = 0.001
I1025 22:56:38.478415 16778 solver.cpp:229] Iteration 9300, loss = 0.473542
I1025 22:56:38.478446 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.207819 (* 1 = 0.207819 loss)
I1025 22:56:38.478451 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.265723 (* 1 = 0.265723 loss)
I1025 22:56:38.478454 16778 sgd_solver.cpp:106] Iteration 9300, lr = 0.001
I1025 22:56:47.418757 16778 solver.cpp:229] Iteration 9400, loss = 0.0493261
I1025 22:56:47.418789 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0340718 (* 1 = 0.0340718 loss)
I1025 22:56:47.418794 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0152543 (* 1 = 0.0152543 loss)
I1025 22:56:47.418798 16778 sgd_solver.cpp:106] Iteration 9400, lr = 0.001
I1025 22:56:56.393956 16778 solver.cpp:229] Iteration 9500, loss = 0.177033
I1025 22:56:56.393988 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.135127 (* 1 = 0.135127 loss)
I1025 22:56:56.393996 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0419063 (* 1 = 0.0419063 loss)
I1025 22:56:56.394001 16778 sgd_solver.cpp:106] Iteration 9500, lr = 0.001
I1025 22:57:05.437656 16778 solver.cpp:229] Iteration 9600, loss = 0.0977992
I1025 22:57:05.437690 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0922007 (* 1 = 0.0922007 loss)
I1025 22:57:05.437695 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00559845 (* 1 = 0.00559845 loss)
I1025 22:57:05.437700 16778 sgd_solver.cpp:106] Iteration 9600, lr = 0.001
I1025 22:57:14.467981 16778 solver.cpp:229] Iteration 9700, loss = 0.394972
I1025 22:57:14.468013 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.202774 (* 1 = 0.202774 loss)
I1025 22:57:14.468017 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.192198 (* 1 = 0.192198 loss)
I1025 22:57:14.468020 16778 sgd_solver.cpp:106] Iteration 9700, lr = 0.001
I1025 22:57:23.380334 16778 solver.cpp:229] Iteration 9800, loss = 0.0472286
I1025 22:57:23.380365 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0330341 (* 1 = 0.0330341 loss)
I1025 22:57:23.380370 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0141945 (* 1 = 0.0141945 loss)
I1025 22:57:23.380374 16778 sgd_solver.cpp:106] Iteration 9800, lr = 0.001
I1025 22:57:32.402175 16778 solver.cpp:229] Iteration 9900, loss = 0.0391388
I1025 22:57:32.402209 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0306183 (* 1 = 0.0306183 loss)
I1025 22:57:32.402216 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0085205 (* 1 = 0.0085205 loss)
I1025 22:57:32.402221 16778 sgd_solver.cpp:106] Iteration 9900, lr = 0.001
I1025 22:57:41.763088 16778 solver.cpp:229] Iteration 10000, loss = 0.202739
I1025 22:57:41.763121 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0395876 (* 1 = 0.0395876 loss)
I1025 22:57:41.763126 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.163152 (* 1 = 0.163152 loss)
I1025 22:57:41.763140 16778 sgd_solver.cpp:106] Iteration 10000, lr = 0.001
I1025 22:57:50.803948 16778 solver.cpp:229] Iteration 10100, loss = 0.123294
I1025 22:57:50.803992 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.11753 (* 1 = 0.11753 loss)
I1025 22:57:50.803997 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00576364 (* 1 = 0.00576364 loss)
I1025 22:57:50.804003 16778 sgd_solver.cpp:106] Iteration 10100, lr = 0.001
I1025 22:57:59.821863 16778 solver.cpp:229] Iteration 10200, loss = 0.126251
I1025 22:57:59.821904 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.10973 (* 1 = 0.10973 loss)
I1025 22:57:59.821909 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.016521 (* 1 = 0.016521 loss)
I1025 22:57:59.821913 16778 sgd_solver.cpp:106] Iteration 10200, lr = 0.001
I1025 22:58:08.821627 16778 solver.cpp:229] Iteration 10300, loss = 0.636838
I1025 22:58:08.821671 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.21189 (* 1 = 0.21189 loss)
I1025 22:58:08.821676 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.424948 (* 1 = 0.424948 loss)
I1025 22:58:08.821681 16778 sgd_solver.cpp:106] Iteration 10300, lr = 0.001
I1025 22:58:17.669244 16778 solver.cpp:229] Iteration 10400, loss = 0.218432
I1025 22:58:17.669277 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0229157 (* 1 = 0.0229157 loss)
I1025 22:58:17.669283 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.195516 (* 1 = 0.195516 loss)
I1025 22:58:17.669287 16778 sgd_solver.cpp:106] Iteration 10400, lr = 0.001
I1025 22:58:26.669571 16778 solver.cpp:229] Iteration 10500, loss = 0.100355
I1025 22:58:26.669605 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0530678 (* 1 = 0.0530678 loss)
I1025 22:58:26.669610 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0472871 (* 1 = 0.0472871 loss)
I1025 22:58:26.669615 16778 sgd_solver.cpp:106] Iteration 10500, lr = 0.001
I1025 22:58:35.673262 16778 solver.cpp:229] Iteration 10600, loss = 0.0926886
I1025 22:58:35.673295 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0671526 (* 1 = 0.0671526 loss)
I1025 22:58:35.673300 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0255359 (* 1 = 0.0255359 loss)
I1025 22:58:35.673305 16778 sgd_solver.cpp:106] Iteration 10600, lr = 0.001
I1025 22:58:44.696019 16778 solver.cpp:229] Iteration 10700, loss = 0.0330877
I1025 22:58:44.696049 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.026205 (* 1 = 0.026205 loss)
I1025 22:58:44.696055 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00688277 (* 1 = 0.00688277 loss)
I1025 22:58:44.696060 16778 sgd_solver.cpp:106] Iteration 10700, lr = 0.001
I1025 22:58:53.743098 16778 solver.cpp:229] Iteration 10800, loss = 0.106315
I1025 22:58:53.743144 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0857794 (* 1 = 0.0857794 loss)
I1025 22:58:53.743149 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0205357 (* 1 = 0.0205357 loss)
I1025 22:58:53.743152 16778 sgd_solver.cpp:106] Iteration 10800, lr = 0.001
I1025 22:59:02.788161 16778 solver.cpp:229] Iteration 10900, loss = 0.373918
I1025 22:59:02.788194 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.251937 (* 1 = 0.251937 loss)
I1025 22:59:02.788200 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.121981 (* 1 = 0.121981 loss)
I1025 22:59:02.788204 16778 sgd_solver.cpp:106] Iteration 10900, lr = 0.001
I1025 22:59:11.813451 16778 solver.cpp:229] Iteration 11000, loss = 0.156564
I1025 22:59:11.813482 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.140409 (* 1 = 0.140409 loss)
I1025 22:59:11.813488 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0161549 (* 1 = 0.0161549 loss)
I1025 22:59:11.813493 16778 sgd_solver.cpp:106] Iteration 11000, lr = 0.001
I1025 22:59:20.858961 16778 solver.cpp:229] Iteration 11100, loss = 0.0704362
I1025 22:59:20.858996 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0398062 (* 1 = 0.0398062 loss)
I1025 22:59:20.859001 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.03063 (* 1 = 0.03063 loss)
I1025 22:59:20.859007 16778 sgd_solver.cpp:106] Iteration 11100, lr = 0.001
I1025 22:59:29.802722 16778 solver.cpp:229] Iteration 11200, loss = 0.120344
I1025 22:59:29.802752 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0993327 (* 1 = 0.0993327 loss)
I1025 22:59:29.802757 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0210115 (* 1 = 0.0210115 loss)
I1025 22:59:29.802762 16778 sgd_solver.cpp:106] Iteration 11200, lr = 0.001
I1025 22:59:38.813169 16778 solver.cpp:229] Iteration 11300, loss = 0.0742293
I1025 22:59:38.813202 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0387062 (* 1 = 0.0387062 loss)
I1025 22:59:38.813207 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.035523 (* 1 = 0.035523 loss)
I1025 22:59:38.813212 16778 sgd_solver.cpp:106] Iteration 11300, lr = 0.001
I1025 22:59:47.852730 16778 solver.cpp:229] Iteration 11400, loss = 0.339683
I1025 22:59:47.852776 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.129773 (* 1 = 0.129773 loss)
I1025 22:59:47.852790 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.209909 (* 1 = 0.209909 loss)
I1025 22:59:47.852794 16778 sgd_solver.cpp:106] Iteration 11400, lr = 0.001
I1025 22:59:56.844750 16778 solver.cpp:229] Iteration 11500, loss = 0.623773
I1025 22:59:56.844779 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.233701 (* 1 = 0.233701 loss)
I1025 22:59:56.844784 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.390072 (* 1 = 0.390072 loss)
I1025 22:59:56.844789 16778 sgd_solver.cpp:106] Iteration 11500, lr = 0.001
I1025 23:00:05.887090 16778 solver.cpp:229] Iteration 11600, loss = 0.102314
I1025 23:00:05.887120 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0564838 (* 1 = 0.0564838 loss)
I1025 23:00:05.887125 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0458303 (* 1 = 0.0458303 loss)
I1025 23:00:05.887128 16778 sgd_solver.cpp:106] Iteration 11600, lr = 0.001
I1025 23:00:15.013685 16778 solver.cpp:229] Iteration 11700, loss = 0.0364136
I1025 23:00:15.013713 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0286021 (* 1 = 0.0286021 loss)
I1025 23:00:15.013718 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00781156 (* 1 = 0.00781156 loss)
I1025 23:00:15.013722 16778 sgd_solver.cpp:106] Iteration 11700, lr = 0.001
I1025 23:00:24.158814 16778 solver.cpp:229] Iteration 11800, loss = 0.0673299
I1025 23:00:24.158859 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0583714 (* 1 = 0.0583714 loss)
I1025 23:00:24.158864 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0089585 (* 1 = 0.0089585 loss)
I1025 23:00:24.158879 16778 sgd_solver.cpp:106] Iteration 11800, lr = 0.001
I1025 23:00:33.154651 16778 solver.cpp:229] Iteration 11900, loss = 0.127751
I1025 23:00:33.154685 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0705857 (* 1 = 0.0705857 loss)
I1025 23:00:33.154690 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0571653 (* 1 = 0.0571653 loss)
I1025 23:00:33.154695 16778 sgd_solver.cpp:106] Iteration 11900, lr = 0.001
I1025 23:00:42.158979 16778 solver.cpp:229] Iteration 12000, loss = 0.0391417
I1025 23:00:42.159023 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0339151 (* 1 = 0.0339151 loss)
I1025 23:00:42.159036 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00522655 (* 1 = 0.00522655 loss)
I1025 23:00:42.159040 16778 sgd_solver.cpp:106] Iteration 12000, lr = 0.001
I1025 23:00:51.030340 16778 solver.cpp:229] Iteration 12100, loss = 0.208389
I1025 23:00:51.030375 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.12819 (* 1 = 0.12819 loss)
I1025 23:00:51.030383 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0801998 (* 1 = 0.0801998 loss)
I1025 23:00:51.030390 16778 sgd_solver.cpp:106] Iteration 12100, lr = 0.001
I1025 23:01:00.068424 16778 solver.cpp:229] Iteration 12200, loss = 0.10498
I1025 23:01:00.068455 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0331552 (* 1 = 0.0331552 loss)
I1025 23:01:00.068461 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0718244 (* 1 = 0.0718244 loss)
I1025 23:01:00.068464 16778 sgd_solver.cpp:106] Iteration 12200, lr = 0.001
I1025 23:01:09.002965 16778 solver.cpp:229] Iteration 12300, loss = 0.135466
I1025 23:01:09.002997 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.112555 (* 1 = 0.112555 loss)
I1025 23:01:09.003002 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0229113 (* 1 = 0.0229113 loss)
I1025 23:01:09.003006 16778 sgd_solver.cpp:106] Iteration 12300, lr = 0.001
I1025 23:01:18.010601 16778 solver.cpp:229] Iteration 12400, loss = 0.138185
I1025 23:01:18.010633 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0700808 (* 1 = 0.0700808 loss)
I1025 23:01:18.010639 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0681041 (* 1 = 0.0681041 loss)
I1025 23:01:18.010643 16778 sgd_solver.cpp:106] Iteration 12400, lr = 0.001
I1025 23:01:26.928284 16778 solver.cpp:229] Iteration 12500, loss = 0.130041
I1025 23:01:26.928318 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0740339 (* 1 = 0.0740339 loss)
I1025 23:01:26.928323 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0560073 (* 1 = 0.0560073 loss)
I1025 23:01:26.928325 16778 sgd_solver.cpp:106] Iteration 12500, lr = 0.001
I1025 23:01:35.904851 16778 solver.cpp:229] Iteration 12600, loss = 0.0594174
I1025 23:01:35.904886 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0477098 (* 1 = 0.0477098 loss)
I1025 23:01:35.904891 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0117076 (* 1 = 0.0117076 loss)
I1025 23:01:35.904896 16778 sgd_solver.cpp:106] Iteration 12600, lr = 0.001
I1025 23:01:44.880496 16778 solver.cpp:229] Iteration 12700, loss = 0.135168
I1025 23:01:44.880540 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.121281 (* 1 = 0.121281 loss)
I1025 23:01:44.880547 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0138872 (* 1 = 0.0138872 loss)
I1025 23:01:44.880551 16778 sgd_solver.cpp:106] Iteration 12700, lr = 0.001
I1025 23:01:54.129566 16778 solver.cpp:229] Iteration 12800, loss = 0.0598864
I1025 23:01:54.129598 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0272468 (* 1 = 0.0272468 loss)
I1025 23:01:54.129603 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0326396 (* 1 = 0.0326396 loss)
I1025 23:01:54.129608 16778 sgd_solver.cpp:106] Iteration 12800, lr = 0.001
I1025 23:02:03.236527 16778 solver.cpp:229] Iteration 12900, loss = 0.0373298
I1025 23:02:03.236560 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0312213 (* 1 = 0.0312213 loss)
I1025 23:02:03.236565 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00610854 (* 1 = 0.00610854 loss)
I1025 23:02:03.236570 16778 sgd_solver.cpp:106] Iteration 12900, lr = 0.001
I1025 23:02:12.190861 16778 solver.cpp:229] Iteration 13000, loss = 0.140249
I1025 23:02:12.190896 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0870647 (* 1 = 0.0870647 loss)
I1025 23:02:12.190901 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0531843 (* 1 = 0.0531843 loss)
I1025 23:02:12.190906 16778 sgd_solver.cpp:106] Iteration 13000, lr = 0.001
I1025 23:02:21.032394 16778 solver.cpp:229] Iteration 13100, loss = 0.161251
I1025 23:02:21.032428 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.116128 (* 1 = 0.116128 loss)
I1025 23:02:21.032434 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0451222 (* 1 = 0.0451222 loss)
I1025 23:02:21.032439 16778 sgd_solver.cpp:106] Iteration 13100, lr = 0.001
I1025 23:02:29.971601 16778 solver.cpp:229] Iteration 13200, loss = 0.0307366
I1025 23:02:29.971633 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0265247 (* 1 = 0.0265247 loss)
I1025 23:02:29.971639 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00421188 (* 1 = 0.00421188 loss)
I1025 23:02:29.971644 16778 sgd_solver.cpp:106] Iteration 13200, lr = 0.001
I1025 23:02:38.917641 16778 solver.cpp:229] Iteration 13300, loss = 0.0496726
I1025 23:02:38.917685 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0444412 (* 1 = 0.0444412 loss)
I1025 23:02:38.917690 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00523143 (* 1 = 0.00523143 loss)
I1025 23:02:38.917695 16778 sgd_solver.cpp:106] Iteration 13300, lr = 0.001
I1025 23:02:48.008265 16778 solver.cpp:229] Iteration 13400, loss = 0.144538
I1025 23:02:48.008298 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.100651 (* 1 = 0.100651 loss)
I1025 23:02:48.008303 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0438873 (* 1 = 0.0438873 loss)
I1025 23:02:48.008308 16778 sgd_solver.cpp:106] Iteration 13400, lr = 0.001
I1025 23:02:57.127774 16778 solver.cpp:229] Iteration 13500, loss = 0.0435419
I1025 23:02:57.127810 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.037564 (* 1 = 0.037564 loss)
I1025 23:02:57.127815 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0059779 (* 1 = 0.0059779 loss)
I1025 23:02:57.127820 16778 sgd_solver.cpp:106] Iteration 13500, lr = 0.001
I1025 23:03:06.153411 16778 solver.cpp:229] Iteration 13600, loss = 0.16264
I1025 23:03:06.153446 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0949137 (* 1 = 0.0949137 loss)
I1025 23:03:06.153451 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0677266 (* 1 = 0.0677266 loss)
I1025 23:03:06.153455 16778 sgd_solver.cpp:106] Iteration 13600, lr = 0.001
I1025 23:03:15.325317 16778 solver.cpp:229] Iteration 13700, loss = 0.156833
I1025 23:03:15.325350 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0897797 (* 1 = 0.0897797 loss)
I1025 23:03:15.325356 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.067053 (* 1 = 0.067053 loss)
I1025 23:03:15.325359 16778 sgd_solver.cpp:106] Iteration 13700, lr = 0.001
I1025 23:03:24.325784 16778 solver.cpp:229] Iteration 13800, loss = 0.301858
I1025 23:03:24.325819 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.117476 (* 1 = 0.117476 loss)
I1025 23:03:24.325824 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.184382 (* 1 = 0.184382 loss)
I1025 23:03:24.325829 16778 sgd_solver.cpp:106] Iteration 13800, lr = 0.001
I1025 23:03:33.459080 16778 solver.cpp:229] Iteration 13900, loss = 0.117451
I1025 23:03:33.459113 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0591682 (* 1 = 0.0591682 loss)
I1025 23:03:33.459118 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0582825 (* 1 = 0.0582825 loss)
I1025 23:03:33.459122 16778 sgd_solver.cpp:106] Iteration 13900, lr = 0.001
I1025 23:03:42.422488 16778 solver.cpp:229] Iteration 14000, loss = 0.0481642
I1025 23:03:42.422521 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0181762 (* 1 = 0.0181762 loss)
I1025 23:03:42.422528 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0299881 (* 1 = 0.0299881 loss)
I1025 23:03:42.422531 16778 sgd_solver.cpp:106] Iteration 14000, lr = 0.001
I1025 23:03:51.460826 16778 solver.cpp:229] Iteration 14100, loss = 0.0196725
I1025 23:03:51.460858 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0137057 (* 1 = 0.0137057 loss)
I1025 23:03:51.460863 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00596679 (* 1 = 0.00596679 loss)
I1025 23:03:51.460868 16778 sgd_solver.cpp:106] Iteration 14100, lr = 0.001
I1025 23:04:00.600122 16778 solver.cpp:229] Iteration 14200, loss = 0.169977
I1025 23:04:00.600157 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0551 (* 1 = 0.0551 loss)
I1025 23:04:00.600165 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.114877 (* 1 = 0.114877 loss)
I1025 23:04:00.600172 16778 sgd_solver.cpp:106] Iteration 14200, lr = 0.001
I1025 23:04:09.618854 16778 solver.cpp:229] Iteration 14300, loss = 0.160799
I1025 23:04:09.618891 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0744065 (* 1 = 0.0744065 loss)
I1025 23:04:09.618899 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0863926 (* 1 = 0.0863926 loss)
I1025 23:04:09.618906 16778 sgd_solver.cpp:106] Iteration 14300, lr = 0.001
I1025 23:04:18.674101 16778 solver.cpp:229] Iteration 14400, loss = 0.0604269
I1025 23:04:18.674132 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0371473 (* 1 = 0.0371473 loss)
I1025 23:04:18.674141 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0232796 (* 1 = 0.0232796 loss)
I1025 23:04:18.674146 16778 sgd_solver.cpp:106] Iteration 14400, lr = 0.001
I1025 23:04:27.777272 16778 solver.cpp:229] Iteration 14500, loss = 0.165441
I1025 23:04:27.777307 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.126581 (* 1 = 0.126581 loss)
I1025 23:04:27.777314 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0388603 (* 1 = 0.0388603 loss)
I1025 23:04:27.777320 16778 sgd_solver.cpp:106] Iteration 14500, lr = 0.001
I1025 23:04:36.695540 16778 solver.cpp:229] Iteration 14600, loss = 0.0416109
I1025 23:04:36.695574 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0288197 (* 1 = 0.0288197 loss)
I1025 23:04:36.695581 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0127912 (* 1 = 0.0127912 loss)
I1025 23:04:36.695587 16778 sgd_solver.cpp:106] Iteration 14600, lr = 0.001
I1025 23:04:45.788722 16778 solver.cpp:229] Iteration 14700, loss = 0.0767837
I1025 23:04:45.788758 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.020267 (* 1 = 0.020267 loss)
I1025 23:04:45.788766 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0565167 (* 1 = 0.0565167 loss)
I1025 23:04:45.788772 16778 sgd_solver.cpp:106] Iteration 14700, lr = 0.001
I1025 23:04:54.835713 16778 solver.cpp:229] Iteration 14800, loss = 0.0523435
I1025 23:04:54.835747 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0427437 (* 1 = 0.0427437 loss)
I1025 23:04:54.835755 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00959981 (* 1 = 0.00959981 loss)
I1025 23:04:54.835762 16778 sgd_solver.cpp:106] Iteration 14800, lr = 0.001
I1025 23:05:03.665709 16778 solver.cpp:229] Iteration 14900, loss = 0.137687
I1025 23:05:03.665745 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0186678 (* 1 = 0.0186678 loss)
I1025 23:05:03.665752 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.119019 (* 1 = 0.119019 loss)
I1025 23:05:03.665758 16778 sgd_solver.cpp:106] Iteration 14900, lr = 0.001
I1025 23:05:12.852308 16778 solver.cpp:229] Iteration 15000, loss = 0.0422309
I1025 23:05:12.852342 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.01021 (* 1 = 0.01021 loss)
I1025 23:05:12.852350 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0320209 (* 1 = 0.0320209 loss)
I1025 23:05:12.852355 16778 sgd_solver.cpp:106] Iteration 15000, lr = 0.001
I1025 23:05:21.954244 16778 solver.cpp:229] Iteration 15100, loss = 0.0610758
I1025 23:05:21.954278 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0287754 (* 1 = 0.0287754 loss)
I1025 23:05:21.954287 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0323004 (* 1 = 0.0323004 loss)
I1025 23:05:21.954293 16778 sgd_solver.cpp:106] Iteration 15100, lr = 0.001
I1025 23:05:30.908869 16778 solver.cpp:229] Iteration 15200, loss = 0.192376
I1025 23:05:30.908903 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0270958 (* 1 = 0.0270958 loss)
I1025 23:05:30.908911 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.16528 (* 1 = 0.16528 loss)
I1025 23:05:30.908917 16778 sgd_solver.cpp:106] Iteration 15200, lr = 0.001
I1025 23:05:40.085120 16778 solver.cpp:229] Iteration 15300, loss = 0.136825
I1025 23:05:40.085155 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0965572 (* 1 = 0.0965572 loss)
I1025 23:05:40.085163 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0402675 (* 1 = 0.0402675 loss)
I1025 23:05:40.085170 16778 sgd_solver.cpp:106] Iteration 15300, lr = 0.001
I1025 23:05:49.011349 16778 solver.cpp:229] Iteration 15400, loss = 0.10976
I1025 23:05:49.011382 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0470644 (* 1 = 0.0470644 loss)
I1025 23:05:49.011387 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0626957 (* 1 = 0.0626957 loss)
I1025 23:05:49.011391 16778 sgd_solver.cpp:106] Iteration 15400, lr = 0.001
I1025 23:05:57.873253 16778 solver.cpp:229] Iteration 15500, loss = 0.43576
I1025 23:05:57.873291 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0915616 (* 1 = 0.0915616 loss)
I1025 23:05:57.873298 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.344199 (* 1 = 0.344199 loss)
I1025 23:05:57.873304 16778 sgd_solver.cpp:106] Iteration 15500, lr = 0.001
I1025 23:06:06.953690 16778 solver.cpp:229] Iteration 15600, loss = 0.0946162
I1025 23:06:06.953724 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0684983 (* 1 = 0.0684983 loss)
I1025 23:06:06.953732 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0261179 (* 1 = 0.0261179 loss)
I1025 23:06:06.953748 16778 sgd_solver.cpp:106] Iteration 15600, lr = 0.001
I1025 23:06:15.966038 16778 solver.cpp:229] Iteration 15700, loss = 0.0240186
I1025 23:06:15.966071 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0101984 (* 1 = 0.0101984 loss)
I1025 23:06:15.966078 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0138202 (* 1 = 0.0138202 loss)
I1025 23:06:15.966081 16778 sgd_solver.cpp:106] Iteration 15700, lr = 0.001
I1025 23:06:24.965873 16778 solver.cpp:229] Iteration 15800, loss = 0.0600722
I1025 23:06:24.965908 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0462157 (* 1 = 0.0462157 loss)
I1025 23:06:24.965912 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0138565 (* 1 = 0.0138565 loss)
I1025 23:06:24.965917 16778 sgd_solver.cpp:106] Iteration 15800, lr = 0.001
I1025 23:06:34.016363 16778 solver.cpp:229] Iteration 15900, loss = 0.0675057
I1025 23:06:34.016397 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0607335 (* 1 = 0.0607335 loss)
I1025 23:06:34.016403 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0067722 (* 1 = 0.0067722 loss)
I1025 23:06:34.016407 16778 sgd_solver.cpp:106] Iteration 15900, lr = 0.001
I1025 23:06:43.067420 16778 solver.cpp:229] Iteration 16000, loss = 0.0528276
I1025 23:06:43.067457 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0487489 (* 1 = 0.0487489 loss)
I1025 23:06:43.067463 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00407863 (* 1 = 0.00407863 loss)
I1025 23:06:43.067467 16778 sgd_solver.cpp:106] Iteration 16000, lr = 0.001
I1025 23:06:52.155580 16778 solver.cpp:229] Iteration 16100, loss = 0.0950265
I1025 23:06:52.155614 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0601939 (* 1 = 0.0601939 loss)
I1025 23:06:52.155619 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0348326 (* 1 = 0.0348326 loss)
I1025 23:06:52.155624 16778 sgd_solver.cpp:106] Iteration 16100, lr = 0.001
I1025 23:07:01.114507 16778 solver.cpp:229] Iteration 16200, loss = 0.0936116
I1025 23:07:01.114542 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0761279 (* 1 = 0.0761279 loss)
I1025 23:07:01.114547 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0174837 (* 1 = 0.0174837 loss)
I1025 23:07:01.114552 16778 sgd_solver.cpp:106] Iteration 16200, lr = 0.001
I1025 23:07:10.150022 16778 solver.cpp:229] Iteration 16300, loss = 0.0514053
I1025 23:07:10.150055 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0330709 (* 1 = 0.0330709 loss)
I1025 23:07:10.150061 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0183344 (* 1 = 0.0183344 loss)
I1025 23:07:10.150066 16778 sgd_solver.cpp:106] Iteration 16300, lr = 0.001
I1025 23:07:19.160637 16778 solver.cpp:229] Iteration 16400, loss = 0.0788338
I1025 23:07:19.160671 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0293683 (* 1 = 0.0293683 loss)
I1025 23:07:19.160677 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0494656 (* 1 = 0.0494656 loss)
I1025 23:07:19.160681 16778 sgd_solver.cpp:106] Iteration 16400, lr = 0.001
I1025 23:07:28.039924 16778 solver.cpp:229] Iteration 16500, loss = 0.0312082
I1025 23:07:28.039957 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.01901 (* 1 = 0.01901 loss)
I1025 23:07:28.039961 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0121982 (* 1 = 0.0121982 loss)
I1025 23:07:28.039965 16778 sgd_solver.cpp:106] Iteration 16500, lr = 0.001
I1025 23:07:37.140678 16778 solver.cpp:229] Iteration 16600, loss = 0.390039
I1025 23:07:37.140712 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0867155 (* 1 = 0.0867155 loss)
I1025 23:07:37.140718 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.303324 (* 1 = 0.303324 loss)
I1025 23:07:37.140733 16778 sgd_solver.cpp:106] Iteration 16600, lr = 0.001
I1025 23:07:46.175637 16778 solver.cpp:229] Iteration 16700, loss = 0.701252
I1025 23:07:46.175680 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.34521 (* 1 = 0.34521 loss)
I1025 23:07:46.175685 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.356042 (* 1 = 0.356042 loss)
I1025 23:07:46.175700 16778 sgd_solver.cpp:106] Iteration 16700, lr = 0.001
I1025 23:07:55.362869 16778 solver.cpp:229] Iteration 16800, loss = 0.29743
I1025 23:07:55.362902 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.113176 (* 1 = 0.113176 loss)
I1025 23:07:55.362907 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.184254 (* 1 = 0.184254 loss)
I1025 23:07:55.362911 16778 sgd_solver.cpp:106] Iteration 16800, lr = 0.001
I1025 23:08:04.436393 16778 solver.cpp:229] Iteration 16900, loss = 0.0426653
I1025 23:08:04.436431 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0287539 (* 1 = 0.0287539 loss)
I1025 23:08:04.436436 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0139114 (* 1 = 0.0139114 loss)
I1025 23:08:04.436440 16778 sgd_solver.cpp:106] Iteration 16900, lr = 0.001
I1025 23:08:13.489524 16778 solver.cpp:229] Iteration 17000, loss = 0.0581651
I1025 23:08:13.489557 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0489858 (* 1 = 0.0489858 loss)
I1025 23:08:13.489562 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00917926 (* 1 = 0.00917926 loss)
I1025 23:08:13.489567 16778 sgd_solver.cpp:106] Iteration 17000, lr = 0.001
I1025 23:08:22.399812 16778 solver.cpp:229] Iteration 17100, loss = 0.0895132
I1025 23:08:22.399845 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0317481 (* 1 = 0.0317481 loss)
I1025 23:08:22.399850 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.057765 (* 1 = 0.057765 loss)
I1025 23:08:22.399854 16778 sgd_solver.cpp:106] Iteration 17100, lr = 0.001
I1025 23:08:31.446221 16778 solver.cpp:229] Iteration 17200, loss = 0.11602
I1025 23:08:31.446255 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0252943 (* 1 = 0.0252943 loss)
I1025 23:08:31.446260 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0907257 (* 1 = 0.0907257 loss)
I1025 23:08:31.446264 16778 sgd_solver.cpp:106] Iteration 17200, lr = 0.001
I1025 23:08:40.454910 16778 solver.cpp:229] Iteration 17300, loss = 0.0866987
I1025 23:08:40.454941 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0558797 (* 1 = 0.0558797 loss)
I1025 23:08:40.454946 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.030819 (* 1 = 0.030819 loss)
I1025 23:08:40.454951 16778 sgd_solver.cpp:106] Iteration 17300, lr = 0.001
I1025 23:08:49.505908 16778 solver.cpp:229] Iteration 17400, loss = 0.069063
I1025 23:08:49.505940 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0420134 (* 1 = 0.0420134 loss)
I1025 23:08:49.505945 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0270496 (* 1 = 0.0270496 loss)
I1025 23:08:49.505949 16778 sgd_solver.cpp:106] Iteration 17400, lr = 0.001
I1025 23:08:58.527221 16778 solver.cpp:229] Iteration 17500, loss = 0.0657334
I1025 23:08:58.527254 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0499069 (* 1 = 0.0499069 loss)
I1025 23:08:58.527259 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0158265 (* 1 = 0.0158265 loss)
I1025 23:08:58.527263 16778 sgd_solver.cpp:106] Iteration 17500, lr = 0.001
I1025 23:09:07.614475 16778 solver.cpp:229] Iteration 17600, loss = 0.0444182
I1025 23:09:07.614507 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00364247 (* 1 = 0.00364247 loss)
I1025 23:09:07.614512 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0407757 (* 1 = 0.0407757 loss)
I1025 23:09:07.614516 16778 sgd_solver.cpp:106] Iteration 17600, lr = 0.001
I1025 23:09:16.608896 16778 solver.cpp:229] Iteration 17700, loss = 0.01894
I1025 23:09:16.608944 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00682775 (* 1 = 0.00682775 loss)
I1025 23:09:16.608949 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0121122 (* 1 = 0.0121122 loss)
I1025 23:09:16.608965 16778 sgd_solver.cpp:106] Iteration 17700, lr = 0.001
I1025 23:09:25.555984 16778 solver.cpp:229] Iteration 17800, loss = 0.0256052
I1025 23:09:25.556016 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0131912 (* 1 = 0.0131912 loss)
I1025 23:09:25.556021 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.012414 (* 1 = 0.012414 loss)
I1025 23:09:25.556027 16778 sgd_solver.cpp:106] Iteration 17800, lr = 0.001
I1025 23:09:34.683456 16778 solver.cpp:229] Iteration 17900, loss = 0.0110221
I1025 23:09:34.683486 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00413754 (* 1 = 0.00413754 loss)
I1025 23:09:34.683495 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00688454 (* 1 = 0.00688454 loss)
I1025 23:09:34.683501 16778 sgd_solver.cpp:106] Iteration 17900, lr = 0.001
I1025 23:09:43.778453 16778 solver.cpp:229] Iteration 18000, loss = 0.0720486
I1025 23:09:43.778489 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0409751 (* 1 = 0.0409751 loss)
I1025 23:09:43.778497 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0310735 (* 1 = 0.0310735 loss)
I1025 23:09:43.778503 16778 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I1025 23:09:52.840559 16778 solver.cpp:229] Iteration 18100, loss = 0.0280323
I1025 23:09:52.840592 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0216311 (* 1 = 0.0216311 loss)
I1025 23:09:52.840597 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00640116 (* 1 = 0.00640116 loss)
I1025 23:09:52.840602 16778 sgd_solver.cpp:106] Iteration 18100, lr = 0.001
I1025 23:10:01.981464 16778 solver.cpp:229] Iteration 18200, loss = 0.183352
I1025 23:10:01.981498 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.102724 (* 1 = 0.102724 loss)
I1025 23:10:01.981504 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.080628 (* 1 = 0.080628 loss)
I1025 23:10:01.981508 16778 sgd_solver.cpp:106] Iteration 18200, lr = 0.001
I1025 23:10:11.051200 16778 solver.cpp:229] Iteration 18300, loss = 0.051126
I1025 23:10:11.051234 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0343792 (* 1 = 0.0343792 loss)
I1025 23:10:11.051239 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0167468 (* 1 = 0.0167468 loss)
I1025 23:10:11.051244 16778 sgd_solver.cpp:106] Iteration 18300, lr = 0.001
I1025 23:10:20.097620 16778 solver.cpp:229] Iteration 18400, loss = 0.138631
I1025 23:10:20.097653 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0438999 (* 1 = 0.0438999 loss)
I1025 23:10:20.097659 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0947313 (* 1 = 0.0947313 loss)
I1025 23:10:20.097663 16778 sgd_solver.cpp:106] Iteration 18400, lr = 0.001
I1025 23:10:29.182878 16778 solver.cpp:229] Iteration 18500, loss = 0.0685122
I1025 23:10:29.182910 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.025821 (* 1 = 0.025821 loss)
I1025 23:10:29.182915 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0426912 (* 1 = 0.0426912 loss)
I1025 23:10:29.182919 16778 sgd_solver.cpp:106] Iteration 18500, lr = 0.001
I1025 23:10:38.213328 16778 solver.cpp:229] Iteration 18600, loss = 0.0638224
I1025 23:10:38.213361 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0462994 (* 1 = 0.0462994 loss)
I1025 23:10:38.213367 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.017523 (* 1 = 0.017523 loss)
I1025 23:10:38.213371 16778 sgd_solver.cpp:106] Iteration 18600, lr = 0.001
I1025 23:10:47.321712 16778 solver.cpp:229] Iteration 18700, loss = 0.758283
I1025 23:10:47.321745 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0924771 (* 1 = 0.0924771 loss)
I1025 23:10:47.321750 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.665806 (* 1 = 0.665806 loss)
I1025 23:10:47.321755 16778 sgd_solver.cpp:106] Iteration 18700, lr = 0.001
I1025 23:10:56.330325 16778 solver.cpp:229] Iteration 18800, loss = 0.056603
I1025 23:10:56.330359 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0172741 (* 1 = 0.0172741 loss)
I1025 23:10:56.330365 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0393289 (* 1 = 0.0393289 loss)
I1025 23:10:56.330369 16778 sgd_solver.cpp:106] Iteration 18800, lr = 0.001
I1025 23:11:05.468498 16778 solver.cpp:229] Iteration 18900, loss = 0.219802
I1025 23:11:05.468534 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0334843 (* 1 = 0.0334843 loss)
I1025 23:11:05.468540 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.186318 (* 1 = 0.186318 loss)
I1025 23:11:05.468545 16778 sgd_solver.cpp:106] Iteration 18900, lr = 0.001
I1025 23:11:14.661916 16778 solver.cpp:229] Iteration 19000, loss = 0.0268817
I1025 23:11:14.661947 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0168815 (* 1 = 0.0168815 loss)
I1025 23:11:14.661952 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0100002 (* 1 = 0.0100002 loss)
I1025 23:11:14.661957 16778 sgd_solver.cpp:106] Iteration 19000, lr = 0.001
I1025 23:11:23.629678 16778 solver.cpp:229] Iteration 19100, loss = 0.0183327
I1025 23:11:23.629712 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00874816 (* 1 = 0.00874816 loss)
I1025 23:11:23.629717 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00958451 (* 1 = 0.00958451 loss)
I1025 23:11:23.629722 16778 sgd_solver.cpp:106] Iteration 19100, lr = 0.001
I1025 23:11:32.642139 16778 solver.cpp:229] Iteration 19200, loss = 0.182815
I1025 23:11:32.642169 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0261684 (* 1 = 0.0261684 loss)
I1025 23:11:32.642175 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.156646 (* 1 = 0.156646 loss)
I1025 23:11:32.642180 16778 sgd_solver.cpp:106] Iteration 19200, lr = 0.001
I1025 23:11:41.580516 16778 solver.cpp:229] Iteration 19300, loss = 0.109639
I1025 23:11:41.580550 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0942513 (* 1 = 0.0942513 loss)
I1025 23:11:41.580556 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0153876 (* 1 = 0.0153876 loss)
I1025 23:11:41.580560 16778 sgd_solver.cpp:106] Iteration 19300, lr = 0.001
I1025 23:11:50.623985 16778 solver.cpp:229] Iteration 19400, loss = 0.441643
I1025 23:11:50.624017 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0689021 (* 1 = 0.0689021 loss)
I1025 23:11:50.624022 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.372741 (* 1 = 0.372741 loss)
I1025 23:11:50.624025 16778 sgd_solver.cpp:106] Iteration 19400, lr = 0.001
I1025 23:11:59.766842 16778 solver.cpp:229] Iteration 19500, loss = 0.0385012
I1025 23:11:59.766875 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0237813 (* 1 = 0.0237813 loss)
I1025 23:11:59.766883 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.01472 (* 1 = 0.01472 loss)
I1025 23:11:59.766890 16778 sgd_solver.cpp:106] Iteration 19500, lr = 0.001
I1025 23:12:08.764879 16778 solver.cpp:229] Iteration 19600, loss = 0.0949193
I1025 23:12:08.764916 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0376756 (* 1 = 0.0376756 loss)
I1025 23:12:08.764924 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0572437 (* 1 = 0.0572437 loss)
I1025 23:12:08.764940 16778 sgd_solver.cpp:106] Iteration 19600, lr = 0.001
I1025 23:12:17.895027 16778 solver.cpp:229] Iteration 19700, loss = 0.119491
I1025 23:12:17.895073 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.043652 (* 1 = 0.043652 loss)
I1025 23:12:17.895090 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0758385 (* 1 = 0.0758385 loss)
I1025 23:12:17.895107 16778 sgd_solver.cpp:106] Iteration 19700, lr = 0.001
I1025 23:12:26.975606 16778 solver.cpp:229] Iteration 19800, loss = 0.0629787
I1025 23:12:26.975644 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0436782 (* 1 = 0.0436782 loss)
I1025 23:12:26.975652 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0193005 (* 1 = 0.0193005 loss)
I1025 23:12:26.975658 16778 sgd_solver.cpp:106] Iteration 19800, lr = 0.001
I1025 23:12:36.023053 16778 solver.cpp:229] Iteration 19900, loss = 0.551731
I1025 23:12:36.023087 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.158254 (* 1 = 0.158254 loss)
I1025 23:12:36.023095 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.393477 (* 1 = 0.393477 loss)
I1025 23:12:36.023111 16778 sgd_solver.cpp:106] Iteration 19900, lr = 0.001
I1025 23:12:45.476488 16778 solver.cpp:229] Iteration 20000, loss = 0.0856948
I1025 23:12:45.476521 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00157099 (* 1 = 0.00157099 loss)
I1025 23:12:45.476526 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0841238 (* 1 = 0.0841238 loss)
I1025 23:12:45.476531 16778 sgd_solver.cpp:106] Iteration 20000, lr = 0.001
I1025 23:12:54.416741 16778 solver.cpp:229] Iteration 20100, loss = 0.0231428
I1025 23:12:54.416772 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0129658 (* 1 = 0.0129658 loss)
I1025 23:12:54.416777 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.010177 (* 1 = 0.010177 loss)
I1025 23:12:54.416781 16778 sgd_solver.cpp:106] Iteration 20100, lr = 0.001
I1025 23:13:03.570025 16778 solver.cpp:229] Iteration 20200, loss = 0.0345893
I1025 23:13:03.570058 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0246628 (* 1 = 0.0246628 loss)
I1025 23:13:03.570063 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00992644 (* 1 = 0.00992644 loss)
I1025 23:13:03.570068 16778 sgd_solver.cpp:106] Iteration 20200, lr = 0.001
I1025 23:13:12.616140 16778 solver.cpp:229] Iteration 20300, loss = 0.0213511
I1025 23:13:12.616175 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0121742 (* 1 = 0.0121742 loss)
I1025 23:13:12.616181 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00917696 (* 1 = 0.00917696 loss)
I1025 23:13:12.616197 16778 sgd_solver.cpp:106] Iteration 20300, lr = 0.001
I1025 23:13:21.772315 16778 solver.cpp:229] Iteration 20400, loss = 0.0774643
I1025 23:13:21.772357 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0512271 (* 1 = 0.0512271 loss)
I1025 23:13:21.772361 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0262371 (* 1 = 0.0262371 loss)
I1025 23:13:21.772366 16778 sgd_solver.cpp:106] Iteration 20400, lr = 0.001
I1025 23:13:30.932490 16778 solver.cpp:229] Iteration 20500, loss = 0.0971417
I1025 23:13:30.932519 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0147782 (* 1 = 0.0147782 loss)
I1025 23:13:30.932524 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0823636 (* 1 = 0.0823636 loss)
I1025 23:13:30.932529 16778 sgd_solver.cpp:106] Iteration 20500, lr = 0.001
I1025 23:13:40.049031 16778 solver.cpp:229] Iteration 20600, loss = 0.0943228
I1025 23:13:40.049065 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0771272 (* 1 = 0.0771272 loss)
I1025 23:13:40.049069 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0171955 (* 1 = 0.0171955 loss)
I1025 23:13:40.049074 16778 sgd_solver.cpp:106] Iteration 20600, lr = 0.001
I1025 23:13:49.218828 16778 solver.cpp:229] Iteration 20700, loss = 0.170581
I1025 23:13:49.218860 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0176445 (* 1 = 0.0176445 loss)
I1025 23:13:49.218865 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.152936 (* 1 = 0.152936 loss)
I1025 23:13:49.218869 16778 sgd_solver.cpp:106] Iteration 20700, lr = 0.001
I1025 23:13:58.292206 16778 solver.cpp:229] Iteration 20800, loss = 0.128935
I1025 23:13:58.292238 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0143293 (* 1 = 0.0143293 loss)
I1025 23:13:58.292243 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.114606 (* 1 = 0.114606 loss)
I1025 23:13:58.292248 16778 sgd_solver.cpp:106] Iteration 20800, lr = 0.001
I1025 23:14:07.353148 16778 solver.cpp:229] Iteration 20900, loss = 0.0465414
I1025 23:14:07.353180 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0307982 (* 1 = 0.0307982 loss)
I1025 23:14:07.353185 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0157433 (* 1 = 0.0157433 loss)
I1025 23:14:07.353190 16778 sgd_solver.cpp:106] Iteration 20900, lr = 0.001
I1025 23:14:16.302965 16778 solver.cpp:229] Iteration 21000, loss = 0.103746
I1025 23:14:16.302999 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0113671 (* 1 = 0.0113671 loss)
I1025 23:14:16.303004 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0923793 (* 1 = 0.0923793 loss)
I1025 23:14:16.303006 16778 sgd_solver.cpp:106] Iteration 21000, lr = 0.001
I1025 23:14:25.262176 16778 solver.cpp:229] Iteration 21100, loss = 0.0583863
I1025 23:14:25.262209 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00762413 (* 1 = 0.00762413 loss)
I1025 23:14:25.262214 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0507621 (* 1 = 0.0507621 loss)
I1025 23:14:25.262218 16778 sgd_solver.cpp:106] Iteration 21100, lr = 0.001
I1025 23:14:34.233999 16778 solver.cpp:229] Iteration 21200, loss = 0.046877
I1025 23:14:34.234032 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.041999 (* 1 = 0.041999 loss)
I1025 23:14:34.234036 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00487806 (* 1 = 0.00487806 loss)
I1025 23:14:34.234041 16778 sgd_solver.cpp:106] Iteration 21200, lr = 0.001
I1025 23:14:43.141882 16778 solver.cpp:229] Iteration 21300, loss = 0.0248771
I1025 23:14:43.141916 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.020101 (* 1 = 0.020101 loss)
I1025 23:14:43.141919 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00477611 (* 1 = 0.00477611 loss)
I1025 23:14:43.141923 16778 sgd_solver.cpp:106] Iteration 21300, lr = 0.001
I1025 23:14:52.314010 16778 solver.cpp:229] Iteration 21400, loss = 0.134061
I1025 23:14:52.314043 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0172056 (* 1 = 0.0172056 loss)
I1025 23:14:52.314049 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.116855 (* 1 = 0.116855 loss)
I1025 23:14:52.314054 16778 sgd_solver.cpp:106] Iteration 21400, lr = 0.001
I1025 23:15:01.361702 16778 solver.cpp:229] Iteration 21500, loss = 0.0283336
I1025 23:15:01.361737 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00195232 (* 1 = 0.00195232 loss)
I1025 23:15:01.361742 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0263813 (* 1 = 0.0263813 loss)
I1025 23:15:01.361745 16778 sgd_solver.cpp:106] Iteration 21500, lr = 0.001
I1025 23:15:10.542266 16778 solver.cpp:229] Iteration 21600, loss = 0.0529345
I1025 23:15:10.542299 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0184858 (* 1 = 0.0184858 loss)
I1025 23:15:10.542304 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0344487 (* 1 = 0.0344487 loss)
I1025 23:15:10.542307 16778 sgd_solver.cpp:106] Iteration 21600, lr = 0.001
I1025 23:15:19.760485 16778 solver.cpp:229] Iteration 21700, loss = 0.572321
I1025 23:15:19.760519 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0863751 (* 1 = 0.0863751 loss)
I1025 23:15:19.760524 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.485946 (* 1 = 0.485946 loss)
I1025 23:15:19.760529 16778 sgd_solver.cpp:106] Iteration 21700, lr = 0.001
I1025 23:15:28.828258 16778 solver.cpp:229] Iteration 21800, loss = 0.104679
I1025 23:15:28.828292 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.050401 (* 1 = 0.050401 loss)
I1025 23:15:28.828299 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0542781 (* 1 = 0.0542781 loss)
I1025 23:15:28.828304 16778 sgd_solver.cpp:106] Iteration 21800, lr = 0.001
I1025 23:15:38.037052 16778 solver.cpp:229] Iteration 21900, loss = 0.0839085
I1025 23:15:38.037087 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0512024 (* 1 = 0.0512024 loss)
I1025 23:15:38.037092 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0327062 (* 1 = 0.0327062 loss)
I1025 23:15:38.037097 16778 sgd_solver.cpp:106] Iteration 21900, lr = 0.001
I1025 23:15:47.110932 16778 solver.cpp:229] Iteration 22000, loss = 0.0882853
I1025 23:15:47.110965 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00511678 (* 1 = 0.00511678 loss)
I1025 23:15:47.110971 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0831686 (* 1 = 0.0831686 loss)
I1025 23:15:47.110975 16778 sgd_solver.cpp:106] Iteration 22000, lr = 0.001
I1025 23:15:56.217993 16778 solver.cpp:229] Iteration 22100, loss = 0.0361221
I1025 23:15:56.218026 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0124578 (* 1 = 0.0124578 loss)
I1025 23:15:56.218031 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0236643 (* 1 = 0.0236643 loss)
I1025 23:15:56.218036 16778 sgd_solver.cpp:106] Iteration 22100, lr = 0.001
I1025 23:16:05.254050 16778 solver.cpp:229] Iteration 22200, loss = 0.0493896
I1025 23:16:05.254084 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0296154 (* 1 = 0.0296154 loss)
I1025 23:16:05.254089 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0197742 (* 1 = 0.0197742 loss)
I1025 23:16:05.254093 16778 sgd_solver.cpp:106] Iteration 22200, lr = 0.001
I1025 23:16:14.188894 16778 solver.cpp:229] Iteration 22300, loss = 0.131447
I1025 23:16:14.188926 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0811736 (* 1 = 0.0811736 loss)
I1025 23:16:14.188931 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0502731 (* 1 = 0.0502731 loss)
I1025 23:16:14.188935 16778 sgd_solver.cpp:106] Iteration 22300, lr = 0.001
I1025 23:16:23.162462 16778 solver.cpp:229] Iteration 22400, loss = 0.0548855
I1025 23:16:23.162495 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0399578 (* 1 = 0.0399578 loss)
I1025 23:16:23.162518 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0149277 (* 1 = 0.0149277 loss)
I1025 23:16:23.162521 16778 sgd_solver.cpp:106] Iteration 22400, lr = 0.001
I1025 23:16:32.336735 16778 solver.cpp:229] Iteration 22500, loss = 0.0869724
I1025 23:16:32.336771 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0213242 (* 1 = 0.0213242 loss)
I1025 23:16:32.336776 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0656482 (* 1 = 0.0656482 loss)
I1025 23:16:32.336781 16778 sgd_solver.cpp:106] Iteration 22500, lr = 0.001
I1025 23:16:41.344480 16778 solver.cpp:229] Iteration 22600, loss = 0.0298515
I1025 23:16:41.344514 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0186323 (* 1 = 0.0186323 loss)
I1025 23:16:41.344519 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0112192 (* 1 = 0.0112192 loss)
I1025 23:16:41.344524 16778 sgd_solver.cpp:106] Iteration 22600, lr = 0.001
I1025 23:16:50.434700 16778 solver.cpp:229] Iteration 22700, loss = 0.22168
I1025 23:16:50.434732 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.102984 (* 1 = 0.102984 loss)
I1025 23:16:50.434738 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.118695 (* 1 = 0.118695 loss)
I1025 23:16:50.434742 16778 sgd_solver.cpp:106] Iteration 22700, lr = 0.001
I1025 23:16:59.432505 16778 solver.cpp:229] Iteration 22800, loss = 0.059219
I1025 23:16:59.432538 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0280101 (* 1 = 0.0280101 loss)
I1025 23:16:59.432544 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0312089 (* 1 = 0.0312089 loss)
I1025 23:16:59.432548 16778 sgd_solver.cpp:106] Iteration 22800, lr = 0.001
I1025 23:17:08.637701 16778 solver.cpp:229] Iteration 22900, loss = 0.0322338
I1025 23:17:08.637732 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00428707 (* 1 = 0.00428707 loss)
I1025 23:17:08.637737 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0279467 (* 1 = 0.0279467 loss)
I1025 23:17:08.637742 16778 sgd_solver.cpp:106] Iteration 22900, lr = 0.001
I1025 23:17:17.797508 16778 solver.cpp:229] Iteration 23000, loss = 0.105635
I1025 23:17:17.797549 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0864437 (* 1 = 0.0864437 loss)
I1025 23:17:17.797554 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0191911 (* 1 = 0.0191911 loss)
I1025 23:17:17.797557 16778 sgd_solver.cpp:106] Iteration 23000, lr = 0.001
I1025 23:17:26.752868 16778 solver.cpp:229] Iteration 23100, loss = 0.0146796
I1025 23:17:26.752900 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0111964 (* 1 = 0.0111964 loss)
I1025 23:17:26.752905 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0034832 (* 1 = 0.0034832 loss)
I1025 23:17:26.752910 16778 sgd_solver.cpp:106] Iteration 23100, lr = 0.001
I1025 23:17:35.692526 16778 solver.cpp:229] Iteration 23200, loss = 0.0584328
I1025 23:17:35.692559 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0560684 (* 1 = 0.0560684 loss)
I1025 23:17:35.692564 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00236443 (* 1 = 0.00236443 loss)
I1025 23:17:35.692569 16778 sgd_solver.cpp:106] Iteration 23200, lr = 0.001
I1025 23:17:44.841349 16778 solver.cpp:229] Iteration 23300, loss = 0.0321214
I1025 23:17:44.841382 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00558384 (* 1 = 0.00558384 loss)
I1025 23:17:44.841389 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0265375 (* 1 = 0.0265375 loss)
I1025 23:17:44.841394 16778 sgd_solver.cpp:106] Iteration 23300, lr = 0.001
I1025 23:17:53.830052 16778 solver.cpp:229] Iteration 23400, loss = 0.0782627
I1025 23:17:53.830088 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0331251 (* 1 = 0.0331251 loss)
I1025 23:17:53.830096 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0451376 (* 1 = 0.0451376 loss)
I1025 23:17:53.830101 16778 sgd_solver.cpp:106] Iteration 23400, lr = 0.001
I1025 23:18:02.828788 16778 solver.cpp:229] Iteration 23500, loss = 0.04284
I1025 23:18:02.828821 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0114582 (* 1 = 0.0114582 loss)
I1025 23:18:02.828827 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0313818 (* 1 = 0.0313818 loss)
I1025 23:18:02.828831 16778 sgd_solver.cpp:106] Iteration 23500, lr = 0.001
I1025 23:18:11.936534 16778 solver.cpp:229] Iteration 23600, loss = 0.0281535
I1025 23:18:11.936568 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0134682 (* 1 = 0.0134682 loss)
I1025 23:18:11.936573 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0146852 (* 1 = 0.0146852 loss)
I1025 23:18:11.936578 16778 sgd_solver.cpp:106] Iteration 23600, lr = 0.001
I1025 23:18:20.901963 16778 solver.cpp:229] Iteration 23700, loss = 0.0181223
I1025 23:18:20.902006 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00934302 (* 1 = 0.00934302 loss)
I1025 23:18:20.902011 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00877932 (* 1 = 0.00877932 loss)
I1025 23:18:20.902014 16778 sgd_solver.cpp:106] Iteration 23700, lr = 0.001
I1025 23:18:29.891705 16778 solver.cpp:229] Iteration 23800, loss = 0.138549
I1025 23:18:29.891736 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0237542 (* 1 = 0.0237542 loss)
I1025 23:18:29.891741 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.114795 (* 1 = 0.114795 loss)
I1025 23:18:29.891746 16778 sgd_solver.cpp:106] Iteration 23800, lr = 0.001
I1025 23:18:39.056912 16778 solver.cpp:229] Iteration 23900, loss = 0.0563294
I1025 23:18:39.056944 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0466781 (* 1 = 0.0466781 loss)
I1025 23:18:39.056949 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00965132 (* 1 = 0.00965132 loss)
I1025 23:18:39.056953 16778 sgd_solver.cpp:106] Iteration 23900, lr = 0.001
I1025 23:18:48.150866 16778 solver.cpp:229] Iteration 24000, loss = 0.611121
I1025 23:18:48.150900 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.049759 (* 1 = 0.049759 loss)
I1025 23:18:48.150905 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.561362 (* 1 = 0.561362 loss)
I1025 23:18:48.150909 16778 sgd_solver.cpp:106] Iteration 24000, lr = 0.001
I1025 23:18:57.176903 16778 solver.cpp:229] Iteration 24100, loss = 0.0493747
I1025 23:18:57.176935 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0319829 (* 1 = 0.0319829 loss)
I1025 23:18:57.176940 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0173918 (* 1 = 0.0173918 loss)
I1025 23:18:57.176945 16778 sgd_solver.cpp:106] Iteration 24100, lr = 0.001
I1025 23:19:06.205657 16778 solver.cpp:229] Iteration 24200, loss = 0.0235361
I1025 23:19:06.205688 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00149537 (* 1 = 0.00149537 loss)
I1025 23:19:06.205693 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0220407 (* 1 = 0.0220407 loss)
I1025 23:19:06.205708 16778 sgd_solver.cpp:106] Iteration 24200, lr = 0.001
I1025 23:19:15.183930 16778 solver.cpp:229] Iteration 24300, loss = 0.0975355
I1025 23:19:15.183959 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0323773 (* 1 = 0.0323773 loss)
I1025 23:19:15.183964 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0651582 (* 1 = 0.0651582 loss)
I1025 23:19:15.183967 16778 sgd_solver.cpp:106] Iteration 24300, lr = 0.001
I1025 23:19:24.170589 16778 solver.cpp:229] Iteration 24400, loss = 0.0738823
I1025 23:19:24.170620 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0149954 (* 1 = 0.0149954 loss)
I1025 23:19:24.170625 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0588869 (* 1 = 0.0588869 loss)
I1025 23:19:24.170629 16778 sgd_solver.cpp:106] Iteration 24400, lr = 0.001
I1025 23:19:33.223227 16778 solver.cpp:229] Iteration 24500, loss = 0.0285244
I1025 23:19:33.223259 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00921552 (* 1 = 0.00921552 loss)
I1025 23:19:33.223263 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0193089 (* 1 = 0.0193089 loss)
I1025 23:19:33.223268 16778 sgd_solver.cpp:106] Iteration 24500, lr = 0.001
I1025 23:19:42.301303 16778 solver.cpp:229] Iteration 24600, loss = 0.0667164
I1025 23:19:42.301337 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0263512 (* 1 = 0.0263512 loss)
I1025 23:19:42.301340 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0403652 (* 1 = 0.0403652 loss)
I1025 23:19:42.301345 16778 sgd_solver.cpp:106] Iteration 24600, lr = 0.001
I1025 23:19:51.455457 16778 solver.cpp:229] Iteration 24700, loss = 0.232553
I1025 23:19:51.455489 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0402213 (* 1 = 0.0402213 loss)
I1025 23:19:51.455493 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.192332 (* 1 = 0.192332 loss)
I1025 23:19:51.455497 16778 sgd_solver.cpp:106] Iteration 24700, lr = 0.001
I1025 23:20:00.374289 16778 solver.cpp:229] Iteration 24800, loss = 0.0124359
I1025 23:20:00.374320 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0072069 (* 1 = 0.0072069 loss)
I1025 23:20:00.374325 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00522901 (* 1 = 0.00522901 loss)
I1025 23:20:00.374328 16778 sgd_solver.cpp:106] Iteration 24800, lr = 0.001
I1025 23:20:09.314178 16778 solver.cpp:229] Iteration 24900, loss = 0.0344107
I1025 23:20:09.314211 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00711468 (* 1 = 0.00711468 loss)
I1025 23:20:09.314215 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.027296 (* 1 = 0.027296 loss)
I1025 23:20:09.314220 16778 sgd_solver.cpp:106] Iteration 24900, lr = 0.001
I1025 23:20:18.366325 16778 solver.cpp:229] Iteration 25000, loss = 0.0562704
I1025 23:20:18.366358 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0156135 (* 1 = 0.0156135 loss)
I1025 23:20:18.366363 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0406569 (* 1 = 0.0406569 loss)
I1025 23:20:18.366366 16778 sgd_solver.cpp:106] Iteration 25000, lr = 0.001
I1025 23:20:27.499352 16778 solver.cpp:229] Iteration 25100, loss = 0.01126
I1025 23:20:27.499385 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00881849 (* 1 = 0.00881849 loss)
I1025 23:20:27.499390 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0024415 (* 1 = 0.0024415 loss)
I1025 23:20:27.499394 16778 sgd_solver.cpp:106] Iteration 25100, lr = 0.001
I1025 23:20:36.567744 16778 solver.cpp:229] Iteration 25200, loss = 0.0380732
I1025 23:20:36.567775 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0142448 (* 1 = 0.0142448 loss)
I1025 23:20:36.567780 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0238284 (* 1 = 0.0238284 loss)
I1025 23:20:36.567785 16778 sgd_solver.cpp:106] Iteration 25200, lr = 0.001
I1025 23:20:45.535531 16778 solver.cpp:229] Iteration 25300, loss = 0.0771434
I1025 23:20:45.535564 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.046377 (* 1 = 0.046377 loss)
I1025 23:20:45.535569 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0307665 (* 1 = 0.0307665 loss)
I1025 23:20:45.535573 16778 sgd_solver.cpp:106] Iteration 25300, lr = 0.001
I1025 23:20:54.659529 16778 solver.cpp:229] Iteration 25400, loss = 0.0423134
I1025 23:20:54.659559 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0102777 (* 1 = 0.0102777 loss)
I1025 23:20:54.659564 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0320356 (* 1 = 0.0320356 loss)
I1025 23:20:54.659567 16778 sgd_solver.cpp:106] Iteration 25400, lr = 0.001
I1025 23:21:03.828490 16778 solver.cpp:229] Iteration 25500, loss = 0.047132
I1025 23:21:03.828523 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0419107 (* 1 = 0.0419107 loss)
I1025 23:21:03.828528 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00522133 (* 1 = 0.00522133 loss)
I1025 23:21:03.828542 16778 sgd_solver.cpp:106] Iteration 25500, lr = 0.001
I1025 23:21:12.933578 16778 solver.cpp:229] Iteration 25600, loss = 0.0205979
I1025 23:21:12.933609 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00817871 (* 1 = 0.00817871 loss)
I1025 23:21:12.933615 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0124192 (* 1 = 0.0124192 loss)
I1025 23:21:12.933619 16778 sgd_solver.cpp:106] Iteration 25600, lr = 0.001
I1025 23:21:22.062952 16778 solver.cpp:229] Iteration 25700, loss = 0.0153214
I1025 23:21:22.062985 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00254744 (* 1 = 0.00254744 loss)
I1025 23:21:22.062990 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0127739 (* 1 = 0.0127739 loss)
I1025 23:21:22.062994 16778 sgd_solver.cpp:106] Iteration 25700, lr = 0.001
I1025 23:21:31.060022 16778 solver.cpp:229] Iteration 25800, loss = 0.00560018
I1025 23:21:31.060055 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00177321 (* 1 = 0.00177321 loss)
I1025 23:21:31.060060 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00382696 (* 1 = 0.00382696 loss)
I1025 23:21:31.060075 16778 sgd_solver.cpp:106] Iteration 25800, lr = 0.001
I1025 23:21:40.130038 16778 solver.cpp:229] Iteration 25900, loss = 0.133365
I1025 23:21:40.130080 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0549913 (* 1 = 0.0549913 loss)
I1025 23:21:40.130085 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0783738 (* 1 = 0.0783738 loss)
I1025 23:21:40.130089 16778 sgd_solver.cpp:106] Iteration 25900, lr = 0.001
I1025 23:21:49.031435 16778 solver.cpp:229] Iteration 26000, loss = 0.0239531
I1025 23:21:49.031467 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.014138 (* 1 = 0.014138 loss)
I1025 23:21:49.031474 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00981519 (* 1 = 0.00981519 loss)
I1025 23:21:49.031479 16778 sgd_solver.cpp:106] Iteration 26000, lr = 0.001
I1025 23:21:58.102995 16778 solver.cpp:229] Iteration 26100, loss = 0.114359
I1025 23:21:58.103029 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0708707 (* 1 = 0.0708707 loss)
I1025 23:21:58.103036 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0434887 (* 1 = 0.0434887 loss)
I1025 23:21:58.103042 16778 sgd_solver.cpp:106] Iteration 26100, lr = 0.001
I1025 23:22:07.230319 16778 solver.cpp:229] Iteration 26200, loss = 0.0102287
I1025 23:22:07.230355 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00138008 (* 1 = 0.00138008 loss)
I1025 23:22:07.230361 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00884862 (* 1 = 0.00884862 loss)
I1025 23:22:07.230366 16778 sgd_solver.cpp:106] Iteration 26200, lr = 0.001
I1025 23:22:16.271086 16778 solver.cpp:229] Iteration 26300, loss = 0.0116963
I1025 23:22:16.271121 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0083239 (* 1 = 0.0083239 loss)
I1025 23:22:16.271128 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00337243 (* 1 = 0.00337243 loss)
I1025 23:22:16.271133 16778 sgd_solver.cpp:106] Iteration 26300, lr = 0.001
I1025 23:22:25.382943 16778 solver.cpp:229] Iteration 26400, loss = 0.108854
I1025 23:22:25.382977 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0436501 (* 1 = 0.0436501 loss)
I1025 23:22:25.382983 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0652039 (* 1 = 0.0652039 loss)
I1025 23:22:25.382988 16778 sgd_solver.cpp:106] Iteration 26400, lr = 0.001
I1025 23:22:34.293830 16778 solver.cpp:229] Iteration 26500, loss = 0.0294345
I1025 23:22:34.293864 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00370438 (* 1 = 0.00370438 loss)
I1025 23:22:34.293871 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0257301 (* 1 = 0.0257301 loss)
I1025 23:22:34.293876 16778 sgd_solver.cpp:106] Iteration 26500, lr = 0.001
I1025 23:22:43.252018 16778 solver.cpp:229] Iteration 26600, loss = 0.0310541
I1025 23:22:43.252053 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00236931 (* 1 = 0.00236931 loss)
I1025 23:22:43.252059 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0286848 (* 1 = 0.0286848 loss)
I1025 23:22:43.252064 16778 sgd_solver.cpp:106] Iteration 26600, lr = 0.001
I1025 23:22:52.442215 16778 solver.cpp:229] Iteration 26700, loss = 0.0473612
I1025 23:22:52.442250 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0129933 (* 1 = 0.0129933 loss)
I1025 23:22:52.442256 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0343679 (* 1 = 0.0343679 loss)
I1025 23:22:52.442261 16778 sgd_solver.cpp:106] Iteration 26700, lr = 0.001
I1025 23:23:01.535974 16778 solver.cpp:229] Iteration 26800, loss = 0.0537064
I1025 23:23:01.536008 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0202318 (* 1 = 0.0202318 loss)
I1025 23:23:01.536015 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0334746 (* 1 = 0.0334746 loss)
I1025 23:23:01.536021 16778 sgd_solver.cpp:106] Iteration 26800, lr = 0.001
I1025 23:23:10.608147 16778 solver.cpp:229] Iteration 26900, loss = 0.0493959
I1025 23:23:10.608181 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0358126 (* 1 = 0.0358126 loss)
I1025 23:23:10.608188 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0135833 (* 1 = 0.0135833 loss)
I1025 23:23:10.608193 16778 sgd_solver.cpp:106] Iteration 26900, lr = 0.001
I1025 23:23:19.771167 16778 solver.cpp:229] Iteration 27000, loss = 0.0895522
I1025 23:23:19.771203 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0182622 (* 1 = 0.0182622 loss)
I1025 23:23:19.771209 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.07129 (* 1 = 0.07129 loss)
I1025 23:23:19.771214 16778 sgd_solver.cpp:106] Iteration 27000, lr = 0.001
I1025 23:23:28.899492 16778 solver.cpp:229] Iteration 27100, loss = 0.0290669
I1025 23:23:28.899524 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00231904 (* 1 = 0.00231904 loss)
I1025 23:23:28.899531 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0267479 (* 1 = 0.0267479 loss)
I1025 23:23:28.899536 16778 sgd_solver.cpp:106] Iteration 27100, lr = 0.001
I1025 23:23:37.993005 16778 solver.cpp:229] Iteration 27200, loss = 0.238075
I1025 23:23:37.993037 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0640583 (* 1 = 0.0640583 loss)
I1025 23:23:37.993042 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.174017 (* 1 = 0.174017 loss)
I1025 23:23:37.993046 16778 sgd_solver.cpp:106] Iteration 27200, lr = 0.001
I1025 23:23:47.093994 16778 solver.cpp:229] Iteration 27300, loss = 0.0703461
I1025 23:23:47.094027 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0519122 (* 1 = 0.0519122 loss)
I1025 23:23:47.094033 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0184339 (* 1 = 0.0184339 loss)
I1025 23:23:47.094036 16778 sgd_solver.cpp:106] Iteration 27300, lr = 0.001
I1025 23:23:56.091231 16778 solver.cpp:229] Iteration 27400, loss = 0.0614226
I1025 23:23:56.091265 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.02376 (* 1 = 0.02376 loss)
I1025 23:23:56.091270 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0376626 (* 1 = 0.0376626 loss)
I1025 23:23:56.091274 16778 sgd_solver.cpp:106] Iteration 27400, lr = 0.001
I1025 23:24:05.283839 16778 solver.cpp:229] Iteration 27500, loss = 0.060288
I1025 23:24:05.283874 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00283634 (* 1 = 0.00283634 loss)
I1025 23:24:05.283879 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0574517 (* 1 = 0.0574517 loss)
I1025 23:24:05.283881 16778 sgd_solver.cpp:106] Iteration 27500, lr = 0.001
I1025 23:24:14.276480 16778 solver.cpp:229] Iteration 27600, loss = 0.0533955
I1025 23:24:14.276525 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0169065 (* 1 = 0.0169065 loss)
I1025 23:24:14.276530 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.036489 (* 1 = 0.036489 loss)
I1025 23:24:14.276533 16778 sgd_solver.cpp:106] Iteration 27600, lr = 0.001
I1025 23:24:23.314775 16778 solver.cpp:229] Iteration 27700, loss = 0.150893
I1025 23:24:23.314810 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0828142 (* 1 = 0.0828142 loss)
I1025 23:24:23.314815 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0680785 (* 1 = 0.0680785 loss)
I1025 23:24:23.314818 16778 sgd_solver.cpp:106] Iteration 27700, lr = 0.001
I1025 23:24:32.245265 16778 solver.cpp:229] Iteration 27800, loss = 0.0179702
I1025 23:24:32.245296 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0140428 (* 1 = 0.0140428 loss)
I1025 23:24:32.245301 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00392738 (* 1 = 0.00392738 loss)
I1025 23:24:32.245304 16778 sgd_solver.cpp:106] Iteration 27800, lr = 0.001
I1025 23:24:41.254823 16778 solver.cpp:229] Iteration 27900, loss = 0.0157
I1025 23:24:41.254854 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00779948 (* 1 = 0.00779948 loss)
I1025 23:24:41.254859 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00790049 (* 1 = 0.00790049 loss)
I1025 23:24:41.254863 16778 sgd_solver.cpp:106] Iteration 27900, lr = 0.001
I1025 23:24:50.291251 16778 solver.cpp:229] Iteration 28000, loss = 0.0454875
I1025 23:24:50.291285 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0210065 (* 1 = 0.0210065 loss)
I1025 23:24:50.291290 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.024481 (* 1 = 0.024481 loss)
I1025 23:24:50.291295 16778 sgd_solver.cpp:106] Iteration 28000, lr = 0.001
I1025 23:24:59.394948 16778 solver.cpp:229] Iteration 28100, loss = 0.0562882
I1025 23:24:59.394980 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0319359 (* 1 = 0.0319359 loss)
I1025 23:24:59.394985 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0243523 (* 1 = 0.0243523 loss)
I1025 23:24:59.394989 16778 sgd_solver.cpp:106] Iteration 28100, lr = 0.001
I1025 23:25:08.457509 16778 solver.cpp:229] Iteration 28200, loss = 0.0709127
I1025 23:25:08.457542 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00751933 (* 1 = 0.00751933 loss)
I1025 23:25:08.457547 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0633934 (* 1 = 0.0633934 loss)
I1025 23:25:08.457551 16778 sgd_solver.cpp:106] Iteration 28200, lr = 0.001
I1025 23:25:17.570147 16778 solver.cpp:229] Iteration 28300, loss = 0.0202496
I1025 23:25:17.570190 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0125569 (* 1 = 0.0125569 loss)
I1025 23:25:17.570204 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00769278 (* 1 = 0.00769278 loss)
I1025 23:25:17.570209 16778 sgd_solver.cpp:106] Iteration 28300, lr = 0.001
I1025 23:25:26.505503 16778 solver.cpp:229] Iteration 28400, loss = 0.111543
I1025 23:25:26.505535 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0268963 (* 1 = 0.0268963 loss)
I1025 23:25:26.505540 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0846468 (* 1 = 0.0846468 loss)
I1025 23:25:26.505544 16778 sgd_solver.cpp:106] Iteration 28400, lr = 0.001
I1025 23:25:35.639068 16778 solver.cpp:229] Iteration 28500, loss = 0.0255156
I1025 23:25:35.639101 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0225362 (* 1 = 0.0225362 loss)
I1025 23:25:35.639106 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00297945 (* 1 = 0.00297945 loss)
I1025 23:25:35.639111 16778 sgd_solver.cpp:106] Iteration 28500, lr = 0.001
I1025 23:25:44.621500 16778 solver.cpp:229] Iteration 28600, loss = 0.0289731
I1025 23:25:44.621532 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0107313 (* 1 = 0.0107313 loss)
I1025 23:25:44.621537 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0182418 (* 1 = 0.0182418 loss)
I1025 23:25:44.621542 16778 sgd_solver.cpp:106] Iteration 28600, lr = 0.001
I1025 23:25:53.620889 16778 solver.cpp:229] Iteration 28700, loss = 0.0114951
I1025 23:25:53.620923 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00612734 (* 1 = 0.00612734 loss)
I1025 23:25:53.620929 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00536772 (* 1 = 0.00536772 loss)
I1025 23:25:53.620934 16778 sgd_solver.cpp:106] Iteration 28700, lr = 0.001
I1025 23:26:02.648931 16778 solver.cpp:229] Iteration 28800, loss = 0.0453237
I1025 23:26:02.648964 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00369192 (* 1 = 0.00369192 loss)
I1025 23:26:02.648972 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0416318 (* 1 = 0.0416318 loss)
I1025 23:26:02.648977 16778 sgd_solver.cpp:106] Iteration 28800, lr = 0.001
I1025 23:26:11.648062 16778 solver.cpp:229] Iteration 28900, loss = 0.0166907
I1025 23:26:11.648095 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00148672 (* 1 = 0.00148672 loss)
I1025 23:26:11.648100 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.015204 (* 1 = 0.015204 loss)
I1025 23:26:11.648104 16778 sgd_solver.cpp:106] Iteration 28900, lr = 0.001
I1025 23:26:20.679236 16778 solver.cpp:229] Iteration 29000, loss = 0.0231418
I1025 23:26:20.679265 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0114037 (* 1 = 0.0114037 loss)
I1025 23:26:20.679270 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0117381 (* 1 = 0.0117381 loss)
I1025 23:26:20.679275 16778 sgd_solver.cpp:106] Iteration 29000, lr = 0.001
I1025 23:26:29.702091 16778 solver.cpp:229] Iteration 29100, loss = 0.0231961
I1025 23:26:29.702126 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00481856 (* 1 = 0.00481856 loss)
I1025 23:26:29.702132 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0183775 (* 1 = 0.0183775 loss)
I1025 23:26:29.702137 16778 sgd_solver.cpp:106] Iteration 29100, lr = 0.001
I1025 23:26:38.806649 16778 solver.cpp:229] Iteration 29200, loss = 0.263459
I1025 23:26:38.806684 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0246178 (* 1 = 0.0246178 loss)
I1025 23:26:38.806691 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.238842 (* 1 = 0.238842 loss)
I1025 23:26:38.806696 16778 sgd_solver.cpp:106] Iteration 29200, lr = 0.001
I1025 23:26:47.804971 16778 solver.cpp:229] Iteration 29300, loss = 0.0222515
I1025 23:26:47.805003 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00994428 (* 1 = 0.00994428 loss)
I1025 23:26:47.805008 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0123072 (* 1 = 0.0123072 loss)
I1025 23:26:47.805012 16778 sgd_solver.cpp:106] Iteration 29300, lr = 0.001
I1025 23:26:56.620265 16778 solver.cpp:229] Iteration 29400, loss = 0.0925319
I1025 23:26:56.620298 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0332727 (* 1 = 0.0332727 loss)
I1025 23:26:56.620303 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0592592 (* 1 = 0.0592592 loss)
I1025 23:26:56.620307 16778 sgd_solver.cpp:106] Iteration 29400, lr = 0.001
I1025 23:27:05.756750 16778 solver.cpp:229] Iteration 29500, loss = 0.0385987
I1025 23:27:05.756783 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0162639 (* 1 = 0.0162639 loss)
I1025 23:27:05.756788 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0223349 (* 1 = 0.0223349 loss)
I1025 23:27:05.756793 16778 sgd_solver.cpp:106] Iteration 29500, lr = 0.001
I1025 23:27:14.876446 16778 solver.cpp:229] Iteration 29600, loss = 0.106591
I1025 23:27:14.876478 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.042845 (* 1 = 0.042845 loss)
I1025 23:27:14.876483 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0637456 (* 1 = 0.0637456 loss)
I1025 23:27:14.876487 16778 sgd_solver.cpp:106] Iteration 29600, lr = 0.001
I1025 23:27:23.951331 16778 solver.cpp:229] Iteration 29700, loss = 0.047345
I1025 23:27:23.951362 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0121966 (* 1 = 0.0121966 loss)
I1025 23:27:23.951367 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0351483 (* 1 = 0.0351483 loss)
I1025 23:27:23.951371 16778 sgd_solver.cpp:106] Iteration 29700, lr = 0.001
I1025 23:27:33.105641 16778 solver.cpp:229] Iteration 29800, loss = 0.0450045
I1025 23:27:33.105674 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0240592 (* 1 = 0.0240592 loss)
I1025 23:27:33.105679 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0209453 (* 1 = 0.0209453 loss)
I1025 23:27:33.105684 16778 sgd_solver.cpp:106] Iteration 29800, lr = 0.001
I1025 23:27:42.136337 16778 solver.cpp:229] Iteration 29900, loss = 0.0922315
I1025 23:27:42.136370 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0508926 (* 1 = 0.0508926 loss)
I1025 23:27:42.136375 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.041339 (* 1 = 0.041339 loss)
I1025 23:27:42.136379 16778 sgd_solver.cpp:106] Iteration 29900, lr = 0.001
I1025 23:27:51.549222 16778 solver.cpp:229] Iteration 30000, loss = 0.0165333
I1025 23:27:51.549273 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00515918 (* 1 = 0.00515918 loss)
I1025 23:27:51.549278 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0113741 (* 1 = 0.0113741 loss)
I1025 23:27:51.549283 16778 sgd_solver.cpp:106] Iteration 30000, lr = 0.001
I1025 23:28:00.367013 16778 solver.cpp:229] Iteration 30100, loss = 0.00676435
I1025 23:28:00.367048 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0034078 (* 1 = 0.0034078 loss)
I1025 23:28:00.367053 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00335655 (* 1 = 0.00335655 loss)
I1025 23:28:00.367055 16778 sgd_solver.cpp:106] Iteration 30100, lr = 0.001
I1025 23:28:09.425093 16778 solver.cpp:229] Iteration 30200, loss = 0.0267401
I1025 23:28:09.425125 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.011815 (* 1 = 0.011815 loss)
I1025 23:28:09.425130 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0149251 (* 1 = 0.0149251 loss)
I1025 23:28:09.425133 16778 sgd_solver.cpp:106] Iteration 30200, lr = 0.001
I1025 23:28:18.572737 16778 solver.cpp:229] Iteration 30300, loss = 0.0352578
I1025 23:28:18.572770 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00493748 (* 1 = 0.00493748 loss)
I1025 23:28:18.572775 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0303204 (* 1 = 0.0303204 loss)
I1025 23:28:18.572779 16778 sgd_solver.cpp:106] Iteration 30300, lr = 0.001
I1025 23:28:27.577463 16778 solver.cpp:229] Iteration 30400, loss = 0.019191
I1025 23:28:27.577497 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00719769 (* 1 = 0.00719769 loss)
I1025 23:28:27.577500 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0119933 (* 1 = 0.0119933 loss)
I1025 23:28:27.577504 16778 sgd_solver.cpp:106] Iteration 30400, lr = 0.001
I1025 23:28:36.729780 16778 solver.cpp:229] Iteration 30500, loss = 0.537633
I1025 23:28:36.729813 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.035921 (* 1 = 0.035921 loss)
I1025 23:28:36.729820 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.501712 (* 1 = 0.501712 loss)
I1025 23:28:36.729823 16778 sgd_solver.cpp:106] Iteration 30500, lr = 0.001
I1025 23:28:45.897940 16778 solver.cpp:229] Iteration 30600, loss = 0.0495126
I1025 23:28:45.897974 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0214902 (* 1 = 0.0214902 loss)
I1025 23:28:45.897979 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0280224 (* 1 = 0.0280224 loss)
I1025 23:28:45.897981 16778 sgd_solver.cpp:106] Iteration 30600, lr = 0.001
I1025 23:28:54.876678 16778 solver.cpp:229] Iteration 30700, loss = 0.0228185
I1025 23:28:54.876711 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0051235 (* 1 = 0.0051235 loss)
I1025 23:28:54.876715 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.017695 (* 1 = 0.017695 loss)
I1025 23:28:54.876720 16778 sgd_solver.cpp:106] Iteration 30700, lr = 0.001
I1025 23:29:03.780586 16778 solver.cpp:229] Iteration 30800, loss = 0.0684955
I1025 23:29:03.780617 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.024389 (* 1 = 0.024389 loss)
I1025 23:29:03.780623 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0441065 (* 1 = 0.0441065 loss)
I1025 23:29:03.780627 16778 sgd_solver.cpp:106] Iteration 30800, lr = 0.001
I1025 23:29:12.911805 16778 solver.cpp:229] Iteration 30900, loss = 0.0627802
I1025 23:29:12.911839 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00438245 (* 1 = 0.00438245 loss)
I1025 23:29:12.911844 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0583977 (* 1 = 0.0583977 loss)
I1025 23:29:12.911847 16778 sgd_solver.cpp:106] Iteration 30900, lr = 0.001
I1025 23:29:22.050408 16778 solver.cpp:229] Iteration 31000, loss = 0.00558093
I1025 23:29:22.050441 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00368259 (* 1 = 0.00368259 loss)
I1025 23:29:22.050446 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00189834 (* 1 = 0.00189834 loss)
I1025 23:29:22.050449 16778 sgd_solver.cpp:106] Iteration 31000, lr = 0.001
I1025 23:29:31.132975 16778 solver.cpp:229] Iteration 31100, loss = 0.0882661
I1025 23:29:31.133008 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0347417 (* 1 = 0.0347417 loss)
I1025 23:29:31.133013 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0535245 (* 1 = 0.0535245 loss)
I1025 23:29:31.133016 16778 sgd_solver.cpp:106] Iteration 31100, lr = 0.001
I1025 23:29:40.087031 16778 solver.cpp:229] Iteration 31200, loss = 0.00729386
I1025 23:29:40.087060 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00423985 (* 1 = 0.00423985 loss)
I1025 23:29:40.087065 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00305402 (* 1 = 0.00305402 loss)
I1025 23:29:40.087069 16778 sgd_solver.cpp:106] Iteration 31200, lr = 0.001
I1025 23:29:49.289199 16778 solver.cpp:229] Iteration 31300, loss = 0.0422258
I1025 23:29:49.289233 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00159779 (* 1 = 0.00159779 loss)
I1025 23:29:49.289238 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.040628 (* 1 = 0.040628 loss)
I1025 23:29:49.289242 16778 sgd_solver.cpp:106] Iteration 31300, lr = 0.001
I1025 23:29:58.343544 16778 solver.cpp:229] Iteration 31400, loss = 0.122198
I1025 23:29:58.343575 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0312371 (* 1 = 0.0312371 loss)
I1025 23:29:58.343580 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0909609 (* 1 = 0.0909609 loss)
I1025 23:29:58.343583 16778 sgd_solver.cpp:106] Iteration 31400, lr = 0.001
I1025 23:30:07.339051 16778 solver.cpp:229] Iteration 31500, loss = 0.0855391
I1025 23:30:07.339085 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0181081 (* 1 = 0.0181081 loss)
I1025 23:30:07.339090 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0674311 (* 1 = 0.0674311 loss)
I1025 23:30:07.339104 16778 sgd_solver.cpp:106] Iteration 31500, lr = 0.001
I1025 23:30:16.209862 16778 solver.cpp:229] Iteration 31600, loss = 0.0246418
I1025 23:30:16.209895 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0147868 (* 1 = 0.0147868 loss)
I1025 23:30:16.209900 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00985502 (* 1 = 0.00985502 loss)
I1025 23:30:16.209904 16778 sgd_solver.cpp:106] Iteration 31600, lr = 0.001
I1025 23:30:25.129402 16778 solver.cpp:229] Iteration 31700, loss = 0.00159308
I1025 23:30:25.129444 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.000528514 (* 1 = 0.000528514 loss)
I1025 23:30:25.129449 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00106457 (* 1 = 0.00106457 loss)
I1025 23:30:25.129453 16778 sgd_solver.cpp:106] Iteration 31700, lr = 0.001
I1025 23:30:34.178890 16778 solver.cpp:229] Iteration 31800, loss = 0.0161201
I1025 23:30:34.178922 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00718768 (* 1 = 0.00718768 loss)
I1025 23:30:34.178927 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00893246 (* 1 = 0.00893246 loss)
I1025 23:30:34.178931 16778 sgd_solver.cpp:106] Iteration 31800, lr = 0.001
I1025 23:30:43.188148 16778 solver.cpp:229] Iteration 31900, loss = 0.780494
I1025 23:30:43.188176 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0598522 (* 1 = 0.0598522 loss)
I1025 23:30:43.188181 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.720642 (* 1 = 0.720642 loss)
I1025 23:30:43.188184 16778 sgd_solver.cpp:106] Iteration 31900, lr = 0.001
I1025 23:30:52.380506 16778 solver.cpp:229] Iteration 32000, loss = 0.0329115
I1025 23:30:52.380539 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0228985 (* 1 = 0.0228985 loss)
I1025 23:30:52.380544 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0100131 (* 1 = 0.0100131 loss)
I1025 23:30:52.380548 16778 sgd_solver.cpp:106] Iteration 32000, lr = 0.001
I1025 23:31:01.375715 16778 solver.cpp:229] Iteration 32100, loss = 0.0144293
I1025 23:31:01.375749 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00111243 (* 1 = 0.00111243 loss)
I1025 23:31:01.375754 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0133169 (* 1 = 0.0133169 loss)
I1025 23:31:01.375758 16778 sgd_solver.cpp:106] Iteration 32100, lr = 0.001
I1025 23:31:10.459399 16778 solver.cpp:229] Iteration 32200, loss = 0.0987232
I1025 23:31:10.459436 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.046077 (* 1 = 0.046077 loss)
I1025 23:31:10.459441 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0526462 (* 1 = 0.0526462 loss)
I1025 23:31:10.459445 16778 sgd_solver.cpp:106] Iteration 32200, lr = 0.001
I1025 23:31:19.411594 16778 solver.cpp:229] Iteration 32300, loss = 0.0425481
I1025 23:31:19.411628 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00290458 (* 1 = 0.00290458 loss)
I1025 23:31:19.411633 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0396435 (* 1 = 0.0396435 loss)
I1025 23:31:19.411636 16778 sgd_solver.cpp:106] Iteration 32300, lr = 0.001
I1025 23:31:28.424923 16778 solver.cpp:229] Iteration 32400, loss = 0.0357541
I1025 23:31:28.424955 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0257512 (* 1 = 0.0257512 loss)
I1025 23:31:28.424960 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0100029 (* 1 = 0.0100029 loss)
I1025 23:31:28.424963 16778 sgd_solver.cpp:106] Iteration 32400, lr = 0.001
I1025 23:31:37.523139 16778 solver.cpp:229] Iteration 32500, loss = 0.0107434
I1025 23:31:37.523172 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00341072 (* 1 = 0.00341072 loss)
I1025 23:31:37.523177 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00733266 (* 1 = 0.00733266 loss)
I1025 23:31:37.523180 16778 sgd_solver.cpp:106] Iteration 32500, lr = 0.001
I1025 23:31:46.810804 16778 solver.cpp:229] Iteration 32600, loss = 0.0149719
I1025 23:31:46.810838 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00243378 (* 1 = 0.00243378 loss)
I1025 23:31:46.810842 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0125381 (* 1 = 0.0125381 loss)
I1025 23:31:46.810847 16778 sgd_solver.cpp:106] Iteration 32600, lr = 0.001
I1025 23:31:55.802819 16778 solver.cpp:229] Iteration 32700, loss = 0.0102568
I1025 23:31:55.802851 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00681204 (* 1 = 0.00681204 loss)
I1025 23:31:55.802856 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00344476 (* 1 = 0.00344476 loss)
I1025 23:31:55.802860 16778 sgd_solver.cpp:106] Iteration 32700, lr = 0.001
I1025 23:32:04.912571 16778 solver.cpp:229] Iteration 32800, loss = 0.463172
I1025 23:32:04.912606 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0518661 (* 1 = 0.0518661 loss)
I1025 23:32:04.912611 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.411306 (* 1 = 0.411306 loss)
I1025 23:32:04.912616 16778 sgd_solver.cpp:106] Iteration 32800, lr = 0.001
I1025 23:32:14.019605 16778 solver.cpp:229] Iteration 32900, loss = 0.0156289
I1025 23:32:14.019637 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00339271 (* 1 = 0.00339271 loss)
I1025 23:32:14.019642 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0122362 (* 1 = 0.0122362 loss)
I1025 23:32:14.019645 16778 sgd_solver.cpp:106] Iteration 32900, lr = 0.001
I1025 23:32:22.998759 16778 solver.cpp:229] Iteration 33000, loss = 0.047105
I1025 23:32:22.998793 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0220313 (* 1 = 0.0220313 loss)
I1025 23:32:22.998798 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0250737 (* 1 = 0.0250737 loss)
I1025 23:32:22.998803 16778 sgd_solver.cpp:106] Iteration 33000, lr = 0.001
I1025 23:32:31.957737 16778 solver.cpp:229] Iteration 33100, loss = 0.0199156
I1025 23:32:31.957769 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00570389 (* 1 = 0.00570389 loss)
I1025 23:32:31.957774 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0142117 (* 1 = 0.0142117 loss)
I1025 23:32:31.957778 16778 sgd_solver.cpp:106] Iteration 33100, lr = 0.001
I1025 23:32:41.016813 16778 solver.cpp:229] Iteration 33200, loss = 0.103846
I1025 23:32:41.016844 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00216204 (* 1 = 0.00216204 loss)
I1025 23:32:41.016849 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.101684 (* 1 = 0.101684 loss)
I1025 23:32:41.016854 16778 sgd_solver.cpp:106] Iteration 33200, lr = 0.001
I1025 23:32:50.095892 16778 solver.cpp:229] Iteration 33300, loss = 0.278006
I1025 23:32:50.095924 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0248429 (* 1 = 0.0248429 loss)
I1025 23:32:50.095929 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.253163 (* 1 = 0.253163 loss)
I1025 23:32:50.095933 16778 sgd_solver.cpp:106] Iteration 33300, lr = 0.001
I1025 23:32:59.159085 16778 solver.cpp:229] Iteration 33400, loss = 0.00683454
I1025 23:32:59.159127 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00503599 (* 1 = 0.00503599 loss)
I1025 23:32:59.159133 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00179855 (* 1 = 0.00179855 loss)
I1025 23:32:59.159147 16778 sgd_solver.cpp:106] Iteration 33400, lr = 0.001
I1025 23:33:08.240521 16778 solver.cpp:229] Iteration 33500, loss = 0.0133152
I1025 23:33:08.240556 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00748226 (* 1 = 0.00748226 loss)
I1025 23:33:08.240561 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00583296 (* 1 = 0.00583296 loss)
I1025 23:33:08.240566 16778 sgd_solver.cpp:106] Iteration 33500, lr = 0.001
I1025 23:33:17.305434 16778 solver.cpp:229] Iteration 33600, loss = 0.0355702
I1025 23:33:17.305476 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0116051 (* 1 = 0.0116051 loss)
I1025 23:33:17.305481 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0239651 (* 1 = 0.0239651 loss)
I1025 23:33:17.305485 16778 sgd_solver.cpp:106] Iteration 33600, lr = 0.001
I1025 23:33:26.304271 16778 solver.cpp:229] Iteration 33700, loss = 0.0195908
I1025 23:33:26.304304 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00819204 (* 1 = 0.00819204 loss)
I1025 23:33:26.304311 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0113988 (* 1 = 0.0113988 loss)
I1025 23:33:26.304313 16778 sgd_solver.cpp:106] Iteration 33700, lr = 0.001
I1025 23:33:35.334897 16778 solver.cpp:229] Iteration 33800, loss = 0.0763825
I1025 23:33:35.334928 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0201502 (* 1 = 0.0201502 loss)
I1025 23:33:35.334933 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0562323 (* 1 = 0.0562323 loss)
I1025 23:33:35.334936 16778 sgd_solver.cpp:106] Iteration 33800, lr = 0.001
I1025 23:33:44.311240 16778 solver.cpp:229] Iteration 33900, loss = 0.0154049
I1025 23:33:44.311274 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00452107 (* 1 = 0.00452107 loss)
I1025 23:33:44.311278 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0108838 (* 1 = 0.0108838 loss)
I1025 23:33:44.311281 16778 sgd_solver.cpp:106] Iteration 33900, lr = 0.001
I1025 23:33:53.369998 16778 solver.cpp:229] Iteration 34000, loss = 0.0218098
I1025 23:33:53.370029 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00556319 (* 1 = 0.00556319 loss)
I1025 23:33:53.370034 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0162466 (* 1 = 0.0162466 loss)
I1025 23:33:53.370038 16778 sgd_solver.cpp:106] Iteration 34000, lr = 0.001
I1025 23:34:02.306542 16778 solver.cpp:229] Iteration 34100, loss = 0.0493265
I1025 23:34:02.306574 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00430767 (* 1 = 0.00430767 loss)
I1025 23:34:02.306579 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0450188 (* 1 = 0.0450188 loss)
I1025 23:34:02.306583 16778 sgd_solver.cpp:106] Iteration 34100, lr = 0.001
I1025 23:34:11.426961 16778 solver.cpp:229] Iteration 34200, loss = 0.0104333
I1025 23:34:11.426992 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00708675 (* 1 = 0.00708675 loss)
I1025 23:34:11.427000 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00334659 (* 1 = 0.00334659 loss)
I1025 23:34:11.427004 16778 sgd_solver.cpp:106] Iteration 34200, lr = 0.001
I1025 23:34:20.310444 16778 solver.cpp:229] Iteration 34300, loss = 0.00901353
I1025 23:34:20.310477 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00347958 (* 1 = 0.00347958 loss)
I1025 23:34:20.310484 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00553395 (* 1 = 0.00553395 loss)
I1025 23:34:20.310489 16778 sgd_solver.cpp:106] Iteration 34300, lr = 0.001
I1025 23:34:29.374299 16778 solver.cpp:229] Iteration 34400, loss = 0.0782522
I1025 23:34:29.374333 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0188043 (* 1 = 0.0188043 loss)
I1025 23:34:29.374341 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0594478 (* 1 = 0.0594478 loss)
I1025 23:34:29.374346 16778 sgd_solver.cpp:106] Iteration 34400, lr = 0.001
I1025 23:34:38.513490 16778 solver.cpp:229] Iteration 34500, loss = 0.0183851
I1025 23:34:38.513525 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00249546 (* 1 = 0.00249546 loss)
I1025 23:34:38.513532 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0158897 (* 1 = 0.0158897 loss)
I1025 23:34:38.513537 16778 sgd_solver.cpp:106] Iteration 34500, lr = 0.001
I1025 23:34:47.610173 16778 solver.cpp:229] Iteration 34600, loss = 0.0307025
I1025 23:34:47.610208 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0185183 (* 1 = 0.0185183 loss)
I1025 23:34:47.610213 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0121842 (* 1 = 0.0121842 loss)
I1025 23:34:47.610216 16778 sgd_solver.cpp:106] Iteration 34600, lr = 0.001
I1025 23:34:56.757248 16778 solver.cpp:229] Iteration 34700, loss = 0.0824695
I1025 23:34:56.757282 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0283399 (* 1 = 0.0283399 loss)
I1025 23:34:56.757287 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0541295 (* 1 = 0.0541295 loss)
I1025 23:34:56.757290 16778 sgd_solver.cpp:106] Iteration 34700, lr = 0.001
I1025 23:35:05.818074 16778 solver.cpp:229] Iteration 34800, loss = 0.188139
I1025 23:35:05.818119 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.012941 (* 1 = 0.012941 loss)
I1025 23:35:05.818122 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.175198 (* 1 = 0.175198 loss)
I1025 23:35:05.818127 16778 sgd_solver.cpp:106] Iteration 34800, lr = 0.001
I1025 23:35:14.809175 16778 solver.cpp:229] Iteration 34900, loss = 0.0431181
I1025 23:35:14.809209 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00317019 (* 1 = 0.00317019 loss)
I1025 23:35:14.809216 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0399479 (* 1 = 0.0399479 loss)
I1025 23:35:14.809219 16778 sgd_solver.cpp:106] Iteration 34900, lr = 0.001
I1025 23:35:23.856632 16778 solver.cpp:229] Iteration 35000, loss = 0.0568957
I1025 23:35:23.856667 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0507204 (* 1 = 0.0507204 loss)
I1025 23:35:23.856673 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00617528 (* 1 = 0.00617528 loss)
I1025 23:35:23.856676 16778 sgd_solver.cpp:106] Iteration 35000, lr = 0.001
I1025 23:35:32.933974 16778 solver.cpp:229] Iteration 35100, loss = 0.0996457
I1025 23:35:32.934008 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0402063 (* 1 = 0.0402063 loss)
I1025 23:35:32.934015 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0594394 (* 1 = 0.0594394 loss)
I1025 23:35:32.934020 16778 sgd_solver.cpp:106] Iteration 35100, lr = 0.001
I1025 23:35:41.935305 16778 solver.cpp:229] Iteration 35200, loss = 0.161499
I1025 23:35:41.935338 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0656343 (* 1 = 0.0656343 loss)
I1025 23:35:41.935343 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.095865 (* 1 = 0.095865 loss)
I1025 23:35:41.935348 16778 sgd_solver.cpp:106] Iteration 35200, lr = 0.001
I1025 23:35:50.866914 16778 solver.cpp:229] Iteration 35300, loss = 0.0361813
I1025 23:35:50.866945 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0191783 (* 1 = 0.0191783 loss)
I1025 23:35:50.866950 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.017003 (* 1 = 0.017003 loss)
I1025 23:35:50.866952 16778 sgd_solver.cpp:106] Iteration 35300, lr = 0.001
I1025 23:35:59.824971 16778 solver.cpp:229] Iteration 35400, loss = 0.0720232
I1025 23:35:59.825006 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00300525 (* 1 = 0.00300525 loss)
I1025 23:35:59.825011 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0690179 (* 1 = 0.0690179 loss)
I1025 23:35:59.825013 16778 sgd_solver.cpp:106] Iteration 35400, lr = 0.001
I1025 23:36:08.837945 16778 solver.cpp:229] Iteration 35500, loss = 0.0143889
I1025 23:36:08.837990 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00831387 (* 1 = 0.00831387 loss)
I1025 23:36:08.837995 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00607499 (* 1 = 0.00607499 loss)
I1025 23:36:08.837998 16778 sgd_solver.cpp:106] Iteration 35500, lr = 0.001
I1025 23:36:17.894556 16778 solver.cpp:229] Iteration 35600, loss = 0.107624
I1025 23:36:17.894589 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0374646 (* 1 = 0.0374646 loss)
I1025 23:36:17.894594 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0701595 (* 1 = 0.0701595 loss)
I1025 23:36:17.894598 16778 sgd_solver.cpp:106] Iteration 35600, lr = 0.001
I1025 23:36:26.906853 16778 solver.cpp:229] Iteration 35700, loss = 0.0200803
I1025 23:36:26.906889 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0121204 (* 1 = 0.0121204 loss)
I1025 23:36:26.906896 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00795987 (* 1 = 0.00795987 loss)
I1025 23:36:26.906901 16778 sgd_solver.cpp:106] Iteration 35700, lr = 0.001
I1025 23:36:35.961619 16778 solver.cpp:229] Iteration 35800, loss = 0.0613077
I1025 23:36:35.961653 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0114085 (* 1 = 0.0114085 loss)
I1025 23:36:35.961661 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0498993 (* 1 = 0.0498993 loss)
I1025 23:36:35.961668 16778 sgd_solver.cpp:106] Iteration 35800, lr = 0.001
I1025 23:36:45.058207 16778 solver.cpp:229] Iteration 35900, loss = 0.0154438
I1025 23:36:45.058238 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00317194 (* 1 = 0.00317194 loss)
I1025 23:36:45.058245 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0122719 (* 1 = 0.0122719 loss)
I1025 23:36:45.058250 16778 sgd_solver.cpp:106] Iteration 35900, lr = 0.001
I1025 23:36:54.003792 16778 solver.cpp:229] Iteration 36000, loss = 0.0157042
I1025 23:36:54.003823 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00170874 (* 1 = 0.00170874 loss)
I1025 23:36:54.003829 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0139954 (* 1 = 0.0139954 loss)
I1025 23:36:54.003834 16778 sgd_solver.cpp:106] Iteration 36000, lr = 0.001
I1025 23:37:02.937088 16778 solver.cpp:229] Iteration 36100, loss = 0.0138271
I1025 23:37:02.937121 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00119816 (* 1 = 0.00119816 loss)
I1025 23:37:02.937126 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.012629 (* 1 = 0.012629 loss)
I1025 23:37:02.937130 16778 sgd_solver.cpp:106] Iteration 36100, lr = 0.001
I1025 23:37:11.844563 16778 solver.cpp:229] Iteration 36200, loss = 0.0722081
I1025 23:37:11.844595 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0301028 (* 1 = 0.0301028 loss)
I1025 23:37:11.844599 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0421053 (* 1 = 0.0421053 loss)
I1025 23:37:11.844604 16778 sgd_solver.cpp:106] Iteration 36200, lr = 0.001
I1025 23:37:20.984572 16778 solver.cpp:229] Iteration 36300, loss = 0.0147327
I1025 23:37:20.984604 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00895859 (* 1 = 0.00895859 loss)
I1025 23:37:20.984609 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00577408 (* 1 = 0.00577408 loss)
I1025 23:37:20.984614 16778 sgd_solver.cpp:106] Iteration 36300, lr = 0.001
I1025 23:37:29.950119 16778 solver.cpp:229] Iteration 36400, loss = 0.0177304
I1025 23:37:29.950150 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.000693132 (* 1 = 0.000693132 loss)
I1025 23:37:29.950155 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0170373 (* 1 = 0.0170373 loss)
I1025 23:37:29.950160 16778 sgd_solver.cpp:106] Iteration 36400, lr = 0.001
I1025 23:37:38.879148 16778 solver.cpp:229] Iteration 36500, loss = 0.0457764
I1025 23:37:38.879181 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00786175 (* 1 = 0.00786175 loss)
I1025 23:37:38.879186 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0379146 (* 1 = 0.0379146 loss)
I1025 23:37:38.879190 16778 sgd_solver.cpp:106] Iteration 36500, lr = 0.001
I1025 23:37:47.826423 16778 solver.cpp:229] Iteration 36600, loss = 0.0149263
I1025 23:37:47.826477 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00171545 (* 1 = 0.00171545 loss)
I1025 23:37:47.826483 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0132109 (* 1 = 0.0132109 loss)
I1025 23:37:47.826486 16778 sgd_solver.cpp:106] Iteration 36600, lr = 0.001
I1025 23:37:56.738867 16778 solver.cpp:229] Iteration 36700, loss = 0.0393113
I1025 23:37:56.738900 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0227215 (* 1 = 0.0227215 loss)
I1025 23:37:56.738905 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0165898 (* 1 = 0.0165898 loss)
I1025 23:37:56.738909 16778 sgd_solver.cpp:106] Iteration 36700, lr = 0.001
I1025 23:38:05.937904 16778 solver.cpp:229] Iteration 36800, loss = 0.0157824
I1025 23:38:05.937937 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00130327 (* 1 = 0.00130327 loss)
I1025 23:38:05.937942 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0144791 (* 1 = 0.0144791 loss)
I1025 23:38:05.937947 16778 sgd_solver.cpp:106] Iteration 36800, lr = 0.001
I1025 23:38:15.230407 16778 solver.cpp:229] Iteration 36900, loss = 0.0260003
I1025 23:38:15.230439 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00781838 (* 1 = 0.00781838 loss)
I1025 23:38:15.230444 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0181819 (* 1 = 0.0181819 loss)
I1025 23:38:15.230449 16778 sgd_solver.cpp:106] Iteration 36900, lr = 0.001
I1025 23:38:24.448891 16778 solver.cpp:229] Iteration 37000, loss = 0.0475492
I1025 23:38:24.448923 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0290598 (* 1 = 0.0290598 loss)
I1025 23:38:24.448928 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0184894 (* 1 = 0.0184894 loss)
I1025 23:38:24.448932 16778 sgd_solver.cpp:106] Iteration 37000, lr = 0.001
I1025 23:38:33.594650 16778 solver.cpp:229] Iteration 37100, loss = 0.0295505
I1025 23:38:33.594682 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0133617 (* 1 = 0.0133617 loss)
I1025 23:38:33.594688 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0161888 (* 1 = 0.0161888 loss)
I1025 23:38:33.594692 16778 sgd_solver.cpp:106] Iteration 37100, lr = 0.001
I1025 23:38:42.673132 16778 solver.cpp:229] Iteration 37200, loss = 0.10133
I1025 23:38:42.673162 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0295171 (* 1 = 0.0295171 loss)
I1025 23:38:42.673167 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0718133 (* 1 = 0.0718133 loss)
I1025 23:38:42.673172 16778 sgd_solver.cpp:106] Iteration 37200, lr = 0.001
I1025 23:38:51.758373 16778 solver.cpp:229] Iteration 37300, loss = 0.0094097
I1025 23:38:51.758404 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0015324 (* 1 = 0.0015324 loss)
I1025 23:38:51.758409 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00787731 (* 1 = 0.00787731 loss)
I1025 23:38:51.758414 16778 sgd_solver.cpp:106] Iteration 37300, lr = 0.001
I1025 23:39:00.769824 16778 solver.cpp:229] Iteration 37400, loss = 0.0253639
I1025 23:39:00.769856 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00308707 (* 1 = 0.00308707 loss)
I1025 23:39:00.769861 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0222769 (* 1 = 0.0222769 loss)
I1025 23:39:00.769865 16778 sgd_solver.cpp:106] Iteration 37400, lr = 0.001
I1025 23:39:09.643754 16778 solver.cpp:229] Iteration 37500, loss = 0.209814
I1025 23:39:09.643788 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0220939 (* 1 = 0.0220939 loss)
I1025 23:39:09.643792 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.18772 (* 1 = 0.18772 loss)
I1025 23:39:09.643796 16778 sgd_solver.cpp:106] Iteration 37500, lr = 0.001
I1025 23:39:18.587975 16778 solver.cpp:229] Iteration 37600, loss = 0.0223828
I1025 23:39:18.588011 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00291782 (* 1 = 0.00291782 loss)
I1025 23:39:18.588017 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.019465 (* 1 = 0.019465 loss)
I1025 23:39:18.588022 16778 sgd_solver.cpp:106] Iteration 37600, lr = 0.001
I1025 23:39:27.660743 16778 solver.cpp:229] Iteration 37700, loss = 0.00770567
I1025 23:39:27.660774 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00149664 (* 1 = 0.00149664 loss)
I1025 23:39:27.660781 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00620904 (* 1 = 0.00620904 loss)
I1025 23:39:27.660786 16778 sgd_solver.cpp:106] Iteration 37700, lr = 0.001
I1025 23:39:36.721570 16778 solver.cpp:229] Iteration 37800, loss = 0.00541843
I1025 23:39:36.721606 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00297606 (* 1 = 0.00297606 loss)
I1025 23:39:36.721613 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00244237 (* 1 = 0.00244237 loss)
I1025 23:39:36.721619 16778 sgd_solver.cpp:106] Iteration 37800, lr = 0.001
I1025 23:39:45.859061 16778 solver.cpp:229] Iteration 37900, loss = 0.0118085
I1025 23:39:45.859097 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0022381 (* 1 = 0.0022381 loss)
I1025 23:39:45.859103 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0095704 (* 1 = 0.0095704 loss)
I1025 23:39:45.859108 16778 sgd_solver.cpp:106] Iteration 37900, lr = 0.001
I1025 23:39:54.867364 16778 solver.cpp:229] Iteration 38000, loss = 0.0326666
I1025 23:39:54.867399 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00761315 (* 1 = 0.00761315 loss)
I1025 23:39:54.867406 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0250535 (* 1 = 0.0250535 loss)
I1025 23:39:54.867411 16778 sgd_solver.cpp:106] Iteration 38000, lr = 0.001
I1025 23:40:03.946355 16778 solver.cpp:229] Iteration 38100, loss = 0.00526548
I1025 23:40:03.946391 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00081331 (* 1 = 0.00081331 loss)
I1025 23:40:03.946399 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00445217 (* 1 = 0.00445217 loss)
I1025 23:40:03.946404 16778 sgd_solver.cpp:106] Iteration 38100, lr = 0.001
I1025 23:40:12.953064 16778 solver.cpp:229] Iteration 38200, loss = 0.0370768
I1025 23:40:12.953099 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0118067 (* 1 = 0.0118067 loss)
I1025 23:40:12.953106 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0252701 (* 1 = 0.0252701 loss)
I1025 23:40:12.953111 16778 sgd_solver.cpp:106] Iteration 38200, lr = 0.001
I1025 23:40:21.892302 16778 solver.cpp:229] Iteration 38300, loss = 0.0261482
I1025 23:40:21.892335 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00762691 (* 1 = 0.00762691 loss)
I1025 23:40:21.892343 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0185213 (* 1 = 0.0185213 loss)
I1025 23:40:21.892348 16778 sgd_solver.cpp:106] Iteration 38300, lr = 0.001
I1025 23:40:30.853004 16778 solver.cpp:229] Iteration 38400, loss = 0.0309693
I1025 23:40:30.853035 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00815271 (* 1 = 0.00815271 loss)
I1025 23:40:30.853040 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0228166 (* 1 = 0.0228166 loss)
I1025 23:40:30.853044 16778 sgd_solver.cpp:106] Iteration 38400, lr = 0.001
I1025 23:40:39.982549 16778 solver.cpp:229] Iteration 38500, loss = 0.0741161
I1025 23:40:39.982583 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0170279 (* 1 = 0.0170279 loss)
I1025 23:40:39.982587 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0570882 (* 1 = 0.0570882 loss)
I1025 23:40:39.982591 16778 sgd_solver.cpp:106] Iteration 38500, lr = 0.001
I1025 23:40:49.023833 16778 solver.cpp:229] Iteration 38600, loss = 0.00912626
I1025 23:40:49.023865 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00448862 (* 1 = 0.00448862 loss)
I1025 23:40:49.023870 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00463765 (* 1 = 0.00463765 loss)
I1025 23:40:49.023875 16778 sgd_solver.cpp:106] Iteration 38600, lr = 0.001
I1025 23:40:58.021114 16778 solver.cpp:229] Iteration 38700, loss = 0.0177426
I1025 23:40:58.021147 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0106695 (* 1 = 0.0106695 loss)
I1025 23:40:58.021152 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0070731 (* 1 = 0.0070731 loss)
I1025 23:40:58.021157 16778 sgd_solver.cpp:106] Iteration 38700, lr = 0.001
I1025 23:41:07.136813 16778 solver.cpp:229] Iteration 38800, loss = 0.0139792
I1025 23:41:07.136847 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00119473 (* 1 = 0.00119473 loss)
I1025 23:41:07.136852 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0127845 (* 1 = 0.0127845 loss)
I1025 23:41:07.136855 16778 sgd_solver.cpp:106] Iteration 38800, lr = 0.001
I1025 23:41:16.197046 16778 solver.cpp:229] Iteration 38900, loss = 0.0709892
I1025 23:41:16.197078 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00519472 (* 1 = 0.00519472 loss)
I1025 23:41:16.197084 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0657944 (* 1 = 0.0657944 loss)
I1025 23:41:16.197088 16778 sgd_solver.cpp:106] Iteration 38900, lr = 0.001
I1025 23:41:25.167135 16778 solver.cpp:229] Iteration 39000, loss = 0.0485323
I1025 23:41:25.167165 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0119338 (* 1 = 0.0119338 loss)
I1025 23:41:25.167170 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0365985 (* 1 = 0.0365985 loss)
I1025 23:41:25.167173 16778 sgd_solver.cpp:106] Iteration 39000, lr = 0.001
I1025 23:41:34.054436 16778 solver.cpp:229] Iteration 39100, loss = 0.0247611
I1025 23:41:34.054469 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00924434 (* 1 = 0.00924434 loss)
I1025 23:41:34.054474 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0155167 (* 1 = 0.0155167 loss)
I1025 23:41:34.054477 16778 sgd_solver.cpp:106] Iteration 39100, lr = 0.001
I1025 23:41:43.000780 16778 solver.cpp:229] Iteration 39200, loss = 0.00236508
I1025 23:41:43.000813 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.000392605 (* 1 = 0.000392605 loss)
I1025 23:41:43.000818 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00197248 (* 1 = 0.00197248 loss)
I1025 23:41:43.000821 16778 sgd_solver.cpp:106] Iteration 39200, lr = 0.001
I1025 23:41:52.212568 16778 solver.cpp:229] Iteration 39300, loss = 0.0706236
I1025 23:41:52.212599 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0284191 (* 1 = 0.0284191 loss)
I1025 23:41:52.212604 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0422045 (* 1 = 0.0422045 loss)
I1025 23:41:52.212610 16778 sgd_solver.cpp:106] Iteration 39300, lr = 0.001
I1025 23:42:01.261433 16778 solver.cpp:229] Iteration 39400, loss = 0.0043644
I1025 23:42:01.261476 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00174615 (* 1 = 0.00174615 loss)
I1025 23:42:01.261481 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00261825 (* 1 = 0.00261825 loss)
I1025 23:42:01.261484 16778 sgd_solver.cpp:106] Iteration 39400, lr = 0.001
I1025 23:42:10.390419 16778 solver.cpp:229] Iteration 39500, loss = 0.0257854
I1025 23:42:10.390451 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00176162 (* 1 = 0.00176162 loss)
I1025 23:42:10.390455 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0240238 (* 1 = 0.0240238 loss)
I1025 23:42:10.390460 16778 sgd_solver.cpp:106] Iteration 39500, lr = 0.001
I1025 23:42:19.439903 16778 solver.cpp:229] Iteration 39600, loss = 0.00603351
I1025 23:42:19.439934 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00296562 (* 1 = 0.00296562 loss)
I1025 23:42:19.439939 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00306789 (* 1 = 0.00306789 loss)
I1025 23:42:19.439942 16778 sgd_solver.cpp:106] Iteration 39600, lr = 0.001
I1025 23:42:28.497954 16778 solver.cpp:229] Iteration 39700, loss = 0.0158437
I1025 23:42:28.497984 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00516715 (* 1 = 0.00516715 loss)
I1025 23:42:28.497990 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0106765 (* 1 = 0.0106765 loss)
I1025 23:42:28.497994 16778 sgd_solver.cpp:106] Iteration 39700, lr = 0.001
I1025 23:42:37.493197 16778 solver.cpp:229] Iteration 39800, loss = 0.0202578
I1025 23:42:37.493229 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.001436 (* 1 = 0.001436 loss)
I1025 23:42:37.493234 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0188218 (* 1 = 0.0188218 loss)
I1025 23:42:37.493238 16778 sgd_solver.cpp:106] Iteration 39800, lr = 0.001
I1025 23:42:46.505220 16778 solver.cpp:229] Iteration 39900, loss = 0.00795097
I1025 23:42:46.505252 16778 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.000661748 (* 1 = 0.000661748 loss)
I1025 23:42:46.505257 16778 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00728922 (* 1 = 0.00728922 loss)
I1025 23:42:46.505261 16778 sgd_solver.cpp:106] Iteration 39900, lr = 0.001
384 182 437]
 [ 44 429  93 475]
 [ 20 411  97 463]
 [  9 402  99 453]
 [  4 393  88 438]
 [ 62 376 106 396]
 [210 202 263 243]
 [276 259 335 431]
 [151 151 245 254]
 [335 250 386 396]
 [372 200 415 236]
 [318 202 388 265]]
500
[[197 245 260 371]
 [228 269 247 305]
 [191 208 244 268]
 [241 148 275 178]
 [298 156 335 296]
 [332 149 359 265]]
500
[[231 186 250 216]
 [ 57 218  80 241]
 [342  79 359 104]
 [381  69 403  80]
 [428  72 448  87]]
500
[[108  88 409 327]]
500
[[ 56  61 430 371]]
500
[[ 36 226 399 274]
 [139  88 392 151]]
500
[[124   1 443 338]]
500
[[290 166 416 287]]
500
[[  0   6 216 247]
 [263  57 335 194]
 [334  10 494 234]
 [469   1 498  40]]
390
[[  6   1 126 230]
 [145   1 247 229]
 [284   0 335 220]
 [ 21 248 124 496]
 [140 254 260 493]
 [272 246 335 485]
 [334   2 386 221]
 [335 262 386 486]]
500
[[ 60   0 335 315]
 [  0   0  62 374]
 [335   1 465 373]]
500
[[101 109 334 295]
 [320  91 497 248]]
500
[[345  96 374 131]]
500
[[ 30  57 266 138]
 [ 54 207 334 261]
 [332 177 446 403]]
500
[[ 14  38 334 450]
 [334  48 497 440]]
375
[[  0  35 334 497]
 [334  96 373 126]]
363
[[  4   1 334 493]
 [335  19 357 122]]
500
[[ 14  59 334 319]
 [334 192 348 242]
 [364 238 449 314]]
500
[[ 22   0 334 273]
 [320  96 457 358]]
500
[[  3  19 121 324]
 [121   7 263 328]
 [263   6 402 310]
 [401  37 499 310]]
374
[[103 109 299 310]
 [243  30 350 142]
 [284 185 313 264]]
500
[[  0   0 240 331]
 [260   0 313 101]
 [279   0 440 166]]
500
[[230 149 394 278]]
500
[[285  96 413 292]]
500
[[151 159 226 215]
 [284 145 477 259]
 [463 289 479 330]]
500
[[ 28   2 481 467]]
500
[[ 39  99 140 243]
 [364 107 469 233]]
500
[[ 91 177 334 291]
 [334 175 368 196]]

done
Preparing training data...
done
roidb len: 1424
Output will be saved to `/home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train`
Filtered 0 roidb entries: 1424 -> 1424
RoiDataLayer: name_to_top: {'gt_boxes': 2, 'data': 0, 'im_info': 1}
Loading pretrained model weights from data/imagenet_models/ZF.v2.caffemodel
Solving...
speed: 0.087s / iter
speed: 0.088s / iter
speed: 0.089s / iter
speed: 0.089s / iter
speed: 0.089s / iter
speed: 0.089s / iter
speed: 0.089s / iter
speed: 0.089s / iter
speed: 0.090s / iter
speed: 0.090s / iter
Wrote snapshot to: /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_rpn_stage1_iter_10000.caffemodel
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
Wrote snapshot to: /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_rpn_stage1_iter_20000.caffemodel
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
Wrote snapshot to: /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_rpn_stage1_iter_30000.caffemodel
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
speed: 0.090s / iter
Wrote snapshot to: /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_rpn_stage1_iter_40000.caffemodel
done solving
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 RPN, generate proposals
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1025 23:42:56.218535 16968 net.cpp:49] Initializing net from parameters: 
name: "ZF"
input: "data"
input: "im_info"
state {
  phase: TEST
}
input_shape {
  dim: 1
  dim: 3
  dim: 224
  dim: 224
}
input_shape {
  dim: 1
  dim: 3
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 96
    pad: 3
    kernel_size: 7
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
    engine: CAFFE
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    stride: 2
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
    engine: CAFFE
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "rpn_conv1"
  type: "Convolution"
  bottom: "conv5"
  top: "rpn_conv1"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "rpn_relu1"
  type: "ReLU"
  bottom: "rpn_conv1"
  top: "rpn_conv1"
}
layer {
  name: "rpn_cls_score"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_cls_score"
  convolution_param {
    num_output: 18
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "rpn_bbox_pred"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_bbox_pred"
  convolution_param {
    num_output: 36
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "rpn_cls_score_reshape"
  type: "Reshape"
  bottom: "rpn_cls_score"
  top: "rpn_cls_score_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 2
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "rpn_cls_prob"
  type: "Softmax"
  bottom: "rpn_cls_score_reshape"
  top: "rpn_cls_prob"
}
layer {
  name: "rpn_cls_prob_reshape"
  type: "Reshape"
  bottom: "rpn_cls_prob"
  top: "rpn_cls_prob_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 18
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "proposal"
  type: "Python"
  bottom: "rpn_cls_prob_reshape"
  bottom: "rpn_bbox_pred"
  bottom: "im_info"
  top: "rois"
  top: "scores"
  python_param {
    module: "rpn.proposal_layer"
    layer: "ProposalLayer"
    param_str: "\'feat_stride\': 16"
  }
}
I1025 23:42:56.218614 16968 net.cpp:413] Input 0 -> data
I1025 23:42:56.224738 16968 net.cpp:413] Input 1 -> im_info
I1025 23:42:56.224789 16968 layer_factory.hpp:77] Creating layer conv1
I1025 23:42:56.224836 16968 net.cpp:106] Creating Layer conv1
I1025 23:42:56.224839 16968 net.cpp:454] conv1 <- data
I1025 23:42:56.224862 16968 net.cpp:411] conv1 -> conv1
I1025 23:42:56.332710 16968 net.cpp:150] Setting up conv1
I1025 23:42:56.332736 16968 net.cpp:157] Top shape: 1 96 112 112 (1204224)
I1025 23:42:56.332738 16968 net.cpp:165] Memory required for data: 4816896
I1025 23:42:56.332762 16968 layer_factory.hpp:77] Creating layer relu1
I1025 23:42:56.332782 16968 net.cpp:106] Creating Layer relu1
I1025 23:42:56.332784 16968 net.cpp:454] relu1 <- conv1
I1025 23:42:56.332798 16968 net.cpp:397] relu1 -> conv1 (in-place)
I1025 23:42:56.333014 16968 net.cpp:150] Setting up relu1
I1025 23:42:56.333021 16968 net.cpp:157] Top shape: 1 96 112 112 (1204224)
I1025 23:42:56.333024 16968 net.cpp:165] Memory required for data: 9633792
I1025 23:42:56.333035 16968 layer_factory.hpp:77] Creating layer norm1
I1025 23:42:56.333042 16968 net.cpp:106] Creating Layer norm1
I1025 23:42:56.333045 16968 net.cpp:454] norm1 <- conv1
I1025 23:42:56.333048 16968 net.cpp:411] norm1 -> norm1
I1025 23:42:56.333129 16968 net.cpp:150] Setting up norm1
I1025 23:42:56.333134 16968 net.cpp:157] Top shape: 1 96 112 112 (1204224)
I1025 23:42:56.333135 16968 net.cpp:165] Memory required for data: 14450688
I1025 23:42:56.333148 16968 layer_factory.hpp:77] Creating layer pool1
I1025 23:42:56.333151 16968 net.cpp:106] Creating Layer pool1
I1025 23:42:56.333153 16968 net.cpp:454] pool1 <- norm1
I1025 23:42:56.333156 16968 net.cpp:411] pool1 -> pool1
I1025 23:42:56.333176 16968 net.cpp:150] Setting up pool1
I1025 23:42:56.333180 16968 net.cpp:157] Top shape: 1 96 57 57 (311904)
I1025 23:42:56.333183 16968 net.cpp:165] Memory required for data: 15698304
I1025 23:42:56.333184 16968 layer_factory.hpp:77] Creating layer conv2
I1025 23:42:56.333190 16968 net.cpp:106] Creating Layer conv2
I1025 23:42:56.333191 16968 net.cpp:454] conv2 <- pool1
I1025 23:42:56.333194 16968 net.cpp:411] conv2 -> conv2
I1025 23:42:56.334753 16968 net.cpp:150] Setting up conv2
I1025 23:42:56.334761 16968 net.cpp:157] Top shape: 1 256 29 29 (215296)
I1025 23:42:56.334764 16968 net.cpp:165] Memory required for data: 16559488
I1025 23:42:56.334769 16968 layer_factory.hpp:77] Creating layer relu2
I1025 23:42:56.334772 16968 net.cpp:106] Creating Layer relu2
I1025 23:42:56.334774 16968 net.cpp:454] relu2 <- conv2
I1025 23:42:56.334777 16968 net.cpp:397] relu2 -> conv2 (in-place)
I1025 23:42:56.335014 16968 net.cpp:150] Setting up relu2
I1025 23:42:56.335031 16968 net.cpp:157] Top shape: 1 256 29 29 (215296)
I1025 23:42:56.335033 16968 net.cpp:165] Memory required for data: 17420672
I1025 23:42:56.335045 16968 layer_factory.hpp:77] Creating layer norm2
I1025 23:42:56.335049 16968 net.cpp:106] Creating Layer norm2
I1025 23:42:56.335052 16968 net.cpp:454] norm2 <- conv2
I1025 23:42:56.335054 16968 net.cpp:411] norm2 -> norm2
I1025 23:42:56.335151 16968 net.cpp:150] Setting up norm2
I1025 23:42:56.335156 16968 net.cpp:157] Top shape: 1 256 29 29 (215296)
I1025 23:42:56.335157 16968 net.cpp:165] Memory required for data: 18281856
I1025 23:42:56.335170 16968 layer_factory.hpp:77] Creating layer pool2
I1025 23:42:56.335173 16968 net.cpp:106] Creating Layer pool2
I1025 23:42:56.335175 16968 net.cpp:454] pool2 <- norm2
I1025 23:42:56.335187 16968 net.cpp:411] pool2 -> pool2
I1025 23:42:56.335218 16968 net.cpp:150] Setting up pool2
I1025 23:42:56.335222 16968 net.cpp:157] Top shape: 1 256 15 15 (57600)
I1025 23:42:56.335224 16968 net.cpp:165] Memory required for data: 18512256
I1025 23:42:56.335225 16968 layer_factory.hpp:77] Creating layer conv3
I1025 23:42:56.335242 16968 net.cpp:106] Creating Layer conv3
I1025 23:42:56.335244 16968 net.cpp:454] conv3 <- pool2
I1025 23:42:56.335248 16968 net.cpp:411] conv3 -> conv3
I1025 23:42:56.336974 16968 net.cpp:150] Setting up conv3
I1025 23:42:56.336985 16968 net.cpp:157] Top shape: 1 384 15 15 (86400)
I1025 23:42:56.336987 16968 net.cpp:165] Memory required for data: 18857856
I1025 23:42:56.337008 16968 layer_factory.hpp:77] Creating layer relu3
I1025 23:42:56.337014 16968 net.cpp:106] Creating Layer relu3
I1025 23:42:56.337016 16968 net.cpp:454] relu3 <- conv3
I1025 23:42:56.337019 16968 net.cpp:397] relu3 -> conv3 (in-place)
I1025 23:42:56.337157 16968 net.cpp:150] Setting up relu3
I1025 23:42:56.337163 16968 net.cpp:157] Top shape: 1 384 15 15 (86400)
I1025 23:42:56.337165 16968 net.cpp:165] Memory required for data: 19203456
I1025 23:42:56.337167 16968 layer_factory.hpp:77] Creating layer conv4
I1025 23:42:56.337173 16968 net.cpp:106] Creating Layer conv4
I1025 23:42:56.337175 16968 net.cpp:454] conv4 <- conv3
I1025 23:42:56.337178 16968 net.cpp:411] conv4 -> conv4
I1025 23:42:56.339515 16968 net.cpp:150] Setting up conv4
I1025 23:42:56.339532 16968 net.cpp:157] Top shape: 1 384 15 15 (86400)
I1025 23:42:56.339534 16968 net.cpp:165] Memory required for data: 19549056
I1025 23:42:56.339540 16968 layer_factory.hpp:77] Creating layer relu4
I1025 23:42:56.339545 16968 net.cpp:106] Creating Layer relu4
I1025 23:42:56.339547 16968 net.cpp:454] relu4 <- conv4
I1025 23:42:56.339551 16968 net.cpp:397] relu4 -> conv4 (in-place)
I1025 23:42:56.339768 16968 net.cpp:150] Setting up relu4
I1025 23:42:56.339776 16968 net.cpp:157] Top shape: 1 384 15 15 (86400)
I1025 23:42:56.339787 16968 net.cpp:165] Memory required for data: 19894656
I1025 23:42:56.339789 16968 layer_factory.hpp:77] Creating layer conv5
I1025 23:42:56.339809 16968 net.cpp:106] Creating Layer conv5
I1025 23:42:56.339812 16968 net.cpp:454] conv5 <- conv4
I1025 23:42:56.339817 16968 net.cpp:411] conv5 -> conv5
I1025 23:42:56.341502 16968 net.cpp:150] Setting up conv5
I1025 23:42:56.341512 16968 net.cpp:157] Top shape: 1 256 15 15 (57600)
I1025 23:42:56.341514 16968 net.cpp:165] Memory required for data: 20125056
I1025 23:42:56.341521 16968 layer_factory.hpp:77] Creating layer relu5
I1025 23:42:56.341526 16968 net.cpp:106] Creating Layer relu5
I1025 23:42:56.341527 16968 net.cpp:454] relu5 <- conv5
I1025 23:42:56.341532 16968 net.cpp:397] relu5 -> conv5 (in-place)
I1025 23:42:56.341761 16968 net.cpp:150] Setting up relu5
I1025 23:42:56.341769 16968 net.cpp:157] Top shape: 1 256 15 15 (57600)
I1025 23:42:56.341770 16968 net.cpp:165] Memory required for data: 20355456
I1025 23:42:56.341773 16968 layer_factory.hpp:77] Creating layer rpn_conv1
I1025 23:42:56.341789 16968 net.cpp:106] Creating Layer rpn_conv1
I1025 23:42:56.341792 16968 net.cpp:454] rpn_conv1 <- conv5
I1025 23:42:56.341805 16968 net.cpp:411] rpn_conv1 -> rpn_conv1
I1025 23:42:56.343191 16968 net.cpp:150] Setting up rpn_conv1
I1025 23:42:56.343201 16968 net.cpp:157] Top shape: 1 256 15 15 (57600)
I1025 23:42:56.343204 16968 net.cpp:165] Memory required for data: 20585856
I1025 23:42:56.343207 16968 layer_factory.hpp:77] Creating layer rpn_relu1
I1025 23:42:56.343225 16968 net.cpp:106] Creating Layer rpn_relu1
I1025 23:42:56.343227 16968 net.cpp:454] rpn_relu1 <- rpn_conv1
I1025 23:42:56.343230 16968 net.cpp:397] rpn_relu1 -> rpn_conv1 (in-place)
I1025 23:42:56.343369 16968 net.cpp:150] Setting up rpn_relu1
I1025 23:42:56.343374 16968 net.cpp:157] Top shape: 1 256 15 15 (57600)
I1025 23:42:56.343376 16968 net.cpp:165] Memory required for data: 20816256
I1025 23:42:56.343387 16968 layer_factory.hpp:77] Creating layer rpn_conv1_rpn_relu1_0_split
I1025 23:42:56.343395 16968 net.cpp:106] Creating Layer rpn_conv1_rpn_relu1_0_split
I1025 23:42:56.343397 16968 net.cpp:454] rpn_conv1_rpn_relu1_0_split <- rpn_conv1
I1025 23:42:56.343401 16968 net.cpp:411] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_0
I1025 23:42:56.343405 16968 net.cpp:411] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_1
I1025 23:42:56.343449 16968 net.cpp:150] Setting up rpn_conv1_rpn_relu1_0_split
I1025 23:42:56.343456 16968 net.cpp:157] Top shape: 1 256 15 15 (57600)
I1025 23:42:56.343468 16968 net.cpp:157] Top shape: 1 256 15 15 (57600)
I1025 23:42:56.343471 16968 net.cpp:165] Memory required for data: 21277056
I1025 23:42:56.343472 16968 layer_factory.hpp:77] Creating layer rpn_cls_score
I1025 23:42:56.343477 16968 net.cpp:106] Creating Layer rpn_cls_score
I1025 23:42:56.343480 16968 net.cpp:454] rpn_cls_score <- rpn_conv1_rpn_relu1_0_split_0
I1025 23:42:56.343483 16968 net.cpp:411] rpn_cls_score -> rpn_cls_score
I1025 23:42:56.344359 16968 net.cpp:150] Setting up rpn_cls_score
I1025 23:42:56.344368 16968 net.cpp:157] Top shape: 1 18 15 15 (4050)
I1025 23:42:56.344380 16968 net.cpp:165] Memory required for data: 21293256
I1025 23:42:56.344385 16968 layer_factory.hpp:77] Creating layer rpn_bbox_pred
I1025 23:42:56.344401 16968 net.cpp:106] Creating Layer rpn_bbox_pred
I1025 23:42:56.344403 16968 net.cpp:454] rpn_bbox_pred <- rpn_conv1_rpn_relu1_0_split_1
I1025 23:42:56.344408 16968 net.cpp:411] rpn_bbox_pred -> rpn_bbox_pred
I1025 23:42:56.345249 16968 net.cpp:150] Setting up rpn_bbox_pred
I1025 23:42:56.345258 16968 net.cpp:157] Top shape: 1 36 15 15 (8100)
I1025 23:42:56.345259 16968 net.cpp:165] Memory required for data: 21325656
I1025 23:42:56.345263 16968 layer_factory.hpp:77] Creating layer rpn_cls_score_reshape
I1025 23:42:56.345276 16968 net.cpp:106] Creating Layer rpn_cls_score_reshape
I1025 23:42:56.345289 16968 net.cpp:454] rpn_cls_score_reshape <- rpn_cls_score
I1025 23:42:56.345294 16968 net.cpp:411] rpn_cls_score_reshape -> rpn_cls_score_reshape
I1025 23:42:56.345326 16968 net.cpp:150] Setting up rpn_cls_score_reshape
I1025 23:42:56.345330 16968 net.cpp:157] Top shape: 1 2 135 15 (4050)
I1025 23:42:56.345332 16968 net.cpp:165] Memory required for data: 21341856
I1025 23:42:56.345333 16968 layer_factory.hpp:77] Creating layer rpn_cls_prob
I1025 23:42:56.345347 16968 net.cpp:106] Creating Layer rpn_cls_prob
I1025 23:42:56.345348 16968 net.cpp:454] rpn_cls_prob <- rpn_cls_score_reshape
I1025 23:42:56.345352 16968 net.cpp:411] rpn_cls_prob -> rpn_cls_prob
I1025 23:42:56.345518 16968 net.cpp:150] Setting up rpn_cls_prob
I1025 23:42:56.345525 16968 net.cpp:157] Top shape: 1 2 135 15 (4050)
I1025 23:42:56.345526 16968 net.cpp:165] Memory required for data: 21358056
I1025 23:42:56.345528 16968 layer_factory.hpp:77] Creating layer rpn_cls_prob_reshape
I1025 23:42:56.345532 16968 net.cpp:106] Creating Layer rpn_cls_prob_reshape
I1025 23:42:56.345535 16968 net.cpp:454] rpn_cls_prob_reshape <- rpn_cls_prob
I1025 23:42:56.345553 16968 net.cpp:411] rpn_cls_prob_reshape -> rpn_cls_prob_reshape
I1025 23:42:56.345569 16968 net.cpp:150] Setting up rpn_cls_prob_reshape
I1025 23:42:56.345573 16968 net.cpp:157] Top shape: 1 18 15 15 (4050)
I1025 23:42:56.345576 16968 net.cpp:165] Memory required for data: 21374256
I1025 23:42:56.345577 16968 layer_factory.hpp:77] Creating layer proposal
I1025 23:42:56.347753 16968 net.cpp:106] Creating Layer proposal
I1025 23:42:56.347764 16968 net.cpp:454] proposal <- rpn_cls_prob_reshape
I1025 23:42:56.347777 16968 net.cpp:454] proposal <- rpn_bbox_pred
I1025 23:42:56.347779 16968 net.cpp:454] proposal <- im_info
I1025 23:42:56.347784 16968 net.cpp:411] proposal -> rois
I1025 23:42:56.347787 16968 net.cpp:411] proposal -> scores
I1025 23:42:56.348553 16968 net.cpp:150] Setting up proposal
I1025 23:42:56.348562 16968 net.cpp:157] Top shape: 1 5 (5)
I1025 23:42:56.348575 16968 net.cpp:157] Top shape: 1 1 1 1 (1)
I1025 23:42:56.348577 16968 net.cpp:165] Memory required for data: 21374280
I1025 23:42:56.348579 16968 net.cpp:228] proposal does not need backward computation.
I1025 23:42:56.348582 16968 net.cpp:228] rpn_cls_prob_reshape does not need backward computation.
I1025 23:42:56.348584 16968 net.cpp:228] rpn_cls_prob does not need backward computation.
I1025 23:42:56.348587 16968 net.cpp:228] rpn_cls_score_reshape does not need backward computation.
I1025 23:42:56.348588 16968 net.cpp:228] rpn_bbox_pred does not need backward computation.
I1025 23:42:56.348590 16968 net.cpp:228] rpn_cls_score does not need backward computation.
I1025 23:42:56.348592 16968 net.cpp:228] rpn_conv1_rpn_relu1_0_split does not need backward computation.
I1025 23:42:56.348594 16968 net.cpp:228] rpn_relu1 does not need backward computation.
I1025 23:42:56.348606 16968 net.cpp:228] rpn_conv1 does not need backward computation.
I1025 23:42:56.348608 16968 net.cpp:228] relu5 does not need backward computation.
I1025 23:42:56.348610 16968 net.cpp:228] conv5 does not need backward computation.
I1025 23:42:56.348611 16968 net.cpp:228] relu4 does not need backward computation.
I1025 23:42:56.348613 16968 net.cpp:228] conv4 does not need backward computation.
I1025 23:42:56.348616 16968 net.cpp:228] relu3 does not need backward computation.
I1025 23:42:56.348618 16968 net.cpp:228] conv3 does not need backward computation.
I1025 23:42:56.348619 16968 net.cpp:228] pool2 does not need backward computation.
I1025 23:42:56.348621 16968 net.cpp:228] norm2 does not need backward computation.
I1025 23:42:56.348623 16968 net.cpp:228] relu2 does not need backward computation.
I1025 23:42:56.348625 16968 net.cpp:228] conv2 does not need backward computation.
I1025 23:42:56.348628 16968 net.cpp:228] pool1 does not need backward computation.
I1025 23:42:56.348629 16968 net.cpp:228] norm1 does not need backward computation.
I1025 23:42:56.348640 16968 net.cpp:228] relu1 does not need backward computation.
I1025 23:42:56.348642 16968 net.cpp:228] conv1 does not need backward computation.
I1025 23:42:56.348644 16968 net.cpp:270] This network produces output rois
I1025 23:42:56.348646 16968 net.cpp:270] This network produces output scores
I1025 23:42:56.348656 16968 net.cpp:283] Network initialization done.
I1025 23:42:56.420928 16968 net.cpp:816] Ignoring source layer input-data
I1025 23:42:56.420949 16968 net.cpp:816] Ignoring source layer data_input-data_0_split
I1025 23:42:56.423705 16968 net.cpp:816] Ignoring source layer rpn_cls_score_rpn_cls_score_0_split
I1025 23:42:56.423727 16968 net.cpp:816] Ignoring source layer rpn-data
I1025 23:42:56.423729 16968 net.cpp:816] Ignoring source layer rpn_loss_cls
I1025 23:42:56.423730 16968 net.cpp:816] Ignoring source layer rpn_loss_bbox
I1025 23:42:56.423732 16968 net.cpp:816] Ignoring source layer dummy_roi_pool_conv5
I1025 23:42:56.423734 16968 net.cpp:816] Ignoring source layer fc6
I1025 23:42:56.423735 16968 net.cpp:816] Ignoring source layer relu6
I1025 23:42:56.423738 16968 net.cpp:816] Ignoring source layer fc7
I1025 23:42:56.423738 16968 net.cpp:816] Ignoring source layer silence_fc7
RPN model: /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_rpn_stage1_iter_40000.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 2000,
          'RPN_PRE_NMS_TOP_N': -1,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': True,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': False,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'selective_search',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for proposal generation
Output will be saved to `/home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train`
im_proposals: 1/712 0.055s
im_proposals: 2/712 0.057s
im_proposals: 3/712 0.055s
im_proposals: 4/712 0.054s
im_proposals: 5/712 0.052s
im_proposals: 6/712 0.051s
im_proposals: 7/712 0.050s
im_proposals: 8/712 0.049s
im_proposals: 9/712 0.049s
im_proposals: 10/712 0.050s
im_proposals: 11/712 0.049s
im_proposals: 12/712 0.049s
im_proposals: 13/712 0.049s
im_proposals: 14/712 0.049s
im_proposals: 15/712 0.050s
im_proposals: 16/712 0.050s
im_proposals: 17/712 0.050s
im_proposals: 18/712 0.049s
im_proposals: 19/712 0.049s
im_proposals: 20/712 0.049s
im_proposals: 21/712 0.049s
im_proposals: 22/712 0.049s
im_proposals: 23/712 0.049s
im_proposals: 24/712 0.049s
im_proposals: 25/712 0.048s
im_proposals: 26/712 0.048s
im_proposals: 27/712 0.048s
im_proposals: 28/712 0.048s
im_proposals: 29/712 0.048s
im_proposals: 30/712 0.048s
im_proposals: 31/712 0.047s
im_proposals: 32/712 0.047s
im_proposals: 33/712 0.047s
im_proposals: 34/712 0.047s
im_proposals: 35/712 0.047s
im_proposals: 36/712 0.046s
im_proposals: 37/712 0.046s
im_proposals: 38/712 0.046s
im_proposals: 39/712 0.046s
im_proposals: 40/712 0.046s
im_proposals: 41/712 0.046s
im_proposals: 42/712 0.046s
im_proposals: 43/712 0.046s
im_proposals: 44/712 0.046s
im_proposals: 45/712 0.046s
im_proposals: 46/712 0.046s
im_proposals: 47/712 0.046s
im_proposals: 48/712 0.046s
im_proposals: 49/712 0.047s
im_proposals: 50/712 0.046s
im_proposals: 51/712 0.046s
im_proposals: 52/712 0.047s
im_proposals: 53/712 0.047s
im_proposals: 54/712 0.047s
im_proposals: 55/712 0.047s
im_proposals: 56/712 0.046s
im_proposals: 57/712 0.047s
im_proposals: 58/712 0.046s
im_proposals: 59/712 0.046s
im_proposals: 60/712 0.046s
im_proposals: 61/712 0.046s
im_proposals: 62/712 0.047s
im_proposals: 63/712 0.047s
im_proposals: 64/712 0.047s
im_proposals: 65/712 0.046s
im_proposals: 66/712 0.047s
im_proposals: 67/712 0.047s
im_proposals: 68/712 0.047s
im_proposals: 69/712 0.047s
im_proposals: 70/712 0.047s
im_proposals: 71/712 0.047s
im_proposals: 72/712 0.046s
im_proposals: 73/712 0.047s
im_proposals: 74/712 0.046s
im_proposals: 75/712 0.047s
im_proposals: 76/712 0.047s
im_proposals: 77/712 0.046s
im_proposals: 78/712 0.047s
im_proposals: 79/712 0.047s
im_proposals: 80/712 0.046s
im_proposals: 81/712 0.046s
im_proposals: 82/712 0.046s
im_proposals: 83/712 0.046s
im_proposals: 84/712 0.046s
im_proposals: 85/712 0.046s
im_proposals: 86/712 0.046s
im_proposals: 87/712 0.046s
im_proposals: 88/712 0.046s
im_proposals: 89/712 0.046s
im_proposals: 90/712 0.046s
im_proposals: 91/712 0.046s
im_proposals: 92/712 0.046s
im_proposals: 93/712 0.046s
im_proposals: 94/712 0.046s
im_proposals: 95/712 0.046s
im_proposals: 96/712 0.046s
im_proposals: 97/712 0.046s
im_proposals: 98/712 0.046s
im_proposals: 99/712 0.046s
im_proposals: 100/712 0.046s
im_proposals: 101/712 0.046s
im_proposals: 102/712 0.046s
im_proposals: 103/712 0.046s
im_proposals: 104/712 0.046s
im_proposals: 105/712 0.046s
im_proposals: 106/712 0.046s
im_proposals: 107/712 0.046s
im_proposals: 108/712 0.046s
im_proposals: 109/712 0.046s
im_proposals: 110/712 0.046s
im_proposals: 111/712 0.046s
im_proposals: 112/712 0.046s
im_proposals: 113/712 0.046s
im_proposals: 114/712 0.046s
im_proposals: 115/712 0.046s
im_proposals: 116/712 0.046s
im_proposals: 117/712 0.046s
im_proposals: 118/712 0.046s
im_proposals: 119/712 0.046s
im_proposals: 120/712 0.046s
im_proposals: 121/712 0.046s
im_proposals: 122/712 0.046s
im_proposals: 123/712 0.046s
im_proposals: 124/712 0.046s
im_proposals: 125/712 0.046s
im_proposals: 126/712 0.046s
im_proposals: 127/712 0.046s
im_proposals: 128/712 0.046s
im_proposals: 129/712 0.046s
im_proposals: 130/712 0.046s
im_proposals: 131/712 0.046s
im_proposals: 132/712 0.046s
im_proposals: 133/712 0.046s
im_proposals: 134/712 0.046s
im_proposals: 135/712 0.046s
im_proposals: 136/712 0.046s
im_proposals: 137/712 0.046s
im_proposals: 138/712 0.046s
im_proposals: 139/712 0.046s
im_proposals: 140/712 0.046s
im_proposals: 141/712 0.046s
im_proposals: 142/712 0.046s
im_proposals: 143/712 0.046s
im_proposals: 144/712 0.046s
im_proposals: 145/712 0.046s
im_proposals: 146/712 0.046s
im_proposals: 147/712 0.046s
im_proposals: 148/712 0.046s
im_proposals: 149/712 0.046s
im_proposals: 150/712 0.046s
im_proposals: 151/712 0.046s
im_proposals: 152/712 0.046s
im_proposals: 153/712 0.046s
im_proposals: 154/712 0.046s
im_proposals: 155/712 0.046s
im_proposals: 156/712 0.046s
im_proposals: 157/712 0.046s
im_proposals: 158/712 0.046s
im_proposals: 159/712 0.046s
im_proposals: 160/712 0.046s
im_proposals: 161/712 0.046s
im_proposals: 162/712 0.046s
im_proposals: 163/712 0.046s
im_proposals: 164/712 0.046s
im_proposals: 165/712 0.046s
im_proposals: 166/712 0.046s
im_proposals: 167/712 0.046s
im_proposals: 168/712 0.046s
im_proposals: 169/712 0.046s
im_proposals: 170/712 0.046s
im_proposals: 171/712 0.046s
im_proposals: 172/712 0.046s
im_proposals: 173/712 0.046s
im_proposals: 174/712 0.046s
im_proposals: 175/712 0.046s
im_proposals: 176/712 0.046s
im_proposals: 177/712 0.046s
im_proposals: 178/712 0.046s
im_proposals: 179/712 0.046s
im_proposals: 180/712 0.046s
im_proposals: 181/712 0.046s
im_proposals: 182/712 0.046s
im_proposals: 183/712 0.046s
im_proposals: 184/712 0.046s
im_proposals: 185/712 0.046s
im_proposals: 186/712 0.046s
im_proposals: 187/712 0.046s
im_proposals: 188/712 0.046s
im_proposals: 189/712 0.046s
im_proposals: 190/712 0.046s
im_proposals: 191/712 0.046s
im_proposals: 192/712 0.046s
im_proposals: 193/712 0.046s
im_proposals: 194/712 0.046s
im_proposals: 195/712 0.046s
im_proposals: 196/712 0.046s
im_proposals: 197/712 0.046s
im_proposals: 198/712 0.046s
im_proposals: 199/712 0.046s
im_proposals: 200/712 0.046s
im_proposals: 201/712 0.046s
im_proposals: 202/712 0.046s
im_proposals: 203/712 0.046s
im_proposals: 204/712 0.046s
im_proposals: 205/712 0.046s
im_proposals: 206/712 0.046s
im_proposals: 207/712 0.046s
im_proposals: 208/712 0.046s
im_proposals: 209/712 0.046s
im_proposals: 210/712 0.046s
im_proposals: 211/712 0.046s
im_proposals: 212/712 0.046s
im_proposals: 213/712 0.046s
im_proposals: 214/712 0.046s
im_proposals: 215/712 0.046s
im_proposals: 216/712 0.046s
im_proposals: 217/712 0.046s
im_proposals: 218/712 0.046s
im_proposals: 219/712 0.046s
im_proposals: 220/712 0.046s
im_proposals: 221/712 0.046s
im_proposals: 222/712 0.046s
im_proposals: 223/712 0.046s
im_proposals: 224/712 0.046s
im_proposals: 225/712 0.045s
im_proposals: 226/712 0.045s
im_proposals: 227/712 0.045s
im_proposals: 228/712 0.045s
im_proposals: 229/712 0.045s
im_proposals: 230/712 0.045s
im_proposals: 231/712 0.045s
im_proposals: 232/712 0.045s
im_proposals: 233/712 0.045s
im_proposals: 234/712 0.045s
im_proposals: 235/712 0.045s
im_proposals: 236/712 0.045s
im_proposals: 237/712 0.045s
im_proposals: 238/712 0.045s
im_proposals: 239/712 0.045s
im_proposals: 240/712 0.045s
im_proposals: 241/712 0.045s
im_proposals: 242/712 0.045s
im_proposals: 243/712 0.045s
im_proposals: 244/712 0.045s
im_proposals: 245/712 0.045s
im_proposals: 246/712 0.045s
im_proposals: 247/712 0.045s
im_proposals: 248/712 0.045s
im_proposals: 249/712 0.045s
im_proposals: 250/712 0.045s
im_proposals: 251/712 0.045s
im_proposals: 252/712 0.045s
im_proposals: 253/712 0.045s
im_proposals: 254/712 0.045s
im_proposals: 255/712 0.045s
im_proposals: 256/712 0.045s
im_proposals: 257/712 0.045s
im_proposals: 258/712 0.045s
im_proposals: 259/712 0.045s
im_proposals: 260/712 0.046s
im_proposals: 261/712 0.046s
im_proposals: 262/712 0.046s
im_proposals: 263/712 0.046s
im_proposals: 264/712 0.046s
im_proposals: 265/712 0.046s
im_proposals: 266/712 0.046s
im_proposals: 267/712 0.046s
im_proposals: 268/712 0.045s
im_proposals: 269/712 0.045s
im_proposals: 270/712 0.046s
im_proposals: 271/712 0.045s
im_proposals: 272/712 0.046s
im_proposals: 273/712 0.046s
im_proposals: 274/712 0.046s
im_proposals: 275/712 0.046s
im_proposals: 276/712 0.046s
im_proposals: 277/712 0.046s
im_proposals: 278/712 0.046s
im_proposals: 279/712 0.046s
im_proposals: 280/712 0.046s
im_proposals: 281/712 0.045s
im_proposals: 282/712 0.045s
im_proposals: 283/712 0.045s
im_proposals: 284/712 0.045s
im_proposals: 285/712 0.045s
im_proposals: 286/712 0.045s
im_proposals: 287/712 0.046s
im_proposals: 288/712 0.046s
im_proposals: 289/712 0.046s
im_proposals: 290/712 0.046s
im_proposals: 291/712 0.046s
im_proposals: 292/712 0.046s
im_proposals: 293/712 0.046s
im_proposals: 294/712 0.046s
im_proposals: 295/712 0.046s
im_proposals: 296/712 0.046s
im_proposals: 297/712 0.046s
im_proposals: 298/712 0.046s
im_proposals: 299/712 0.046s
im_proposals: 300/712 0.046s
im_proposals: 301/712 0.046s
im_proposals: 302/712 0.046s
im_proposals: 303/712 0.046s
im_proposals: 304/712 0.046s
im_proposals: 305/712 0.046s
im_proposals: 306/712 0.046s
im_proposals: 307/712 0.046s
im_proposals: 308/712 0.046s
im_proposals: 309/712 0.046s
im_proposals: 310/712 0.046s
im_proposals: 311/712 0.046s
im_proposals: 312/712 0.046s
im_proposals: 313/712 0.046s
im_proposals: 314/712 0.046s
im_proposals: 315/712 0.046s
im_proposals: 316/712 0.046s
im_proposals: 317/712 0.046s
im_proposals: 318/712 0.046s
im_proposals: 319/712 0.046s
im_proposals: 320/712 0.046s
im_proposals: 321/712 0.046s
im_proposals: 322/712 0.046s
im_proposals: 323/712 0.046s
im_proposals: 324/712 0.046s
im_proposals: 325/712 0.046s
im_proposals: 326/712 0.046s
im_proposals: 327/712 0.046s
im_proposals: 328/712 0.046s
im_proposals: 329/712 0.046s
im_proposals: 330/712 0.046s
im_proposals: 331/712 0.046s
im_proposals: 332/712 0.046s
im_proposals: 333/712 0.046s
im_proposals: 334/712 0.046s
im_proposals: 335/712 0.046s
im_proposals: 336/712 0.046s
im_proposals: 337/712 0.046s
im_proposals: 338/712 0.046s
im_proposals: 339/712 0.046s
im_proposals: 340/712 0.046s
im_proposals: 341/712 0.046s
im_proposals: 342/712 0.046s
im_proposals: 343/712 0.046s
im_proposals: 344/712 0.046s
im_proposals: 345/712 0.046s
im_proposals: 346/712 0.046s
im_proposals: 347/712 0.046s
im_proposals: 348/712 0.046s
im_proposals: 349/712 0.046s
im_proposals: 350/712 0.046s
im_proposals: 351/712 0.046s
im_proposals: 352/712 0.046s
im_proposals: 353/712 0.046s
im_proposals: 354/712 0.046s
im_proposals: 355/712 0.046s
im_proposals: 356/712 0.046s
im_proposals: 357/712 0.046s
im_proposals: 358/712 0.046s
im_proposals: 359/712 0.046s
im_proposals: 360/712 0.046s
im_proposals: 361/712 0.046s
im_proposals: 362/712 0.046s
im_proposals: 363/712 0.046s
im_proposals: 364/712 0.046s
im_proposals: 365/712 0.046s
im_proposals: 366/712 0.046s
im_proposals: 367/712 0.046s
im_proposals: 368/712 0.046s
im_proposals: 369/712 0.046s
im_proposals: 370/712 0.046s
im_proposals: 371/712 0.046s
im_proposals: 372/712 0.046s
im_proposals: 373/712 0.046s
im_proposals: 374/712 0.046s
im_proposals: 375/712 0.046s
im_proposals: 376/712 0.046s
im_proposals: 377/712 0.046s
im_proposals: 378/712 0.046s
im_proposals: 379/712 0.046s
im_proposals: 380/712 0.046s
im_proposals: 381/712 0.046s
im_proposals: 382/712 0.046s
im_proposals: 383/712 0.046s
im_proposals: 384/712 0.046s
im_proposals: 385/712 0.046s
im_proposals: 386/712 0.045s
im_proposals: 387/712 0.045s
im_proposals: 388/712 0.045s
im_proposals: 389/712 0.045s
im_proposals: 390/712 0.045s
im_proposals: 391/712 0.045s
im_proposals: 392/712 0.045s
im_proposals: 393/712 0.045s
im_proposals: 394/712 0.045s
im_proposals: 395/712 0.045s
im_proposals: 396/712 0.045s
im_proposals: 397/712 0.045s
im_proposals: 398/712 0.045s
im_proposals: 399/712 0.045s
im_proposals: 400/712 0.045s
im_proposals: 401/712 0.045s
im_proposals: 402/712 0.045s
im_proposals: 403/712 0.045s
im_proposals: 404/712 0.045s
im_proposals: 405/712 0.045s
im_proposals: 406/712 0.045s
im_proposals: 407/712 0.045s
im_proposals: 408/712 0.045s
im_proposals: 409/712 0.045s
im_proposals: 410/712 0.045s
im_proposals: 411/712 0.045s
im_proposals: 412/712 0.045s
im_proposals: 413/712 0.045s
im_proposals: 414/712 0.045s
im_proposals: 415/712 0.045s
im_proposals: 416/712 0.045s
im_proposals: 417/712 0.045s
im_proposals: 418/712 0.045s
im_proposals: 419/712 0.045s
im_proposals: 420/712 0.045s
im_proposals: 421/712 0.045s
im_proposals: 422/712 0.045s
im_proposals: 423/712 0.045s
im_proposals: 424/712 0.045s
im_proposals: 425/712 0.045s
im_proposals: 426/712 0.045s
im_proposals: 427/712 0.045s
im_proposals: 428/712 0.045s
im_proposals: 429/712 0.045s
im_proposals: 430/712 0.045s
im_proposals: 431/712 0.045s
im_proposals: 432/712 0.045s
im_proposals: 433/712 0.045s
im_proposals: 434/712 0.045s
im_proposals: 435/712 0.045s
im_proposals: 436/712 0.045s
im_proposals: 437/712 0.045s
im_proposals: 438/712 0.045s
im_proposals: 439/712 0.045s
im_proposals: 440/712 0.045s
im_proposals: 441/712 0.045s
im_proposals: 442/712 0.045s
im_proposals: 443/712 0.045s
im_proposals: 444/712 0.045s
im_proposals: 445/712 0.045s
im_proposals: 446/712 0.045s
im_proposals: 447/712 0.045s
im_proposals: 448/712 0.045s
im_proposals: 449/712 0.045s
im_proposals: 450/712 0.045s
im_proposals: 451/712 0.045s
im_proposals: 452/712 0.045s
im_proposals: 453/712 0.045s
im_proposals: 454/712 0.045s
im_proposals: 455/712 0.045s
im_proposals: 456/712 0.045s
im_proposals: 457/712 0.045s
im_proposals: 458/712 0.045s
im_proposals: 459/712 0.045s
im_proposals: 460/712 0.045s
im_proposals: 461/712 0.045s
im_proposals: 462/712 0.045s
im_proposals: 463/712 0.045s
im_proposals: 464/712 0.045s
im_proposals: 465/712 0.045s
im_proposals: 466/712 0.045s
im_proposals: 467/712 0.045s
im_proposals: 468/712 0.045s
im_proposals: 469/712 0.045s
im_proposals: 470/712 0.045s
im_proposals: 471/712 0.045s
im_proposals: 472/712 0.045s
im_proposals: 473/712 0.045s
im_proposals: 474/712 0.045s
im_proposals: 475/712 0.045s
im_proposals: 476/712 0.045s
im_proposals: 477/712 0.045s
im_proposals: 478/712 0.045s
im_proposals: 479/712 0.045s
im_proposals: 480/712 0.045s
im_proposals: 481/712 0.045s
im_proposals: 482/712 0.045s
im_proposals: 483/712 0.045s
im_proposals: 484/712 0.045s
im_proposals: 485/712 0.045s
im_proposals: 486/712 0.045s
im_proposals: 487/712 0.045s
im_proposals: 488/712 0.045s
im_proposals: 489/712 0.045s
im_proposals: 490/712 0.045s
im_proposals: 491/712 0.045s
im_proposals: 492/712 0.045s
im_proposals: 493/712 0.045s
im_proposals: 494/712 0.045s
im_proposals: 495/712 0.045s
im_proposals: 496/712 0.045s
im_proposals: 497/712 0.045s
im_proposals: 498/712 0.045s
im_proposals: 499/712 0.045s
im_proposals: 500/712 0.045s
im_proposals: 501/712 0.045s
im_proposals: 502/712 0.045s
im_proposals: 503/712 0.045s
im_proposals: 504/712 0.045s
im_proposals: 505/712 0.045s
im_proposals: 506/712 0.045s
im_proposals: 507/712 0.045s
im_proposals: 508/712 0.045s
im_proposals: 509/712 0.045s
im_proposals: 510/712 0.045s
im_proposals: 511/712 0.045s
im_proposals: 512/712 0.045s
im_proposals: 513/712 0.045s
im_proposals: 514/712 0.045s
im_proposals: 515/712 0.045s
im_proposals: 516/712 0.045s
im_proposals: 517/712 0.045s
im_proposals: 518/712 0.045s
im_proposals: 519/712 0.045s
im_proposals: 520/712 0.045s
im_proposals: 521/712 0.045s
im_proposals: 522/712 0.045s
im_proposals: 523/712 0.046s
im_proposals: 524/712 0.046s
im_proposals: 525/712 0.046s
im_proposals: 526/712 0.046s
im_proposals: 527/712 0.045s
im_proposals: 528/712 0.045s
im_proposals: 529/712 0.045s
im_proposals: 530/712 0.045s
im_proposals: 531/712 0.045s
im_proposals: 532/712 0.045s
im_proposals: 533/712 0.045s
im_proposals: 534/712 0.045s
im_proposals: 535/712 0.045s
im_proposals: 536/712 0.045s
im_proposals: 537/712 0.045s
im_proposals: 538/712 0.045s
im_proposals: 539/712 0.045s
im_proposals: 540/712 0.045s
im_proposals: 541/712 0.046s
im_proposals: 542/712 0.045s
im_proposals: 543/712 0.045s
im_proposals: 544/712 0.046s
im_proposals: 545/712 0.045s
im_proposals: 546/712 0.046s
im_proposals: 547/712 0.045s
im_proposals: 548/712 0.046s
im_proposals: 549/712 0.046s
im_proposals: 550/712 0.046s
im_proposals: 551/712 0.045s
im_proposals: 552/712 0.045s
im_proposals: 553/712 0.045s
im_proposals: 554/712 0.046s
im_proposals: 555/712 0.046s
im_proposals: 556/712 0.045s
im_proposals: 557/712 0.046s
im_proposals: 558/712 0.045s
im_proposals: 559/712 0.045s
im_proposals: 560/712 0.045s
im_proposals: 561/712 0.045s
im_proposals: 562/712 0.045s
im_proposals: 563/712 0.045s
im_proposals: 564/712 0.045s
im_proposals: 565/712 0.045s
im_proposals: 566/712 0.045s
im_proposals: 567/712 0.045s
im_proposals: 568/712 0.045s
im_proposals: 569/712 0.045s
im_proposals: 570/712 0.045s
im_proposals: 571/712 0.045s
im_proposals: 572/712 0.045s
im_proposals: 573/712 0.045s
im_proposals: 574/712 0.045s
im_proposals: 575/712 0.045s
im_proposals: 576/712 0.045s
im_proposals: 577/712 0.045s
im_proposals: 578/712 0.045s
im_proposals: 579/712 0.045s
im_proposals: 580/712 0.045s
im_proposals: 581/712 0.045s
im_proposals: 582/712 0.045s
im_proposals: 583/712 0.045s
im_proposals: 584/712 0.045s
im_proposals: 585/712 0.045s
im_proposals: 586/712 0.045s
im_proposals: 587/712 0.045s
im_proposals: 588/712 0.045s
im_proposals: 589/712 0.045s
im_proposals: 590/712 0.045s
im_proposals: 591/712 0.045s
im_proposals: 592/712 0.045s
im_proposals: 593/712 0.045s
im_proposals: 594/712 0.045s
im_proposals: 595/712 0.045s
im_proposals: 596/712 0.045s
im_proposals: 597/712 0.045s
im_proposals: 598/712 0.045s
im_proposals: 599/712 0.045s
im_proposals: 600/712 0.045s
im_proposals: 601/712 0.045s
im_proposals: 602/712 0.045s
im_proposals: 603/712 0.045s
im_proposals: 604/712 0.045s
im_proposals: 605/712 0.045s
im_proposals: 606/712 0.045s
im_proposals: 607/712 0.045s
im_proposals: 608/712 0.045s
im_proposals: 609/712 0.045s
im_proposals: 610/712 0.045s
im_proposals: 611/712 0.045s
im_proposals: 612/712 0.045s
im_proposals: 613/712 0.045s
im_proposals: 614/712 0.045s
im_proposals: 615/712 0.045s
im_proposals: 616/712 0.045s
im_proposals: 617/712 0.045s
im_proposals: 618/712 0.045s
im_proposals: 619/712 0.045s
im_proposals: 620/712 0.045s
im_proposals: 621/712 0.045s
im_proposals: 622/712 0.045s
im_proposals: 623/712 0.045s
im_proposals: 624/712 0.045s
im_proposals: 625/712 0.045s
im_proposals: 626/712 0.045s
im_proposals: 627/712 0.045s
im_proposals: 628/712 0.045s
im_proposals: 629/712 0.045s
im_proposals: 630/712 0.045s
im_proposals: 631/712 0.045s
im_proposals: 632/712 0.045s
im_proposals: 633/712 0.045s
im_proposals: 634/712 0.045s
im_proposals: 635/712 0.045s
im_proposals: 636/712 0.045s
im_proposals: 637/712 0.045s
im_proposals: 638/712 0.045s
im_proposals: 639/712 0.045s
im_proposals: 640/712 0.045s
im_proposals: 641/712 0.045s
im_proposals: 642/712 0.045s
im_proposals: 643/712 0.045s
im_proposals: 644/712 0.045s
im_proposals: 645/712 0.045s
im_proposals: 646/712 0.045s
im_proposals: 647/712 0.045s
im_proposals: 648/712 0.045s
im_proposals: 649/712 0.045s
im_proposals: 650/712 0.045s
im_proposals: 651/712 0.045s
im_proposals: 652/712 0.045s
im_proposals: 653/712 0.045s
im_proposals: 654/712 0.045s
im_proposals: 655/712 0.045s
im_proposals: 656/712 0.045s
im_proposals: 657/712 0.045s
im_proposals: 658/712 0.045s
im_proposals: 659/712 0.045s
im_proposals: 660/712 0.045s
im_proposals: 661/712 0.045s
im_proposals: 662/712 0.045s
im_proposals: 663/712 0.045s
im_proposals: 664/712 0.045s
im_proposals: 665/712 0.045s
im_proposals: 666/712 0.045s
im_proposals: 667/712 0.045s
im_proposals: 668/712 0.045s
im_proposals: 669/712 0.045s
im_proposals: 670/712 0.045s
im_proposals: 671/712 0.045s
im_proposals: 672/712 0.045s
im_proposals: 673/712 0.045s
im_proposals: 674/712 0.045s
im_proposals: 675/712 0.045s
im_proposals: 676/712 0.045s
im_proposals: 677/712 0.045s
im_proposals: 678/712 0.045s
im_proposals: 679/712 0.045s
im_proposals: 680/712 0.045s
im_proposals: 681/712 0.045s
im_proposals: 682/712 0.045s
im_proposals: 683/712 0.045s
im_proposals: 684/712 0.045s
im_proposals: 685/712 0.045s
im_proposals: 686/712 0.045s
im_proposals: 687/712 0.045s
im_proposals: 688/712 0.045s
im_proposals: 689/712 0.045s
im_proposals: 690/712 0.045s
im_proposals: 691/712 0.045s
im_proposals: 692/712 0.045s
im_proposals: 693/712 0.045s
im_proposals: 694/712 0.045s
im_proposals: 695/712 0.045s
im_proposals: 696/712 0.045s
im_proposals: 697/712 0.045s
im_proposals: 698/712 0.045s
im_proposals: 699/712 0.045s
im_proposals: 700/712 0.045s
im_proposals: 701/712 0.045s
im_proposals: 702/712 0.045s
im_proposals: 703/712 0.045s
im_proposals: 704/712 0.045s
im_proposals: 705/712 0.045s
im_proposals: 706/712 0.045s
im_proposals: 707/712 0.045s
im_proposals: 708/712 0.045s
im_proposals: 709/712 0.045s
im_proposals: 710/712 0.045s
im_proposals: 711/712 0.045s
im_proposals: 712/712 0.045s
Wrote RPN proposals to /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_rpn_stage1_iter_40000_proposals.pkl
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 1 Fast R-CNN using RPN proposals, init from ImageNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Init model: data/imagenet_models/ZF.v2.caffemodel
RPN proposals: /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_rpn_stage1_iter_40000_proposals.pkl
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': True,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': False,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'rpn',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage1',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: rpn
Appending horizontally-flipped training examples...

loading /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_rpn_stage1_iter_40000_proposals.pkl
375
[[ 160.          191.          227.          248.        ]
 [   3.80105972  153.38137817  196.92352295  499.375     ]
 [   0.           77.47154236  185.81477356  468.74588013]
 ..., 
 [ 230.25079346  362.35379028  258.04818726  387.64056396]
 [ 151.36703491  129.97418213  181.11114502  158.59210205]
 [ 129.13356018  384.6022644   162.19580078  426.55172729]]
500
[[ 208.          164.          227.          183.        ]
 [ 207.72610474  181.1088562   225.33091736  199.73016357]
 [ 203.22561646  167.70228577  220.14105225  189.387146  ]
 ..., 
 [ 300.26712036  238.147995    342.67080688  265.3291626 ]
 [ 164.1144104    44.87060165  195.04751587   68.79955292]
 [ 464.71881104   42.12200546  488.55801392   73.62076569]]
333
[[  26.          294.          176.          425.        ]
 [ 147.          378.          265.          435.        ]
 [ 252.          309.          290.          352.        ]
 ..., 
 [ 290.70819092  413.07339478  310.47320557  451.3454895 ]
 [   0.          163.8883667    66.25428772  303.7696228 ]
 [ 127.22799683  441.38360596  194.31724548  480.0753479 ]]
333
[[  19.          224.           82.          356.        ]
 [ 250.          235.          293.          291.        ]
 [ 225.          299.          264.          339.        ]
 ..., 
 [ 227.31774902   62.25869751  247.60340881   80.80292511]
 [ 289.7946167   259.37530518  332.44500732  323.68811035]
 [ 281.26596069  224.93562317  310.37231445  246.62950134]]
375
[[  95.           60.          259.          452.        ]
 [ 107.27024078   88.05604553  315.63690186  499.375     ]
 [  92.66990662    0.          341.15139771  429.29385376]
 ..., 
 [ 197.57746887   36.87156296  257.95880127  116.41870117]
 [ 145.56654358  287.03552246  266.23916626  401.57049561]
 [ 100.09484863  101.09229279  143.85150146  144.51948547]]
500
[[ 138.          225.          343.          342.        ]
 [ 131.            0.          333.          193.        ]
 [ 142.94558716  223.01986694  303.3175354   374.375     ]
 ..., 
 [ 250.42063904  244.17709351  463.72668457  347.46722412]
 [ 124.82283783   89.02651978  156.31617737  111.55116272]
 [ 307.6890564   104.13169861  383.39651489  147.98609924]]
375
[[ 160.          122.          261.          231.        ]
 [ 121.95640564    0.          370.78601074  435.93557739]
 [ 160.46257019  118.36412048  253.7250061   213.73674011]
 ..., 
 [ 332.52514648   51.86996078  354.56759644   86.08573151]
 [ 308.17547607  213.8653717   338.21316528  241.5328064 ]
 [ 213.5740509    29.60027695  374.375       161.12397766]]
375
[[  36.           83.          307.          453.        ]
 [   0.            0.          251.76753235  499.375     ]
 [  71.10079956  120.59403992  289.73846436  499.375     ]
 ..., 
 [ 270.90737915  117.12753296  303.88024902  150.88877869]
 [ 314.08578491   89.41039276  345.51223755  122.06967163]
 [ 154.62136841  410.68603516  236.94665527  458.10598755]]
333
[[   0.           17.          177.          476.        ]
 [ 140.          198.          189.          400.        ]
 [ 159.           49.          301.          219.        ]
 ..., 
 [  15.63501263  192.04356384   34.27856445  226.8026123 ]
 [  18.45184517   82.62199402  106.36909485  119.3289566 ]
 [ 311.2979126   418.01968384  332.44500732  499.5       ]]
333
[[  10.           78.          312.          465.        ]
 [ 122.70655823    0.          332.44500732  407.08633423]
 [ 106.25720978   54.26441956  332.44500732  293.71743774]
 ..., 
 [  37.12982559    0.          109.26473236   23.81025124]
 [ 203.62921143  411.32183838  283.23086548  474.0774231 ]
 [   0.          276.63247681   56.4782753   328.21008301]]
376
[[ 100.           88.          285.          425.        ]
 [  59.87901306    0.          286.48468018  456.67514038]
 [  41.02111435    6.17605543  364.97692871  340.44052124]
 ..., 
 [ 269.12939453  368.47366333  286.83404541  424.00466919]
 [ 179.33659363  337.44314575  210.40184021  407.15155029]
 [  96.6518631    60.21857452  196.31713867  129.83583069]]
333
[[  49.            0.          285.          499.        ]
 [ 265.            0.          332.          136.        ]
 [   0.            0.          212.34179688  496.32867432]
 ..., 
 [ 247.76928711  354.3326416   270.97348022  380.868927  ]
 [  62.00857544  428.88516235   79.06462097  457.2946167 ]
 [ 103.49909973  449.02789307  179.87498474  496.24691772]]
333
[[  24.           32.          317.          471.        ]
 [ 100.99867249  201.99537659  284.50619507  499.5       ]
 [  64.32868958    0.          256.04968262  499.5       ]
 ..., 
 [  37.01607895  378.70666504  105.90306854  449.73373413]
 [ 274.62347412   36.08568573  296.51733398   64.01982117]
 [ 201.00350952   51.6787262   240.05007935   87.3266983 ]]
376
[[  57.          145.          309.          417.        ]
 [ 128.47567749   46.94610596  348.9569397   499.45330811]
 [  27.14120865   48.58521652  264.1519165   499.45330811]
 ..., 
 [ 128.01329041  398.28515625  150.89892578  417.34204102]
 [  36.28381348  445.85827637   61.46802902  474.41668701]
 [ 291.33740234  179.51060486  307.54333496  196.55529785]]
316
[[  50.           28.          277.          470.        ]
 [  94.91576385   60.38980103  285.71731567  499.27999878]
 [ 105.49051666    0.          287.05755615  391.79351807]
 ..., 
 [ 213.57333374    0.          262.29425049   20.35581207]
 [ 272.07992554  274.11206055  305.19546509  343.32943726]
 [ 214.29855347  387.6388855   286.00241089  463.45123291]]
500
[[ 254.           88.          306.          248.        ]
 [ 236.37255859  148.44033813  430.47537231  331.44665527]
 [ 280.90600586  160.86444092  499.66000366  331.44665527]
 ..., 
 [  78.32102966  199.22245789   96.86139679  215.0816803 ]
 [  68.58247375    5.27264118   90.0431366    29.84398079]
 [ 117.5322876   129.75244141  196.33000183  166.61129761]]
333
[[ 167.          378.          268.          453.        ]
 [ 270.          211.          287.          245.        ]
 [ 104.          209.          273.          293.        ]
 ..., 
 [ 127.73332214    7.72043753  321.09091187   98.55287933]
 [  73.10373688   40.12646484   92.75911713   81.53715515]
 [  27.2253933   277.65911865   63.0265274   305.35614014]]
500
[[ 142.          153.          253.          276.        ]
 [ 256.          212.          265.          227.        ]
 [ 250.          202.          302.          287.        ]
 ..., 
 [ 444.98236084  305.0345459   473.36279297  330.52511597]
 [ 460.86993408   39.64356995  499.375       157.68031311]
 [ 460.79876709  198.20788574  499.375       318.92440796]]
500
[[ 192.          136.          366.          281.        ]
 [ 192.67448425  167.25897217  360.58932495  307.83853149]
 [ 220.39837646    0.          471.72583008  395.33999634]
 ..., 
 [ 339.23303223  154.08085632  393.33377075  198.32696533]
 [ 106.10063171  194.91362     122.20259094  214.77424622]
 [ 444.00839233  172.89347839  465.04162598  225.10969543]]
500
[[ 133.            0.          231.           64.        ]
 [ 278.            0.          314.           76.        ]
 [ 304.           37.          370.           67.        ]
 ..., 
 [ 378.0970459     0.          499.375        41.639431  ]
 [  38.56412506   87.06519318   65.12252808  104.78686523]
 [ 381.53674316  154.83381653  401.48876953  181.33805847]]
500
[[ 184.           98.          307.          228.        ]
 [ 176.91116333   96.02601624  313.9359436   233.24827576]
 [ 184.85520935  117.86869812  312.93603516  252.12998962]
 ..., 
 [ 442.41799927    0.          466.9100647    15.8951292 ]
 [ 284.45788574  240.40008545  301.71310425  261.54354858]
 [ 271.13104248  115.29115295  287.76071167  149.51713562]]
500
[[ 203.           74.          277.          148.        ]
 [ 204.76760864   85.0062561   276.69998169  153.89804077]
 [  96.36574554    0.          357.32714844  276.06057739]
 ..., 
 [ 252.90150452  227.44467163  277.85455322  256.11129761]
 [  86.1706543   120.6835022   108.43008423  153.91894531]
 [ 323.50732422   45.63619232  356.91143799   78.69176483]]
375
[[ 197.          274.          324.          467.        ]
 [ 150.90072632  118.69508362  368.76831055  499.375     ]
 [ 156.28318787  192.33572388  312.64053345  499.375     ]
 ..., 
 [ 121.54602814   91.63419342  166.71907043  140.59901428]
 [ 255.78936768   61.95442963  297.20401001  100.33024597]
 [   6.78252697  387.68704224  126.8415451   499.375     ]]
500
[[ 240.          147.          285.          308.        ]
 [ 282.3704834    76.86340332  453.11184692  332.44500732]
 [ 218.25300598  142.05085754  499.5         332.44500732]
 ..., 
 [ 443.28491211   59.12342453  458.76495361   76.11582947]
 [ 181.57240295   66.13480377  241.25382996  126.78125763]
 [ 456.47293091  201.52102661  472.79345703  235.73699951]]
375
[[   0.          234.          155.          332.        ]
 [ 141.          122.          211.          210.        ]
 [  99.          332.          182.          497.        ]
 ..., 
 [   7.87395     368.66824341   35.78602219  410.27087402]
 [ 136.07661438  261.14498901  166.25817871  286.87554932]
 [ 110.7559967     0.          148.71391296   21.88646698]]
500
[[ 280.          210.          339.          273.        ]
 [ 277.27133179  207.37243652  344.87091064  278.40304565]
 [ 283.98873901  209.37586975  336.22393799  269.98388672]
 ..., 
 [ 305.78182983  301.79244995  329.41571045  345.07037354]
 [ 304.43869019   62.96749878  334.51019287   95.41111755]
 [ 315.72976685  106.92055511  384.60232544  157.58120728]]
500
[[  49.           71.           79.          116.        ]
 [  67.          113.           94.          153.        ]
 [  49.          206.           72.          229.        ]
 ..., 
 [  89.12515259  101.09587097  111.59604645  121.11238861]
 [ 309.18466187  297.41772461  349.59017944  328.43548584]
 [ 411.89978027  262.62417603  440.2097168   282.75360107]]
500
[[  99.           28.          430.          344.        ]
 [  67.91641235    0.          371.37283325  374.375     ]
 [ 176.49778748    0.          434.41168213  374.375     ]
 ..., 
 [ 440.72796631  153.50128174  459.72262573  185.80381775]
 [ 440.0942688   322.51177979  461.1842041   348.74005127]
 [ 161.55093384  354.20391846  230.38363647  374.375     ]]
413
[[  76.          302.          250.          471.        ]
 [  68.80322266  326.83847046  251.52192688  474.26269531]
 [  50.91117477  295.337677    280.63619995  457.64529419]
 ..., 
 [ 252.82032776  150.42564392  308.91958618  207.08145142]
 [ 191.84307861  173.3795166   253.80349731  230.59529114]
 [ 261.28771973    0.          354.28924561  189.90504456]]
500
[[  53.          156.          147.          264.        ]
 [ 141.          179.          194.          263.        ]
 [ 193.          180.          248.          269.        ]
 ..., 
 [ 174.30735779    0.          391.66644287  151.92701721]
 [ 276.05230713  279.0168457   304.73361206  311.95999146]
 [ 361.03619385  252.10441589  395.58163452  295.16928101]]
500
[[  17.           25.          458.          207.        ]
 [ 275.8711853     0.          499.5         226.5       ]
 [ 223.58210754    0.          439.20043945  226.5       ]
 ..., 
 [ 460.6373291    88.65019226  491.58032227  148.76583862]
 [ 428.31732178   97.14299011  450.48907471  120.13569641]
 [ 359.277771     56.11369705  395.35528564   78.17465973]]
500
[[  80.           92.          371.          377.        ]
 [ 159.04522705   69.3081131   419.45074463  404.32501221]
 [  77.03981781   11.08863831  393.08352661  404.32501221]
 ..., 
 [  31.64953613  109.1055069    65.24241638  142.45440674]
 [ 156.05426025  157.43263245  190.07209778  199.83045959]
 [   0.          253.10523987   96.74877167  309.72271729]]
375
[[  97.           70.          292.          498.        ]
 [ 168.10868835    0.          365.66116333  499.375     ]
 [  79.24493408    7.7519989   374.375       375.79656982]
 ..., 
 [ 146.26625061   14.81884003  273.36422729  179.12290955]
 [ 195.11740112  391.03634644  222.92684937  426.01208496]
 [ 197.18333435  186.43110657  266.34619141  266.32284546]]
320
[[ 156.          148.          202.          183.        ]
 [ 187.53649902   51.71953201  319.6000061   239.6000061 ]
 [ 200.68751526   96.52070618  319.6000061   239.6000061 ]
 ..., 
 [ 269.95013428  133.27642822  285.92510986  154.84825134]
 [  54.44659424  157.63699341   87.0247345   181.03349304]
 [ 261.33782959   65.32917786  291.58779907  131.75488281]]
500
[[  99.           21.          362.          345.        ]
 [  39.14656448    0.          292.27206421  374.375     ]
 [ 213.95828247    6.55971527  419.70202637  374.375     ]
 ..., 
 [ 423.13174438   29.95719337  447.63201904   45.44496155]
 [ 211.70326233   71.15985107  241.15222168   92.57988739]
 [  58.87439346  320.39877319   75.41933441  342.53851318]]
500
[[ 374.          122.          431.          203.        ]
 [  50.          128.          110.          200.        ]
 [ 365.03994751  140.83609009  427.20095825  238.12728882]
 ..., 
 [ 203.17666626   36.9578476   247.63848877   91.10005951]
 [ 240.5854187   174.8684845   295.88146973  240.88801575]
 [ 150.9672699    81.27945709  189.93302917  109.47382355]]
375
[[  51.          164.          316.          480.        ]
 [ 273.          330.          297.          371.        ]
 [ 278.          376.          304.          451.        ]
 ..., 
 [  51.20459366  132.59248352  124.84425354  179.5693512 ]
 [ 267.36752319  178.51014709  287.07180786  197.69815063]
 [ 133.03436279  245.01190186  172.21459961  319.64172363]]
480
[[ 180.           78.          407.          115.        ]
 [ 176.          135.          438.          319.        ]
 [ 267.05056763   37.19895935  476.14764404  319.4666748 ]
 ..., 
 [ 108.38648224  285.76501465  130.12919617  305.605896  ]
 [ 291.81143188  136.47966003  322.46408081  157.27003479]
 [ 108.36362457   41.96558762  160.12948608   82.72683716]]
406
[[   0.           41.          405.          499.        ]
 [ 108.47241974    0.          288.10949707  466.31558228]
 [ 183.37397766    0.          405.32333374  482.70516968]
 ..., 
 [  13.91627121  417.56057739   32.98913956  451.58486938]
 [ 142.63209534  156.16691589  302.8742981   330.22332764]
 [   0.          113.65073395   56.96634674  176.14942932]]
500
[[ 216.           59.          403.          267.        ]
 [ 207.57701111   94.43836212  438.58584595  240.08232117]
 [ 209.70187378   72.33517456  418.98291016  270.05999756]
 ..., 
 [ 399.93521118   90.59598541  447.15820312  153.8613739 ]
 [  82.52468109  324.91867065  153.37966919  374.375     ]
 [ 377.83206177  111.99459076  425.28866577  170.39505005]]
337
[[  88.          193.          158.          353.        ]
 [ 166.          193.          234.          351.        ]
 [ 109.9031601   207.49253845  162.09820557  366.0864563 ]
 ..., 
 [ 272.89541626   69.26447296  289.92245483   88.11270142]
 [ 127.86727142  321.74255371  156.5226593   362.60351562]
 [  93.08037567   42.2503891   114.13989258   69.69541931]]
500
[[ 159.           46.          337.          349.        ]
 [ 215.48591614    3.78358841  350.59906006  374.375     ]
 [ 180.31562805    0.          326.51074219  374.375     ]
 ..., 
 [ 341.13006592   56.53564072  461.01904297  177.58525085]
 [ 399.17095947   87.55738831  433.70510864  187.19451904]
 [ 140.78866577  105.12799835  233.60864258  182.63845825]]
500
[[ 155.            7.          406.          331.        ]
 [ 226.42684937  119.42169189  353.23919678  332.44500732]
 [ 190.32266235   36.12511826  307.68969727  332.44500732]
 ..., 
 [ 128.37559509   73.77310944  157.92344666  105.66326904]
 [  98.91343689  315.88769531  120.20896912  332.44500732]
 [ 393.68075562  299.18981934  455.89556885  332.44500732]]
375
[[   0.            0.          373.          499.        ]
 [  84.82723236    0.          312.41986084  466.96609497]
 [   7.96291351    0.          374.375       448.62258911]
 ..., 
 [ 265.44515991  238.03312683  307.40863037  287.68838501]
 [  80.50960541  271.6572876   226.96717834  343.54467773]
 [ 126.68656921   19.50465202  214.41552734  121.22573853]]
500
[[ 221.          117.          350.          283.        ]
 [ 222.4094696   126.11897278  364.46224976  281.35055542]
 [ 195.89022827  128.59671021  412.45080566  267.17269897]
 ..., 
 [ 196.81280518  264.50549316  221.99147034  294.09609985]
 [  94.25983429   66.22054291  196.4185791   103.09374237]
 [ 105.81191254  173.99935913  194.65838623  218.11390686]]
500
[[   9.          205.          117.          256.        ]
 [ 123.          180.          259.          232.        ]
 [ 264.          170.          300.          208.        ]
 ..., 
 [ 170.56121826   56.75323486  254.05128479   98.88824463]
 [ 251.29266357   64.02251434  275.0072937    90.05474091]
 [ 151.33912659  133.7338562   175.48384094  160.00593567]]
500
[[ 250.           62.          370.          193.        ]
 [ 242.92350769   70.48547363  371.22250366  192.17417908]
 [ 263.55526733   74.20726776  400.5663147   204.3270874 ]
 ..., 
 [ 186.39733887   90.87224579  202.50608826  111.40777588]
 [ 399.33026123   65.76121521  428.07467651  116.6554718 ]
 [ 266.82397461    0.          307.22930908   23.67390823]]
500
[[  36.           61.          424.          288.        ]
 [ 123.          235.          249.          299.        ]
 [ 137.25466919   48.28937149  330.25488281  332.44500732]
 ..., 
 [ 466.14587402  213.69735718  489.27694702  233.82095337]
 [   0.          141.64749146   33.31754303  192.30821228]
 [ 288.33309937  209.26867676  378.74810791  266.89108276]]
294
[[  55.          186.          198.          335.        ]
 [  73.17172241  198.85142517  200.48504639  324.58929443]
 [ 159.87692261  220.52650452  293.5         499.5       ]
 ..., 
 [  21.67983246  378.01916504   41.20565033  413.26629639]
 [   5.1312294   406.60717773   70.45289612  468.58032227]
 [  15.53377342  446.38397217   78.35024261  489.84014893]]
500
[[  27.          122.           69.          168.        ]
 [ 181.           24.          232.          114.        ]
 [ 230.           52.          259.          114.        ]
 ..., 
 [ 423.04766846   13.17787266  464.41897583   51.07109451]
 [ 354.90988159  207.6869812   411.48464966  253.24017334]
 [ 306.53851318  445.45648193  333.95050049  461.23001099]]
333
[[   1.           40.          324.          463.        ]
 [   0.           18.74513817  186.89131165  484.47821045]
 [  46.13781357   40.61709595  280.70581055  499.5       ]
 ..., 
 [ 123.79460907  167.60647583  194.90971375  243.17153931]
 [  58.38881683   40.77298355  174.08305359  181.42260742]
 [  70.24460602  350.45510864  117.17733765  428.13952637]]
333
[[ 142.          171.          161.          187.        ]
 [ 107.          150.          164.          231.        ]
 [  93.          177.          103.          191.        ]
 ..., 
 [  37.95010757  250.79760742   55.57077408  275.15710449]
 [ 173.31544495  476.16870117  244.92373657  499.5       ]
 [ 187.82350159  183.66992188  206.2903595   210.28678894]]
500
[[ 179.          175.          321.          246.        ]
 [ 163.          215.          175.          234.        ]
 [ 151.          192.          167.          213.        ]
 ..., 
 [ 201.49150085  263.12985229  222.30363464  288.72875977]
 [ 410.05889893  172.36959839  430.65151978  195.02861023]
 [ 309.07315063   46.3266449   371.4407959   103.56344604]]
375
[[  98.           42.          246.          209.        ]
 [  59.          103.           91.          265.        ]
 [ 276.           79.          330.          225.        ]
 ..., 
 [ 319.12112427  414.96151733  353.08276367  480.0866394 ]
 [ 115.94434357  188.60227966  140.36462402  220.50543213]
 [  15.63530159  473.77871704   42.21040726  499.375     ]]
375
[[ 125.          297.          225.          430.        ]
 [  86.           50.          130.          118.        ]
 [ 268.           24.          307.          156.        ]
 ..., 
 [  53.17375183    0.          251.69711304   79.41704559]
 [ 252.44338989  115.91207123  299.98815918  177.20800781]
 [ 168.0070343   420.10214233  200.57633972  455.4524231 ]]
375
[[  72.          140.          233.          280.        ]
 [  70.33705902  161.28326416  239.56176758  294.8371582 ]
 [  44.48714066  130.65501404  264.59011841  257.98306274]
 ..., 
 [ 113.33787537    0.          190.31512451   97.0718689 ]
 [ 259.71615601   41.06496811  342.70504761   90.87535095]
 [ 169.49992371   54.37369156  206.84863281   87.33687592]]
500
[[ 217.          123.          278.          193.        ]
 [ 220.55570984  135.78147888  284.27981567  205.55422974]
 [ 212.68453979  154.73416138  286.440979    224.46003723]
 ..., 
 [ 437.76113892  135.08473206  499.5         183.78915405]
 [ 283.78207397  271.46228027  302.01531982  291.70227051]
 [  59.58956146   17.45137787  193.71444702  161.24203491]]
375
[[  45.           99.          195.          398.        ]
 [ 194.          178.          335.          412.        ]
 [ 167.44216919    0.          374.375       499.375     ]
 ..., 
 [  55.04654694  300.17080688  122.43487549  422.84521484]
 [ 267.15847778  382.73236084  311.56539917  433.13540649]
 [ 134.33892822  386.58221436  167.50845337  433.86181641]]
500
[[ 146.           34.          300.          303.        ]
 [ 135.82527161   30.43738365  308.95651245  331.93759155]
 [  77.41744232    0.          314.64938354  374.375     ]
 ..., 
 [  68.81072998   93.37388611   88.24500275  114.63039398]
 [ 455.9942627    33.65572739  491.13479614   87.71027374]
 [ 146.39590454   38.081604    167.91148376   64.73720551]]
373
[[  40.           40.          318.          465.        ]
 [ 178.85412598   54.49906158  359.38623047  499.19833374]
 [  14.67623234    0.          254.99142456  499.19833374]
 ..., 
 [  92.20404816   75.71751404  134.35920715  110.72544098]
 [ 143.71153259  331.14758301  283.41470337  418.70401001]
 [ 175.51004028  231.43159485  224.64318848  285.19171143]]
353
[[   6.            2.          335.          485.        ]
 [  10.16424084   85.67720795  141.84143066  391.43197632]
 [   3.59304738    0.          210.73130798  499.49499512]
 ..., 
 [ 280.6913147   293.32949829  304.58709717  365.90606689]
 [ 213.98661804  360.37677002  257.29586792  386.03540039]
 [  80.87140656   90.68934631  149.70933533  142.65679932]]
333
[[ 156.          379.          230.          497.        ]
 [ 113.          262.          165.          328.        ]
 [ 115.          151.          188.          238.        ]
 ..., 
 [  46.63316345    0.          179.05107117  114.91775513]
 [ 194.04428101  273.806427    332.44500732  410.13705444]
 [   0.          260.51104736   70.5644455   320.31054688]]
271
[[  13.           52.          241.          493.        ]
 [  50.21485901    0.          214.2771759   450.39230347]
 [  55.38304901  126.64715576  254.90859985  499.5       ]
 ..., 
 [ 198.10388184  135.36993408  224.1567688   171.76663208]
 [ 238.38674927  258.62103271  263.86730957  290.53167725]
 [  98.58699799  349.04040527  176.7901001   413.94116211]]
375
[[  70.            0.          301.          461.        ]
 [  25.          246.          101.          359.        ]
 [ 143.12330627  211.92202759  374.375       499.375     ]
 ..., 
 [   0.          181.31355286   53.80775452  313.62890625]
 [ 182.64189148  349.92892456  244.62213135  384.12335205]
 [ 199.05932617    0.          313.61990356  235.64039612]]
500
[[ 207.          106.          331.          242.        ]
 [ 196.39128113  114.95048523  342.63946533  252.41160583]
 [ 211.65948486   66.83769989  329.11883545  243.51031494]
 ..., 
 [ 302.60580444  260.97888184  336.94689941  299.51351929]
 [ 340.21011353   58.82230759  376.95120239   91.75717926]
 [ 353.29882812    0.72798491  382.9281311    21.47875595]]
500
[[  62.           14.          333.          338.        ]
 [  95.86443329    0.          319.70443726  316.85488892]
 [ 142.82925415    0.          348.30447388  339.43331909]
 ..., 
 [  49.48001862  170.04109192   74.28024292  200.17762756]
 [  70.09008789    0.          162.53993225  248.76538086]
 [   1.7102679    69.47827148   74.14089966  122.45768738]]
333
[[  83.           88.          211.          401.        ]
 [ 119.86205292   21.18437386  230.44308472  305.66339111]
 [ 104.71547699   64.79425049  228.11448669  332.78833008]
 ..., 
 [ 129.83174133  432.28427124  162.7230835   467.51031494]
 [ 233.13536072  143.65835571  251.58758545  175.73646545]
 [ 189.00794983    0.          296.43701172  168.8066864 ]]
316
[[ 114.          232.          201.          255.        ]
 [ 125.          199.          200.          241.        ]
 [ 129.37724304  205.81639099  194.97427368  244.09329224]
 ..., 
 [ 298.91064453  231.25961304  315.47332764  339.43887329]
 [   0.          429.89520264  273.83917236  471.3666687 ]
 [  91.46730042  322.92175293  131.56318665  377.8536377 ]]
375
[[  68.           82.          277.          441.        ]
 [  46.77482605   58.01832199  286.1661377   499.375     ]
 [  31.88434601   62.73078918  221.99809265  499.375     ]
 ..., 
 [  59.73743057   88.53751373  109.41201782  154.45500183]
 [ 239.71731567  396.73660278  263.18481445  431.34558105]
 [  71.78813171  297.78585815   94.96826935  356.88674927]]
333
[[  33.           53.           92.          394.        ]
 [ 138.           49.          275.          448.        ]
 [ 264.          451.          280.          472.        ]
 ..., 
 [ 203.53794861  120.41404724  225.31356812  172.39459229]
 [   9.13309669  218.4881134    41.98096848  266.43899536]
 [ 148.08143616  209.81558228  166.71972656  241.17507935]]
500
[[ 178.          116.          299.          289.        ]
 [ 192.           79.          226.          116.        ]
 [ 229.           69.          270.          111.        ]
 ..., 
 [  65.63381195  246.83837891  139.12844849  310.29165649]
 [  25.62727928  230.28833008   55.30609894  271.2416687 ]
 [ 283.29272461  190.07575989  317.91427612  220.85289001]]
500
[[ 176.          189.          242.          241.        ]
 [   0.            0.          141.65003967  357.87200928]
 [ 171.4468689   195.59403992  236.75776672  240.9821167 ]
 ..., 
 [ 436.0710144    84.41041565  463.65072632  108.44865417]
 [ 172.54707336  156.92100525  278.25561523  299.21508789]
 [ 209.27027893   78.95053864  275.48239136  149.49395752]]
500
[[ 204.          142.          302.          237.        ]
 [ 210.19174194  155.09161377  281.9442749   232.54466248]
 [ 215.8105011   157.73416138  264.16119385  206.6481781 ]
 ..., 
 [  23.82345772   92.50565338   88.44480896  135.32489014]
 [ 121.5266037   169.70074463  145.31999207  204.90310669]
 [ 451.3934021    32.58541107  471.51712036   58.31258774]]
500
[[ 381.          100.          448.          240.        ]
 [ 325.76330566    0.          499.375       374.375     ]
 [ 286.99737549    0.          499.375       310.2958374 ]
 ..., 
 [ 214.1033783   286.75354004  237.22738647  325.74920654]
 [ 421.19909668   50.08862305  447.22640991   88.2657547 ]
 [ 393.88568115  224.14785767  428.47750854  276.55178833]]
334
[[  28.            5.          297.          484.        ]
 [  46.28937531    0.          294.98275757  456.21783447]
 [ 113.40442657    0.          333.44332886  433.56503296]
 ..., 
 [  36.01704407  342.90133667   82.33654022  426.54067993]
 [ 220.89744568  274.85092163  255.56829834  338.6930542 ]
 [ 224.59251404  174.05770874  333.44332886  335.66607666]]
375
[[  87.          220.          195.          411.        ]
 [  78.12865448  233.32084656  196.16752625  394.45980835]
 [  19.42331314    0.          199.38943481  342.5093689 ]
 ..., 
 [ 306.49206543  182.69502258  337.81668091  206.17181396]
 [ 337.75787354   93.26950073  363.79263306  131.28953552]
 [ 129.35646057  374.33584595  196.87594604  421.81555176]]
250
[[  31.           13.          222.          476.        ]
 [   0.           34.32292175  182.69891357  499.5       ]
 [  26.34738922  101.92358398  230.71282959  499.5       ]
 ..., 
 [  27.73502731   59.76298904   43.93889236   85.66897583]
 [  59.82819366  194.35610962  105.09374237  272.07592773]
 [ 195.51977539  322.81860352  217.25463867  356.02288818]]
333
[[   5.          194.          194.          496.        ]
 [   0.           93.9067688   189.91358948  499.5       ]
 [   0.          231.53372192  232.3671875   499.5       ]
 ..., 
 [  84.4952774   173.51283264  157.70799255  218.14047241]
 [  90.93043518  390.06060791  214.14338684  499.5       ]
 [ 198.9563446   147.75672913  332.44500732  239.72727966]]
305
[[  16.            4.          301.          489.        ]
 [  86.34204102    0.          304.4916687   469.33883667]
 [  28.12619591  206.77059937  154.56166077  499.69165039]
 ..., 
 [   0.          268.36691284   31.01753235  313.13717651]
 [ 235.14964294   14.84113598  304.4916687    59.16920471]
 [  53.58088684    0.          135.38398743  191.95890808]]
143
[[  10.           10.          136.          480.        ]
 [   0.            0.          142.5         399.08206177]
 [   0.           74.08653259  142.5         499.5       ]
 ..., 
 [ 122.8924942   161.91320801  142.5         252.15792847]
 [ 112.8223877   197.02746582  139.00784302  262.69760132]
 [ 115.43977356  201.57940674  136.79570007  239.55865479]]
500
[[ 208.          154.          288.          211.        ]
 [ 142.          133.          200.          189.        ]
 [ 355.8416748    39.47498322  499.5         332.44500732]
 ..., 
 [ 447.02432251   23.05839729  485.89938354   87.54212189]
 [ 297.56219482  209.03355408  326.75125122  234.00563049]
 [ 168.21116638   27.28688431  398.57614136  140.34014893]]
217
[[   5.            1.          215.          446.        ]
 [  17.84078407   75.35681915  186.2124939   451.54797363]
 [  78.06302643    0.          216.50799561  324.70626831]
 ..., 
 [ 116.81703186  142.67642212  206.65693665  298.33233643]
 [ 190.76010132  372.41952515  206.62934875  398.41430664]
 [   0.           81.65328979   28.91132545  131.5234375 ]]
333
[[  22.           71.          265.          376.        ]
 [  25.5654583    78.02764893  291.29800415  350.5486145 ]
 [  27.99658966    0.          270.24862671  455.34646606]
 ..., 
 [  28.12349892  419.72879028   55.8894043   446.38516235]
 [ 160.6096344    47.91524124  229.61976624   89.4931488 ]
 [  16.73964119   98.97929382  101.1752243   143.27497864]]
182
[[   4.            0.          175.          486.        ]
 [   8.86520386    0.          181.5         499.5       ]
 [  19.94410706  140.45013428  181.5         487.08270264]
 ..., 
 [ 142.34875488  250.52444458  181.5         316.94393921]
 [ 135.62428284   85.75646973  156.70869446  116.94216919]
 [  61.94443512  461.67538452   92.45787811  479.17471313]]
375
[[  54.           24.          278.          464.        ]
 [  24.02797699    0.          237.24211121  498.13204956]
 [ 105.0220871     0.          336.56292725  405.30484009]
 ..., 
 [ 349.39312744   27.42852592  374.375       163.32894897]
 [   2.81451392  127.5575943    19.4838829   147.61383057]
 [  97.93315125  317.20715332  188.31187439  378.30633545]]
600
[[ 187.          162.          289.          249.        ]
 [ 421.18450928   23.58771515  599.25        449.25      ]
 [ 354.51498413    0.          599.25        449.25      ]
 ..., 
 [  68.09255219   95.25552368  234.68840027  264.96884155]
 [ 281.09207153   92.27024078  445.828125    360.99951172]
 [ 124.2414093    64.96318054  210.47973633  127.12355804]]
338
[[ 176.            0.          300.          225.        ]
 [ 188.07019043   25.43270111  297.71636963  197.71559143]
 [ 172.73233032    0.          337.43667603  313.61730957]
 ..., 
 [ 302.56246948  248.5289917   337.43667603  292.82623291]
 [   0.          332.02191162   39.96665573  372.90896606]
 [ 267.01040649  353.7713623   322.57293701  411.150177  ]]
333
[[  49.           35.          163.          174.        ]
 [  20.39673424    0.          176.38945007  248.58499146]
 [  77.94650269    0.          198.99133301  248.58499146]
 ..., 
 [ 144.13461304   26.97047997  164.92907715   48.89384079]
 [  35.24266434   88.02903748   60.5329628   124.50891113]
 [  13.80831146   56.76554108  109.43262482  132.6343689 ]]
291
[[  26.            0.          258.          499.        ]
 [ 133.78033447    0.          290.5         437.38516235]
 [   0.          125.79269409  247.006073    499.5       ]
 ..., 
 [ 215.08517456  365.05014038  258.1897583   422.21212769]
 [  24.83492661   19.74769592  106.68621826  234.24081421]
 [  56.15308762  174.68693542  157.16932678  260.17544556]]
375
[[ 136.          347.          210.          423.        ]
 [ 191.          227.          226.          249.        ]
 [ 222.          218.          232.          232.        ]
 ..., 
 [ 337.52261353  123.78003693  362.3527832   147.58158875]
 [  38.13325882   60.15680313  141.67300415  138.24740601]
 [  16.27840042   97.33757019  102.53148651  153.91905212]]
333
[[  14.          337.           96.          427.        ]
 [  43.61383057   80.75705719  194.38088989  499.5       ]
 [   0.          188.66111755  186.55245972  499.5       ]
 ..., 
 [ 186.97370911  196.22689819  256.10421753  235.76638794]
 [ 114.44745636  132.88142395  226.77001953  499.5       ]
 [ 152.37815857  450.73803711  178.33859253  468.19677734]]
297
[[ 155.          215.          191.          272.        ]
 [  18.41007996    0.          215.61734009  321.46450806]
 [  24.52383041    0.          170.48719788  270.1902771 ]
 ..., 
 [ 164.88848877  331.98944092  187.99139404  357.19909668]
 [  93.79510498  280.12060547  113.44095612  314.01965332]
 [   0.          173.44270325   66.17420959  277.99685669]]
275
[[  36.           42.          227.          465.        ]
 [  51.6447525     0.81404114  219.10125732  388.75518799]
 [   0.           64.67433167  272.06005859  321.4833374 ]
 ..., 
 [ 151.19975281    1.50728798  218.73912048   34.33049774]
 [  18.29402161  166.8813324    48.38233185  205.21775818]
 [ 195.86572266  276.6574707   212.78158569  302.0546875 ]]
360
[[ 129.           76.          255.          218.        ]
 [  93.65590668   76.44950867  259.48840332  158.91766357]
 [ 114.90520477   69.9022522   245.6056366   169.27455139]
 ..., 
 [ 301.13381958   49.7110939   320.02050781   77.22180176]
 [ 147.83047485   12.64282894  208.92944336   44.72201157]
 [  10.39680481  162.6958313    27.47249985  188.37962341]]
500
[[ 148.           47.          378.          254.        ]
 [ 160.23690796   23.94264221  331.58956909  331.44665527]
 [ 163.74563599    0.          390.97827148  331.44665527]
 ..., 
 [  73.98099518  233.56707764  190.32847595  315.77713013]
 [ 396.24771118  106.15460205  427.06903076  166.54222107]
 [ 148.09133911  138.2484436   172.0177002   170.99647522]]
500
[[ 253.          133.          375.          273.        ]
 [ 154.56962585    0.          366.26989746  246.60675049]
 [ 121.85890198    0.          390.76330566  284.60131836]
 ..., 
 [ 368.82858276  165.51742554  426.24304199  232.49942017]
 [ 313.74655151  356.20773315  414.75515747  374.375     ]
 [   0.           19.51981926   33.46341705   77.19032288]]
375
[[ 213.           64.          319.          167.        ]
 [ 208.18824768   71.60548401  304.16873169  163.8793335 ]
 [ 143.37898254   57.76290512  374.375       158.81282043]
 ..., 
 [ 204.82119751  178.96731567  220.93692017  208.22703552]
 [ 107.50084686  113.24947357  126.88011932  136.66921997]
 [ 128.72251892  248.24931335  155.3835907   280.96432495]]
333
[[  46.          113.          319.          396.        ]
 [  79.94861603    0.          312.0920105   472.9208374 ]
 [  77.23826599  104.39940643  332.44500732  341.58099365]
 ..., 
 [ 184.39060974  221.95054626  225.37519836  261.36657715]
 [  34.41003799   53.46356583   76.12602234   96.35067749]
 [   3.60935879  262.61288452   76.44012451  317.27050781]]
352
[[ 119.          138.          248.          287.        ]
 [  83.62819672    0.          295.48809814  287.52001953]
 [  56.74489212    0.          249.40814209  287.52001953]
 ..., 
 [   0.          228.50650024   72.62325287  287.52001953]
 [  12.41752911  233.16494751   32.69492722  257.73986816]
 [ 137.19030762   42.36753845  157.14761353   75.46655273]]
500
[[ 170.          130.          181.          145.        ]
 [ 182.          141.          194.          159.        ]
 [ 196.          136.          207.          151.        ]
 ..., 
 [   0.           96.27763367   34.200634    154.01275635]
 [   0.          108.9863739   147.01383972  229.77842712]
 [  10.86452484  304.89221191   42.63500595  354.63009644]]
500
[[ 359.          112.          375.          135.        ]
 [ 376.          125.          393.          145.        ]
 [ 360.81442261  121.08449554  377.38043213  144.19143677]
 ..., 
 [ 263.26531982  141.9372406   330.36053467  195.39849854]
 [ 333.37857056   38.58664322  415.45870972   93.75823212]
 [ 273.12399292  144.52505493  293.36636353  160.8449707 ]]
375
[[ 215.          243.          233.          264.        ]
 [ 142.46806335   53.24207306  374.375       499.375     ]
 [  64.81891632  222.1139679   374.375       499.375     ]
 ..., 
 [ 174.52836609   24.28318405  193.30343628   42.61590958]
 [  33.188591    129.08094788   52.55357742  153.91662598]
 [ 186.90644836  436.4984436   240.44721985  481.26849365]]
332
[[ 108.           20.          290.          498.        ]
 [  94.08552551    0.          285.40585327  499.66000366]
 [  72.18041229    0.          267.17874146  410.60324097]
 ..., 
 [ 209.88569641  246.51239014  258.375       296.73196411]
 [ 111.51289368  325.51016235  136.78222656  367.45013428]
 [  14.53536701  134.17956543   37.52027512  169.47857666]]
378
[[ 126.           76.          268.          183.        ]
 [ 269.16812134    0.          377.33334351  282.52835083]
 [ 112.97368622   88.24900055  263.61935425  203.40536499]
 ..., 
 [ 223.69760132  265.89807129  277.27496338  282.52835083]
 [  20.25128365  193.86209106   54.29835892  216.70211792]
 [ 114.77935791    0.          210.40087891   97.14007568]]
232
[[  14.            3.          229.          494.        ]
 [  58.11212921  251.12854004  216.08172607  499.5       ]
 [  43.01023865  192.12869263  231.5         499.5       ]
 ..., 
 [ 152.03730774  480.43859863  187.83113098  499.5       ]
 [ 113.55056       0.          153.77241516   18.49096298]
 [ 187.45787048  296.41436768  214.42958069  325.08129883]]
500
[[ 197.           37.          406.          224.        ]
 [ 278.          195.          291.          240.        ]
 [ 212.42018127   27.4711113   379.8343811   199.13981628]
 ..., 
 [  43.39570999   26.63707924   65.85179138   46.35799789]
 [  52.2742691    26.63527679   74.73108673   46.35736084]
 [   0.          132.03285217  104.36577606  271.13397217]]
334
[[ 201.          252.          210.          275.        ]
 [ 119.09700012    0.          333.44332886  342.5824585 ]
 [ 207.05693054  267.56887817  222.59909058  296.71844482]
 ..., 
 [ 158.96580505  141.57949829  211.45487976  207.61329651]
 [   5.01101065   18.2143116   116.04242706  301.06671143]
 [ 156.42393494  354.50015259  209.45710754  431.23623657]]
500
[[ 313.          283.          400.          343.        ]
 [ 148.          160.          373.          303.        ]
 [  16.98888779    0.          300.01620483  374.375     ]
 ..., 
 [   0.          172.95240784   29.97632408  197.36474609]
 [  55.30709839  140.4486084    72.42321014  160.40536499]
 [ 309.51309204  171.75238037  345.11160278  199.8192749 ]]
500
[[ 212.          166.          264.          223.        ]
 [ 205.86112976  161.13742065  268.96032715  235.2270813 ]
 [ 203.25791931  165.6504364   280.11523438  244.60960388]
 ..., 
 [ 107.98925781  274.59298706  139.53096008  309.45663452]
 [  12.14075184  372.73007202   39.51051331  394.81002808]
 [ 108.60456848  202.59286499  135.74623108  229.08901978]]
375
[[ 125.          112.          258.          356.        ]
 [  93.17692566   26.02024078  308.32577515  499.375     ]
 [ 148.02864075  127.20558929  273.93508911  357.31848145]
 ..., 
 [ 137.50878906  194.45094299  210.43318176  249.22923279]
 [  93.71258545  234.27987671  115.79701233  264.64584351]
 [ 307.89993286  137.87892151  326.53973389  178.61450195]]
500
[[  75.           89.          105.          123.        ]
 [ 109.          174.          256.          218.        ]
 [ 330.          221.          430.          279.        ]
 ..., 
 [ 450.89633179  332.88562012  484.52838135  365.35015869]
 [ 305.23257446  349.20193481  394.73370361  374.375     ]
 [ 348.67245483    0.          458.23550415  270.44329834]]
339
[[   8.            9.          119.          168.        ]
 [  29.           93.          189.          393.        ]
 [  20.87380028  113.45625305  240.16523743  499.45999146]
 ..., 
 [   3.9077127   367.28488159   35.91492844  411.23123169]
 [   0.          253.94996643   93.50706482  411.99133301]
 [ 104.10860443  130.07269287  137.15005493  167.98362732]]
339
[[  30.          119.           67.          249.        ]
 [  45.          104.          300.          457.        ]
 [  99.35459137  197.69021606  318.72714233  499.45999146]
 ..., 
 [ 105.64174652  340.61532593  152.44619751  394.86865234]
 [ 196.42498779  390.67880249  231.97250366  436.28097534]
 [ 317.9538269   263.43048096  338.43499756  332.36257935]]
500
[[ 107.           46.          400.          355.        ]
 [  44.70123291    0.          273.39706421  374.375     ]
 [   0.          123.26589203  326.25494385  374.375     ]
 ..., 
 [ 307.88900757  340.50909424  369.88513184  365.30020142]
 [ 100.64832306   76.96265411  120.34664154  100.64134216]
 [  52.75225067  224.25848389  118.76866913  275.91009521]]
500
[[ 127.           48.          414.          336.        ]
 [ 173.67152405    0.          412.63442993  374.375     ]
 [ 154.92430115  114.7651062   429.23049927  365.07528687]
 ..., 
 [ 395.12442017  103.9128952   415.01882935  129.17112732]
 [  94.51537323  203.38204956  130.17520142  261.43295288]
 [ 132.6038208   275.91125488  232.80664062  374.375     ]]
500
[[ 145.           10.          403.          321.        ]
 [ 105.9012146     0.          400.62103271  374.375     ]
 [ 113.92216492  157.81721497  378.64624023  374.375     ]
 ..., 
 [ 258.15603638  278.79211426  291.47277832  304.60079956]
 [ 106.76483154  137.40109253  123.99436951  164.33370972]
 [ 151.50924683   24.01554108  181.51332092   60.23962021]]
500
[[ 175.           51.          363.          259.        ]
 [ 176.21383667   65.82367706  364.93795776  254.47840881]
 [ 158.70378113    0.          395.15829468  374.375     ]
 ..., 
 [ 377.215271    322.30038452  434.28283691  370.94500732]
 [ 144.13296509  210.02653503  169.51982117  254.52996826]
 [  69.010849    101.79904938   86.46300507  121.8234024 ]]
356
[[   5.           20.          338.          479.        ]
 [ 102.96363068  144.32254028  338.3059082   499.58666992]
 [  96.73695374  209.34150696  300.97897339  499.58666992]
 ..., 
 [  51.45749283  321.21377563   82.06770325  379.76293945]
 [  29.82384872  307.94720459   88.66675568  357.3230896 ]
 [  56.42282867  456.31829834   87.22992706  486.3704834 ]]
375
[[ 103.          162.          256.          355.        ]
 [ 115.81151581  164.80311584  267.00622559  362.62265015]
 [  52.78722763    0.          263.69299316  499.375     ]
 ..., 
 [ 310.50970459   32.5788765   354.0774231    97.54331207]
 [ 181.13299561  418.43405151  200.98963928  441.31033325]
 [  44.6069603   254.68534851  122.60086823  302.97116089]]
500
[[   0.           24.          169.          204.        ]
 [  56.          129.          157.          250.        ]
 [ 155.          126.          337.          281.        ]
 ..., 
 [ 138.69155884   57.87733078  155.58097839   84.04046631]
 [  58.43518066   84.43817139   82.06378937  102.60636902]
 [ 433.38369751  339.78643799  455.41098022  362.07214355]]
500
[[   0.          323.          226.          374.        ]
 [  28.           50.          451.          309.        ]
 [   0.            0.          268.23864746  374.375     ]
 ..., 
 [ 341.77685547  270.05477905  364.67144775  296.70065308]
 [ 266.54302979  109.66278839  310.52740479  151.65583801]
 [ 113.24508667  109.23679352  131.75956726  133.70347595]]
448
[[ 113.           84.          298.          225.        ]
 [ 322.49786377  143.16210938  447.5         299.5       ]
 [ 286.90411377  157.98397827  443.25286865  299.5       ]
 ..., 
 [ 177.7023468   180.09388733  224.76246643  235.39744568]
 [ 362.27655029   59.09616852  390.80181885   96.43376923]
 [ 201.76170349   27.05387115  384.2288208    94.33914948]]
500
[[  27.           74.          499.          311.        ]
 [  71.            1.          343.           31.        ]
 [ 413.            0.          498.           45.        ]
 ..., 
 [ 310.24517822   40.7057457   435.39279175   81.8682251 ]
 [ 161.32521057   51.31361389  419.39108276  179.75772095]
 [  61.63686752   12.44495296  115.46228027   34.59915924]]
487
[[  20.            0.          484.          479.        ]
 [ 158.95237732    0.          430.98199463  499.17498779]
 [   0.           14.86406612  304.52731323  499.17498779]
 ..., 
 [ 335.66348267  312.84439087  486.18832397  479.63153076]
 [ 266.75274658  345.6121521   321.39154053  412.69744873]
 [ 301.99343872  438.79544067  328.21243286  460.68347168]]
375
[[  38.           65.          331.          416.        ]
 [   5.95711708   46.96735382  224.76806641  499.375     ]
 [   0.          164.79025269  295.3757019   459.61074829]
 ..., 
 [ 339.84957886    9.62388229  356.58535767   43.20648193]
 [ 149.57341003   23.48561478  166.43855286   40.26807785]
 [  52.44814301   96.03322601  133.99813843  142.54618835]]
500
[[ 182.           75.          329.          197.        ]
 [ 178.68086243   82.42490387  325.3263855   201.21012878]
 [ 162.70516968   54.88533401  321.77853394  194.29136658]
 ..., 
 [ 339.28164673  218.94784546  357.71908569  250.74189758]
 [ 416.73983765  287.3453064   471.09509277  346.269104  ]
 [ 212.650177    354.73019409  238.39950562  374.375     ]]
500
[[ 154.          117.          400.          274.        ]
 [ 238.48693848    0.          484.74691772  374.375     ]
 [  61.03796005    0.          305.1512146   374.375     ]
 ..., 
 [ 400.17288208  276.91583252  432.75588989  295.89596558]
 [ 395.25543213  301.58584595  422.13095093  319.35079956]
 [ 410.11233521  356.49694824  486.06616211  374.375     ]]
500
[[  89.          124.          414.          162.        ]
 [ 166.58050537  117.39980316  397.23858643  156.39677429]
 [ 123.64078522  121.45578003  343.63018799  159.06939697]
 ..., 
 [ 437.50637817   54.71618271  466.41995239   79.1309967 ]
 [ 215.68751526  299.85955811  305.8152771   374.375     ]
 [   0.          129.78926086   22.93895721  172.87963867]]
500
[[ 105.           20.          340.          302.        ]
 [ 187.          356.          272.          372.        ]
 [  71.93195343    0.          282.23617554  374.375     ]
 ..., 
 [ 177.63792419  315.01156616  219.8316803   332.16021729]
 [ 124.94224548   69.17297363  168.0973053   106.69556427]
 [ 418.6388855    37.56487656  449.07391357   71.77120972]]
398
[[   5.            6.          375.          493.        ]
 [  12.           98.           70.          235.        ]
 [  68.61599731    0.          350.15548706  499.48999023]
 ..., 
 [ 113.58942413  300.87762451  154.30532837  351.5255127 ]
 [ 320.19769287  146.68049622  370.20724487  206.20957947]
 [  79.18672943  219.87034607  175.11962891  282.65011597]]
310
[[  74.          219.          200.          295.        ]
 [ 146.          111.          192.          144.        ]
 [ 135.          168.          196.          205.        ]
 ..., 
 [  96.32806396  103.72115326  146.37095642  160.11903381]
 [ 236.95245361  318.20056152  257.9432373   344.49435425]
 [ 235.20257568  389.71707153  252.35742188  405.99755859]]
334
[[   0.          251.           69.          388.        ]
 [  18.           91.          331.          485.        ]
 [ 136.21720886  217.68055725  333.44332886  499.32998657]
 ..., 
 [  89.77381897  221.00740051  128.17300415  275.94799805]
 [ 101.38298035  290.37161255  206.98391724  353.0135498 ]
 [ 162.69615173  183.94520569  217.0511322   232.33721924]]
352
[[  30.          390.           49.          442.        ]
 [ 239.          287.          304.          340.        ]
 [  64.          206.          272.          474.        ]
 ..., 
 [  15.86037827  313.95516968  106.7638092   366.18139648]
 [  62.9872551   201.8475647   158.82710266  268.78005981]
 [ 270.02935791   85.42071533  295.21920776  112.38045502]]
334
[[  38.          294.           95.          486.        ]
 [  61.          190.          288.          461.        ]
 [ 103.659729     71.73453522  333.44332886  499.32998657]
 ..., 
 [ 218.46810913  354.1706543   272.30361938  416.44232178]
 [ 187.34196472  106.03258514  214.64321899  136.68998718]
 [ 126.86545563  251.36271667  281.11050415  451.09466553]]
375
[[  53.           64.          361.          391.        ]
 [ 150.03895569    0.          374.375       475.66543579]
 [ 117.78459167   70.49816132  374.375       391.94735718]
 ..., 
 [   4.83083248  128.96124268   93.48653412  204.78862   ]
 [  60.02973938   89.88352966   81.10913849  121.51890564]
 [ 224.5735321   343.38470459  288.15170288  382.21893311]]
439
[[   7.            0.          220.          478.        ]
 [ 214.            0.          421.          473.        ]
 [ 220.55534363    0.          438.26834106  498.99667358]
 ..., 
 [ 179.8740387     0.          214.42582703   16.57110596]
 [ 107.38751221  449.8142395   206.9025116   498.99667358]
 [ 192.15879822  465.04400635  220.6966095   498.99667358]]
286
[[  59.            3.          267.          493.        ]
 [  48.8422699     0.          285.5         466.63116455]
 [  55.57579803    0.          239.05233765  411.11456299]
 ..., 
 [ 183.92404175   14.76473236  268.29525757   65.04482269]
 [ 236.14932251   68.64654541  270.15359497  148.6806488 ]
 [ 124.07136536  383.03179932  157.50984192  489.07434082]]
500
[[  56.          238.           85.          283.        ]
 [ 146.          175.          212.          221.        ]
 [ 220.          215.          238.          264.        ]
 ..., 
 [ 483.43191528    0.          499.59002686   21.78249741]
 [ 465.71292114   73.67620087  498.4203186   116.72589874]
 [ 417.7064209   290.24380493  493.95089722  332.9291687 ]]
333
[[  72.          120.          249.          380.        ]
 [ 130.38887024  144.70205688  252.09599304  441.36148071]
 [ 143.2286377    85.10647583  256.81118774  411.5262146 ]
 ..., 
 [ 124.63000488  301.13327026  166.67480469  333.59750366]
 [ 131.65612793  357.26254272  220.18066406  499.5       ]
 [ 100.43856049  382.22903442  120.68889618  417.20806885]]
500
[[ 142.          125.          323.          238.        ]
 [ 255.13183594   99.53004456  496.300354    374.375     ]
 [ 209.31045532  168.9315033   499.375       374.375     ]
 ..., 
 [ 116.21524811  134.35003662  201.58828735  204.23471069]
 [  81.68967438   53.5768013   103.52646637   72.34124756]
 [ 444.8314209   178.01881409  461.84985352  194.2649231 ]]
375
[[  52.           65.          334.          498.        ]
 [   0.            0.          206.18113708  499.375     ]
 [  63.86574554    0.          242.96089172  396.92593384]
 ..., 
 [ 251.89891052  301.35913086  368.41244507  400.4324646 ]
 [ 163.02125549  372.71185303  191.02822876  419.89242554]
 [ 201.19512939  239.02441406  314.24819946  334.80688477]]
500
[[  24.          318.          360.          374.        ]
 [ 313.          126.          498.          370.        ]
 [  45.            0.          381.          239.        ]
 ..., 
 [ 222.2114563   135.76246643  351.00802612  221.29768372]
 [ 260.99475098  338.33441162  315.85003662  354.19775391]
 [  26.12301826  196.74147034   57.40177917  259.66146851]]
375
[[ 128.           85.          374.          337.        ]
 [ 256.          304.          374.          494.        ]
 [  64.98123169    0.          294.06536865  493.64822388]
 ..., 
 [ 158.46099854  453.20205688  226.3837738   499.375     ]
 [  12.49055576   45.04428864   78.00643921   99.88710022]
 [ 100.28310394  222.59046936  150.91127014  262.57263184]]
375
[[  12.            7.          297.          498.        ]
 [ 154.47549438    0.          374.375       372.34280396]
 [  39.89204407    0.          313.27380371  461.01278687]
 ..., 
 [ 142.31088257  347.21569824  196.28805542  433.81304932]
 [ 104.72874451  162.72660828  180.21669006  267.49389648]
 [ 244.02375793  245.33305359  285.46533203  296.91928101]]
333
[[  74.          130.          241.          359.        ]
 [  25.49295807   40.31163406  280.11117554  496.73266602]
 [  53.65619278    0.          251.32481384  429.82189941]
 ..., 
 [   0.          338.14929199   89.76869202  477.18377686]
 [ 265.91775513  123.02451324  331.94262695  164.91680908]
 [ 118.91947174  101.99100494  240.30841064  227.28211975]]
474
[[  95.            8.          366.          232.        ]
 [ 198.95379639    0.          354.30386353  278.7119751 ]
 [   0.           75.35734558  196.55255127  278.7119751 ]
 ..., 
 [ 233.23829651  137.14222717  267.96084595  171.50714111]
 [ 147.84616089  244.1486969   220.05438232  278.7119751 ]
 [ 165.32884216  219.50343323  191.28948975  241.92576599]]
398
[[  99.          164.          280.          438.        ]
 [  48.43128967    0.          291.88195801  486.54003906]
 [  61.02199936   74.39837646  239.14125061  499.48999023]
 ..., 
 [ 144.63534546  426.6831665   198.90406799  458.97805786]
 [ 258.57232666  437.96716309  307.75592041  486.15914917]
 [ 113.8236084     0.          336.82025146  112.84233093]]
345
[[ 116.          244.          163.          299.        ]
 [ 199.          300.          226.          333.        ]
 [ 190.53935242  304.94772339  223.12606812  344.18826294]
 ..., 
 [  14.34850788  234.67884827   51.43864822  304.80136108]
 [  75.43668365   85.10335541  213.34375     307.77212524]
 [ 146.66496277  157.30955505  224.97631836  204.67141724]]
335
[[  57.           51.          265.          485.        ]
 [  61.68790054    0.          274.50967407  416.33963013]
 [  15.61016846  148.67324829  245.39546204  499.70831299]
 ..., 
 [  96.78913116  469.12905884  175.84280396  499.70831299]
 [ 313.96859741   67.31539154  334.44165039  133.39642334]
 [  81.04219818  322.30661011  174.99998474  381.43218994]]
375
[[ 103.          140.          305.          436.        ]
 [ 164.88374329   37.50833511  354.06234741  499.375     ]
 [ 107.10832214   29.34066772  318.57147217  499.375     ]
 ..., 
 [ 179.64311218  132.081604    205.47875977  155.46852112]
 [ 169.33410645   32.05047226  310.02810669  180.22422791]
 [ 122.39720917   44.61381531  141.71499634   68.84634399]]
500
[[ 113.          185.          135.          214.        ]
 [ 359.          194.          374.          220.        ]
 [ 137.          146.          349.          236.        ]
 ..., 
 [  24.92423439    0.           97.9957428    21.98370743]
 [ 124.64771271   35.29994202  144.18032837   57.63560104]
 [  42.52823257  287.89425659   72.21598053  306.84899902]]
375
[[ 112.           36.          301.          482.        ]
 [ 283.           66.          296.           87.        ]
 [  65.51174164    0.          289.01898193  435.95297241]
 ..., 
 [ 329.47665405  318.40100098  345.21420288  339.37069702]
 [   0.           18.63512993   42.85531616   90.28935242]
 [ 113.24806213   59.75795746  155.59687805  151.88973999]]
300
[[  61.           73.          262.          339.        ]
 [ 126.93989563    0.          299.5         399.5       ]
 [  61.90765381  129.16600037  299.5         399.5       ]
 ..., 
 [   0.          119.06455994   32.31509399  167.28007507]
 [  37.753685    130.35818481   95.49715424  181.76705933]
 [   0.            0.           54.29672241  119.75274658]]
500
[[ 302.          123.          337.          150.        ]
 [ 283.44598389   16.33384895  499.5         332.44500732]
 [ 309.26080322  107.86737061  497.05981445  332.44500732]
 ..., 
 [ 364.87033081  113.29129028  388.42028809  140.07830811]
 [ 439.90335083  176.26321411  499.5         213.07003784]
 [ 248.59060669  127.38551331  285.65722656  150.74433899]]
500
[[  55.          112.          287.          321.        ]
 [  91.91310883  124.25010681  269.72052002  325.45666504]
 [  69.9238205    14.60485268  269.74102783  325.45666504]
 ..., 
 [ 410.19699097   13.25178337  499.32333374  120.39781952]
 [ 285.91290283   45.86074448  338.51351929   94.34768677]
 [   0.            0.           62.5670166   112.90382385]]
500
[[  85.           78.          448.          223.        ]
 [ 145.35528564   72.09786224  467.24990845  241.37329102]
 [  86.40605927   80.16052246  404.97381592  226.52449036]
 ..., 
 [  51.67235184  224.58087158  139.37225342  260.69171143]
 [ 456.01242065   63.83158493  499.375       120.71018982]
 [ 120.90550232  357.59396362  148.88438416  374.375     ]]
289
[[  76.            4.          194.          491.        ]
 [  44.80463409    0.          236.24737549  395.25878906]
 [  68.10406494   31.07214355  239.884552    499.5       ]
 ..., 
 [   0.          201.65141296  123.2334137   329.10327148]
 [ 136.45365906  434.45767212  236.60621643  499.5       ]
 [  82.90194702  289.95922852  151.94194031  345.58843994]]
500
[[  24.           49.          466.          261.        ]
 [ 108.           77.          181.          135.        ]
 [ 158.           47.          235.           94.        ]
 ..., 
 [ 309.17453003  332.94607544  393.23916626  369.65362549]
 [ 476.99865723  206.0852356   494.78546143  252.98171997]
 [ 418.07043457   92.52736664  479.65216064  144.7359314 ]]
375
[[  80.          278.          159.          387.        ]
 [  85.50013733    0.          345.01095581  340.13595581]
 [  88.10998535  300.31854248  148.75268555  402.74459839]
 ..., 
 [   0.          198.67152405   35.12517548  256.51522827]
 [  91.57754517  210.75007629  113.80271912  257.95941162]
 [ 214.91906738  118.9354248   261.37091064  167.93139648]]
375
[[ 106.          192.          129.          221.        ]
 [ 125.           89.          276.          305.        ]
 [ 175.          161.          189.          198.        ]
 ..., 
 [ 146.18501282   89.9046402   195.07917786  133.75830078]
 [  69.20671082   29.68751907  106.39253235   56.21403503]
 [ 268.88235474  366.49847412  319.44793701  499.375     ]]
500
[[ 121.            1.          321.          362.        ]
 [ 111.83451843    0.          309.69848633  374.375     ]
 [ 148.03764343    0.          377.77606201  365.85357666]
 ..., 
 [ 463.64926147    0.          499.375       299.15112305]
 [  20.69548607  201.46833801   36.71094513  224.55053711]
 [  98.72661591  144.71099854  163.64451599  187.74176025]]
375
[[ 175.           72.          207.           95.        ]
 [ 136.           93.          221.          465.        ]
 [ 141.75526428  187.2442627   220.64341736  428.50292969]
 ..., 
 [ 281.98226929  149.0821228   336.40820312  209.66970825]
 [  21.34243011  299.64224243   97.52919769  370.29708862]
 [   8.45405293  256.74893188   55.42757034  298.53848267]]
375
[[ 141.           38.          246.          377.        ]
 [ 148.22717285   21.57726288  263.90637207  350.30758667]
 [ 126.21492004   42.41703796  242.16699219  346.61453247]
 ..., 
 [ 104.51581573  103.56811523  170.70722961  175.036026  ]
 [  78.1568985   105.76450348  172.49217224  177.9637146 ]
 [ 194.61038208  279.06896973  257.04244995  343.33813477]]
500
[[ 202.           64.          238.          104.        ]
 [ 204.39230347   62.05908966  244.03546143  114.18981171]
 [ 202.35691833   59.85934067  244.23872375   99.71572876]
 ..., 
 [  44.0954361   122.93097687   80.31638336  147.24049377]
 [ 190.49871826  119.75380707  232.03723145  157.40008545]
 [ 167.27125549   26.18191338  213.26649475   66.0279541 ]]
500
[[ 145.          211.          225.          290.        ]
 [ 150.35879517  217.36885071  223.61192322  289.4630127 ]
 [ 135.74717712  248.92526245  241.68807983  334.37142944]
 ..., 
 [ 280.83279419  262.85415649  300.36251831  282.78967285]
 [ 179.04214478  285.23959351  218.80307007  309.16275024]
 [ 103.68338013  152.60908508  133.36436462  178.17993164]]
375
[[ 225.           79.          372.          269.        ]
 [   0.          386.          105.          432.        ]
 [  76.          394.          225.          498.        ]
 ..., 
 [ 214.25978088   44.31825256  253.92861938  161.67338562]
 [ 357.03460693  264.50857544  374.375       354.36508179]
 [ 350.27911377  254.14413452  374.375       343.57513428]]
375
[[ 176.           90.          262.          266.        ]
 [ 159.1499939     0.          374.375       499.375     ]
 [ 160.53392029  202.00515747  374.375       454.55215454]
 ..., 
 [ 243.04698181  239.97406006  272.77590942  266.78204346]
 [  86.20635223  261.16146851  124.04934692  305.86325073]
 [  39.36564636  254.58763123   66.66188049  281.82287598]]
500
[[  22.           29.          206.          343.        ]
 [  56.          132.           75.          160.        ]
 [  78.          157.          101.          185.        ]
 ..., 
 [  63.05194473   81.7075882   108.24868774  140.26994324]
 [ 323.93875122  290.26135254  430.61325073  374.375     ]
 [ 133.81135559  274.41748047  223.53399658  346.99908447]]
500
[[  33.            0.          497.          331.        ]
 [ 229.89317322    9.20151424  463.42166138  331.44665527]
 [ 160.50074768    0.          394.77520752  311.60299683]
 ..., 
 [   0.           71.32073212  157.91844177  242.5997467 ]
 [ 177.04566956   43.98012543  197.60881042  103.51993561]
 [  75.50527191  304.36062622  165.19543457  331.44665527]]
333
[[  36.            0.          208.          499.        ]
 [   0.            0.          174.57392883  499.5       ]
 [  78.3467865     0.          259.76654053  415.01412964]
 ..., 
 [ 289.97106934  198.8092041   315.60269165  249.78787231]
 [  78.08135986  153.97515869  184.36859131  229.57659912]
 [  22.99643898  197.39096069   40.76741409  239.777771  ]]
500
[[ 284.           44.          409.          250.        ]
 [ 285.00735474   67.46529388  400.73086548  243.35861206]
 [ 343.41574097   95.39913177  499.375       374.375     ]
 ..., 
 [ 250.49128723  183.49333191  272.93392944  214.74275208]
 [ 239.5740509    50.4785881   275.28582764  101.49987793]
 [ 357.371521    253.87117004  429.18203735  301.26275635]]
500
[[ 262.           49.          354.          144.        ]
 [ 125.          211.          160.          263.        ]
 [ 159.          207.          242.          283.        ]
 ..., 
 [ 397.20523071  201.97053528  472.0953064   258.44970703]
 [ 187.18031311  130.13804626  208.67163086  153.53367615]
 [  70.00029755   62.98102188   91.47402954  123.95909119]]
500
[[   0.           40.           88.          235.        ]
 [ 108.           60.          176.          117.        ]
 [ 303.           73.          372.          134.        ]
 ..., 
 [ 361.48251343   77.04951477  457.63101196  121.06137085]
 [ 240.49786377  203.31033325  332.25238037  256.26846313]
 [ 290.01522827  111.72869873  324.18063354  139.78598022]]
466
[[  52.           28.          392.          447.        ]
 [ 271.97796631    0.          465.22332764  440.85421753]
 [  78.10837555    0.          327.23458862  499.39666748]
 ..., 
 [ 268.34094238   61.52514648  296.11602783   86.72135162]
 [ 319.08584595   21.0563488   362.60070801   73.73352051]
 [  69.5366745   157.80740356   93.97140503  189.26766968]]
500
[[ 253.           70.          433.          283.        ]
 [ 101.          123.          340.          302.        ]
 [ 252.02676392    5.79160404  459.40530396  376.37167358]
 ..., 
 [ 136.82730103    0.          211.77476501   21.5080719 ]
 [ 417.99468994  252.30122375  453.02304077  284.43771362]
 [ 148.6829071   224.12316895  178.58059692  277.72192383]]
500
[[ 357.          219.          417.          313.        ]
 [ 268.          137.          309.          165.        ]
 [ 143.37382507   21.28257751  365.92410278  374.375     ]
 ..., 
 [ 273.26477051  124.11006927  291.0055542   161.31370544]
 [ 220.57673645  311.855896    237.30128479  327.90786743]
 [ 179.68051147  339.56375122  207.83340454  357.73178101]]
375
[[  54.            0.          240.          498.        ]
 [  70.64700317    0.          204.80883789  499.375     ]
 [   5.06678581   80.26055908  295.49862671  499.375     ]
 ..., 
 [ 203.34136963  226.80796814  345.38946533  367.07977295]
 [ 322.59274292  171.39808655  374.375       458.98480225]
 [  16.62384796  240.71290588   36.69454956  277.59164429]]
333
[[  58.           47.          266.          403.        ]
 [  45.          318.           63.          399.        ]
 [  36.68461609   64.21437073  245.53103638  454.79159546]
 ..., 
 [ 151.08183289  383.36730957  208.92669678  423.59375   ]
 [   0.          480.65161133   53.97154999  499.5       ]
 [ 188.29429626  330.86965942  268.48257446  397.99716187]]
333
[[ 221.          113.          281.          203.        ]
 [ 198.          253.          276.          472.        ]
 [  51.          188.           91.          215.        ]
 ..., 
 [ 171.31776428  229.44992065  237.04397583  292.17727661]
 [  94.55200958  145.93200684  162.73057556  202.66226196]
 [ 247.41572571   77.32527161  325.19003296  124.92961121]]
500
[[ 120.           29.          375.          312.        ]
 [ 173.67570496    0.          394.91171265  365.39001465]
 [ 143.15815735    0.          286.72015381  365.39001465]
 ..., 
 [ 388.90896606   35.25772476  425.65893555   70.12564087]
 [  45.23130798  150.4690094    69.18974304  178.34907532]
 [  66.2559967    92.83042145   92.07233429  131.30299377]]
500
[[ 151.           72.          370.          255.        ]
 [  81.80580139   47.73195267  290.84646606  374.375     ]
 [ 123.68346405    0.          325.95474243  374.375     ]
 ..., 
 [ 314.72814941   78.88941956  343.51715088  107.50487518]
 [ 476.66140747  161.00753784  499.375       224.38613892]
 [ 350.04229736  186.54425049  384.25189209  266.11639404]]
334
[[ 149.           88.          168.          153.        ]
 [ 218.           89.          251.          121.        ]
 [ 168.          188.          204.          223.        ]
 ..., 
 [ 304.10684204  170.32516479  325.25149536  212.81866455]
 [ 139.90802002  250.04678345  162.88183594  296.55374146]
 [ 197.90556335  319.83364868  267.56280518  362.57614136]]
334
[[  46.            1.          316.          498.        ]
 [   0.            0.          220.47277832  499.32998657]
 [  42.8904953     0.          331.03100586  499.32998657]
 ..., 
 [ 248.90414429  122.20267487  307.56277466  237.02705383]
 [ 114.17324829   40.36603928  185.50547791   90.49595642]
 [ 227.75671387  409.14041138  331.42687988  473.78381348]]
500
[[ 119.           39.          372.          321.        ]
 [ 151.80973816   69.8494339   321.68151855  374.375     ]
 [ 117.92935944    0.          306.50579834  374.375     ]
 ..., 
 [  81.84913635  221.22619629  115.61410522  274.73391724]
 [ 415.59368896   27.45137978  448.17294312   77.15309143]
 [ 279.2953186    90.9023819   317.31307983  125.0895462 ]]
375
[[  80.           10.          371.          470.        ]
 [   0.            0.          258.60046387  456.1975708 ]
 [  77.77500153    0.          374.375       452.12161255]
 ..., 
 [ 214.7172699   331.97018433  265.88757324  378.99661255]
 [ 128.19256592  408.42544556  163.5426178   449.04693604]
 [ 334.17486572  366.28070068  357.29864502  409.78192139]]
375
[[ 102.           29.          287.          467.        ]
 [ 151.84866333  175.28578186  285.56555176  499.375     ]
 [   0.            0.          161.04866028  499.375     ]
 ..., 
 [ 145.92723083  425.16744995  162.90657043  455.31835938]
 [  31.47093201   58.16007614   56.78213501   92.39081573]
 [ 144.85446167   96.02624512  251.35620117  336.42370605]]
375
[[  99.           93.          270.          470.        ]
 [  90.25428772   49.92713928  274.75332642  499.375     ]
 [  40.0803566   125.16742706  365.93490601  474.64398193]
 ..., 
 [ 181.15234375  446.92428589  266.54360962  484.72122192]
 [ 114.14281464  172.27957153  179.91569519  258.58972168]
 [ 116.33821106   67.54621887  254.29872131  162.52336121]]
500
[[ 261.          138.          316.          163.        ]
 [ 281.09829712    0.          499.375       374.375     ]
 [   0.           12.06899643  243.72119141  374.375     ]
 ..., 
 [ 183.13568115   28.5870266   217.67837524   53.2025795 ]
 [  81.03167725   61.09083939  102.24448395   81.32820129]
 [ 210.48869324  292.28735352  240.69862366  310.50839233]]
500
[[ 152.          122.          399.          211.        ]
 [ 301.          196.          373.          219.        ]
 [   0.            0.          181.21057129  332.44500732]
 ..., 
 [ 179.28674316   73.08045197  242.68946838  120.15142059]
 [   7.26149273  155.23204041   45.64161682  187.85758972]
 [  68.6026001   130.59260559  116.88548279  180.38903809]]
453
[[  0.00000000e+00   0.00000000e+00   4.20000000e+02   4.99000000e+02]
 [  1.39339060e-01   0.00000000e+00   2.67690552e+02   4.99054993e+02]
 [  0.00000000e+00   7.12826843e+01   3.28668182e+02   4.77090607e+02]
 ..., 
 [  3.75418488e+02   2.04775894e+02   4.52244995e+02   2.90976166e+02]
 [  3.72875122e+02   4.77182434e+02   3.98956482e+02   4.99054993e+02]
 [  8.42821426e+01   2.08804016e+02   1.21038414e+02   2.56146667e+02]]
375
[[  93.           96.          268.          357.        ]
 [ 208.           75.          245.          106.        ]
 [  78.            0.          235.           81.        ]
 ..., 
 [ 231.75360107  299.39785767  254.83229065  320.25775146]
 [ 192.26403809  404.40048218  250.88461304  476.25808716]
 [ 246.6547699   265.62567139  294.7510376   346.54620361]]
500
[[  16.           24.          228.          139.        ]
 [  31.          162.          209.          252.        ]
 [ 274.           22.          469.          162.        ]
 ..., 
 [ 479.37191772  217.53344727  499.13998413  288.34616089]
 [  24.2798233   158.9644165    79.33998108  221.45892334]
 [  55.2497673   219.35923767  117.63665009  276.78948975]]
316
[[  11.           48.          288.          492.        ]
 [  38.6544838     0.          268.31509399  395.94821167]
 [  41.87295532    0.          184.62837219  312.73672485]
 ..., 
 [  43.32970428  132.70362854   70.14958954  168.09162903]
 [  53.2553978    86.46325684   95.22605896  134.85824585]
 [  20.85201645  339.09405518   44.76960754  372.8288269 ]]
333
[[ 104.          184.          172.          249.        ]
 [  90.05319977    0.          281.89916992  296.75723267]
 [ 109.43099976  194.56507874  174.78265381  246.57417297]
 ..., 
 [   8.07790852   37.23641586   30.2007637    61.48736572]
 [ 167.64726257  197.86442566  192.40939331  216.50648499]
 [   0.          285.30142212   76.03617859  461.22769165]]
254
[[  14.           11.          240.          494.        ]
 [   0.            0.          189.99789429  388.11230469]
 [   0.           16.06788635  226.5138855   298.48760986]
 ..., 
 [ 171.73858643  299.33337402  199.96212769  324.91540527]
 [  68.75561523  217.39953613   95.00515747  259.95922852]
 [  77.98300171  418.66262817  147.48068237  477.29891968]]
500
[[  27.          202.           46.          223.        ]
 [   2.          211.           85.          324.        ]
 [ 277.          331.          349.          366.        ]
 ..., 
 [  58.72989655   81.62463379   85.37815857  102.42671204]
 [ 454.81536865  193.92692566  493.96047974  238.33503723]
 [ 301.85910034  183.91017151  446.85583496  304.92706299]]
500
[[ 369.          240.          420.          287.        ]
 [ 389.          350.          413.          372.        ]
 [ 297.36099243    0.          446.24243164  260.40246582]
 ..., 
 [ 150.30812073  299.11016846  169.5395813   337.77163696]
 [ 316.84020996   43.02630997  349.38299561   80.95730591]
 [ 137.36225891  294.60577393  168.13485718  331.84207153]]
500
[[  39.           68.          470.          276.        ]
 [  71.72840118    0.          268.14453125  374.375     ]
 [   1.02946281   28.82415771  317.86993408  374.375     ]
 ..., 
 [ 104.29302216  157.53746033  182.22084045  312.92843628]
 [ 425.78201294    0.          460.92901611   36.27497482]
 [ 346.25811768    0.          460.04925537  109.66500854]]
500
[[  66.           61.          475.          202.        ]
 [  91.1939621     0.          391.19586182  374.375     ]
 [ 137.12043762    0.          367.45697021  316.03857422]
 ..., 
 [ 471.07015991   57.93085861  499.375       197.36643982]
 [ 202.36022949  329.24621582  265.70635986  372.94836426]
 [  29.69578934  302.42498779   51.21795654  326.95205688]]
500
[[ 176.           34.          387.          368.        ]
 [ 234.01156616    0.          438.44973755  374.375     ]
 [ 150.66061401    0.          341.64642334  374.375     ]
 ..., 
 [  62.72779083  309.796875     93.14622498  340.81463623]
 [ 385.61627197  157.65843201  499.375       340.90527344]
 [ 457.57711792  265.76138306  478.59024048  294.10662842]]
500
[[  47.          134.          466.          348.        ]
 [ 166.2943573    62.33636856  365.63015747  374.375     ]
 [  90.71773529    0.          387.50473022  374.375     ]
 ..., 
 [ 123.72224426  181.35264587  149.10746765  217.75604248]
 [  24.0920105    98.85612488   63.50767517  156.93385315]
 [ 292.84176636  231.04376221  333.92578125  278.27133179]]
500
[[ 126.           26.          413.          312.        ]
 [ 201.          110.          211.          125.        ]
 [  93.06767273    0.          290.09631348  374.375     ]
 ..., 
 [  18.65000343   26.95114326   48.84054565   85.26399231]
 [ 383.81329346   68.88437653  423.78497314  111.57743073]
 [ 340.41098022  229.2224884   394.57855225  286.27212524]]
375
[[   0.           94.          284.          389.        ]
 [   7.          185.           36.          219.        ]
 [ 135.           98.          167.          122.        ]
 ..., 
 [ 305.88821411  286.20172119  349.23355103  372.5100708 ]
 [  30.57758713  285.03735352  129.63471985  385.30307007]
 [ 327.41308594   44.34225845  354.6880188    80.09101105]]
500
[[ 102.            6.          364.          359.        ]
 [  94.30310822    0.          321.09915161  374.375     ]
 [  96.12277985   58.03036499  391.30297852  357.87524414]
 ..., 
 [  14.63140869   46.33716583   44.03371429   85.17008972]
 [ 174.48587036  187.26776123  249.46305847  255.65071106]
 [ 365.71594238   37.0505867   451.86248779  102.80202484]]
500
[[  73.          132.          326.          259.        ]
 [ 154.5022583    99.86836243  342.97167969  332.44500732]
 [ 203.97142029   71.49664307  307.54516602  332.44500732]
 ..., 
 [ 295.01217651   82.79598999  322.06442261  119.18983459]
 [  69.14463806    0.          206.09069824  146.03182983]
 [  23.15274239  232.52635193   38.85660934  254.29682922]]
375
[[ 117.           19.          290.          361.        ]
 [ 119.47662354    0.          291.98031616  347.32540894]
 [  26.55402184    0.          157.36752319  271.76382446]
 ..., 
 [ 265.7232666   172.44093323  351.45129395  266.72354126]
 [  46.06751251  162.32168579   65.90726471  204.81416321]
 [   0.          151.15170288   53.92269897  321.06274414]]
375
[[  58.            0.          269.          395.        ]
 [  33.          292.           52.          363.        ]
 [ 100.          438.          141.          484.        ]
 ..., 
 [ 331.68777466  279.97845459  348.8765564   316.50125122]
 [ 323.05737305  406.60147095  347.10848999  441.13327026]
 [   0.            0.           16.81369019   17.28785324]]
480
[[  68.           23.          404.          296.        ]
 [  44.          298.           80.          358.        ]
 [  72.          319.          121.          357.        ]
 ..., 
 [ 134.24398804   77.10829926  154.29098511  104.99862671]
 [ 147.16802979  254.5219574   199.52992249  316.31872559]
 [ 283.28137207   80.67789459  323.15841675  135.6149292 ]]
375
[[  88.            0.          296.          411.        ]
 [  82.          271.           95.          302.        ]
 [ 195.          430.          220.          449.        ]
 ..., 
 [  72.03836823   33.88747787  200.04949951  156.98320007]
 [ 129.42578125   96.59752655  146.40032959  138.00291443]
 [  22.82081604  398.44119263   47.68299103  426.27932739]]
480
[[  84.           32.          348.          302.        ]
 [ 143.045578      0.          431.36254883  206.56546021]
 [ 190.91882324    0.          341.44760132  359.3999939 ]
 ..., 
 [ 300.25622559  274.4463501   366.41726685  345.11758423]
 [ 354.16970825   73.78816986  371.77581787   92.67485809]
 [   2.2638061    39.90295792   34.85409927   81.29798126]]
375
[[ 134.           48.          264.          425.        ]
 [ 117.99292755   22.44682312  289.59851074  499.375     ]
 [  53.78012466  130.87878418  356.94934082  481.99783325]
 ..., 
 [ 183.37051392   49.34930801  257.83950806  126.40586853]
 [ 286.10998535  270.63986206  304.83737183  290.32736206]
 [ 191.30046082  343.26269531  281.87094116  499.375     ]]
500
[[ 179.          165.          275.          248.        ]
 [ 269.           95.          350.          161.        ]
 [ 175.78623962  163.52111816  283.11398315  250.02693176]
 ..., 
 [  81.50863647  117.42599487  289.98135376  305.25131226]
 [ 177.3478241   293.3770752   201.16664124  314.89416504]
 [ 135.10093689   91.90013123  196.58427429  136.01226807]]
500
[[ 154.           72.          364.          275.        ]
 [ 139.11309814    0.          292.19625854  374.375     ]
 [ 109.12757111    0.          335.29620361  374.375     ]
 ..., 
 [ 118.27428436  235.38821411  221.24838257  374.375     ]
 [ 142.02900696  291.51837158  210.33618164  364.20742798]
 [ 204.31391907   15.75240707  234.3939209    48.56447601]]
500
[[ 127.          133.          348.          200.        ]
 [ 133.          192.          177.          235.        ]
 [ 177.          201.          224.          249.        ]
 ..., 
 [ 150.73614502  262.14031982  179.76257324  299.30499268]
 [  51.05369568  202.79579163  125.27310944  272.38360596]
 [  63.31515121  166.80636597  112.1556778   227.9513092 ]]
375
[[ 178.           58.          337.          450.        ]
 [ 192.05622864    0.          374.375       499.375     ]
 [ 143.15026855    0.          353.18634033  407.64953613]
 ..., 
 [ 305.35510254  197.84356689  353.45767212  258.37261963]
 [ 226.61231995   22.95265579  358.5246582   106.08551025]
 [ 296.34466553  417.23071289  337.34646606  478.07995605]]
500
[[  63.           79.          475.          250.        ]
 [  30.86409187    0.          283.96221924  332.44500732]
 [ 257.10940552   14.6401577   489.28713989  332.44500732]
 ..., 
 [ 379.42211914  100.23699951  435.96124268  129.15370178]
 [ 305.07070923   77.53874969  324.89468384   99.06729126]
 [  84.3134613    74.0185318   180.62380981  176.97973633]]
500
[[ 356.          181.          381.          208.        ]
 [ 368.          207.          386.          227.        ]
 [ 384.          225.          396.          242.        ]
 ..., 
 [ 103.17897797   19.77143478  168.09638977   70.1497879 ]
 [ 269.44024658  316.88928223  297.68267822  332.44500732]
 [  64.31960297    6.58557987  131.20832825   55.33030701]]
375
[[  53.           28.          338.          496.        ]
 [   0.            0.          247.39434814  499.375     ]
 [ 114.15811157    0.          353.40881348  499.375     ]
 ..., 
 [ 261.39169312   44.07892227  351.59597778   66.16750336]
 [  27.73812675  355.98822021   51.96955109  387.19641113]
 [  24.24196053  103.72940063   94.21841431  160.1348877 ]]
256
[[  25.           17.          242.          485.        ]
 [  99.43480682    0.          255.5         395.74871826]
 [   9.76026154   48.14498901  207.48892212  499.5       ]
 ..., 
 [  92.01338196  297.18829346  139.28869629  318.24176025]
 [  37.56368256  434.99160767  135.87103271  499.5       ]
 [ 120.73678589  369.78900146  159.75473022  416.35559082]]
500
[[  97.           59.          331.          224.        ]
 [ 194.          355.          211.          374.        ]
 [ 209.          359.          222.          380.        ]
 ..., 
 [ 342.95669556  140.76945496  366.70462036  172.45568848]
 [   0.            0.           29.2742691    22.11328697]
 [ 104.38729095  170.1661377   266.00326538  428.28500366]]
375
[[  59.           60.          239.          461.        ]
 [ 214.          384.          234.          422.        ]
 [ 182.          289.          231.          370.        ]
 ..., 
 [  17.76542091  395.7210083    55.90791702  437.60623169]
 [ 197.06085205  143.52236938  288.86947632  202.79704285]
 [  69.62413788   58.33612823  136.36506653  102.48723602]]
500
[[  82.           38.          301.          330.        ]
 [  22.08190346    0.          168.15821838  333.44332886]
 [  68.13742828    0.          275.13626099  333.44332886]
 ..., 
 [   0.          301.53817749   72.43040466  333.44332886]
 [ 437.10702515   20.37867928  458.07214355   53.0719223 ]
 [  35.68635559  216.65270996   98.41656494  274.54760742]]
500
[[ 170.          172.          280.          269.        ]
 [ 198.          266.          233.          318.        ]
 [ 105.          231.          132.          372.        ]
 ..., 
 [ 165.71601868   23.75153732  204.36347961   63.5714035 ]
 [  66.87539673  206.47758484   91.72314453  252.48927307]
 [ 194.1683197   227.22994995  314.11212158  374.375     ]]
375
[[ 130.          149.          289.          429.        ]
 [ 117.6971817    60.10410309  299.65289307  499.375     ]
 [ 121.50096893  137.51829529  340.80233765  499.375     ]
 ..., 
 [  29.64465332  473.44448853   64.02021027  499.375     ]
 [ 316.32443237  322.41516113  374.375       461.99697876]
 [ 147.35644531   68.56530762  187.15266418  126.07343292]]
500
[[  72.            0.          402.          373.        ]
 [  65.68170166    0.          320.36657715  345.51953125]
 [  30.1934433     0.          366.90338135  221.84783936]
 ..., 
 [ 169.28086853  329.18731689  229.59809875  361.69485474]
 [  29.33737755  120.82946777  117.81830597  374.375     ]
 [  65.88314819  318.93563843  101.02786255  356.49664307]]
500
[[ 189.          242.          281.          296.        ]
 [ 200.          311.          345.          344.        ]
 [ 244.          131.          439.          322.        ]
 ..., 
 [ 144.68453979  216.58061218  166.16020203  244.67097473]
 [  74.5229187   337.21881104   95.2541275   374.375     ]
 [   6.03426695  165.09985352   50.31467438  226.61897278]]
500
[[  62.           73.          358.          292.        ]
 [   0.            0.          219.2666626   406.32168579]
 [   0.           72.17017365  273.69085693  406.32168579]
 ..., 
 [ 394.04852295  159.82901001  422.22662354  189.53172302]
 [ 289.10284424  282.15759277  319.64382935  304.62472534]
 [ 352.73626709  152.79971313  389.86129761  205.12565613]]
500
[[ 166.          115.          203.          138.        ]
 [   0.           38.34663391  173.98200989  374.375     ]
 [ 158.05856323  112.97928619  199.51776123  144.34727478]
 ..., 
 [ 324.33172607  250.3397522   354.28817749  294.51879883]
 [   2.15447426  124.70803833   25.52132607  160.11561584]
 [  91.44016266  260.96099854  186.26541138  324.69726562]]
281
[[  71.           14.           92.           56.        ]
 [  73.           60.          120.          121.        ]
 [  97.          126.          139.          184.        ]
 ..., 
 [  54.13826752   58.08935547  161.54290771  188.70877075]
 [  83.59701538  128.94644165  140.66793823  182.21368408]
 [  78.39582825  173.32754517  160.35261536  211.09954834]]
500
[[ 379.          187.          396.          208.        ]
 [ 384.          201.          414.          234.        ]
 [  86.          147.          116.          191.        ]
 ..., 
 [ 371.71499634   73.71807098  419.29962158  129.10658264]
 [ 396.98669434   47.6962738   481.55395508  105.75357056]
 [ 395.69561768  174.14634705  474.73110962  243.15867615]]
375
[[ 195.           20.          374.          472.        ]
 [   0.           23.          181.          486.        ]
 [ 203.59286499  106.05805969  374.375       499.375     ]
 ..., 
 [ 237.43479919  408.87680054  278.65393066  448.17785645]
 [ 143.7684021    71.61377716  338.76348877  224.68864441]
 [ 191.94572449  400.77633667  224.49549866  446.26507568]]
500
[[  34.            0.          499.          373.        ]
 [  13.83049965    0.          250.5974884   374.375     ]
 [  73.47370911    0.          338.79302979  374.375     ]
 ..., 
 [  56.93096924   20.18265724  147.08294678  169.0539856 ]
 [ 201.28672791  130.65939331  307.91275024  218.28149414]
 [  91.52236938  333.54492188  181.7191925   374.375     ]]
269
[[   8.          123.          259.          498.        ]
 [   0.            0.          268.5         387.54217529]
 [   0.           10.16799927  268.5         267.15762329]
 ..., 
 [  17.31664658  398.62036133   41.7682991   444.22058105]
 [  99.7950058   256.25692749  140.3992157   306.75985718]
 [ 140.97891235   24.80681801  178.13833618   51.97379303]]
375
[[  38.           67.          330.          431.        ]
 [  27.34401703    0.          222.19271851  473.03274536]
 [   0.            0.          308.07897949  359.60925293]
 ..., 
 [  58.68096924   72.63664246  160.76759338  178.1806488 ]
 [ 273.16381836  268.25979614  315.44631958  309.00881958]
 [ 325.38909912  421.75192261  374.375       475.00296021]]
500
[[   0.          184.           77.          363.        ]
 [  37.96987152  336.86621094   64.23715973  374.27755737]
 [  20.23947144  202.6060791    98.22673798  363.17440796]
 ..., 
 [  18.45495987  171.65307617   61.37385178  246.4132843 ]
 [ 312.48651123  222.88442993  344.60562134  248.92919922]
 [   0.          149.42318726  175.90586853  301.85842896]]
333
[[   8.          293.          321.          428.        ]
 [ 161.          411.          171.          422.        ]
 [ 107.          418.          117.          432.        ]
 ..., 
 [ 273.03848267   54.63490295  309.02514648  117.75824738]
 [ 215.67677307  142.77598572  242.44340515  176.33248901]
 [ 265.6668396    58.30707932  332.44500732  162.44419861]]
375
[[ 104.          188.          301.          414.        ]
 [ 176.          430.          259.          460.        ]
 [ 257.          428.          295.          452.        ]
 ..., 
 [ 249.99285889    0.          374.375        54.83792114]
 [  67.69602966  102.33106232   87.60761261  128.95713806]
 [ 332.62716675   67.78686523  349.18960571   99.0455246 ]]
375
[[   5.           77.          174.          376.        ]
 [  27.37816238   77.80391693  170.42727661  439.8197937 ]
 [ 186.21221924    0.          369.662323    306.79721069]
 ..., 
 [  72.67685699  315.09976196  106.31928253  358.00579834]
 [  87.62734222  336.40118408  107.90177155  366.34539795]
 [ 118.17140198  121.12598419  152.0670166   157.03520203]]
375
[[   0.            0.          374.          498.        ]
 [ 170.97331238   88.68843079  374.375       499.375     ]
 [ 217.26667786  130.6947937   374.375       499.375     ]
 ..., 
 [ 301.50015259    0.          374.375        33.05060196]
 [ 193.63037109  147.13121033  298.48828125  245.38130188]
 [  36.1862793    31.62557602   60.34246063   59.17667389]]
375
[[ 124.           96.          278.          469.        ]
 [ 159.          206.          198.          256.        ]
 [ 156.65065002  207.78726196  196.55490112  259.31066895]
 ..., 
 [ 272.54425049  224.2791748   360.83404541  272.96322632]
 [  34.23120499  318.45907593   59.49289322  374.84716797]
 [   0.          412.23007202   53.01085281  475.36962891]]
334
[[ 117.          115.          233.          350.        ]
 [  96.89242554    0.          261.69534302  411.00601196]
 [  96.45156097  147.59851074  244.00775146  316.28549194]
 ..., 
 [   0.           51.93585968  180.3150177   129.6197052 ]
 [ 216.32949829   41.28348923  245.00411987   80.45513153]
 [ 271.11157227  446.64468384  325.53839111  487.50286865]]
500
[[  37.          190.           91.          274.        ]
 [ 321.          171.          392.          255.        ]
 [ 310.9107666   167.20114136  356.33496094  206.93067932]
 ..., 
 [  26.50810623  117.37312317   49.69852448  150.43847656]
 [ 264.37667847  151.26020813  363.6892395   188.43147278]
 [ 282.33709717   57.67198563  303.14074707   87.77003479]]
500
[[ 173.          125.          259.          211.        ]
 [ 170.8319397   131.71281433  244.57473755  204.45840454]
 [  85.23020935    0.          253.62045288  374.375     ]
 ..., 
 [ 328.5645752    22.20001411  350.48779297   51.74819946]
 [ 439.5791626   246.9903717   475.10253906  304.56997681]
 [ 277.39819336  271.14971924  312.0022583   341.02545166]]
500
[[ 230.          127.          323.          287.        ]
 [ 131.47169495   75.4703064   358.51815796  426.28833008]
 [   0.           57.87000275  140.01477051  426.28833008]
 ..., 
 [  77.89582825  133.28588867  125.64833832  210.45535278]
 [   0.          229.87480164   28.99981499  273.84091187]
 [ 482.36361694  204.64239502  499.58999634  426.28833008]]
500
[[ 274.           75.          293.          115.        ]
 [ 221.          109.          428.          276.        ]
 [   0.            0.          204.17481995  374.375     ]
 ..., 
 [ 397.15454102  162.43792725  488.85375977  210.72932434]
 [ 118.15005493   25.87843132  141.20404053   51.75077438]
 [ 393.99932861  173.08174133  487.22702026  221.96983337]]
498
[[  23.           68.          144.          323.        ]
 [ 122.          101.          348.          357.        ]
 [ 200.           38.          264.           78.        ]
 ..., 
 [ 464.34231567  472.0871582   485.43618774  498.82998657]
 [ 346.06582642  440.87060547  371.18395996  461.23965454]
 [  35.75214005  456.21188354  129.34262085  498.82998657]]
332
[[  56.          258.          149.          321.        ]
 [  68.65948486  271.50366211  130.56149292  319.82507324]
 [  75.14784241  274.06777954  137.37243652  312.68814087]
 ..., 
 [ 257.60073853  438.20718384  319.63085938  493.2355957 ]
 [ 164.54873657  355.93841553  230.93360901  428.21520996]
 [ 196.65478516  343.32275391  221.50283813  404.23590088]]
332
[[ 217.          294.          275.          313.        ]
 [ 213.17205811  298.92694092  270.55056763  318.20352173]
 [ 200.11581421  152.2641449   310.44775391  499.66000366]
 ..., 
 [ 295.13647461  302.987854    331.44665527  379.36730957]
 [  83.17485809  158.07559204  138.25643921  197.69734192]
 [  63.08360672   64.10299683  128.27529907   87.67922211]]
500
[[ 146.          192.          163.          253.        ]
 [ 169.          209.          200.          312.        ]
 [ 211.          213.          257.          330.        ]
 ..., 
 [  62.253479    322.43054199   93.38443756  365.33724976]
 [ 408.66299438  143.52229309  468.05422974  208.09890747]
 [ 283.58901978   16.04462051  367.07336426   69.98247528]]
375
[[  78.          256.          247.          494.        ]
 [  82.34240723  272.43780518  248.33877563  489.14984131]
 [  78.36785126   91.31820679  309.0932312   499.375     ]
 ..., 
 [  10.42837143  351.5458374   224.7951355   483.02761841]
 [  29.0940609   388.56893921   52.71516418  421.83782959]
 [  43.27903748  348.38363647  112.94705963  395.66726685]]
300
[[  40.           27.          279.          471.        ]
 [  38.13443756  152.56448364  195.74539185  499.5       ]
 [   0.          252.95567322  236.12893677  499.5       ]
 ..., 
 [ 181.20014954  335.68447876  199.0092926   353.29873657]
 [ 126.37586212  140.92562866  164.39639282  189.4387207 ]
 [ 226.11212158   19.99085045  286.68804932   63.34906769]]
334
[[  71.          133.          100.          178.        ]
 [ 219.          131.          239.          161.        ]
 [ 243.          131.          314.          288.        ]
 ..., 
 [  12.41030025  167.95904541   42.74290848  210.86062622]
 [ 155.93284607  357.31271362  197.22383118  413.63601685]
 [ 265.06152344  203.83584595  296.83718872  274.89907837]]
375
[[ 137.          189.          226.          349.        ]
 [ 141.27508545  210.14254761  240.4122467   357.90585327]
 [ 127.46051788  190.63148499  232.73048401  331.75253296]
 ..., 
 [  66.87293243  333.36026001   96.70703125  363.67739868]
 [ 249.89059448   73.30052948  268.16687012   90.74015808]
 [ 172.75619507   34.43140793  196.82879639   65.97011566]]
500
[[ 356.          140.          372.          159.        ]
 [ 349.          155.          371.          164.        ]
 [ 349.85290527  167.00912476  365.20959473  186.8374176 ]
 ..., 
 [  43.87813187   66.97306061  104.88459015  132.73138428]
 [ 137.73553467   94.31765747  202.22460938  153.66540527]
 [ 185.89259338   83.27941895  251.09104919  141.09516907]]
333
[[  22.            0.          258.          468.        ]
 [   0.            0.          228.35977173  456.60400391]
 [   0.           36.0990181   217.30677795  361.1902771 ]
 ..., 
 [  46.24612045  103.38993073  106.14569092  197.38224792]
 [ 101.58820343  263.95495605  154.42434692  353.3631897 ]
 [ 276.478302     95.61314392  309.38388062  140.60949707]]
442
[[ 131.           42.          319.          197.        ]
 [ 157.26132202    0.          368.51986694  270.54833984]
 [ 126.35377502    0.          288.86895752  270.54833984]
 ..., 
 [  39.21886826  198.18383789  102.18934631  240.78378296]
 [ 375.05752563   61.60318756  431.7003479   126.27449036]
 [  22.13422966  252.77415466   92.77992249  270.54833984]]
500
[[   8.            5.          492.          343.        ]
 [ 155.44895935   23.54594994  337.06216431  347.42001343]
 [ 192.53178406   41.97666931  326.82189941  347.42001343]
 ..., 
 [ 441.29083252  192.06787109  499.38000488  254.51469421]
 [ 422.37506104  122.83108521  443.06665039  146.43025208]
 [ 471.30709839   63.2009697   499.38000488  129.43035889]]
367
[[ 199.            1.          349.          359.        ]
 [  75.           81.          127.          154.        ]
 [ 163.45443726    4.60133171  366.38830566  493.93551636]
 ..., 
 [ 270.27557373  383.08172607  302.4276123   458.98425293]
 [ 161.32849121  161.45695496  311.15289307  362.93099976]
 [ 248.42440796  302.59481812  266.72094727  347.14868164]]
375
[[  37.            1.          373.          497.        ]
 [ 178.9414978     0.          365.652771    290.94250488]
 [  52.00099945    0.          289.94009399  330.93292236]
 ..., 
 [ 272.02075195  237.86280823  374.375       382.93899536]
 [ 304.01452637  165.7472229   363.56497192  215.2596283 ]
 [  15.86889267  170.20158386  176.37770081  327.60287476]]
303
[[  77.          230.          259.          487.        ]
 [  59.84068298  161.12609863  242.30383301  499.44500732]
 [  17.48768234  143.48269653  224.13687134  497.36224365]
 ..., 
 [ 254.81610107  311.13409424  284.61950684  356.40982056]
 [  65.21089172  363.2328186   149.97372437  419.53463745]
 [ 194.78009033  225.68280029  218.86982727  257.2361145 ]]
305
[[  31.           30.          284.          440.        ]
 [ 115.          432.          134.          455.        ]
 [  86.16964722    0.          304.4916687   456.4833374 ]
 ..., 
 [   0.          103.25949097   29.03669167  154.47514343]
 [ 175.37129211  174.79718018  236.43254089  229.85525513]
 [ 187.83657837  121.05523682  304.4916687   204.823349  ]]
500
[[  17.            6.          459.          305.        ]
 [ 149.34591675    0.          339.84060669  257.55892944]
 [ 170.9365387     0.          392.02856445  322.71575928]
 ..., 
 [  30.5404129    11.08473492   85.32324982   62.70851898]
 [ 412.20297241  225.12600708  478.23944092  271.06866455]
 [ 381.28479004  100.79743958  396.92520142  119.65386963]]
400
[[ 173.          135.          306.          266.        ]
 [ 178.87345886  136.1414032   317.67816162  266.55499268]
 [ 164.77850342    0.          335.50180054  266.55499268]
 ..., 
 [  40.25030518   96.73034668   56.45821381  115.942276  ]
 [ 286.20672607    0.          352.46884155  102.01309204]
 [ 340.12374878  198.19264221  389.41644287  235.89857483]]
375
[[  78.            2.          296.          485.        ]
 [  39.96041107  179.50315857  289.23519897  499.375     ]
 [  68.72206879  112.22618103  322.85437012  499.375     ]
 ..., 
 [ 297.52456665  338.45736694  335.17306519  390.85223389]
 [ 263.38583374  466.16107178  285.76889038  489.68829346]
 [ 339.33441162  159.29702759  361.96875     221.58488464]]
500
[[  78.           35.          422.          202.        ]
 [ 130.          232.          202.          250.        ]
 [ 318.          238.          355.          250.        ]
 ..., 
 [ 270.81866455   65.38976288  301.62713623   86.28159332]
 [   0.            0.           95.12767029   32.06879425]
 [ 193.06134033   68.23257446  268.60873413  119.99206543]]
411
[[ 187.          221.          242.          283.        ]
 [ 185.97019958  227.70259094  246.58045959  303.10797119]
 [ 192.05325317  242.95088196  234.10144043  295.2986145 ]
 ..., 
 [ 350.82901001  322.50064087  395.96734619  365.67440796]
 [ 264.9937439   361.40530396  285.1522522   383.43383789]
 [ 221.54197693   65.13391876  298.14685059  149.96432495]]
333
[[ 130.           44.          330.          293.        ]
 [ 175.60621643    4.02976274  332.44500732  362.07571411]
 [ 127.15374756   69.00026703  332.44500732  350.76318359]
 ..., 
 [ 176.46902466  269.92657471  205.29986572  290.73269653]
 [ 222.97076416  287.98654175  244.35122681  304.53790283]
 [ 282.64154053  116.63089752  305.597229    169.13122559]]
415
[[  18.            0.          219.          497.        ]
 [  56.          334.           83.          404.        ]
 [ 245.            5.          409.          477.        ]
 ..., 
 [  75.7800827   423.21359253  106.13597107  468.6857605 ]
 [  89.08783722  299.47341919  135.64860535  385.83099365]
 [ 181.58247375  391.3999939   203.45549011  497.08432007]]
500
[[ 365.          223.          404.          356.        ]
 [  24.          244.          170.          356.        ]
 [ 164.          266.          186.          289.        ]
 ..., 
 [  49.04542923  175.15612793   68.29019165  193.69242859]
 [ 230.55528259  114.33293152  266.20904541  170.29786682]
 [ 355.96594238    0.          471.86437988   25.44324303]]
500
[[  78.           64.          121.           89.        ]
 [   0.            0.          136.48976135  280.5       ]
 [   0.            0.          197.57543945  247.41711426]
 ..., 
 [   0.            0.           31.04970169   31.36735535]
 [ 196.33251953  106.47942352  213.23632812  129.40464783]
 [ 156.70584106  252.17233276  178.39727783  271.15664673]]
375
[[  57.            8.          321.          495.        ]
 [ 115.45239258   65.79837799  354.1289978   448.2947998 ]
 [  23.05837631   49.02954102  374.375       391.78192139]
 ..., 
 [  54.61436081  299.25506592   82.28204346  354.09008789]
 [   7.80415535   17.57387924   86.03760529   85.31724548]
 [  46.18084717  113.56824493  113.02029419  184.30519104]]
333
[[ 158.          131.          259.          303.        ]
 [ 169.8931427   166.39215088  264.93792725  308.71734619]
 [ 187.88458252    0.          332.44500732  305.84490967]
 ..., 
 [  50.40106964   99.71337891   67.18314362  116.36452484]
 [   5.42264891  335.20706177  128.92648315  499.5       ]
 [  25.25169945   52.8973465    57.72616959   93.46505737]]
334
[[   0.          116.           17.          142.        ]
 [ 121.          104.          172.          153.        ]
 [ 147.          190.          260.          323.        ]
 ..., 
 [  94.97484589  177.96226501  157.44949341  226.22625732]
 [ 211.16079712   36.35426712  238.65844727   63.07867432]
 [ 107.48014069  173.75942993  250.44163513  499.32998657]]
375
[[  37.            0.          308.          494.        ]
 [  39.05557632    0.          295.39822388  409.99816895]
 [  90.29660034    0.          360.09902954  499.375     ]
 ..., 
 [ 127.46190643  357.42514038  209.05271912  413.7638855 ]
 [ 129.83598328  423.125       150.44029236  449.62194824]
 [ 282.95666504  294.76550293  328.95123291  344.11633301]]
375
[[  41.           51.          342.          463.        ]
 [ 245.            0.          314.           73.        ]
 [ 172.27142334  178.96325684  374.375       499.375     ]
 ..., 
 [ 221.11160278   23.22965431  296.75512695   71.81285858]
 [  35.13374329   91.23645782   55.53539276  196.5512085 ]
 [  61.41081238  165.45344543   84.9887619   223.02459717]]
262
[[  43.           12.          223.          487.        ]
 [  38.52546692    0.          247.23582458  395.46740723]
 [  31.57662964   82.06469727  214.8598175   499.5       ]
 ..., 
 [ 188.34002686  375.12533569  213.5652771   417.04037476]
 [ 188.25282288   31.08678055  221.47096252   74.22612   ]
 [   0.          118.45083618   26.56165123  166.31787109]]
500
[[ 141.           82.          332.          255.        ]
 [  46.08551788    0.          268.26239014  374.375     ]
 [ 207.61126709   58.96924973  352.62683105  374.375     ]
 ..., 
 [ 438.74371338  253.77609253  499.375       323.94689941]
 [ 365.27304077   90.44167328  391.14630127  125.98110962]
 [ 355.77255249  246.94602966  473.29595947  311.78515625]]
375
[[  97.          148.          263.          366.        ]
 [  90.06352234   90.55162048  258.97067261  499.375     ]
 [ 149.14256287  150.68331909  291.63424683  499.375     ]
 ..., 
 [  83.90826416  370.23907471  196.56745911  453.87142944]
 [ 236.89179993   48.51891327  314.95953369  101.9311676 ]
 [  84.82263184   85.73639679  174.95321655  129.44158936]]
331
[[ 100.           10.          231.          491.        ]
 [  77.78603363    0.          222.05889893  364.09054565]
 [  91.90716553    0.          198.92198181  309.54803467]
 ..., 
 [ 189.02180481  306.69256592  230.17686462  349.5897522 ]
 [ 138.81622314  278.56808472  180.00146484  351.82507324]
 [  43.94879913   76.99796295   72.19075775  121.95987701]]
375
[[  83.          205.          155.          298.        ]
 [ 179.          189.          213.          243.        ]
 [ 322.          264.          367.          372.        ]
 ..., 
 [  28.41029167  156.87948608   53.31243515  205.0271759 ]
 [ 342.81591797  353.99252319  368.74685669  401.54724121]
 [ 209.68382263  206.67771912  280.19998169  254.86807251]]
375
[[  86.          356.          158.          407.        ]
 [ 182.          377.          236.          431.        ]
 [ 245.          364.          286.          408.        ]
 ..., 
 [ 131.61322021  168.35826111  197.85371399  220.32946777]
 [ 143.5710144   191.15458679  210.32595825  248.93901062]
 [   0.          166.08529663  166.06417847  312.28582764]]
246
[[  31.            6.          218.          476.        ]
 [   0.            0.          196.22473145  481.35934448]
 [   0.          148.58065796  227.37161255  295.38247681]
 ..., 
 [  25.95465279  294.24731445   64.68212128  358.68634033]
 [  10.54444027  367.65328979   31.14836884  409.36087036]
 [  56.45042419   46.78288269  100.57801819  104.4082489 ]]
375
[[  66.            1.          338.          430.        ]
 [ 165.32380676    0.          374.375       381.20715332]
 [  86.2414093     0.          325.4647522   463.43609619]
 ..., 
 [ 164.79806519  126.43306732  230.77261353  189.71766663]
 [ 152.81428528  138.67080688  188.04161072  164.46337891]
 [ 347.67242432  247.41030884  374.375       366.25546265]]
392
[[  60.           84.          283.          382.        ]
 [ 194.          103.          228.          142.        ]
 [ 284.          297.          302.          350.        ]
 ..., 
 [ 227.86973572   31.72743797  257.84970093   65.88949585]
 [  69.84445953    0.          177.61782837  103.24436951]
 [  53.97634506  293.07647705  110.41992188  350.63104248]]
500
[[ 208.           76.          335.          241.        ]
 [ 194.3506012    87.31583405  330.28146362  307.69256592]
 [ 225.05279541   93.40396118  327.08276367  259.97521973]
 ..., 
 [  38.65934372  226.78468323  248.11279297  333.44332886]
 [ 303.41717529   77.04870605  323.78607178   99.31044769]
 [ 212.08755493  140.37705994  244.24238586  174.15498352]]
333
[[   0.          287.           79.          445.        ]
 [  83.          224.          233.          352.        ]
 [ 165.          127.          203.          163.        ]
 ..., 
 [ 142.94740295   82.3498764   169.89810181  125.41659546]
 [ 246.12791443  142.35327148  332.44500732  200.45591736]
 [ 277.82620239  184.18618774  304.21484375  204.52677917]]
333
[[ 141.          298.          184.          342.        ]
 [ 141.50947571  301.92684937  191.30030823  355.05773926]
 [ 146.24946594  297.91552734  191.42277527  343.48358154]
 ..., 
 [ 105.11992645   23.81039429  139.90959167   60.29625702]
 [  58.15510559  419.82919312  138.01786804  460.83535767]
 [ 112.59905243  164.76681519  132.64295959  180.7144165 ]]
333
[[ 182.          356.          241.          422.        ]
 [  33.          246.          231.          353.        ]
 [ 193.          140.          236.          245.        ]
 ..., 
 [ 171.08241272   43.47934723  194.25460815   61.88175201]
 [ 179.5380249   143.23588562  206.53164673  171.71871948]
 [ 278.05859375   45.36971664  316.60168457   84.26798248]]
333
[[ 105.           71.          164.          172.        ]
 [ 119.          468.          173.          498.        ]
 [  18.          327.          104.          341.        ]
 ..., 
 [  98.07369232  166.42991638  145.11883545  231.66426086]
 [  11.51418686   84.16943359   30.68528557  115.53033447]
 [ 138.79792786   25.31559944  209.71066284   86.10098267]]
333
[[ 140.          234.          211.          300.        ]
 [ 253.          235.          263.          264.        ]
 [ 240.          266.          265.          302.        ]
 ..., 
 [  30.10651207  263.3069458    48.9030304   311.03890991]
 [ 287.17660522  123.74645996  322.11999512  154.47590637]
 [  45.26889038  249.93769836   64.01718903  287.97680664]]
333
[[ 107.          189.          265.          274.        ]
 [ 288.          427.          317.          459.        ]
 [ 104.47519684  188.26747131  266.13592529  276.33508301]
 ..., 
 [ 212.35902405  457.13635254  304.47875977  499.5       ]
 [ 292.70376587    0.          332.44500732  185.55574036]
 [ 176.84004211  477.58786011  202.76545715  496.66220093]]
333
[[  77.          237.           93.          258.        ]
 [  75.          275.           85.          291.        ]
 [  98.          268.          121.          292.        ]
 ..., 
 [  67.7947998   230.23556519   99.9139328   253.32955933]
 [  25.2740078   257.89614868   56.68154144  286.84408569]
 [  88.73712921  347.87149048  191.95176697  499.5       ]]
333
[[  14.           51.          246.          437.        ]
 [   0.          101.84480286  186.85426331  499.5       ]
 [   0.            0.          225.91311646  441.90432739]
 ..., 
 [   0.          420.0617981    32.95174408  478.75549316]
 [   0.          425.70184326   45.85416794  472.6161499 ]
 [  18.84269333  234.0196991    83.81545258  314.33673096]]
333
[[  23.          134.          166.          449.        ]
 [ 155.          327.          187.          346.        ]
 [ 124.          114.          187.          178.        ]
 ..., 
 [  30.00617027  344.15377808  116.6007843   428.49368286]
 [  88.02133179  406.76296997  143.62992859  455.57904053]
 [ 178.68057251   63.37462616  212.95951843   91.79645538]]
500
[[ 164.          214.          187.          242.        ]
 [ 175.          250.          197.          286.        ]
 [ 141.          240.          156.          258.        ]
 ..., 
 [ 296.46749878  104.71971893  337.29141235  143.36755371]
 [ 361.65777588  183.26757812  389.93777466  202.49633789]
 [ 133.68612671  225.47407532  381.0557251   304.81460571]]
500
[[  89.          119.          333.          249.        ]
 [  70.          244.           80.          278.        ]
 [ 220.          110.          235.          134.        ]
 ..., 
 [ 115.54716492  136.93528748  167.11398315  161.67469788]
 [  47.78432465  151.96858215  116.9486084   208.26371765]
 [ 121.83650208  250.41899109  271.89550781  332.44500732]]
333
[[  36.          182.           95.          394.        ]
 [  89.          295.          144.          414.        ]
 [ 101.          144.          301.          440.        ]
 ..., 
 [   9.5707674   398.27374268   74.15606689  441.92416382]
 [ 278.01620483   98.31549072  296.53338623  121.35070801]
 [   0.          183.08618164   48.18287277  247.72291565]]
500
[[ 145.           19.          346.          328.        ]
 [ 136.87220764    0.          316.40658569  334.44165039]
 [ 180.9586792     0.          355.73083496  334.44165039]
 ..., 
 [ 278.16015625  168.62477112  316.26000977  222.28962708]
 [ 322.66052246   14.43024445  402.23419189   52.33636093]
 [ 168.22932434   34.79071426  197.83216858   72.91216278]]
333
[[   5.          168.          315.          279.        ]
 [ 155.35881042  173.45948792  259.65441895  229.56532288]
 [ 141.19894409  171.5063324   250.62631226  237.53303528]
 ..., 
 [  62.70140839  450.02157593  135.94213867  490.95864868]
 [ 282.32165527  173.20913696  299.48989868  208.65318298]
 [ 312.16195679   88.90577698  332.44500732  206.18211365]]
333
[[  80.          157.          231.          407.        ]
 [  85.4551239   164.95899963  226.94178772  410.2638855 ]
 [  62.87090683    0.          264.23336792  499.5       ]
 ..., 
 [ 207.47210693   69.62561798  225.22377014   91.38696289]
 [ 210.17918396  318.80722046  235.00852966  353.3432312 ]
 [   0.          311.42544556   33.16128159  364.21609497]]
500
[[  75.          148.          234.          208.        ]
 [  58.57104111  145.04949951  217.86076355  209.50216675]
 [   5.89151907    8.24563599  245.59779358  349.41665649]
 ..., 
 [ 225.66667175  222.61351013  287.95010376  257.87512207]
 [ 205.83401489  219.79434204  237.87272644  239.35975647]
 [ 265.08270264  158.27908325  325.47979736  212.50344849]]
222
[[   5.            4.          216.          492.        ]
 [  10.40390015  142.35887146  211.77391052  499.5       ]
 [   0.            0.          219.01318359  499.5       ]
 ..., 
 [ 185.10964966  232.83865356  221.5         290.12966919]
 [  12.6701107    63.76852417   55.21245575  162.63046265]
 [  61.46250916  296.85110474  169.00553894  490.16055298]]
333
[[  80.            0.          226.          496.        ]
 [  71.71072388    0.          214.99938965  499.5       ]
 [  73.03174591    0.          292.32901001  354.81698608]
 ..., 
 [ 105.94528961  171.16566467  132.02516174  203.00402832]
 [ 308.73519897  131.92124939  332.44500732  170.365448  ]
 [   1.91807652  396.31588745   74.43710327  447.63293457]]
402
[[ 129.          243.          296.          315.        ]
 [ 166.17323303   24.07299423  401.33001709  499.15002441]
 [ 105.72632599  238.72239685  239.26786804  325.19042969]
 ..., 
 [ 306.18447876   95.99202728  330.25057983  125.98234558]
 [  17.0522728   191.69963074   45.66665268  229.86239624]
 [  19.09274292  243.26335144   89.09474182  336.51251221]]
375
[[   0.            0.           36.          203.        ]
 [   2.            0.          270.          495.        ]
 [ 238.          136.          362.          497.        ]
 ..., 
 [ 277.64755249  202.15690613  366.015625    267.53973389]
 [ 158.17808533   11.04630947  208.86903381   64.27246094]
 [  18.70936203  462.62844849   60.94301224  497.46395874]]
338
[[ 123.          241.          163.          303.        ]
 [ 143.          301.          175.          321.        ]
 [ 173.          274.          190.          294.        ]
 ..., 
 [ 228.82705688   27.8353138   254.89776611   66.37117004]
 [  10.47679043  165.79959106   39.40428925  218.64888   ]
 [  76.09906769  187.06573486   99.19081879  223.85620117]]
336
[[ 160.           99.          305.          395.        ]
 [ 178.36123657    7.99620104  335.44000244  499.52001953]
 [ 142.96807861   22.93316841  312.13122559  440.46868896]
 ..., 
 [ 189.47612     449.47802734  212.70809937  484.69351196]
 [ 270.07666016    0.          335.44000244   62.38382339]
 [  12.63792324  225.39535522   47.7638588   287.20379639]]
353
[[  98.            1.          277.          497.        ]
 [ 143.83920288    0.          323.9019165   445.04528809]
 [  72.20929718    0.          271.66470337  435.62863159]
 ..., 
 [  81.17056274  444.5663147   100.30978394  464.10174561]
 [  17.57608223  430.28948975   42.67567825  467.60580444]
 [ 224.20257568  122.11454773  324.71057129  186.38586426]]
253
[[  40.          106.          190.          271.        ]
 [  19.4608345    46.81713486  180.49024963  296.98406982]
 [  49.78232574   51.45246887  195.76567078  270.97476196]
 ..., 
 [ 130.13238525  167.86105347  160.40985107  194.75112915]
 [ 132.57814026  271.58300781  234.56536865  336.48999023]
 [ 149.96859741   28.31691742  173.5778656    68.94979858]]
500
[[ 120.           16.          365.          331.        ]
 [ 175.40736389   33.45413208  348.44754028  374.375     ]
 [ 158.38873291  101.93767548  330.09054565  374.375     ]
 ..., 
 [  59.52396774  241.09011841   98.94561768  291.47952271]
 [ 146.99188232  160.99085999  201.73805237  200.31394958]
 [ 295.82415771  204.0322876   331.72216797  231.76928711]]
345
[[ 118.           33.          199.          477.        ]
 [ 113.           12.          120.           19.        ]
 [  97.           32.          101.           39.        ]
 ..., 
 [ 212.63581848  280.79104614  237.45681763  344.32601929]
 [  71.68455505  404.66107178  104.53824615  485.00720215]
 [ 269.36004639  279.13619995  310.00305176  320.43945312]]
500
[[  69.           40.           76.           66.        ]
 [  47.           59.           57.           81.        ]
 [  51.          115.           62.          141.        ]
 ..., 
 [  63.08141708   94.03633881  200.92115784  288.91278076]
 [ 155.91375732   26.63257027  403.69927979   94.29640961]
 [ 277.70574951  308.65078735  359.76239014  341.31600952]]
500
[[ 138.           36.          373.          352.        ]
 [ 191.36683655    0.          466.06610107  374.375     ]
 [ 119.42829895    0.          333.79803467  374.375     ]
 ..., 
 [  58.45701599  296.87142944   77.21810913  326.37496948]
 [   8.01103592  207.51223755  166.17446899  340.00357056]
 [ 462.92553711  161.54745483  479.7507019   190.31530762]]
500
[[  92.           10.          332.          279.        ]
 [   0.            6.14466715  298.78338623  332.44500732]
 [  47.27060318    0.          228.04515076  332.44500732]
 ..., 
 [ 248.78489685  127.67670441  291.98086548  155.88182068]
 [ 139.59367371   27.07120132  308.16311646  134.0952301 ]
 [ 211.59248352  221.30297852  239.42683411  251.52287292]]
333
[[ 102.           47.          258.          498.        ]
 [ 105.          296.          121.          317.        ]
 [ 102.07543945  298.75738525  119.34137726  320.97851562]
 ..., 
 [ 265.09048462  215.87992859  285.75787354  244.00688171]
 [ 133.30845642    0.          238.89317322  147.15632629]
 [  72.09345245  302.85446167   89.56706238  338.80032349]]
353
[[  36.           28.          150.          465.        ]
 [ 198.           28.          299.          454.        ]
 [   0.          153.86608887  178.13316345  499.49499512]
 ..., 
 [  18.96334076   22.99746323  139.69750977  161.31921387]
 [ 290.42105103  330.50109863  317.99075317  394.87591553]
 [ 137.12553406  483.31573486  206.32797241  499.49499512]]
333
[[  69.          264.          108.          293.        ]
 [  67.54658508  273.1831665   103.63840485  295.30038452]
 [  67.76399994  266.9354248   105.45301819  290.81594849]
 ..., 
 [ 146.91264343  358.08618164  168.02737427  378.97027588]
 [  81.78106689  138.36817932  124.35497284  195.65730286]
 [ 180.8163147   327.73199463  250.01683044  373.43859863]]
500
[[ 216.          198.          351.          372.        ]
 [ 202.02700806  211.40319824  359.40795898  373.72427368]
 [ 254.11048889    0.          499.375       273.43438721]
 ..., 
 [  68.05947876  231.43777466   87.62268066  247.44393921]
 [   0.          183.80793762  109.7845459   352.85900879]
 [ 407.09796143    0.          499.375        38.94498825]]
333
[[  72.          219.          166.          350.        ]
 [  60.84966278  220.61485291  168.60420227  372.44992065]
 [  89.84959412  229.18095398  180.98226929  396.10879517]
 ..., 
 [ 267.12811279  262.08511353  332.44500732  305.71957397]
 [  83.9655838   193.79545593  161.21566772  245.11683655]
 [  10.79178238   61.13724518   69.88806915  133.61636353]]
375
[[ 149.          136.          332.          222.        ]
 [ 151.23899841  140.99125671  339.10528564  221.52893066]
 [ 162.7464447   125.07675171  333.57641602  233.59588623]
 ..., 
 [ 213.37832642  240.24462891  326.31808472  407.33248901]
 [   0.43783545  153.56616211   18.94371414  177.29438782]
 [  83.99478912   18.85158348  165.09269714   81.87260437]]
333
[[  93.          190.          215.          497.        ]
 [ 144.          481.          164.          498.        ]
 [ 200.          448.          211.          474.        ]
 ..., 
 [  79.59661865  436.21218872  101.26844788  465.82434082]
 [ 184.56031799   32.11005402  211.77041626   56.78424072]
 [  92.46235657  188.89038086  117.43231201  213.44918823]]
500
[[ 134.          205.          218.          316.        ]
 [ 131.57870483  203.6169281   222.33302307  299.94003296]
 [ 135.83918762  224.08247375  225.43251038  307.96820068]
 ..., 
 [ 237.10417175   45.63440323  356.41311646  219.65856934]
 [ 242.21551514  288.02572632  375.77392578  374.375     ]
 [   8.22253704   35.31339264   35.02222443   83.88211823]]
375
[[ 122.          111.          275.          417.        ]
 [  89.5609436    41.31267548  263.89181519  499.375     ]
 [ 105.5737381   118.87646484  281.984375    404.11608887]
 ..., 
 [  41.85817337  219.58158875  140.36619568  273.39794922]
 [ 316.05703735  468.77767944  374.375       499.375     ]
 [  81.5124054   447.41824341  100.73208618  499.375     ]]
500
[[   0.          106.           64.          177.        ]
 [  64.          128.          262.          248.        ]
 [ 272.           85.          383.          371.        ]
 ..., 
 [ 333.92047119  213.86012268  384.11380005  265.26745605]
 [ 122.14968872  292.54504395  143.01387024  326.38769531]
 [  96.69320679   29.57188034  121.80238342   88.51715088]]
333
[[  88.          128.          249.          424.        ]
 [ 116.26447296   77.27404785  252.84819031  489.97421265]
 [ 134.17672729   98.53565216  311.25674438  499.5       ]
 ..., 
 [ 231.50854492   82.0592804   248.95774841   98.93599701]
 [   0.          163.97120667  130.80073547  315.7003479 ]
 [ 173.97302246  278.20672607  212.39494324  333.86199951]]
500
[[ 148.          178.          198.          230.        ]
 [  74.95103455  118.50773621  308.59661865  333.44332886]
 [  71.69163513  178.229599    324.20474243  333.44332886]
 ..., 
 [ 406.30880737  281.18280029  436.19299316  310.6277771 ]
 [ 134.80706787  106.8457489   280.96456909  195.34629822]
 [ 195.67237854  123.16578674  212.04460144  139.7517395 ]]
500
[[ 303.          246.          338.          285.        ]
 [ 194.          251.          244.          332.        ]
 [ 196.77384949  260.41705322  247.59942627  354.10083008]
 ..., 
 [ 339.98751831   54.41423035  376.43505859   86.21993256]
 [ 434.21585083  199.01647949  459.95779419  238.3399353 ]
 [ 191.5071106   174.23400879  238.92062378  242.2520752 ]]
500
[[  23.            3.          193.          396.        ]
 [  46.          363.           55.          396.        ]
 [ 198.            5.          363.          393.        ]
 ..., 
 [ 392.78701782  305.52636719  468.96224976  397.33666992]
 [ 398.23318481  243.05422974  443.99014282  397.33666992]
 [  45.44076157  373.8637085    92.93980408  397.33666992]]
500
[[  64.          253.          370.          374.        ]
 [ 259.          324.          324.          350.        ]
 [ 293.          321.          322.          334.        ]
 ..., 
 [ 331.91931152   82.87029266  392.46450806  153.4161377 ]
 [ 245.16288757  157.81268311  499.375       300.4850769 ]
 [ 417.14337158    0.          499.375        34.17615509]]
439
[[   0.          403.          295.          498.        ]
 [ 143.          207.          406.          414.        ]
 [ 156.           59.          362.          244.        ]
 ..., 
 [ 401.14361572  302.47113037  426.80322266  347.28338623]
 [   0.          313.13983154   37.75108337  380.92959595]
 [ 302.81253052  431.82620239  333.68579102  465.69586182]]
500
[[ 232.          136.          341.          230.        ]
 [   0.            0.          249.26554871  324.35513306]
 [   0.            0.          195.06716919  374.375     ]
 ..., 
 [ 283.66018677   44.65065384  312.48440552   80.4331131 ]
 [ 366.3757019    88.18357086  401.33206177  129.84886169]
 [  65.9957962   198.45065308   88.47431946  254.92630005]]
500
[[ 170.          109.          301.          143.        ]
 [ 231.          136.          256.          189.        ]
 [ 262.          134.          268.          139.        ]
 ..., 
 [ 312.16137695  299.87692261  328.91781616  331.51416016]
 [   2.05196857  321.35131836   19.01037407  353.60510254]
 [ 400.09384155  306.24716187  415.74575806  321.78076172]]
375
[[  42.           78.          345.          451.        ]
 [ 100.52497101    0.          325.29846191  393.59576416]
 [  42.70940781    1.75653458  257.48449707  499.375     ]
 ..., 
 [ 270.03927612  149.4624939   361.63031006  222.03918457]
 [ 219.21089172   98.11049652  255.66375732  151.25274658]
 [  79.28457642  196.64801025  171.75950623  246.832901  ]]
400
[[  83.           72.          345.          438.        ]
 [   0.            0.          244.78186035  477.13314819]
 [   0.          276.22869873  150.98106384  499.33334351]
 ..., 
 [  97.04119873   50.90922928  194.67948914  117.5533371 ]
 [ 197.75914001   13.54242992  271.00543213   57.60641861]
 [ 164.95559692  338.95001221  192.96659851  371.69815063]]
500
[[ 177.          205.          408.          332.        ]
 [ 415.          128.          498.          332.        ]
 [ 436.          198.          449.          207.        ]
 ..., 
 [ 261.63311768    0.          485.46401978   37.88073349]
 [ 483.87710571    0.          499.5          72.67365265]
 [ 352.89538574   57.52716827  437.86364746  112.11808014]]
500
[[ 138.          145.          150.          178.        ]
 [ 327.49935913  132.76893616  497.96389771  332.44500732]
 [ 343.3425293   160.50094604  473.12304688  332.44500732]
 ..., 
 [ 414.67245483  223.85597229  445.9831543   249.81057739]
 [   0.          238.73651123   43.44245911  297.9324646 ]
 [ 152.35112      21.38171577  169.49961853   40.43497086]]
500
[[  28.           76.          243.          317.        ]
 [ 358.          130.          443.          301.        ]
 [  19.05131149   30.03582954  243.85896301  371.38000488]
 ..., 
 [ 152.66011047   95.31604767  190.29333496  140.71087646]
 [ 320.60980225  117.23202515  348.19314575  181.72229004]
 [ 198.61465454   89.45690918  230.2644043   120.79619598]]
500
[[   0.            0.           81.          188.        ]
 [  77.            0.          287.          228.        ]
 [ 298.           86.          445.          373.        ]
 ..., 
 [ 471.6529541     8.36096287  499.375       133.09065247]
 [  78.66881561  147.33436584  113.56861115  196.0322876 ]
 [ 100.29803467    0.          181.21279907   32.66086578]]
447
[[  52.           51.          374.          442.        ]
 [ 100.35427856   80.98069     302.86834717  499.14996338]
 [ 116.85514832   31.76612663  379.19366455  499.14996338]
 ..., 
 [ 266.853302    372.60577393  291.10824585  390.25616455]
 [  72.95001221   98.08699036  154.17698669  187.59535217]
 [ 319.50460815  398.51068115  337.10314941  438.63491821]]
500
[[  33.          109.          178.          233.        ]
 [ 209.          155.          267.          213.        ]
 [ 312.          106.          455.          233.        ]
 ..., 
 [ 279.05215454  253.61247253  308.21002197  269.8757019 ]
 [ 169.70623779   73.80873871  199.69084167  101.40821838]
 [ 452.50180054    0.          499.5         265.46630859]]
500
[[  54.           66.          259.          317.        ]
 [  61.08168411    0.          498.99667358  438.26834106]
 [  17.27462769    0.          250.28019714  399.34344482]
 ..., 
 [ 336.41387939  397.47164917  401.47158813  426.88980103]
 [ 363.60076904  285.85864258  397.20108032  316.97982788]
 [ 192.91790771  361.3883667   244.743927    422.37542725]]
333
[[  57.          136.          179.          264.        ]
 [   0.           85.           83.          263.        ]
 [   0.           11.34603024  150.31414795  438.57385254]
 ..., 
 [ 292.70599365  480.85339355  325.19232178  499.5       ]
 [ 130.1519165    95.80751801  237.97361755  233.1938324 ]
 [   8.21914196   94.00662994   46.60065079  150.63317871]]
500
[[  72.           76.          240.          222.        ]
 [ 237.           65.          408.          208.        ]
 [  88.          234.          248.          398.        ]
 ..., 
 [   0.           63.75114822   98.95002747  143.26490784]
 [ 441.48529053   15.57626343  478.86450195   43.64923096]
 [ 170.15516663  159.90794373  369.12249756  307.80404663]]
375
[[  52.           52.          358.          469.        ]
 [  24.01249886    0.          230.49990845  464.03933716]
 [  51.06079102    0.          275.72180176  401.58966064]
 ..., 
 [  52.7769928   400.1572876   298.00085449  487.34973145]
 [  94.55810547  133.87969971  156.16281128  208.09970093]
 [ 231.50857544  195.39549255  333.26327515  262.89389038]]
500
[[ 170.           88.          235.          188.        ]
 [ 173.57421875   83.27909088  235.84301758  197.22164917]
 [ 176.51940918   85.73698425  243.81573486  176.57461548]
 ..., 
 [ 438.85668945  223.06217957  470.0788269   257.16619873]
 [ 129.6378479   125.62102509  161.08280945  160.47431946]
 [ 115.443573     28.70443916  306.08441162  150.53521729]]
375
[[  1.22000000e+02   2.53000000e+02   3.08000000e+02   4.05000000e+02]
 [  2.22000000e+02   3.01000000e+02   2.43000000e+02   3.19000000e+02]
 [  0.00000000e+00   1.45072937e-01   1.72197830e+02   4.29320190e+02]
 ..., 
 [  2.57416077e+02   4.74973938e+02   3.26527466e+02   4.99375000e+02]
 [  1.33812332e+02   3.66882385e+02   1.56953064e+02   3.88449097e+02]
 [  2.29281708e+02   1.66961624e+02   2.54905609e+02   1.94237671e+02]]
375
[[  67.           50.          310.          494.        ]
 [ 158.42205811   11.03549957  353.93405151  462.59991455]
 [  69.05491638  104.74639893  282.32696533  499.375     ]
 ..., 
 [ 332.79876709   14.24345016  364.11291504   45.52627945]
 [ 309.46212769  424.98309326  328.76635742  447.64981079]
 [ 300.87109375  260.23321533  338.00646973  365.8263855 ]]
500
[[ 288.          216.          351.          301.        ]
 [ 239.          200.          424.          225.        ]
 [ 286.97805786  213.36790466  357.35632324  297.29406738]
 ..., 
 [   7.5469389   274.8013916    37.25391006  302.12164307]
 [ 217.86091614   60.28781128  288.45791626  114.16807556]
 [ 182.8812561   219.77566528  203.21408081  251.37898254]]
307
[[  42.           23.          262.          492.        ]
 [  58.64235687    0.          301.3890686   366.59848022]
 [  65.75562286    0.          306.48831177  223.06541443]
 ..., 
 [  93.06462097  175.28045654  189.49595642  238.93486023]
 [  74.39421082   67.32310486   93.02488708   88.51291656]
 [ 173.59375      37.05702591  204.88642883   60.11445618]]
500
[[ 306.          137.          363.          200.        ]
 [  32.68529892    0.          183.43859863  318.86679077]
 [   0.            0.          264.50799561  247.84928894]
 ..., 
 [ 376.38806152  276.82684326  424.68206787  322.37533569]
 [ 200.86505127  228.43936157  243.16638184  261.62792969]
 [ 198.53875732  256.24008179  226.16078186  290.44796753]]
403
[[  87.            0.          294.          293.        ]
 [ 148.54574585    0.          363.51467896  460.54690552]
 [  95.06411743   30.58532715  295.43276978  442.62084961]
 ..., 
 [  46.64112854  310.81918335   71.06702423  331.91885376]
 [   0.          154.13676453   42.29030609  217.35897827]
 [  18.10464478  358.10498047   50.97772217  406.56323242]]
500
[[  87.          107.          303.          253.        ]
 [ 267.          110.          295.          134.        ]
 [ 298.          118.          332.          144.        ]
 ..., 
 [ 301.2755127    37.68795395  332.69665527   57.31372452]
 [ 437.61108398  232.96775818  498.02008057  280.34313965]
 [ 436.7159729   222.7177124   496.46139526  273.36465454]]
500
[[ 178.          158.          305.          248.        ]
 [  94.35262299  140.65704346  362.51913452  253.64845276]
 [ 133.54582214  153.56639099  363.94384766  275.08013916]
 ..., 
 [ 334.04711914  340.70541382  355.09671021  356.83065796]
 [ 205.74855042   68.11277771  278.46679688  116.14070892]
 [ 477.68054199  112.84442902  499.375       155.4046936 ]]
500
[[ 427.          266.          447.          290.        ]
 [   5.           67.           86.          267.        ]
 [   0.            0.          112.37489319  374.375     ]
 ..., 
 [ 129.00389099  140.96588135  217.52207947  217.57815552]
 [ 340.04162598  148.09732056  415.66378784  211.76446533]
 [ 134.45368958    0.          181.42337036   23.9392395 ]]
362
[[ 101.          107.          197.          204.        ]
 [ 149.08757019  135.79086304  185.7408905   190.52888489]
 [ 135.15750122  145.10798645  190.07850647  219.07417297]
 ..., 
 [ 293.35354614  324.37750244  361.39666748  367.37524414]
 [  76.7227478   478.33837891  361.39666748  499.55999756]
 [  14.25439453  159.31509399   34.25894928  186.73306274]]
500
[[   1.          143.          396.          294.        ]
 [   0.            1.          395.          144.        ]
 [ 404.            5.          498.          291.        ]
 ..., 
 [  98.27359772   24.49981499  131.82006836   44.98307037]
 [ 264.42874146  195.66851807  356.93222046  250.42660522]
 [ 118.06030273  273.42770386  165.82556152  297.5       ]]
375
[[  30.          229.          342.          367.        ]
 [ 139.14891052    0.          374.375       499.375     ]
 [  88.93943787   98.80536652  290.14663696  462.05526733]
 ..., 
 [ 111.56806946  160.807724    135.83718872  181.40742493]
 [ 141.24864197   80.52403259  165.43379211  112.20913696]
 [  77.159935     84.13546753  110.5729599   108.38043976]]
500
[[  14.           50.          480.          321.        ]
 [ 236.41044617    0.          499.375       374.375     ]
 [ 116.08514404    0.          388.64935303  374.375     ]
 ..., 
 [  22.09444237  194.4596405    84.49008942  260.49990845]
 [ 183.39097595    0.          249.51654053   38.71482086]
 [ 353.63256836  290.29766846  396.80136108  326.33200073]]
500
[[  28.           74.          308.          333.        ]
 [  51.47566223    0.          284.54037476  333.44332886]
 [  55.39800644   85.2492981   338.01638794  333.44332886]
 ..., 
 [ 129.13208008   21.26236343  207.93025208   65.95236206]
 [  83.70290375  167.66531372  102.00402069  192.56921387]
 [  97.1932373   108.47332001  123.95535278  136.43930054]]
500
[[ 278.          160.          314.          180.        ]
 [ 280.43481445  170.16398621  311.3458252   190.57843018]
 [ 277.64190674  173.71740723  308.92333984  196.1089325 ]
 ..., 
 [ 200.22720337   54.1966629   414.13650513  183.76538086]
 [  67.98051453  244.46690369   93.04834747  305.84317017]
 [ 219.11616516  118.77158356  247.81065369  147.42724609]]
291
[[  14.           33.          263.          461.        ]
 [  37.82363129    0.          272.03045654  489.5456543 ]
 [  16.64350128  259.27423096  253.58450317  499.5       ]
 ..., 
 [ 226.46469116  242.36300659  257.36218262  292.8074646 ]
 [ 191.91676331   84.17694092  212.8639679   106.46694946]
 [  93.06759644  302.44018555  117.56323242  346.25140381]]
500
[[  66.          152.          242.          429.        ]
 [  64.51015472  160.49147034  230.37245178  456.56243896]
 [   0.            0.          239.47683716  499.16665649]
 ..., 
 [ 171.44258118  432.93087769  229.42234802  489.90100098]
 [  16.84789658  315.08529663   44.09739685  388.65332031]
 [ 469.53347778  168.34980774  499.16665649  258.76834106]]
500
[[ 132.          160.          336.          374.        ]
 [ 397.          254.          419.          290.        ]
 [ 329.7571106    82.34555817  499.375       374.375     ]
 ..., 
 [ 433.06838989   86.55011749  474.01184082  122.1905899 ]
 [ 343.70953369  296.18914795  365.67675781  322.09854126]
 [ 381.54348755  127.70309448  435.7321167   176.30006409]]
375
[[ 123.          200.          151.          256.        ]
 [ 159.          210.          176.          239.        ]
 [ 184.          212.          214.          241.        ]
 ..., 
 [  57.54623032  304.50863647  132.43174744  367.00518799]
 [ 123.5553894     0.          173.52259827   19.38788033]
 [ 159.42573547  257.21487427  281.04815674  459.3296814 ]]
500
[[ 338.           91.          425.          273.        ]
 [ 340.07470703  105.90594482  425.45974731  297.80303955]
 [ 342.94439697   47.73299789  436.57421875  255.14602661]
 ..., 
 [ 279.87496948    0.          423.84039307  109.25949097]
 [   3.75237393  253.95085144   33.85983658  299.33517456]
 [ 167.36717224  122.36172485  197.97198486  160.76924133]]
333
[[  64.          138.          174.          297.        ]
 [  59.82954788  145.59346008  183.77496338  298.90814209]
 [  67.05178833  137.02679443  171.0716095   275.03082275]
 ..., 
 [ 312.97607422   48.61332703  328.91986084   86.72927094]
 [ 106.20716095  123.448349    195.99786377  235.41159058]
 [ 306.37347412  274.33148193  332.44500732  327.65530396]]
500
[[  41.          128.          332.          332.        ]
 [  92.68920135    0.          310.63751221  332.44500732]
 [ 111.31557465  115.31691742  277.27120972  296.88632202]
 ..., 
 [  12.01507568  196.75431824   50.13603973  247.18852234]
 [ 440.46694946  237.0456543   480.16152954  265.50970459]
 [ 145.68009949  172.02728271  161.92889404  189.86878967]]
333
[[ 156.          394.          173.          417.        ]
 [ 155.53585815  387.44271851  172.31816101  411.91305542]
 [ 154.04600525  404.4367981   175.96047974  428.84466553]
 ..., 
 [ 238.68023682   54.15450287  254.29566956   71.00437164]
 [ 191.23953247  205.56338501  213.72984314  232.03523254]
 [ 311.58670044  365.12322998  330.57958984  409.75018311]]
333
[[  30.          206.          278.          324.        ]
 [   0.           39.94878769  177.29785156  446.3447876 ]
 [  89.72872925  200.24705505  243.40682983  344.65371704]
 ..., 
 [   0.           89.4956665   163.59312439  181.98031616]
 [  67.71074677  291.64196777  105.04262543  312.17053223]
 [  21.3991642   388.28967285   48.49312592  405.69168091]]
500
[[   0.            2.          499.          483.        ]
 [  33.5201416     0.          337.01501465  484.19168091]
 [ 123.99006653   65.88506317  499.55001831  484.19168091]
 ..., 
 [ 297.1986084    44.16001892  324.65792847   73.33145905]
 [ 377.0458374    69.64588928  428.90841675  127.64859009]
 [ 348.11523438  267.42922974  447.33630371  379.22970581]]
375
[[  51.          287.          337.          440.        ]
 [   0.          284.           48.          439.        ]
 [  74.84075165  148.71124268  314.42193604  499.375     ]
 ..., 
 [ 312.10800171  254.63768005  340.13977051  277.82736206]
 [ 153.05960083   71.77696228  291.56356812  207.4296875 ]
 [ 265.52722168  235.55419922  299.27029419  255.14598083]]
500
[[  40.          164.          374.          364.        ]
 [  75.12558746    0.          268.68673706  402.32830811]
 [ 128.24690247   62.90602493  358.76754761  402.32830811]
 ..., 
 [ 297.72653198   49.9304924   391.93893433  121.37217712]
 [ 121.0231781   338.64593506  177.38226318  386.80578613]
 [ 188.69081116  278.02111816  286.84921265  317.14489746]]
500
[[  28.           64.          199.          310.        ]
 [ 280.           35.          454.          310.        ]
 [ 277.75915527   79.99790192  462.66540527  304.14855957]
 ..., 
 [ 462.77911377  264.09399414  485.03964233  315.07901001]
 [ 174.36289978  107.30996704  192.83073425  143.72686768]
 [ 377.63201904  246.10075378  469.5491333   293.76464844]]
500
[[   2.           22.          136.          397.        ]
 [ 171.           12.          320.          408.        ]
 [ 342.           25.          492.          409.        ]
 ..., 
 [  68.60418701  261.02615356  231.26679993  408.29199219]
 [   0.           39.01515961   66.90989685   57.07474136]
 [ 153.99363708  119.15229797  275.36538696  224.92103577]]
393
[[ 101.          409.          144.          433.        ]
 [ 154.          414.          297.          440.        ]
 [  65.           69.          318.          419.        ]
 ..., 
 [  14.87400913  282.42654419   37.1045723   313.30108643]
 [  26.04791832  258.07687378   49.99702072  288.06976318]
 [   9.6919136   186.28121948   40.78552246  231.67474365]]
375
[[ 115.          147.          263.          320.        ]
 [ 101.57688141  164.96766663  270.26751709  319.7074585 ]
 [  84.94561005  162.64579773  311.37060547  295.07675171]
 ..., 
 [ 151.54608154  210.588974    332.45291138  417.87139893]
 [  25.59971237  436.18643188   90.42778015  496.67120361]
 [ 297.8989563   266.52175903  316.98312378  296.04275513]]
375
[[  96.           71.          317.          439.        ]
 [ 170.62416077    0.          373.9871521   455.26358032]
 [  61.2154007    35.74022293  321.7845459   499.375     ]
 ..., 
 [ 178.4630127    46.28084564  199.6889801    69.83197784]
 [  99.19694519  112.47885895  118.51724243  137.85449219]
 [  97.89913177  308.29077148  138.80052185  396.75979614]]
333
[[ 173.          174.          287.          329.        ]
 [ 265.          135.          332.          399.        ]
 [ 102.          146.          181.          384.        ]
 ..., 
 [ 220.12554932  367.42098999  308.19454956  415.7427063 ]
 [  96.79009247  172.94451904  224.43818665  303.77536011]
 [ 285.77035522  347.38589478  332.44500732  418.03295898]]
500
[[  50.           21.          410.          294.        ]
 [ 183.17233276    0.          329.26824951  319.8366394 ]
 [ 151.54684448    0.          308.50253296  324.90048218]
 ..., 
 [  95.12227631   37.45744324  132.27987671   91.4281311 ]
 [   5.89016771  164.87817383  127.8680191   332.44500732]
 [  97.37155914  127.10855103  154.56848145  190.78887939]]
375
[[  86.           58.          293.          410.        ]
 [  66.62474823    0.          317.59130859  418.52185059]
 [  51.25363159    0.          234.27790833  380.16403198]
 ..., 
 [ 332.24890137  226.5300293   374.375       395.73010254]
 [  54.45337677  373.24734497  183.30093384  499.375     ]
 [ 241.12586975  129.49205017  301.24639893  240.76025391]]
335
[[  69.            0.          221.          425.        ]
 [  33.2413826   126.31889343  260.21176147  499.70831299]
 [   0.          197.61418152  272.09289551  459.75479126]
 ..., 
 [ 142.77507019  330.79660034  221.2449646   371.41213989]
 [ 180.76799011  164.79980469  207.55752563  232.77883911]
 [ 186.35772705  136.6466217   260.94021606  201.82113647]]
394
[[  79.          420.          138.          484.        ]
 [ 220.          334.          272.          481.        ]
 [ 230.          491.          244.          499.        ]
 ..., 
 [ 296.53424072  341.45587158  318.42037964  358.24841309]
 [ 215.7923584   273.65457153  237.96403503  313.25750732]
 [  92.27619171   69.42671204  223.84738159  499.06665039]]
500
[[ 283.           85.          456.          278.        ]
 [ 301.9263916     0.          499.5         280.5       ]
 [ 278.63439941    0.          456.21252441  280.5       ]
 ..., 
 [ 302.27032471   55.77745056  325.88665771   76.32502747]
 [ 133.26141357  241.29171753  163.34927368  266.90856934]
 [  38.10235977    0.          118.66812134  157.31893921]]
500
[[ 300.          310.          465.          362.        ]
 [ 308.45108032  307.11395264  448.29800415  348.48199463]
 [ 300.11520386  314.52172852  455.68621826  360.1086731 ]
 ..., 
 [ 368.86434937  250.6386261   440.49539185  295.27713013]
 [ 280.53381348  418.29296875  307.07989502  438.24923706]
 [ 384.95223999  326.95120239  427.25778198  354.08157349]]
500
[[   0.            0.          186.          181.        ]
 [ 186.            0.          263.          107.        ]
 [ 131.          104.          257.          291.        ]
 ..., 
 [ 318.53219604  152.19206238  402.05728149  213.72927856]
 [ 145.23162842  390.09051514  180.82125854  412.93148804]
 [ 364.76913452  143.73330688  393.93948364  164.33895874]]
500
[[ 145.          262.          274.          307.        ]
 [ 212.          294.          292.          347.        ]
 [ 305.          431.          379.          485.        ]
 ..., 
 [ 243.4198761   291.81326294  439.70553589  429.05386353]
 [ 242.06732178  455.23986816  357.43173218  494.17498779]
 [ 202.16954041   26.85419273  233.75083923   60.90446091]]
500
[[  83.          102.          199.          449.        ]
 [  29.86455536    0.          281.41564941  495.17330933]
 [   0.            0.          307.46258545  401.39053345]
 ..., 
 [ 159.21377563  267.88818359  260.90481567  356.64047241]
 [ 350.6081543   220.59983826  371.02288818  245.63298035]
 [ 192.25120544  295.72299194  213.6217041   381.43521118]]
389
[[  68.           68.          330.          469.        ]
 [   3.03956699    0.          216.98376465  499.2166748 ]
 [   0.           63.91057587  321.17608643  499.2166748 ]
 ..., 
 [  56.45294189  113.29030609  110.34629822  289.4800415 ]
 [ 300.14968872   52.73593521  325.33352661   83.27729797]
 [ 207.50050354   66.29843903  241.20532227   96.28179169]]
333
[[ 113.          389.          234.          499.        ]
 [ 145.          165.          239.          321.        ]
 [  65.          129.          131.          209.        ]
 ..., 
 [ 209.07626343  255.49017334  232.85935974  274.90896606]
 [ 238.21630859  225.42129517  260.64135742  251.24761963]
 [ 117.91265869  219.28344727  149.87072754  248.65858459]]
375
[[  30.          122.          326.          466.        ]
 [ 276.          235.          294.          265.        ]
 [ 274.          167.          291.          186.        ]
 ..., 
 [  74.55329895  441.84576416  107.8523407   472.15560913]
 [ 134.91598511    2.27561951  274.15292358  225.92840576]
 [ 217.43545532  326.37619019  276.55322266  380.07125854]]
500
[[  92.           98.          402.          203.        ]
 [ 104.96735382    0.          332.4430542   333.44332886]
 [ 108.6870575    88.55776215  405.55606079  207.6452179 ]
 ..., 
 [ 336.1295166   190.57536316  365.61587524  212.95472717]
 [ 276.46374512   42.77013397  296.72293091   67.22341156]
 [  91.70899963  103.59693909  214.99488831  333.44332886]]
500
[[ 227.          215.          309.          313.        ]
 [ 314.          214.          389.          290.        ]
 [ 389.          184.          461.          245.        ]
 ..., 
 [  13.79970741  104.49486542   43.21056366  145.12774658]
 [ 365.90002441   61.92345047  386.35839844  110.86856842]
 [ 109.4389267   291.76556396  130.33096313  324.03381348]]
500
[[ 232.          119.          361.          272.        ]
 [ 123.07629395    0.          394.22073364  374.375     ]
 [ 235.74916077   85.99407196  386.52728271  374.375     ]
 ..., 
 [  91.37753296  143.61811829  111.08847809  180.07022095]
 [ 287.11581421  117.49610901  499.375       235.16479492]
 [ 197.0459137    93.13451385  217.95823669  108.60810852]]
375
[[  86.          122.          287.          385.        ]
 [ 304.          170.          328.          210.        ]
 [ 297.          268.          333.          307.        ]
 ..., 
 [  70.36044312  130.11465454  130.89562988  198.39709473]
 [ 126.52991486  447.80215454  172.63667297  485.10174561]
 [  34.82326126   57.94822693   70.1241684    95.72705841]]
500
[[ 314.          139.          421.          231.        ]
 [ 320.80526733  164.08602905  382.99108887  229.56959534]
 [ 322.78289795  159.13832092  414.41040039  215.04414368]
 ..., 
 [ 390.95968628  268.78781128  413.73901367  291.15328979]
 [  70.17934418  251.75956726   85.97573853  268.77349854]
 [  35.73445129   16.96183968  215.11524963  137.69494629]]
500
[[ 331.           29.          398.          133.        ]
 [  18.           41.          156.           90.        ]
 [  19.02509308   43.59285736  140.98754883   92.08487701]
 ..., 
 [ 405.11450195   75.1149826   459.11468506  128.13858032]
 [ 276.35473633  174.58799744  308.16394043  273.1137085 ]
 [ 252.95877075   14.60672379  357.53872681  130.63569641]]
375
[[ 171.          218.          201.          248.        ]
 [ 175.          249.          195.          282.        ]
 [ 173.96391296  251.67692566  190.97810364  276.88729858]
 ..., 
 [ 180.16992188   15.3780508   219.9261322    49.72779083]
 [ 223.10484314  175.74105835  247.9883728   219.51483154]
 [ 172.40153503  440.16207886  189.28157043  463.6022644 ]]
375
[[  82.          123.          374.          499.        ]
 [   0.            0.          321.5383606   499.375     ]
 [ 105.47260284    0.          374.375       499.375     ]
 ..., 
 [ 128.83937073  275.58447266  169.56695557  327.22311401]
 [ 282.36730957  310.91052246  301.2807312   351.17825317]
 [ 144.19920349  250.52786255  174.65643311  279.91967773]]
304
[[  46.          122.          303.          496.        ]
 [ 108.53869629  134.21279907  292.67724609  499.57333374]
 [  39.75507355  104.13606262  244.65808105  499.57333374]
 ..., 
 [   0.          410.96456909   78.19205475  471.20489502]
 [  77.33010864   76.47856903  107.20233917  108.82643127]
 [ 228.98139954  114.07222748  262.8024292   137.27345276]]
300
[[  39.          148.          111.          205.        ]
 [   0.          127.           43.          171.        ]
 [ 177.           64.          204.          106.        ]
 ..., 
 [   0.           75.08228302   23.99554825  104.05872345]
 [ 230.04118347   60.34772873  279.38653564   92.60177612]
 [ 268.99493408  119.4683075   292.91738892  146.51797485]]
500
[[  27.           19.          479.          394.        ]
 [ 287.97628784    0.          499.41500854  398.33499146]
 [  31.6883316     0.          282.8454895   398.33499146]
 ..., 
 [ 284.35134888  210.33557129  310.00869751  234.48680115]
 [  67.6632309   296.18115234  116.04905701  342.19714355]
 [ 281.24993896  193.63452148  308.51791382  217.60812378]]
333
[[   7.           67.          328.          413.        ]
 [  91.88001251   22.06689072  276.45257568  499.5       ]
 [ 102.60858917   85.92915344  237.85684204  499.5       ]
 ..., 
 [ 150.28216553   44.12649536  332.44500732  132.67227173]
 [ 194.4296875   306.01852417  236.97790527  355.02923584]
 [ 220.01695251  169.68092346  323.79006958  236.91464233]]
334
[[  46.           25.          237.          477.        ]
 [  19.          141.          120.          498.        ]
 [  31.65435982  184.38737488  252.39718628  499.32998657]
 ..., 
 [  66.04161835  438.93994141  173.35780334  499.32998657]
 [  16.38963699  199.03102112   46.98632431  250.26077271]
 [ 123.28140259  388.42987061  162.44857788  437.66156006]]
333
[[  99.           25.          210.          475.        ]
 [ 207.            0.          249.          118.        ]
 [ 104.94006348  178.07659912  296.03009033  499.5       ]
 ..., 
 [ 241.34558105  259.81698608  332.44500732  412.31863403]
 [ 102.55893707  427.37774658  142.61747742  499.5       ]
 [  70.08230591  214.11859131   86.9181366   244.43684387]]
400
[[  89.           10.          303.          486.        ]
 [   0.            0.          269.46212769  472.36523438]
 [   0.            0.          329.80395508  325.85302734]
 ..., 
 [ 109.28835297  257.2828064   154.62438965  361.34341431]
 [ 175.94067383  422.45193481  231.92915344  466.86691284]
 [ 121.40209961  265.58285522  153.53636169  380.15475464]]
333
[[  29.           23.          313.          493.        ]
 [  63.32679749    0.          282.2791748   499.5       ]
 [ 108.42336273    0.          262.63912964  439.71038818]
 ..., 
 [ 109.06721497  127.75227356  174.74702454  180.80151367]
 [ 288.34118652   72.77570343  305.12054443  105.16127777]
 [  94.14538574  199.12052917  146.80479431  243.74656677]]
375
[[  80.            5.          272.          494.        ]
 [  60.89729309    0.          321.47232056  450.94433594]
 [  27.77953148    0.          275.75430298  393.40032959]
 ..., 
 [ 269.69528198  378.67584229  310.11257935  399.10140991]
 [ 201.43966675  267.08654785  268.30993652  377.72802734]
 [  11.05625534  327.08883667   38.19833755  364.58984375]]
334
[[  82.           19.          242.          476.        ]
 [  77.4994812     0.          257.99508667  378.86569214]
 [  38.49334335    0.          320.66015625  245.05273438]
 ..., 
 [ 294.2427063   118.36719513  313.07891846  150.27616882]
 [ 305.20788574  114.08880615  323.43307495  154.9385376 ]
 [ 293.00561523  297.46710205  314.599823    328.58477783]]
334
[[  40.          407.           82.          499.        ]
 [  19.           29.          310.          484.        ]
 [ 136.62266541    0.          287.13491821  294.06085205]
 ..., 
 [  23.56937027  254.78738403  137.6239624   315.24926758]
 [ 184.52806091  254.69378662  333.44332886  397.83898926]
 [  14.20765018  441.33987427   31.44100952  473.09616089]]
500
[[ 208.          195.          278.          276.        ]
 [ 353.56323242    0.          499.32998657  333.44332886]
 [ 205.28752136  198.13035583  276.89642334  278.90734863]
 ..., 
 [ 323.47210693  133.32054138  347.60848999  148.83004761]
 [ 271.7918396     9.90626431  371.83724976  192.73944092]
 [ 182.256073    181.54722595  209.05891418  206.27017212]]
500
[[  1.70000000e+01   4.00000000e+01   2.19000000e+02   2.25000000e+02]
 [  2.83000000e+02   5.30000000e+01   4.35000000e+02   1.59000000e+02]
 [  2.18550949e+01   0.00000000e+00   2.21109009e+02   2.51500000e+02]
 ..., 
 [  2.66517975e+02   4.37973022e-01   3.27004608e+02   2.50376053e+02]
 [  4.43581696e+02   1.36045670e+02   4.60440033e+02   1.54371017e+02]
 [  2.36999619e+02   2.10494659e+02   3.63424591e+02   2.51500000e+02]]
500
[[ 202.          212.          289.          304.        ]
 [ 248.61598206    0.          499.375       327.48321533]
 [ 313.39926147    0.          499.375       305.75891113]
 ..., 
 [ 143.15597534   64.50241089  165.04656982   81.07749939]
 [ 400.21151733   97.24538422  423.7963562   115.22451019]
 [  66.62071228   22.01085472  105.9929657    45.48549271]]
375
[[  14.            0.          367.          458.        ]
 [ 166.            0.          209.           61.        ]
 [ 232.            3.          273.           40.        ]
 ..., 
 [ 246.03056335  405.49008179  336.47451782  442.30151367]
 [ 270.55981445  184.56437683  316.93884277  225.64881897]
 [  85.73059082  296.9649353   200.97059631  418.29946899]]
500
[[ 404.          313.          465.          374.        ]
 [ 439.          328.          471.          353.        ]
 [ 442.          312.          461.          323.        ]
 ..., 
 [ 299.60906982  180.33302307  436.87094116  309.8263855 ]
 [ 269.50262451   84.5401001   295.24304199  113.56507874]
 [ 279.95846558  140.32209778  302.66516113  155.7766571 ]]
336
[[ 260.          277.          308.          383.        ]
 [  90.          323.          103.          332.        ]
 [  86.          305.          130.          364.        ]
 ..., 
 [  53.48696518    6.44342327  116.24124908   39.75442505]
 [  26.9323616    48.87155914   59.054245     80.48038483]
 [ 213.58952332  218.08757019  235.88468933  245.838974  ]]
375
[[ 126.          151.          214.          339.        ]
 [ 179.           98.          290.          401.        ]
 [   0.           96.           28.          127.        ]
 ..., 
 [ 280.23391724   62.62138367  374.375       166.86203003]
 [ 349.01797485   29.44959068  372.88372803   70.03298187]
 [ 189.30636597  360.20681763  220.82733154  393.44488525]]
472
[[  54.           35.          162.          105.        ]
 [ 199.          180.          229.          206.        ]
 [ 222.          133.          327.          166.        ]
 ..., 
 [ 121.17399597   70.2116394   165.26853943  128.54699707]
 [ 130.53974915  242.26873779  164.17445374  290.62280273]
 [ 143.51934814   31.68311882  234.61804199   80.10764313]]
500
[[ 232.          140.          376.          200.        ]
 [ 174.          137.          229.          210.        ]
 [ 224.70851135  143.28074646  347.69018555  198.79768372]
 ..., 
 [   0.          179.2849884   138.10231018  298.82931519]
 [   0.          216.64248657   52.83506393  275.57919312]
 [ 209.25357056  288.81253052  278.7638855   329.56838989]]
342
[[  87.           84.          252.          344.        ]
 [  91.51438141   66.8660202   233.7950592   380.18289185]
 [  61.81538391  104.04999542  240.5586853   432.74243164]
 ..., 
 [ 280.32037354  297.19366455  308.50759888  363.65960693]
 [ 268.37225342  378.90640259  300.25097656  440.16998291]
 [  53.81684113    0.          196.97598267   53.5295639 ]]
500
[[  56.           92.          377.          245.        ]
 [  66.96862793   92.66280365  370.39678955  230.84941101]
 [ 108.70511627   94.67559814  411.97180176  245.81344604]
 ..., 
 [ 247.92753601  103.66909027  267.25378418  127.60375977]
 [ 158.6105957    92.00015259  176.16571045  111.26208496]
 [ 357.16738892  131.9261322   378.77001953  157.71821594]]
377
[[ 193.          188.          274.          284.        ]
 [ 205.          184.          252.          247.        ]
 [ 178.          183.          209.          258.        ]
 ..., 
 [ 211.58544922  139.15740967  312.9750061   217.271698  ]
 [ 284.43325806  410.91061401  326.34091187  448.27819824]
 [  47.3412323   182.40258789   85.31122589  220.89859009]]
333
[[   0.          290.          332.          499.        ]
 [ 142.49038696   91.52520752  332.44500732  499.5       ]
 [ 122.40253448  295.94787598  332.44500732  499.5       ]
 ..., 
 [ 262.28982544  466.44067383  296.87567139  491.39712524]
 [ 196.75920105   25.18096161  229.75747681   68.85328674]
 [ 186.53179932   40.23623276  217.98617554   87.35384369]]
333
[[  31.          103.          220.          215.        ]
 [  31.64255333   99.05062866  216.22694397  202.57049561]
 [   0.          204.16018677  222.07817078  499.5       ]
 ..., 
 [ 198.56684875  224.6829834   231.53152466  244.18450928]
 [ 164.28794861   70.93675232  272.51800537  123.50533295]
 [ 108.11113739   32.11668777  287.60794067  152.33631897]]
364
[[   9.           14.          345.          466.        ]
 [  31.86418152    0.          216.65544128  369.78198242]
 [   0.            0.          312.83547974  314.58483887]
 ..., 
 [ 328.80526733   17.0175209   363.39334106   85.64305878]
 [ 254.32595825  174.6337738   326.51596069  225.7897644 ]
 [ 238.32435608  270.17880249  319.3302002   346.21218872]]
500
[[ 203.          244.          323.          455.        ]
 [ 330.          186.          344.          212.        ]
 [ 244.          185.          283.          251.        ]
 ..., 
 [   0.          328.05380249   84.28011322  404.91931152]
 [  57.14897156   31.0993824   102.7572403    87.07408905]
 [ 251.72180176  168.88188171  276.01702881  238.20463562]]
375
[[  55.           16.          322.          485.        ]
 [  66.37493134    0.          325.99279785  499.375     ]
 [  81.21640778   24.88323212  267.76205444  499.375     ]
 ..., 
 [ 218.64859009  180.35218811  274.68560791  248.70545959]
 [ 117.03047943  442.83328247  214.52430725  475.1043396 ]
 [  90.60443115  401.23626709  138.26942444  445.01989746]]
500
[[ 163.            5.          337.          374.        ]
 [ 200.66757202    0.          392.01937866  374.375     ]
 [ 204.66439819   34.96566772  331.94522095  374.375     ]
 ..., 
 [   9.68168259  188.24720764   28.78736496  243.68203735]
 [ 403.72946167  199.73059082  425.81442261  227.62445068]
 [  23.35391235   35.67933655   58.87282181   66.06237793]]
500
[[ 244.          160.          348.          271.        ]
 [ 210.76783752    0.          426.15386963  374.375     ]
 [ 259.45944214  162.13809204  336.71118164  243.85623169]
 ..., 
 [  73.12911224  116.18514252  106.55291748  152.50744629]
 [ 403.26159668  238.81338501  420.34191895  266.30621338]
 [ 414.24594116  100.51473236  444.29064941  132.95387268]]
500
[[ 338.          221.          365.          269.        ]
 [ 338.15228271    0.          499.375       374.375     ]
 [  53.48049164    0.          227.39901733  374.375     ]
 ..., 
 [ 438.04397583    9.48503971  499.375       134.60905457]
 [ 188.64877319   82.38122559  218.56306458  108.73470306]
 [ 204.8028717   258.94909668  245.1519165   293.92501831]]
500
[[ 224.          153.          357.          333.        ]
 [ 238.25144958    0.          426.37698364  374.375     ]
 [ 223.90063477  176.83029175  371.22024536  337.70239258]
 ..., 
 [ 223.25854492  170.8057251   309.50961304  222.44874573]
 [ 230.25572205   83.43497467  251.69790649  112.33439636]
 [ 104.43467712  176.16239929  126.04863739  205.37004089]]
375
[[ 120.           36.          326.          335.        ]
 [  87.83000183   86.53249359  279.90026855  499.375     ]
 [ 121.11898041   30.72525024  328.91021729  484.59216309]
 ..., 
 [ 165.05335999  115.83520508  200.83961487  159.10472107]
 [ 141.11122131  202.7645874   190.55947876  253.5687561 ]
 [  94.2219162   397.63946533  177.26945496  460.89208984]]
375
[[ 132.          134.          321.          465.        ]
 [ 181.0188446   160.72801208  370.97494507  499.375     ]
 [  67.92037964    0.          275.97918701  459.48858643]
 ..., 
 [ 290.52459717  113.27529907  317.52523804  150.83589172]
 [ 244.99116516  256.48834229  265.68307495  290.49511719]
 [ 176.55496216  391.93331909  196.56083679  439.26638794]]
500
[[ 138.           98.          316.          278.        ]
 [ 220.           66.          235.           76.        ]
 [ 153.59661865   92.72232056  318.46810913  262.2472229 ]
 ..., 
 [ 244.40565491   49.24702454  359.73019409  248.09315491]
 [ 408.54083252  264.33190918  469.72528076  280.5       ]
 [  62.05989456  157.77572632   81.2323761   191.57119751]]
450
[[  22.           34.          377.          272.        ]
 [  60.2373848     0.          339.08349609  337.43667603]
 [ 178.99926758    0.          393.86535645  337.43667603]
 ..., 
 [ 101.51496124   80.34070587  127.24288177  100.05622101]
 [ 249.95039368  143.79489136  332.00842285  221.45942688]
 [ 183.91195679  207.86399841  241.0447998   286.04568481]]
500
[[  35.           76.          376.          273.        ]
 [ 142.87451172    0.          468.45297241  374.375     ]
 [ 274.26608276    0.          499.375       374.375     ]
 ..., 
 [ 132.16772461    0.          169.63279724   15.8977356 ]
 [ 134.45524597    0.          233.28237915  282.54046631]
 [ 450.79516602   62.8777771   479.57522583  112.29022217]]
416
[[  81.           31.          326.          454.        ]
 [  91.79433441    0.          326.93209839  488.18731689]
 [   1.50344241    0.          270.42022705  499.19998169]
 ..., 
 [ 371.14440918  360.75125122  390.21182251  396.88043213]
 [ 136.14761353  140.29760742  236.11219788  204.59783936]
 [ 370.62063599  327.40673828  389.90136719  364.09301758]]
282
[[  59.           81.          224.          401.        ]
 [  95.82576752   35.87042236  271.04623413  409.44433594]
 [  73.42124176   18.5352478   260.46337891  477.60238647]
 ..., 
 [ 134.93688965  362.4156189   212.27764893  401.74453735]
 [  79.54035187  355.48110962   97.24620819  371.42849731]
 [ 185.64030457  444.98400879  226.37150574  488.77313232]]
450
[[  34.           11.          377.          305.        ]
 [  68.05117798    0.          327.18984985  337.43667603]
 [  28.92393875    0.          275.59091187  335.53952026]
 ..., 
 [ 180.77801514  172.86700439  211.55522156  195.22079468]
 [ 314.46746826  115.61799622  331.48684692  132.09629822]
 [ 233.42016602  133.35939026  335.95391846  197.05700684]]
450
[[  44.           54.          377.          276.        ]
 [ 152.52363586    0.          318.49307251  337.43667603]
 [  32.28889465    0.          308.07519531  337.43667603]
 ..., 
 [ 150.50126648  288.71075439  179.21916199  318.27853394]
 [ 325.85742188   83.08912659  364.61273193  117.64883423]
 [ 286.02862549  153.65455627  384.0675354   196.82009888]]
500
[[ 179.           29.          377.          359.        ]
 [ 227.34712219    0.          499.33334351  358.87704468]
 [ 296.2774353     0.          499.33334351  252.12463379]
 ..., 
 [ 455.13442993   57.31884766  477.95800781   99.38974762]
 [  61.8571167   108.44263458  105.72094727  155.77438354]
 [   0.          334.2059021    20.66777992  359.59854126]]
375
[[  60.          151.          149.          367.        ]
 [  51.52836227  163.43719482  144.59953308  370.69592285]
 [  34.35871124    0.          187.21472168  474.77407837]
 ..., 
 [  79.92699432  384.34689331   96.01279449  413.13458252]
 [  59.61961365  238.23748779   81.00476837  278.53225708]
 [ 321.56829834  245.23509216  374.375       295.37338257]]
375
[[  44.            0.          322.          480.        ]
 [ 120.93539429    0.          340.27493286  463.50912476]
 [  73.58436584    0.          371.82922363  396.06069946]
 ..., 
 [  11.28172112   42.71541595   27.67421532   67.77454376]
 [ 191.16616821  350.58163452  225.52133179  386.38729858]
 [  15.88217831  235.50619507   44.49717331  268.34924316]]
500
[[ 247.          134.          274.          258.        ]
 [ 244.25216675  161.43655396  274.31591797  267.5411377 ]
 [ 243.2068634   148.82318115  277.13497925  246.37266541]
 ..., 
 [ 353.56936646  132.50137329  391.80316162  180.15007019]
 [ 169.20707703   66.41519165  187.6943512    88.76885986]
 [ 460.25772095  352.7598877   484.1272583   374.375     ]]
333
[[  31.          234.           85.          327.        ]
 [   0.          115.55895233  166.57574463  499.5       ]
 [  23.96413422  242.61772156  282.37948608  499.5       ]
 ..., 
 [  66.93029022  150.48471069  183.24531555  313.1194458 ]
 [  75.61089325  212.98835754  108.51809692  237.46110535]
 [  10.36339855  323.19802856  187.22340393  465.01367188]]
298
[[  34.          159.          108.          252.        ]
 [ 127.          212.          203.          287.        ]
 [  37.85813141  193.13101196   88.85823059  256.22698975]
 ..., 
 [ 240.61149597  434.84362793  258.39169312  453.11340332]
 [  20.69433594  210.94491577   50.93467712  239.68624878]
 [ 137.40048218  451.23986816  160.83627319  475.15637207]]
321
[[  32.          284.          126.          371.        ]
 [  57.41519165  307.68661499  119.96640015  366.86505127]
 [  47.8497963   300.11148071  113.71568298  361.02487183]
 ..., 
 [ 163.08810425  308.88864136  320.46499634  415.78244019]
 [  37.65288162  157.67996216   55.2735405   176.6374054 ]
 [ 242.07481384  414.89324951  259.05322266  442.34994507]]
333
[[  34.          334.          135.          466.        ]
 [  56.          256.           75.          325.        ]
 [  21.          257.           50.          283.        ]
 ..., 
 [ 222.99455261  396.03192139  307.2276001   439.84109497]
 [ 263.41265869  435.38735962  291.14382935  457.23590088]
 [ 250.19511414  415.20309448  332.44500732  451.0166626 ]]
294
[[  25.           20.          272.          498.        ]
 [  42.26071167    0.          202.13684082  418.10113525]
 [  60.44792938    0.          270.91693115  331.60870361]
 ..., 
 [  30.26493835  160.85702515   54.03477478  200.29318237]
 [ 133.39779663  238.28765869  224.57818604  289.99713135]
 [ 258.61425781  222.61878967  281.64770508  266.21255493]]
340
[[  64.           13.          267.          480.        ]
 [ 111.53863525   43.67053223  302.1307373   469.82192993]
 [  65.91204071   17.74032593  220.12443542  499.2333374 ]
 ..., 
 [ 167.22422791  443.1194458   195.74333191  470.0112915 ]
 [ 213.12153625  347.97561646  251.28178406  415.75073242]
 [  61.45064163  111.96240997  120.59801483  184.36956787]]
500
[[ 163.          192.          377.          238.        ]
 [ 141.3888855   187.4730072   293.98544312  237.78616333]
 [ 172.47001648  183.01097107  335.83859253  239.04006958]
 ..., 
 [ 442.30429077  156.57466125  464.32333374  182.26776123]
 [  60.84062195   22.97913933  210.73504639  160.7845459 ]
 [ 322.21594238   84.71406555  353.95391846  112.25823975]]
405
[[  15.            1.          377.          496.        ]
 [ 162.3372345    83.06399536  376.34884644  499.50003052]
 [ 123.32456207  112.40861511  327.14083862  499.50003052]
 ..., 
 [ 108.18148041  182.30337524  199.61990356  282.82022095]
 [  41.81602859  139.86483765   92.16821289  225.20982361]
 [ 289.64038086    0.          324.7281189    19.17069244]]
500
[[  93.          133.          365.          330.        ]
 [  13.32277203    0.          303.46923828  430.28164673]
 [   7.25083017  130.36616516  310.44848633  430.28164673]
 ..., 
 [ 362.10379028  336.31399536  391.41513062  369.59591675]
 [ 327.85830688  104.69762421  411.46362305  164.81816101]
 [ 220.26174927  146.66670227  268.16635132  195.28749084]]
500
[[  31.           51.          377.          332.        ]
 [ 177.92094421    0.          446.16687012  356.40499878]
 [ 177.5304718   120.30086517  499.20498657  349.27716064]
 ..., 
 [ 148.26831055  173.92276001  255.8911438   356.40499878]
 [ 325.65771484   36.86782837  418.61508179   77.30870819]
 [ 140.36700439  255.04959106  192.35804749  298.41625977]]
500
[[  88.          132.          284.          334.        ]
 [ 189.          266.          286.          373.        ]
 [ 250.           77.          283.          117.        ]
 ..., 
 [ 366.80529785   98.05696106  392.36798096  121.41407013]
 [ 261.31469727  320.19424438  335.40036011  365.19271851]
 [ 420.10699463   24.92033195  460.37539673   76.74168396]]
299
[[  27.           17.          251.          496.        ]
 [  35.15319824  207.49198914  204.05444336  499.5       ]
 [  92.5658493   188.65516663  288.52716064  499.5       ]
 ..., 
 [ 258.43847656  165.84983826  274.43951416  193.22050476]
 [  78.46389771  417.07876587  143.32200623  499.5       ]
 [ 159.12628174   15.1299305   186.69329834   51.60012817]]
344
[[  50.          204.          252.          445.        ]
 [ 105.06781006   80.87541199  259.66027832  466.24078369]
 [ 111.97434998    0.          308.90975952  366.46853638]
 ..., 
 [ 154.25854492    0.          313.07904053   39.14187241]
 [ 318.56417847  429.67199707  338.55932617  482.84439087]
 [  93.47098541    0.          218.23716736   27.89777756]]
361
[[  54.           64.          225.          489.        ]
 [  73.63381195  132.80111694  213.65415955  499.3833313 ]
 [  15.72062588  174.51473999  265.51031494  460.17715454]
 ..., 
 [ 109.89141083  320.27865601  136.53216553  362.67852783]
 [ 230.2610321   316.48516846  254.83920288  372.74301147]
 [  35.9994812   403.33828735   55.78289795  445.18518066]]
500
[[  92.           30.          377.          313.        ]
 [  70.5169754     0.          330.47244263  374.375     ]
 [ 126.90007782    0.          384.8934021   374.375     ]
 ..., 
 [ 102.52236938    0.          168.21818542   24.45149612]
 [  99.89151764  237.50444031  117.3415451   264.71755981]
 [ 472.63607788   71.8391037   499.375       193.75865173]]
500
[[ 183.          301.          250.          346.        ]
 [ 179.          214.          281.          274.        ]
 [ 104.          255.          170.          301.        ]
 ..., 
 [ 344.84857178  335.27761841  425.09292603  397.01712036]
 [ 245.95301819   14.94645214  384.61309814  194.98936462]
 [ 479.63323975  344.03030396  497.43136597  372.98684692]]
500
[[  77.          148.          356.          215.        ]
 [  68.44808197  142.94004822  389.25405884  219.39955139]
 [  79.80109406  158.2454834   369.64788818  228.12364197]
 ..., 
 [ 453.17724609   93.38718414  471.33276367  110.96650696]
 [ 338.36383057  244.67758179  364.95587158  270.75469971]
 [ 420.20663452   55.58372116  437.08468628   76.78556824]]
375
[[ 148.          243.          246.          348.        ]
 [ 154.61630249  254.22106934  246.49957275  342.67736816]
 [ 174.19348145  286.09744263  248.30445862  373.36499023]
 ..., 
 [  86.78987122  309.46899414  254.39935303  499.375     ]
 [  80.29506683  180.55366516  120.51139832  242.44754028]
 [ 157.743927    143.51797485  185.07196045  167.58407593]]
500
[[  52.           33.          377.          405.        ]
 [ 114.37270355    0.          324.24078369  401.25210571]
 [  19.49729538    0.          356.11279297  299.64486694]
 ..., 
 [   0.          169.0284729    26.59716606  210.3263092 ]
 [  59.1342392    51.51150894   78.70833588   77.57607269]
 [ 156.95358276   28.33241463  173.26785278   51.20084   ]]
466
[[  28.           19.          145.          443.        ]
 [ 169.           17.          308.          440.        ]
 [ 326.           27.          377.          437.        ]
 ..., 
 [   4.27321625   96.26612091   66.18068695  206.82281494]
 [ 251.88824463  399.48294067  282.24716187  433.98068237]
 [ 372.23492432   25.01002121  424.7868042    62.22554016]]
500
[[  25.           74.          214.          417.        ]
 [ 259.           78.          377.          396.        ]
 [ 251.62770081    0.          499.55001831  484.19168091]
 ..., 
 [ 248.93875122  266.69232178  280.76757812  342.08261108]
 [ 471.3119812   380.97927856  499.55001831  446.27017212]
 [ 185.75337219  468.81332397  271.92559814  484.19168091]]
319
[[  92.          428.          147.          443.        ]
 [ 175.          390.          194.          416.        ]
 [  19.          174.          189.          257.        ]
 ..., 
 [  84.20482635  277.23181152  115.78764343  298.76815796]
 [   0.           34.97067261   31.55221558   86.16629791]
 [ 189.64471436  255.62106323  218.1945343   297.46749878]]
500
[[ 282.          131.          376.          266.        ]
 [  60.          136.          199.          274.        ]
 [ 268.71298218  154.49145508  404.78079224  282.6812439 ]
 ..., 
 [ 392.21350098  139.08648682  460.31848145  179.6146698 ]
 [ 458.97460938  142.70704651  499.5         214.65536499]
 [  61.96237183   90.75003052  133.01139832  119.62327576]]
500
[[  29.           19.          377.          368.        ]
 [ 180.60148621    0.          456.33651733  374.375     ]
 [ 117.32055664    0.          396.35986328  374.375     ]
 ..., 
 [ 454.07049561   57.87065125  487.24838257  121.51868439]
 [ 348.82659912   52.56650925  392.6505127    82.04155731]
 [ 303.24362183   99.28777313  388.49334717  213.7212677 ]]
333
[[  22.          259.          163.          497.        ]
 [ 176.          313.          261.          498.        ]
 [  85.            0.          332.          244.        ]
 ..., 
 [ 270.54559326  238.89402771  332.44500732  308.16021729]
 [ 253.73242188   86.58848572  278.82537842  144.13647461]
 [  58.43569946   48.60916901   90.7744751    91.42462158]]
500
[[ 280.          139.          379.          264.        ]
 [ 175.           72.          233.          230.        ]
 [ 109.          132.          179.          262.        ]
 ..., 
 [ 399.4989624    24.81233597  499.375       136.35267639]
 [ 229.9861908    91.90686798  354.77539062  216.57148743]
 [ 174.99946594   15.17357349  225.0050354    66.33792114]]
332
[[ 104.          280.          201.          319.        ]
 [ 104.          185.          202.          302.        ]
 [ 131.18074036  153.37634277  214.36689758  320.73822021]
 ..., 
 [  48.74442291  116.07662201   79.76737213  150.11595154]
 [  61.06241226  119.45775604  136.40066528  162.57531738]
 [ 195.69734192  324.22598267  264.01467896  359.5375061 ]]
375
[[  67.            9.          321.          497.        ]
 [ 125.08095551  180.88903809  342.48394775  499.375     ]
 [  77.98035431   15.06656647  349.1946106   499.375     ]
 ..., 
 [   0.          104.42443848   69.55271912  277.01449585]
 [  38.76220322   90.77477264  140.51191711  157.29255676]
 [ 356.37911987  110.83757019  374.375       179.12239075]]
432
[[ 165.          193.          268.          258.        ]
 [ 163.7963562   196.74995422  253.14411926  260.64187622]
 [ 170.83328247  194.26712036  264.40740967  247.48204041]
 ..., 
 [  80.09723663   66.31121826  357.76248169  258.61032104]
 [ 300.1852417    56.96281815  388.73739624   98.0298996 ]
 [  28.44133568  422.86462402  120.77914429  473.605896  ]]
500
[[ 187.          128.          291.          209.        ]
 [ 237.71870422    0.          499.66000366  331.44665527]
 [ 330.84936523    0.          499.66000366  331.44665527]
 ..., 
 [ 314.00006104   55.17861557  343.24389648   87.81993103]
 [ 268.00738525  192.19073486  334.00326538  237.92417908]
 [  71.26296997  213.86174011   96.62905121  229.85157776]]
500
[[ 122.            6.          332.          353.        ]
 [ 105.37528992    0.          285.86859131  374.375     ]
 [ 134.13372803    0.          369.9369812   374.375     ]
 ..., 
 [ 113.54319763  139.67959595  135.19508362  178.33227539]
 [ 386.43597412  203.77236938  404.80953979  224.73219299]
 [ 386.44793701  193.65296936  404.71377563  214.57258606]]
500
[[ 139.          149.          288.          211.        ]
 [  86.          253.          134.          301.        ]
 [ 113.9670639   144.13961792  372.50518799  242.2558136 ]
 ..., 
 [ 394.5362854   242.08013916  417.51071167  269.20324707]
 [ 296.37017822   77.52098846  342.16079712  125.61899567]
 [ 426.80938721  241.39733887  452.3182373   268.80020142]]
500
[[  96.          112.          323.          372.        ]
 [  31.63674355    0.          295.42514038  374.375     ]
 [ 150.55735779    0.          416.96377563  374.375     ]
 ..., 
 [  93.86556244   84.44966888  110.76545715  104.95946503]
 [ 415.95629883  261.00430298  451.87564087  304.27615356]
 [  67.20700836  130.91757202  147.74897766  192.95114136]]
500
[[ 107.          140.          241.          277.        ]
 [  97.45731354  138.66767883  244.81866455  258.83724976]
 [ 108.04608917  153.92333984  252.51464844  279.99014282]
 ..., 
 [ 291.44570923  277.29827881  358.05535889  327.84545898]
 [   0.           86.50758362   39.04051971  203.85641479]
 [   0.          123.85353088   39.0560112   241.20710754]]
375
[[ 103.          180.          332.          279.        ]
 [ 126.53630066  179.84872437  364.0149231   270.10305786]
 [  98.04690552  169.82287598  332.0831604   276.72628784]
 ..., 
 [ 331.5501709   288.66812134  361.64318848  341.44888306]
 [ 260.57702637  288.51876831  287.59146118  308.39868164]
 [ 257.55618286  145.06964111  336.37738037  192.88400269]]
375
[[  81.          221.          331.          334.        ]
 [ 113.52296448  222.62187195  353.921875    335.02227783]
 [ 146.79187012  221.89379883  304.04284668  365.34689331]
 ..., 
 [  34.25629425  327.30212402  102.91882324  389.68783569]
 [  68.69667053   47.06111908   86.06658173   76.9644928 ]
 [  30.90959549  292.97924805   99.8055954   341.15481567]]
257
[[  29.           15.          230.          459.        ]
 [  28.49913788    0.          221.75875854  456.03582764]
 [  64.10946655    0.          212.73007202  313.26168823]
 ..., 
 [   5.42432022   88.47118378   95.17391968  152.1602478 ]
 [ 167.08834839  435.7684021   235.07785034  475.78158569]
 [ 150.974823    323.50140381  181.43295288  377.86938477]]
375
[[ 237.          272.          292.          331.        ]
 [ 209.          312.          239.          338.        ]
 [ 138.          322.          202.          375.        ]
 ..., 
 [ 321.47573853  226.35591125  354.60388184  276.74453735]
 [ 131.22492981    0.          305.00033569   46.5351944 ]
 [ 167.37669373  224.92269897  196.05653381  246.13546753]]
375
[[  69.           31.          332.          446.        ]
 [  63.93021393   89.2201767   309.18136597  499.375     ]
 [  60.4096489   105.2721405   268.44396973  450.93859863]
 ..., 
 [  38.79605865   56.80459595  111.11968994  100.8033371 ]
 [  84.08149719   76.20966339  188.67240906  194.17628479]
 [   0.          261.74819946   50.58844376  304.20553589]]
500
[[  28.            0.          332.          301.        ]
 [   0.            0.          228.40353394  374.375     ]
 [ 242.89332581    0.          499.375       374.375     ]
 ..., 
 [ 437.95901489  330.89108276  474.21908569  374.375     ]
 [  12.46927261  169.52677917  103.84616852  255.19648743]
 [ 315.3644104   222.79327393  355.81738281  258.57327271]]
333
[[  42.          117.          245.          388.        ]
 [   0.            0.          232.87838745  499.5       ]
 [   8.47790909  162.6633606   313.63891602  459.24197388]
 ..., 
 [ 168.01589966  119.67749786  204.40348816  154.9664917 ]
 [ 280.04873657  358.76916504  310.85665894  401.78564453]
 [ 135.79646301  388.56219482  200.64266968  445.89785767]]
334
[[   0.           17.          332.          497.        ]
 [  44.86463547    0.          292.12390137  315.59686279]
 [  43.98583221   31.32136726  295.51596069  440.40911865]
 ..., 
 [  13.10146618  352.46051025   44.26802826  413.27001953]
 [ 238.65034485  158.88626099  333.44332886  210.77049255]
 [ 274.71578979  269.11795044  305.5340271   327.6763916 ]]
500
[[ 245.          183.          315.          227.        ]
 [ 148.          230.          299.          333.        ]
 [ 169.32597351    0.          390.25140381  333.44332886]
 ..., 
 [ 328.48001099    0.          427.60595703  162.59016418]
 [  61.35542297  227.64984131  100.57108307  283.77932739]
 [ 475.51528931  126.4508667   499.32998657  178.67636108]]
500
[[  82.            7.          332.          492.        ]
 [   1.          271.           78.          378.        ]
 [   0.          220.           66.          261.        ]
 ..., 
 [ 369.23721313  191.29753113  406.69726562  246.00923157]
 [ 461.06411743   30.64284515  482.48300171   66.54817963]
 [  42.25077057  102.35670471  212.06144714  286.10238647]]
333
[[  82.           78.          241.          435.        ]
 [  82.56284332    0.          253.21627808  428.72875977]
 [  22.24288559    0.          316.91720581  303.98904419]
 ..., 
 [ 131.39242554  398.76977539  176.99801636  431.81985474]
 [ 183.68495178  102.26810455  214.07952881  150.57226562]
 [  44.01647949  155.39775085   70.76399231  186.76046753]]
500
[[  24.          264.          122.          324.        ]
 [  33.86150742  267.82345581  106.72479248  321.16619873]
 [  63.89676666    0.          382.79232788  310.53387451]
 ..., 
 [ 177.64187622  233.89929199  207.75079346  275.23910522]
 [ 172.9519043   286.42379761  190.8707428   310.71887207]
 [ 250.71347046    0.          296.73196411   46.71611404]]
375
[[  93.           99.          267.          424.        ]
 [  79.9887085    51.6126442   256.43536377  440.98135376]
 [ 146.30500793  109.33441162  269.44189453  433.66543579]
 ..., 
 [  43.49119568   71.3957901   182.62792969  197.23272705]
 [ 275.91900635  176.83349609  340.40539551  217.22697449]
 [ 115.9552002   228.68693542  143.62937927  251.58732605]]
333
[[  78.          288.          153.          367.        ]
 [  30.           67.          256.          183.        ]
 [  53.79028702   53.75872421  285.75125122  175.3346405 ]
 ..., 
 [  93.27075958  440.31503296  128.08023071  476.3894043 ]
 [ 140.10098267  168.08044434  167.99345398  199.193573  ]
 [ 130.24107361   79.56898499  161.44221497   98.30126953]]
460
[[ 247.          126.          332.          208.        ]
 [   0.          199.86413574  114.06311798  499.09997559]
 [   0.          245.09820557  286.9460144   499.09997559]
 ..., 
 [ 440.21398926  325.57208252  456.98483276  372.02331543]
 [ 363.43676758  137.74029541  421.49215698  195.31674194]
 [ 272.13009644   75.47428131  342.68066406  151.72198486]]
403
[[  72.           39.          332.          493.        ]
 [  64.11872864    0.          282.70895386  499.04830933]
 [   0.           44.53337479  354.65563965  494.75262451]
 ..., 
 [  50.82424545  394.76104736   78.55186462  422.60552979]
 [  55.07991028  351.84170532  166.39221191  425.14535522]
 [ 136.96827698  225.28660583  173.07014465  279.98660278]]
375
[[ 135.           89.          257.          207.        ]
 [ 105.37380981  105.5109024   270.20724487  214.17948914]
 [ 126.46188354  116.97675323  260.74957275  225.51423645]
 ..., 
 [ 155.28486633  181.67967224  192.5960083   211.78269958]
 [  57.44826126  399.56265259   84.09230804  438.18301392]
 [ 309.51937866   36.5476799   340.65127563   79.15396881]]
500
[[ 254.          141.          332.          220.        ]
 [ 195.00778198   60.56341171  395.03317261  325.45666504]
 [ 173.65032959    7.15026855  369.80657959  325.45666504]
 ..., 
 [ 338.67892456  248.57243347  360.31713867  280.52426147]
 [ 240.44857788    6.23915577  393.79055786  130.22915649]
 [ 324.3560791   100.94186401  340.77670288  117.94480896]]
334
[[ 100.          398.          186.          444.        ]
 [   0.            0.          188.12133789  425.98873901]
 [   0.           71.03924561  216.70549011  381.73388672]
 ..., 
 [   0.          296.19506836  102.02355194  446.41943359]
 [ 118.87425232  448.58282471  189.81745911  489.14553833]
 [  88.21907806    6.83516407  107.67692566   24.99475861]]
331
[[   0.            1.          322.           99.        ]
 [   3.          112.          323.          225.        ]
 [  20.          246.          326.          342.        ]
 ..., 
 [   0.           95.11657715   24.72980309  115.43115997]
 [  92.84405518  145.57104492  140.06642151  187.93678284]
 [ 121.27442932   67.62953949  209.36927795  217.93022156]]
333
[[  48.            0.          309.          496.        ]
 [ 128.56307983    0.          332.44500732  356.97268677]
 [   0.            0.          218.85491943  499.5       ]
 ..., 
 [ 111.52529144  171.46903992  161.41513062  235.67848206]
 [   0.          268.29870605   78.97631073  467.81997681]
 [ 284.00186157    0.          332.44500732   75.621521  ]]
333
[[  76.           45.          281.          439.        ]
 [  45.12284851   22.71877098  260.6262207   499.5       ]
 [  91.49165344    0.          269.30386353  291.37081909]
 ..., 
 [  12.84347343  384.09765625   39.77605438  430.16659546]
 [ 255.24963379  335.26907349  284.4831543   376.3190918 ]
 [ 105.14257812  187.30805969  152.17852783  225.64208984]]
333
[[  24.          144.          275.          436.        ]
 [ 100.66390228   39.10915756  268.32958984  431.5760498 ]
 [  32.0880394    93.2936554   190.88946533  499.5       ]
 ..., 
 [  65.22735596  428.38241577   99.92027283  460.54443359]
 [  92.10357666  296.58551025  126.86636353  348.76931763]
 [   4.74562836  199.31736755   57.52035904  253.29701233]]
333
[[  19.          162.          303.          454.        ]
 [  58.46111679   76.33026886  214.7882843   457.21517944]
 [  83.72031403   36.28488541  233.52516174  453.74902344]
 ..., 
 [ 216.57614136  284.16699219  313.09191895  345.26412964]
 [ 253.26957703  377.46948242  271.96585083  401.6953125 ]
 [  34.57732391  447.45297241   70.34916687  479.55462646]]
333
[[  34.          170.          259.          453.        ]
 [ 124.35385895  127.50667572  276.14950562  487.55636597]
 [ 144.92575073  220.06854248  282.50744629  499.5       ]
 ..., 
 [  32.65026093  241.42414856   52.38326645  263.63244629]
 [ 197.66259766  412.24255371  226.36129761  462.65072632]
 [   9.90035915   72.11920929   83.9331131   128.53544617]]
333
[[  69.           47.          249.          366.        ]
 [   0.            0.          166.27105713  310.30181885]
 [  87.1762085     0.          319.01193237  381.15148926]
 ..., 
 [ 221.3903656   235.71783447  288.22198486  271.13046265]
 [ 304.70727539  409.63543701  320.24008179  445.70596313]
 [  91.87567139  282.9163208   125.17087555  321.37530518]]
333
[[  67.           67.          260.          435.        ]
 [  67.3263092   124.81612396  230.4541626   499.5       ]
 [  42.59347153  218.15394592  187.04864502  499.5       ]
 ..., 
 [ 201.68508911  244.87217712  279.84683228  298.71542358]
 [  69.2334137   391.87844849  121.94752502  436.8633728 ]
 [ 302.07995605  464.04098511  322.9442749   496.9453125 ]]
333
[[  39.           50.          282.          442.        ]
 [  47.01881409    3.97442865  295.45553589  475.85931396]
 [  82.12851715   44.29871368  276.8225708   417.246521  ]
 ..., 
 [ 248.46009827  310.76834106  316.54824829  357.31826782]
 [ 132.93147278  432.99655151  149.1796875   450.92123413]
 [  43.9996109    47.05591583  258.5206604   217.32702637]]
360
[[  25.           33.          331.          433.        ]
 [ 124.83505249    0.          359.3999939   464.40002441]
 [  12.25232887    0.          228.7275238   464.40002441]
 ..., 
 [  29.74159813  365.90811157   74.10605621  435.14044189]
 [ 176.34042358   25.17758179  230.42640686   82.44926453]
 [ 159.37689209  337.18829346  202.03892517  363.96334839]]
341
[[  38.            2.          303.          438.        ]
 [ 148.12034607   86.65904236  340.43167114  499.56500244]
 [  73.4517746   112.07983398  334.70974731  499.56500244]
 ..., 
 [  13.88146782  301.4647522    77.08609772  356.18612671]
 [ 221.8508606   176.77151489  244.89222717  194.10945129]
 [ 307.28033447  324.22259521  329.70111084  355.78729248]]
333
[[   2.          372.           73.          464.        ]
 [  34.69884109  136.14213562  233.65849304  499.5       ]
 [  74.40969849  196.10014343  237.78579712  499.5       ]
 ..., 
 [ 180.10543823  227.23704529  332.44500732  369.29971313]
 [ 129.15429688  256.49987793  149.0974884   284.63314819]
 [ 173.72523499  339.7784729   209.10772705  373.71936035]]
500
[[  80.           92.          331.          330.        ]
 [  40.61968231    0.          233.26890564  332.44500732]
 [   5.41298628   22.48583221  278.11517334  291.29797363]
 ..., 
 [ 233.29502869  126.79402161  329.32144165  203.12567139]
 [ 426.44024658   96.46407318  456.53482056  129.92059326]
 [ 266.65835571    4.7567215   292.6842041    29.50197792]]
500
[[ 161.            3.          286.          330.        ]
 [ 152.17976379   32.77091217  293.4574585   306.80532837]
 [   0.            0.          224.30212402  297.41860962]
 ..., 
 [ 141.58503723  157.71720886  163.06513977  188.81790161]
 [ 154.8631897    92.08929443  233.67951965  140.9707489 ]
 [ 370.56140137  119.58308411  395.69906616  143.46153259]]
333
[[  50.          345.          285.          448.        ]
 [  44.74577332  355.7623291   288.6166687   452.94064331]
 [ 186.4425354   206.25094604  326.19711304  499.5       ]
 ..., 
 [ 179.16239929  139.93721008  257.25170898  186.77416992]
 [  30.44508553  346.18304443   57.32313156  374.05938721]
 [ 261.34259033  474.82623291  283.91809082  494.03973389]]
500
[[  57.          165.           73.          190.        ]
 [ 116.          233.          198.          280.        ]
 [ 247.          264.          290.          281.        ]
 ..., 
 [ 132.68902588   77.96708679  159.07385254   99.74676514]
 [ 358.39050293    0.          460.51364136   23.22083855]
 [   0.           76.07137299   47.33012772  211.20300293]]
243
[[  11.            7.          242.          496.        ]
 [   0.            0.          232.54827881  499.5       ]
 [   0.           42.59011841  182.43357849  471.771698  ]
 ..., 
 [ 113.50253296  138.53909302  207.66088867  194.16256714]
 [   0.          125.54193115   34.47616577  190.54574585]
 [   0.          135.32937622  235.26435852  289.75900269]]
375
[[  51.          169.          158.          423.        ]
 [  50.43216705  152.38249207  151.84942627  397.82775879]
 [  41.55152893  196.29473877  174.71244812  355.05081177]
 ..., 
 [   0.          254.23068237   68.60732269  311.14898682]
 [   0.          222.88702393   35.93841553  269.29074097]
 [ 224.39138794  259.94335938  268.80990601  303.49523926]]
333
[[ 122.          161.          228.          255.        ]
 [ 112.          207.          329.          493.        ]
 [ 140.84078979  266.78955078  250.42753601  499.5       ]
 ..., 
 [   8.92605495  343.52441406   43.12343979  393.44885254]
 [ 279.39599609  282.60494995  296.275177    309.41372681]
 [ 244.00164795  128.96846008  306.36013794  188.45079041]]
375
[[  60.          236.          207.          412.        ]
 [ 202.          233.          229.          283.        ]
 [ 231.          203.          241.          237.        ]
 ..., 
 [  51.38580322  353.0819397    92.01200104  396.1343689 ]
 [  77.28180695  171.63363647  142.6473999   210.50494385]
 [  69.48634338    4.68076229  101.98700714   45.1260643 ]]
500
[[ 174.           72.          332.          215.        ]
 [ 167.57295227   83.60525513  334.28125     208.38430786]
 [ 158.16253662   73.17487335  372.72909546  202.59251404]
 ..., 
 [ 124.3219986   227.80075073  145.99969482  263.47305298]
 [ 319.87539673  316.62384033  359.47564697  333.44332886]
 [ 283.43902588  233.80195618  372.81906128  286.59378052]]
313
[[  79.          118.          230.          353.        ]
 [  99.20967865    7.63793993  253.94041443  383.73339844]
 [  51.18225861   99.21350861  197.10456848  454.65109253]
 ..., 
 [ 142.48722839  350.06021118  160.85194397  365.98217773]
 [ 112.4324646    91.9368515   255.00352478  237.81668091]
 [  18.0369339   113.89313507  106.81167603  179.29219055]]
500
[[ 182.           46.          314.          331.        ]
 [ 181.52713013   58.44640732  312.53936768  302.42211914]
 [ 202.47825623   91.53514099  330.09521484  353.61624146]
 ..., 
 [ 383.57406616    0.          499.375        26.31364822]
 [ 158.96658325   16.34930992  191.25776672   34.15981674]
 [ 427.50997925   14.17118073  456.08352661   35.07517624]]
333
[[ 118.          158.          206.          401.        ]
 [   0.            0.          276.99185181  475.41989136]
 [ 183.9776001     0.          332.44500732  373.35818481]
 ..., 
 [ 183.30764771  318.61239624  207.02973938  354.19534302]
 [ 151.70391846  117.10354614  326.21008301  188.21488953]
 [ 119.85676575  335.04318237  194.03523254  404.69458008]]
500
[[ 111.          114.          331.          286.        ]
 [ 132.83926392    0.          344.60943604  373.37667847]
 [  90.93364716  135.34971619  381.96347046  373.37667847]
 ..., 
 [  31.35655022  212.16744995  287.14505005  320.06658936]
 [ 247.17567444  287.75952148  285.38690186  309.62704468]
 [ 124.94166565  229.09729004  150.23832703  264.45535278]]
364
[[  79.           44.          208.          349.        ]
 [  78.79290009   46.89722443  237.58786011  347.08630371]
 [  66.1055069     4.37209892  260.45288086  413.2885437 ]
 ..., 
 [ 316.50366211    0.          363.39334106   42.59194183]
 [ 259.47360229   72.96268463  279.7276001    89.89265442]
 [ 261.48492432   17.75680733  291.4546814    68.71572113]]
345
[[   6.           39.          307.          362.        ]
 [ 174.          336.          246.          469.        ]
 [  51.5920372     0.          258.12368774  447.38778687]
 ..., 
 [ 130.60404968  417.71105957  168.57666016  468.3243103 ]
 [ 291.04855347  105.70598602  324.00183105  142.93092346]
 [  40.40812302  114.42793274  108.47490692  171.67852783]]
500
[[  71.            6.          446.          320.        ]
 [  16.49690437    0.          251.10411072  332.44500732]
 [ 166.04980469    0.          344.29519653  332.44500732]
 ..., 
 [ 195.69699097    0.          338.38345337   61.66391754]
 [  83.5875473   255.94837952  107.07400513  288.18060303]
 [ 221.76701355   98.98125458  274.40979004  202.56016541]]
375
[[  95.           80.          228.          467.        ]
 [  88.37036133   49.07506943  239.04986572  467.37243652]
 [ 116.75678253   70.66812134  221.26467896  499.20913696]
 ..., 
 [  76.97049713  395.9246521    99.16656494  424.16064453]
 [  25.2415638   412.52828979   76.26004028  471.85394287]
 [  74.60514069  386.36618042  218.68554688  499.375     ]]
500
[[ 304.          142.          354.          209.        ]
 [ 368.          324.          400.          373.        ]
 [ 332.06640625   30.46272278  499.375       374.375     ]
 ..., 
 [  27.39470863  286.41567993  140.07217407  374.375     ]
 [   0.          267.56268311  120.56382751  374.375     ]
 [  93.03327179  254.13949585  179.0151825   309.92306519]]
484
[[ 132.          299.          262.          446.        ]
 [ 127.50426483  296.88571167  276.10018921  441.71936035]
 [ 156.92558289  305.17776489  270.43823242  421.58901978]
 ..., 
 [ 319.93115234   89.12516022  380.98977661  136.29762268]
 [ 289.60864258  242.19374084  388.275177    333.28598022]
 [ 310.44873047  342.47079468  352.6227417   419.20181274]]
500
[[ 255.          321.          268.          333.        ]
 [ 276.          314.          288.          336.        ]
 [ 300.          323.          311.          335.        ]
 ..., 
 [  28.78874779  177.54560852  122.38964844  224.60656738]
 [ 480.47119141  267.67895508  498.01367188  310.34783936]
 [ 152.54354858   29.98497963  245.9021759   327.24752808]]
500
[[   0.           76.          498.          373.        ]
 [  26.97594643  124.97505188  305.39276123  374.375     ]
 [  73.63150787    0.          314.98455811  374.375     ]
 ..., 
 [ 481.05328369  170.57173157  499.375       233.76493835]
 [ 159.42071533   90.86328125  223.59960938  132.01257324]
 [ 309.8934021   114.66820526  337.99291992  140.00872803]]
500
[[ 419.          342.          484.          371.        ]
 [ 394.          234.          468.          341.        ]
 [ 405.10717773  342.01199341  437.20898438  376.70056152]
 ..., 
 [ 202.09236145  169.55023193  220.91447449  190.50531006]
 [ 180.72727966  158.87153625  199.53314209  179.81648254]
 [ 244.72639465  201.77760315  263.49783325  222.68699646]]
370
[[ 278.          151.          318.          195.        ]
 [ 287.          421.          322.          458.        ]
 [ 269.09835815  182.27874756  308.08990479  214.49002075]
 ..., 
 [  53.0334549    76.54641724  134.2663269   119.18264008]
 [  52.88749313   66.6917038   134.27072144  109.46839905]
 [   8.86008835  262.47338867   82.59347534  318.60848999]]
500
[[ 170.          122.          194.          231.        ]
 [ 191.           92.          304.          270.        ]
 [ 196.91967773   91.14157867  312.78305054  262.04336548]
 ..., 
 [  54.76462555   26.32624626   72.95396423   61.84204483]
 [ 264.10961914   94.19200897  327.53677368  147.20028687]
 [ 336.73638916  182.04368591  360.78207397  213.29208374]]
375
[[ 181.          178.          283.          368.        ]
 [ 197.03067017    0.          363.44274902  346.68200684]
 [ 143.08267212  153.90483093  319.44284058  480.70355225]
 ..., 
 [ 296.03240967   50.76253891  361.83447266   85.87041473]
 [  64.44270325    0.          157.62915039  122.69760895]
 [ 113.10005188  384.23590088  210.28060913  414.12109375]]
375
[[  55.           24.          304.          308.        ]
 [  57.          291.           57.          291.        ]
 [ 343.           76.          373.          187.        ]
 ..., 
 [ 223.50457764  452.12588501  253.56878662  470.49481201]
 [ 132.66026306    0.          360.16845703   68.61847687]
 [ 150.34126282  293.52932739  181.24369812  332.59121704]]
500
[[ 181.          199.          224.          233.        ]
 [ 221.          288.          252.          337.        ]
 [ 170.          142.          216.          181.        ]
 ..., 
 [ 159.88400269  153.80397034  177.86132812  198.63612366]
 [  69.0595932   143.77749634  156.12269592  203.36672974]
 [  50.61694717  145.84136963   93.1169281   172.15956116]]
500
[[   5.           15.          154.          359.        ]
 [ 158.           15.          324.          360.        ]
 [ 320.           15.          466.          354.        ]
 ..., 
 [ 462.80151367  196.46989441  499.55999756  269.34576416]
 [ 275.37609863  162.92959595  348.03363037  255.64605713]
 [  47.6979599     0.          268.00799561   76.64593506]]
500
[[   8.           76.           49.          171.        ]
 [  34.           16.          115.           70.        ]
 [  24.          177.          118.          344.        ]
 ..., 
 [ 209.72177124  186.47489929  365.03848267  337.34561157]
 [ 415.20223999  104.98140717  438.484375    142.36528015]
 [ 368.17669678  252.00212097  485.55703735  331.35025024]]
500
[[ 142.           72.          337.          219.        ]
 [ 176.92381287    0.          392.06411743  318.46832275]
 [ 131.29833984    0.          348.58602905  318.46832275]
 ..., 
 [ 436.54400635   70.63895416  484.37271118  101.11228943]
 [ 263.40142822  214.27157593  295.64962769  236.96379089]
 [ 445.00714111   78.29042053  467.75808716  100.66749573]]
351
[[  27.           42.          270.          427.        ]
 [  40.55211639   26.27981377  288.69805908  472.19067383]
 [   0.            0.          204.31391907  499.58999634]
 ..., 
 [ 211.90228271  354.8520813   308.69119263  414.76318359]
 [  88.42449188  154.68388367  166.85697937  203.97227478]
 [  61.10768127  308.41210938  114.329422    369.26223755]]
500
[[  25.            0.          147.           61.        ]
 [  28.            1.          492.          246.        ]
 [ 133.82687378    0.          365.96643066  272.5       ]
 ..., 
 [  18.36135864   60.6984787    41.27422333  105.95548248]
 [ 194.90304565   15.56931591  228.70187378   31.64788818]
 [  22.92997742  149.24993896   56.37944031  182.78448486]]
500
[[ 123.           96.          403.          198.        ]
 [ 153.34593201   96.6071167   362.75872803  236.02629089]
 [ 123.19348145   88.97576904  354.52871704  198.83732605]
 ..., 
 [ 211.19969177  106.2987442   242.54399109  134.32452393]
 [ 472.38031006   54.1000824   499.70831299  110.18659973]
 [  57.62758636   54.31610489  133.33900452   85.78549957]]
500
[[  17.          119.          491.          327.        ]
 [ 213.37309265    0.          457.67547607  374.375     ]
 [ 111.65377808    0.          317.7383728   374.375     ]
 ..., 
 [ 219.57670593  312.27444458  315.7868042   358.4888916 ]
 [ 424.81832886  301.68533325  445.30447388  325.0803833 ]
 [ 461.10446167  277.09591675  477.06958008  310.05834961]]
500
[[ 246.          146.          435.          234.        ]
 [  74.           99.          240.          216.        ]
 [ 177.          373.          177.          373.        ]
 ..., 
 [ 388.39422607   68.77263641  406.00970459   88.39904785]
 [ 177.06829834  241.96420288  287.33312988  373.37667847]
 [ 131.79066467   78.84395599  214.82206726  107.84590149]]
284
[[  44.           17.          255.          493.        ]
 [  36.74995422    0.          210.26602173  388.76379395]
 [  64.22671509    0.          190.70614624  355.87872314]
 ..., 
 [ 211.81321716  187.64259338  235.39204407  220.30351257]
 [  57.86273956   66.02935791   84.21901703   92.9776001 ]
 [ 131.22219849  143.78768921  161.33517456  226.99478149]]
500
[[  20.           26.          201.          233.        ]
 [ 262.            2.          438.          227.        ]
 [   0.            0.          171.86302185  234.5       ]
 ..., 
 [ 405.1633606   110.36645508  463.18502808  156.03369141]
 [ 414.05651855  117.84598541  467.4385376   163.99021912]
 [ 129.85913086  146.17456055  156.43780518  168.49264526]]
345
[[ 113.          150.          227.          467.        ]
 [  95.66518402  152.57820129  285.88058472  499.67501831]
 [ 120.5741272   136.08808899  253.68101501  499.67501831]
 ..., 
 [  73.28038788  411.43814087   92.0189209   445.21890259]
 [  91.0749054   274.69555664  135.55703735  329.62792969]
 [ 165.487854    369.48651123  208.13096619  392.18005371]]
375
[[  14.            2.          357.          495.        ]
 [  83.08259583    0.          281.03598022  499.375     ]
 [ 115.0976181    57.79043198  249.6282959   499.375     ]
 ..., 
 [ 283.84780884  218.63540649  315.31576538  265.62454224]
 [ 303.62646484  415.02655029  340.56686401  460.36437988]
 [ 245.48449707  148.8117981   262.86178589  177.13792419]]
358
[[  74.           12.          269.          490.        ]
 [ 270.           11.          301.          257.        ]
 [ 259.          230.          285.          306.        ]
 ..., 
 [  54.72229767    0.          130.37097168   16.25004768]
 [ 145.36508179  372.51019287  202.78175354  447.54473877]
 [  69.43521118  405.39080811   96.12415314  431.65310669]]
500
[[ 157.            1.          266.          313.        ]
 [ 170.31282043    0.          276.98867798  216.04872131]
 [ 168.42010498   25.39566994  278.24520874  274.70913696]
 ..., 
 [ 254.85522461   92.06355286  463.88204956  208.94966125]
 [ 246.11413574  204.56007385  458.03521729  332.44500732]
 [  29.76951599  196.57276917   46.50115204  220.56916809]]
375
[[  74.           48.          296.          459.        ]
 [  53.03506851    0.          279.82247925  460.16271973]
 [ 106.05603027  101.16239166  333.090271    499.375     ]
 ..., 
 [ 204.79246521  357.54458618  236.96427917  407.32128906]
 [ 184.14485168  451.62130737  228.99455261  492.42575073]
 [   9.98479366  275.13232422  101.95064545  319.20239258]]
500
[[ 300.          154.          369.          279.        ]
 [ 302.39260864  176.42378235  362.95455933  246.61260986]
 [ 255.15582275    0.          430.99038696  310.07592773]
 ..., 
 [ 267.46627808  286.01239014  499.06665039  367.56356812]
 [ 133.39231873  270.42807007  162.68115234  305.29772949]
 [  88.63855743  100.50460815  233.25332642  252.99691772]]
500
[[ 102.          229.          200.          312.        ]
 [ 204.          182.          255.          241.        ]
 [  98.10241699  253.56036377  156.1991272   309.00906372]
 ..., 
 [ 309.87875366  125.98454285  342.15179443  154.35253906]
 [   0.           91.34376526   38.53259659  147.68797302]
 [  11.85343742   25.00627327   98.78269958   67.61421967]]
500
[[ 153.           61.          256.          156.        ]
 [ 274.          114.          325.          156.        ]
 [ 296.           70.          377.          155.        ]
 ..., 
 [ 423.61254883   96.96239471  499.375       147.92478943]
 [   4.71145725  105.59487915   39.71609116  159.81771851]
 [   3.98345828  253.99511719   39.42443466  309.39572144]]
500
[[ 110.          165.          227.          257.        ]
 [ 250.           87.          399.          175.        ]
 [ 260.           52.          326.           78.        ]
 ..., 
 [ 194.09095764  304.95672607  252.92440796  324.00292969]
 [  39.94440079   84.99478912   74.26202393  131.66914368]
 [ 299.48382568   91.65772247  351.68417358  139.18656921]]
500
[[ 178.          124.          294.          205.        ]
 [ 130.7197876   107.95017242  347.35351562  209.90541077]
 [ 149.60366821   95.16766357  328.91781616  222.35618591]
 ..., 
 [  59.34770966   83.37640381   80.48743439  109.2901535 ]
 [ 300.90536499   95.92931366  362.15155029  165.89736938]
 [ 274.31680298  131.12258911  329.15884399  211.71629333]]
500
[[ 127.          159.          287.          244.        ]
 [ 156.13912964  159.70199585  329.04650879  253.98635864]
 [ 130.53031921  169.46792603  300.74978638  262.58728027]
 ..., 
 [   0.91983795   96.97624969   86.63365936  147.20594788]
 [ 457.91278076   45.79327011  478.04669189   78.40268707]
 [ 437.90930176   85.80726624  461.15844727  109.3927002 ]]
333
[[ 111.           35.          235.          450.        ]
 [  93.05757141  276.69638062  229.72756958  499.5       ]
 [ 113.46768951  237.91784668  219.72235107  499.5       ]
 ..., 
 [  45.39267731   73.2651825    66.37848663   96.65460205]
 [ 108.9037323   247.07917786  132.67955017  283.49642944]
 [ 291.83413696  407.93954468  313.99810791  455.53009033]]
192
[[  74.          130.          141.          188.        ]
 [  60.46189499   41.7837677   169.50187683  239.67999268]
 [  68.18997192  136.034729    143.14108276  192.48149109]
 ..., 
 [ 167.47117615    0.          184.08210754   26.98771477]
 [ 148.0144043     0.          175.83558655   32.13710403]
 [ 156.56915283   64.83022308  187.14361572  128.45855713]]
333
[[ 105.          242.          231.          298.        ]
 [ 129.66555786  259.54660034  262.36410522  297.24334717]
 [ 103.09577179  243.14353943  228.49406433  295.2588501 ]
 ..., 
 [ 243.8502655   166.95257568  262.69207764  204.96655273]
 [  39.34458542    6.07684183   68.94676208   46.47789383]
 [  19.50334358   86.1284256    47.51438904  125.44824982]]
173
[[  63.          272.          169.          441.        ]
 [   7.            0.          153.          217.        ]
 [  66.22690582  298.75875854  163.67141724  443.55374146]
 ..., 
 [ 125.92906189  346.96618652  142.66279602  371.99420166]
 [ 100.1743927   229.52600098  156.0861969   265.60906982]
 [  93.87413025  429.23513794  123.07049561  457.40097046]]
500
[[ 135.          235.          177.          285.        ]
 [ 129.43792725  244.66261292  181.73788452  291.83666992]
 [ 132.69590759  229.02781677  193.02438354  293.33221436]
 ..., 
 [ 274.91796875  128.91473389  309.22476196  163.09809875]
 [ 188.16287231  145.02296448  221.98306274  177.51914978]
 [ 259.44717407   25.0693264   349.7784729    67.33310699]]
500
[[ 117.          236.          220.          330.        ]
 [ 289.          186.          396.          299.        ]
 [   0.          244.16630554  404.83428955  499.16665649]
 ..., 
 [  58.75931931  428.69784546   77.29454041  455.62390137]
 [ 359.46240234    0.          499.16665649   52.05640411]
 [ 174.98379517  270.7303772   392.71652222  453.28585815]]
500
[[ 246.           19.          376.          312.        ]
 [ 388.35110474    0.          499.32998657  269.21853638]
 [ 330.8956604     0.          499.32998657  225.87841797]
 ..., 
 [ 111.21781921  179.21743774  134.24760437  212.62556458]
 [ 187.38447571  133.80250549  376.16241455  271.42288208]
 [ 111.12550354   94.1627121   175.68395996  157.34840393]]
333
[[  37.          148.          303.          462.        ]
 [ 115.          314.          171.          359.        ]
 [   5.87998009   40.84762955  196.93400574  472.01495361]
 ..., 
 [ 271.21124268  364.15283203  326.54281616  440.43823242]
 [ 298.56933594  433.35699463  320.44082642  481.11383057]
 [  32.77098465  446.37124634   99.62294006  497.61605835]]
335
[[ 123.           56.          296.          424.        ]
 [ 109.73511505    0.          287.32052612  408.74627686]
 [  87.6690979   154.43391418  206.68804932  499.70831299]
 ..., 
 [   9.33689594  205.21212769   28.21237373  245.66529846]
 [ 136.23069763  411.71337891  208.70243835  456.30831909]
 [  15.03818798  264.62826538   35.19585037  296.26593018]]
500
[[  94.          197.          403.          323.        ]
 [ 321.12423706    0.          499.52001953  335.44000244]
 [ 129.32632446  181.86791992  427.24398804  319.26480103]
 ..., 
 [ 310.63134766   87.22078705  341.29928589  124.80451202]
 [ 162.77986145  222.88414001  187.95829773  253.7335968 ]
 [ 297.2729187    88.01728821  313.77493286  103.98205566]]
500
[[ 182.          128.          297.          244.        ]
 [ 171.03182983  149.22781372  302.4833374   261.00814819]
 [ 174.395401    127.32045746  314.19369507  241.98670959]
 ..., 
 [  96.96617889   93.83000183  118.09166718  109.45900726]
 [ 327.44055176    0.          470.32272339  235.41694641]
 [  84.76773834   88.22956848  112.83149719  103.81008911]]
354
[[  80.           36.          332.          474.        ]
 [ 173.87796021    0.          353.41000366  499.13998413]
 [  92.47444916    0.          345.0619812   499.13998413]
 ..., 
 [ 255.11877441   49.0760994   353.41000366  145.9503479 ]
 [ 111.97875977  332.74429321  181.31658936  396.12478638]
 [  22.71089935  396.92749023   61.74944305  467.73165894]]
333
[[ 189.          298.          332.          498.        ]
 [ 145.69711304  222.26997375  280.86865234  499.5       ]
 [ 119.8441925   258.20419312  332.44500732  499.5       ]
 ..., 
 [ 307.58029175  199.47927856  332.44500732  308.36373901]
 [ 113.28488922  118.97356415  158.33815002  147.87731934]
 [   0.           86.25296021  176.05535889  265.52929688]]
371
[[  74.           91.          274.          409.        ]
 [  43.74059677   43.89136505  173.14805603  451.89508057]
 [  66.94471741   99.68084717  312.65917969  499.61334229]
 ..., 
 [ 343.07086182  124.1651001   363.45089722  159.62931824]
 [  28.51563835  134.19606018   47.97798157  159.77996826]
 [  23.94737434  427.41546631   84.2486496   485.00735474]]
333
[[  62.            0.          201.          419.        ]
 [  15.35245419    0.          258.01565552  499.5       ]
 [  24.23838234   30.18872833  310.70007324  428.76538086]
 ..., 
 [  86.73046875  212.08590698  120.89989471  267.39266968]
 [ 275.12286377  289.43035889  302.02206421  320.80541992]
 [ 113.12565613  342.45657349  147.24101257  423.63217163]]
500
[[ 140.          111.          354.          299.        ]
 [  82.20075226    0.          322.81854248  332.44500732]
 [ 164.69290161    0.          410.06008911  332.44500732]
 ..., 
 [ 125.42193604  109.77466583  147.53160095  132.49847412]
 [ 195.43859863  161.96749878  289.82827759  207.90985107]
 [  91.20927429  247.54266357  119.26544952  284.90551758]]
333
[[ 199.          287.          304.          373.        ]
 [ 101.          285.          141.          327.        ]
 [ 100.30388641  292.97860718  131.25099182  329.21716309]
 ..., 
 [ 161.81037903  117.80062866  218.69451904  151.4473114 ]
 [ 185.13519287   71.85315704  213.50334167  103.04763794]
 [ 157.89439392  215.57963562  223.05184937  264.26889038]]
334
[[  79.          160.          310.          488.        ]
 [  86.077034      0.          317.18365479  425.27651978]
 [  60.628582      0.          261.150177    425.00109863]
 ..., 
 [ 105.37239075  143.38619995  192.58190918  246.51373291]
 [ 219.46563721   83.49673462  255.50723267  126.34824371]
 [ 132.07374573   90.08460236  161.73442078  120.71174622]]
375
[[  95.           37.          213.          361.        ]
 [ 100.544487     59.7029686   223.92134094  351.3526001 ]
 [  80.06336212  127.42807007  240.35624695  499.375     ]
 ..., 
 [ 263.7472229    52.71602249  303.72964478   90.84603119]
 [   0.            0.          116.91886902  229.75219727]
 [  19.49852753  364.45599365   95.83988953  418.84869385]]
308
[[  48.           67.          162.          175.        ]
 [  67.38801575    0.          204.92170715  230.61500549]
 [ 188.4384613     0.          307.61499023  230.61500549]
 ..., 
 [  32.70120239    7.85915565   51.5811882    47.41858673]
 [  59.95530319   50.98543167   98.16280365   81.00373077]
 [ 157.77314758   21.77877235  182.25039673   42.92827988]]
494
[[  11.           36.          408.          445.        ]
 [ 109.43934631   56.34383392  374.23739624  498.93997192]
 [   0.          209.07507324  382.37466431  498.93997192]
 ..., 
 [  81.0219574   158.15866089  140.49191284  195.49378967]
 [ 384.76828003  268.64123535  442.95773315  324.14968872]
 [  65.25993347  473.05856323   98.68151855  488.84924316]]
375
[[  27.          202.          208.          494.        ]
 [  68.            1.          271.          174.        ]
 [  25.50852776    0.          286.92370605  499.375     ]
 ..., 
 [ 275.75613403  124.17182922  295.89920044  169.91676331]
 [   0.          105.17184448  114.80479431  259.34207153]
 [  96.62137604  274.6847229   142.04756165  328.42514038]]
500
[[  18.           25.          350.          365.        ]
 [ 380.          233.          486.          282.        ]
 [ 361.           28.          479.          198.        ]
 ..., 
 [ 232.5015564    98.03083038  288.97296143  188.66186523]
 [ 312.64035034   30.4686451   341.09103394  129.81803894]
 [  84.86618805   22.21791267  112.15435791   40.60966873]]
375
[[ 141.           39.          368.          441.        ]
 [ 142.64936829   12.00922012  351.93270874  499.375     ]
 [ 184.58766174    3.39542389  362.26690674  393.44729614]
 ..., 
 [ 342.73434448  124.92093658  374.375       172.82249451]
 [   0.          472.85687256  242.17292786  499.375     ]
 [ 155.27536011  114.47429657  214.75782776  185.29420471]]
500
[[  73.          125.          342.          228.        ]
 [ 166.12107849    0.          344.62127686  265.5       ]
 [ 115.57936096    0.          286.37091064  265.5       ]
 ..., 
 [ 363.40655518   45.11301804  380.59576416   67.0249939 ]
 [  70.84436035  167.20925903  208.73770142  265.5       ]
 [ 445.41262817  220.6055603   486.61947632  251.56326294]]
500
[[ 279.           23.          435.          214.        ]
 [ 152.          154.          215.          276.        ]
 [  72.          257.          197.          399.        ]
 ..., 
 [ 362.55172729  197.03166199  499.33334351  336.39831543]
 [ 283.42401123    7.17309332  303.78643799   27.38109398]
 [ 454.98535156   60.9775238   494.92861938  113.69793701]]
500
[[  26.           57.          446.          316.        ]
 [ 236.94258118    0.          434.37088013  363.39334106]
 [  90.2017746     0.          369.62695312  363.39334106]
 ..., 
 [  30.39818192  105.02751923   51.40103531  126.68238068]
 [ 452.66516113  114.12963867  481.93792725  147.60585022]
 [ 335.44540405  264.75823975  382.85760498  302.10162354]]
443
[[  48.            2.          383.          451.        ]
 [  38.36368179    0.          227.70927429  472.49938965]
 [   0.           33.89289856  257.87472534  499.11331177]
 ..., 
 [ 350.18005371  322.74508667  379.70584106  375.08660889]
 [ 263.92959595  191.60739136  304.92263794  237.38298035]
 [  76.64797211   51.8992691   264.28598022  147.67256165]]
500
[[  69.          213.          276.          319.        ]
 [  53.14400482  213.4029541   281.76574707  323.40765381]
 [  90.4309845   209.64736938  311.11608887  317.57391357]
 ..., 
 [ 211.4563446    28.56290817  231.9526825    57.5387764 ]
 [ 369.23431396  304.03302002  403.53518677  326.76608276]
 [ 366.8237915   260.97137451  391.09460449  294.87597656]]
333
[[  84.           38.          211.          205.        ]
 [ 102.          207.          236.          381.        ]
 [  79.45417786   48.24461365  214.77160645  208.99713135]
 ..., 
 [  99.77228546  104.11309814  131.50360107  147.54093933]
 [ 305.41964722  233.56256104  324.30880737  263.51013184]
 [  15.81167793   27.12537956   87.84851837   90.9101944 ]]
500
[[  94.           56.          371.          354.        ]
 [  77.76251984   21.09586334  299.49499512  380.36502075]
 [ 151.84835815    0.          366.8130188   374.70861816]
 ..., 
 [  25.10033798  263.07995605   50.53398895  311.42572021]
 [   0.           52.64907455   52.89539719  143.37425232]
 [ 236.02549744   26.32022476  265.87252808   51.12842941]]
357
[[  82.          204.          286.          404.        ]
 [  90.89551544  221.06643677  278.23962402  382.76010132]
 [ 124.70780945   54.48426437  316.36618042  472.03579712]
 ..., 
 [ 326.61203003  207.20529175  356.40499878  313.07022095]
 [ 267.96594238  276.40484619  340.52926636  318.86425781]
 [ 211.72483826  181.92127991  238.5249939   198.90370178]]
375
[[ 114.          150.          250.          362.        ]
 [ 113.          127.          129.          157.        ]
 [ 105.39753723  138.15435791  125.10254669  163.50166321]
 ..., 
 [ 347.02581787   21.3552475   374.375        92.0044632 ]
 [ 259.23742676  274.97396851  350.50009155  340.77716064]
 [ 120.01927948   11.90543175  151.82554626   34.5376091 ]]
375
[[ 159.            2.          284.          477.        ]
 [ 152.31259155    0.          286.96151733  499.375     ]
 [ 151.86206055  110.02040863  340.83529663  499.375     ]
 ..., 
 [ 143.37341309  285.38574219  205.57197571  409.70169067]
 [ 355.77423096  258.7026062   374.375       327.04968262]
 [ 276.72460938  247.41136169  304.55914307  299.38885498]]
375
[[  65.            0.          274.          498.        ]
 [  63.51235199    0.          312.34051514  482.59796143]
 [  40.54462433   11.29188538  306.69036865  376.99154663]
 ..., 
 [ 155.68011475  428.80322266  204.98390198  466.59313965]
 [  13.64560032  289.20843506   41.34748459  328.36767578]
 [ 344.3081665   190.78361511  366.66824341  232.15461731]]
375
[[  36.           42.          336.          472.        ]
 [  54.18884277    0.          331.72476196  482.394104  ]
 [  57.46566772  218.8603363   345.76940918  499.375     ]
 ..., 
 [ 155.66993713  329.62982178  184.13348389  374.13198853]
 [  65.14220428  290.09936523  233.94851685  441.28817749]
 [ 306.85644531   97.65670013  374.375       177.42182922]]
500
[[ 233.          316.          463.          450.        ]
 [ 228.91784668  316.16079712  446.31976318  461.23001099]
 [ 203.99731445    0.          474.02438354  375.41235352]
 ..., 
 [  13.32627487  440.28671265  111.24060822  461.23001099]
 [  78.23938751  178.74752808  171.74804688  288.25997925]
 [ 113.57403564   39.38478851  221.60240173  101.54728699]]
500
[[  83.           79.          393.          322.        ]
 [ 196.58073425    0.52318573  388.20394897  374.375     ]
 [ 157.51356506   99.72446442  460.93154907  328.11541748]
 ..., 
 [ 147.66325378   94.02159882  177.83013916  122.9324646 ]
 [ 260.21383667  132.14953613  367.19335938  245.31471252]
 [ 380.30548096  288.65359497  485.83847046  374.375     ]]
500
[[ 144.          104.          203.          160.        ]
 [ 243.           62.          445.          238.        ]
 [ 189.            0.          222.           23.        ]
 ..., 
 [  23.82329941   39.64313507  287.25967407  259.10220337]
 [ 465.66473389    0.          499.60830688   23.34431648]
 [ 395.55197144  124.31948853  444.61889648  171.81625366]]
352
[[  34.          227.          251.          471.        ]
 [ 226.          283.          263.          307.        ]
 [   9.57205868   88.49636078  227.20741272  499.25332642]
 ..., 
 [ 190.17303467   68.11762238  209.80091858   89.91124725]
 [ 154.48033142  154.66673279  319.76913452  419.80908203]
 [   0.          183.38072205  135.45950317  314.31210327]]
500
[[  54.          211.          193.          353.        ]
 [  51.75344849  225.77398682  196.21516418  357.40332031]
 [ 299.19921875    0.          488.86245728  357.40332031]
 ..., 
 [  75.25475311  264.04156494   95.44393158  298.5062561 ]
 [ 213.24229431  312.33233643  253.84989929  352.1048584 ]
 [ 447.99505615   23.49414444  499.40997314   85.63401031]]
332
[[  47.          252.          171.          483.        ]
 [ 161.          269.          242.          350.        ]
 [  46.14300156  264.47116089  175.28440857  491.80511475]
 ..., 
 [   0.           62.28461838   27.45629501  113.30334473]
 [   0.           71.53540039   26.99019432  122.58850098]
 [ 148.05540466  153.67948914  282.59863281  324.50811768]]
500
[[ 111.           12.          404.          333.        ]
 [ 133.34580994    0.          390.92999268  333.44332886]
 [ 140.88310242   57.588871    341.03823853  333.44332886]
 ..., 
 [ 303.3147583    83.48432159  343.9324646   118.7692337 ]
 [ 292.57675171  297.75802612  324.75259399  327.17947388]
 [ 466.80874634   56.00737762  489.94500732  115.07423401]]
500
[[  41.           88.          473.          274.        ]
 [ 109.32969666    0.          423.27191162  332.44500732]
 [  80.8441925     0.          341.06799316  332.44500732]
 ..., 
 [ 263.43954468  185.63340759  315.71447754  257.12460327]
 [ 321.27337646    0.          409.13519287   20.12717247]
 [ 391.92572021    0.          483.16427612   18.63158226]]
333
[[ 107.           78.          235.          394.        ]
 [  80.51983643   27.48252678  222.54841614  418.59353638]
 [  56.66828156    0.          239.76708984  390.97555542]
 ..., 
 [  50.53001022    9.87185001   93.45300293   61.3035965 ]
 [ 101.95175934   33.07778549  128.28305054   54.59631348]
 [  28.37613297  469.70510864   62.16892242  493.76623535]]
500
[[ 172.          143.          259.          330.        ]
 [ 280.          154.          362.          337.        ]
 [ 181.53952026  152.97079468  270.06201172  316.8291626 ]
 ..., 
 [ 168.3868866   201.08554077  188.6390686   264.16381836]
 [  33.04554749  128.81272888   67.96764374  151.12071228]
 [ 417.49594116  207.18341064  441.41098022  229.151474  ]]
500
[[ 132.          178.          225.          292.        ]
 [ 131.48023987  175.80183411  231.84625244  295.6267395 ]
 [ 114.69123077   55.7756424   320.90649414  374.375     ]
 ..., 
 [ 178.7802124    35.93867111  373.78201294  168.35223389]
 [ 246.26885986  171.83947754  284.56945801  220.69824219]
 [ 334.61599731  196.37506104  379.32119751  240.41146851]]
500
[[  10.           95.          478.          261.        ]
 [ 171.10812378   10.81596375  419.77401733  374.375     ]
 [ 111.389328      0.          377.96539307  374.375     ]
 ..., 
 [ 122.32073975  332.85855103  147.56820679  350.31298828]
 [ 322.88134766    0.          404.82745361   64.92442322]
 [ 314.01022339  203.5949707   343.21508789  228.27514648]]
500
[[ 135.          268.          152.          290.        ]
 [ 156.          229.          184.          269.        ]
 [ 176.          174.          240.          249.        ]
 ..., 
 [ 439.62374878  315.43484497  489.38983154  365.95785522]
 [ 419.55245972   62.65848541  499.15002441  195.92140198]
 [ 478.12121582  225.21878052  496.35464478  256.26242065]]
500
[[  41.           33.          404.          163.        ]
 [ 222.24349976    0.          382.43069458  233.5       ]
 [ 149.80148315    0.          364.43154907  233.5       ]
 ..., 
 [ 283.105896     32.65650558  300.64746094   55.15623856]
 [  13.85154533    0.           68.67532349   15.56296444]
 [ 206.97875977  122.56884003  306.39196777  233.5       ]]
500
[[ 114.          115.          379.          221.        ]
 [ 119.78599548  101.96621704  365.58242798  215.99414062]
 [ 148.22134399  137.19612122  449.9135437   235.86024475]
 ..., 
 [  61.52264404  186.87510681  140.17280579  220.64555359]
 [ 434.73388672  318.29510498  456.74719238  336.47192383]
 [ 453.5697937    77.99938965  475.27990723  119.20508575]]
500
[[ 181.          128.          328.          344.        ]
 [ 302.           95.          334.          121.        ]
 [ 307.00521851   96.9453125   334.54290771  116.88052368]
 ..., 
 [  14.04442501    6.47022247   82.63145447   74.99208069]
 [ 418.90350342  297.09463501  438.62213135  325.11935425]
 [ 194.47528076   82.99790192  263.5055542   162.22224426]]
500
[[  46.           26.          466.          365.        ]
 [  85.69834137    0.          389.51535034  487.18667603]
 [  93.99422455  108.82089233  296.68145752  487.18667603]
 ..., 
 [ 140.59417725   79.8058548   163.74598694  106.26080322]
 [ 425.55667114  187.29177856  463.29138184  246.64816284]
 [  37.31786346  382.47940063  130.72451782  421.22183228]]
500
[[ 143.          122.          398.          256.        ]
 [ 189.74971008  118.06468201  445.06481934  284.20904541]
 [ 174.74353027  114.03585052  348.24468994  277.08959961]
 ..., 
 [ 338.24749756  110.69085693  355.159729    129.11843872]
 [ 247.90744019    0.          396.7265625    98.86869049]
 [ 379.5765686   115.94192505  398.92630005  133.51419067]]
500
[[  80.           74.          121.          103.        ]
 [ 164.          190.          383.          291.        ]
 [ 137.           49.          229.          209.        ]
 ..., 
 [ 285.21710205  249.83325195  311.26434326  268.22634888]
 [ 409.48028564   96.47682953  439.64477539  133.43197632]
 [  80.51358032   39.12398911  104.2618103    74.08501434]]
489
[[ 249.           73.          415.          265.        ]
 [ 261.2829895    88.95485687  427.76031494  263.57730103]
 [ 220.42881775   54.29856873  403.41690063  258.65246582]
 ..., 
 [ 142.30937195  476.14990234  251.82855225  498.77996826]
 [  47.39777756    0.           86.80484772   15.94724655]
 [ 256.48873901  225.10105896  286.67636108  256.20547485]]
405
[[  79.           55.          309.          379.        ]
 [  30.10995865   47.16463852  267.15948486  499.50003052]
 [  75.20104218    0.          328.07440186  499.50003052]
 ..., 
 [ 121.36444092  396.31942749  145.93377686  418.02349854]
 [  63.87156296  392.04330444   80.04982758  410.58255005]
 [ 143.46437073   47.58197021  161.8057251    66.82159424]]
500
[[  57.          158.          335.          307.        ]
 [  14.48658943    0.          218.5710907   286.11129761]
 [   0.            0.          150.39834595  276.47979736]
 ..., 
 [  22.63985062  134.14411926   61.26348877  175.84181213]
 [ 460.69122314  228.87411499  499.375       257.12213135]
 [ 123.70175171    0.          186.73492432   71.41204834]]
500
[[   8.           49.          353.          226.        ]
 [ 121.           22.          148.           44.        ]
 [ 216.41749573    0.          417.91619873  357.40332031]
 ..., 
 [ 332.78536987  317.90762329  378.56045532  348.49356079]
 [ 147.5512085    16.38560867  234.22651672   59.59767914]
 [ 179.99160767  240.68856812  198.9236908   261.18991089]]
333
[[ 141.          211.          219.          364.        ]
 [ 151.07476807  229.66159058  233.72402954  359.62969971]
 [ 140.51199341  238.4848938   245.15718079  379.4161377 ]
 ..., 
 [  12.76280212  427.01889038   39.39630508  453.57717896]
 [ 210.53669739  471.80645752  275.44619751  499.5       ]
 [  99.04592896  184.9445343   117.42773438  227.60948181]]
332
[[ 222.          148.          282.          313.        ]
 [  60.          173.          102.          298.        ]
 [  70.38089752  177.78849792  107.72872162  302.51678467]
 ..., 
 [  81.94075012  398.55990601  108.72297668  436.81866455]
 [  28.11805725  202.66426086   47.06291962  240.08538818]
 [ 309.89556885  435.1925354   326.44107056  459.52764893]]
500
[[  73.          213.          209.          276.        ]
 [  78.06080627  217.82662964  209.26820374  269.5453186 ]
 [  15.26069069   44.14767075  312.65231323  391.34667969]
 ..., 
 [ 140.3874054   325.49945068  197.27285767  367.17538452]
 [ 412.77502441    0.          493.65988159   86.84314728]
 [ 351.89260864  147.9649353   491.07089233  306.03219604]]
334
[[ 216.          173.          333.          497.        ]
 [ 215.95909119  222.56828308  333.44332886  499.32998657]
 [ 164.33952332   32.57325745  333.44332886  437.08685303]
 ..., 
 [  26.1794796   421.92880249   42.98019409  439.86334229]
 [ 300.2980957   105.20519257  333.44332886  226.56948853]
 [   0.          155.57810974   28.67843246  214.95707703]]
269
[[  74.           90.          250.          484.        ]
 [ 134.49578857  244.87765503  254.78625488  499.5       ]
 [  90.26008606   86.70027161  264.69793701  449.83569336]
 ..., 
 [ 122.38897705  338.52276611  208.63519287  390.92089844]
 [ 211.03250122  211.94036865  268.5         260.56900024]
 [   0.          317.32373047   35.91339874  361.53192139]]
344
[[  70.          126.          259.          380.        ]
 [  32.29697037   67.61833954  299.5149231   439.17333984]
 [  83.00976562  128.69555664  308.98776245  439.17333984]
 ..., 
 [ 187.98771667   24.06032562  219.70875549   68.45766449]
 [ 294.39770508   24.32277107  314.33148193   56.60648727]
 [ 192.28384399  116.04692078  213.59596252  134.972229  ]]
333
[[   0.          105.          331.          494.        ]
 [  93.           22.          264.          120.        ]
 [  72.38087463  153.12979126  325.70687866  499.5       ]
 ..., 
 [  28.02688217  145.07955933  122.9202652   346.46881104]
 [ 196.77590942  187.91653442  236.73139954  256.20117188]
 [ 159.62660217   87.08171844  241.63531494  182.84017944]]
408
[[  31.           54.           63.           76.        ]
 [   0.          107.           20.          138.        ]
 [  51.           76.          326.          244.        ]
 ..., 
 [ 195.85166931   72.77848816  271.96963501  166.62831116]
 [   0.          218.3506012    77.07518005  305.48999023]
 [  27.73987198  170.84359741  107.53430939  222.52571106]]
448
[[   1.           18.          425.          495.        ]
 [   0.            0.          287.00415039  499.51998901]
 [ 103.82379913    0.          365.4046936   499.51998901]
 ..., 
 [ 262.45077515    2.41041994  378.09176636   99.62549591]
 [ 246.95088196   87.96047974  293.13330078  132.70755005]
 [  53.80373764  251.39517212  135.83468628  327.61898804]]
450
[[ 157.           40.          180.           61.        ]
 [ 154.           72.          273.          340.        ]
 [ 151.29316711  125.96224213  287.57492065  369.35705566]
 ..., 
 [ 109.27548981  325.35916138  128.38540649  349.54266357]
 [ 348.06072998  412.46795654  366.21670532  434.77624512]
 [  46.18555069  384.34570312   67.37524414  408.7383728 ]]
375
[[  38.          221.          141.          477.        ]
 [  96.          394.          166.          497.        ]
 [  69.          282.          187.          379.        ]
 ..., 
 [ 283.50405884  156.8321228   311.67333984  183.63031006]
 [ 116.08615875  214.24447632  158.4584198   261.6602478 ]
 [ 291.18075562  307.17333984  319.81121826  342.67651367]]
450
[[  49.           56.          395.          209.        ]
 [ 128.70063782    0.          302.00662231  254.69998169]
 [ 243.07925415    0.          383.25149536  254.69998169]
 ..., 
 [   0.            0.           43.47307968   79.93017578]
 [ 160.62600708   81.01174927  183.03256226  103.64463043]
 [  75.53044128   40.20972824  102.34429932   67.66417694]]
333
[[  78.           67.          252.          432.        ]
 [  95.02079773    4.38126183  262.99038696  384.85006714]
 [  59.63530731  164.74452209  270.56784058  499.5       ]
 ..., 
 [ 168.51455688  282.806427    235.87557983  371.71862793]
 [  30.69729996  357.41033936   91.21743011  411.20706177]
 [  62.76088715  351.46478271   80.80120087  376.82455444]]
333
[[  23.           42.          288.          468.        ]
 [ 105.47582245    0.          280.52099609  343.23809814]
 [  58.82413864    0.          265.22113037  330.13705444]
 ..., 
 [ 273.71661377  196.8009491   295.41046143  234.96736145]
 [ 271.57827759  147.14901733  294.65301514  168.36434937]
 [   0.            7.92360592   50.34474564   58.4100647 ]]
500
[[ 230.          146.          259.          166.        ]
 [ 229.05206299  154.60884094  254.92965698  176.37545776]
 [ 234.70474243  133.51922607  259.49258423  158.62547302]
 ..., 
 [ 375.90048218  283.22930908  409.16668701  313.39346313]
 [ 453.69247437  179.32180786  480.09408569  218.8213501 ]
 [ 400.6477356    49.68857193  420.32217407   73.60704803]]
455
[[ 148.          230.          327.          428.        ]
 [  97.9214859   232.54574585  427.54257202  410.80187988]
 [ 154.42990112  262.84051514  344.7883606   426.79769897]
 ..., 
 [ 360.9180603    48.76754379  450.5713501   108.56610107]
 [  98.64775848  413.59631348  143.47979736  447.23614502]
 [ 270.33746338  272.20492554  316.22418213  333.60827637]]
334
[[ 103.           97.          229.          316.        ]
 [  47.55452347    0.          262.75405884  455.65686035]
 [  32.47363663   42.15584564  309.52828979  405.5692749 ]
 ..., 
 [ 313.69229126  235.67778015  333.44332886  300.73388672]
 [  65.5128479   277.13192749  302.20785522  357.00985718]
 [  58.25386429  464.11630249   83.1344223   491.67816162]]
150
[[  42.           30.          107.          142.        ]
 [  19.49906921    0.          116.3735199   179.75      ]
 [   0.            7.85411835   76.38637543  179.75      ]
 ..., 
 [ 128.76319885  140.62164307  149.75        177.42926025]
 [ 103.11572266  144.56648254  130.82673645  163.67417908]
 [ 119.77397919  150.62931824  145.15667725  179.75      ]]
332
[[   0.            0.          252.          466.        ]
 [ 101.           98.          147.          120.        ]
 [   0.            0.          167.58981323  321.86853027]
 ..., 
 [ 182.84207153   80.68517303  206.37643433  112.0569458 ]
 [ 271.65365601   25.82666206  331.44665527   76.02455139]
 [  87.95293427  139.83709717  135.94172668  177.3129425 ]]
343
[[   0.            0.          338.          487.        ]
 [   0.            0.          232.96191406  389.62249756]
 [  35.08330154    0.          298.72592163  466.87643433]
 ..., 
 [ 205.25891113  377.95666504  264.70303345  453.91616821]
 [ 238.95095825   38.89570999  277.2862854    85.48082733]
 [ 210.73847961  183.19708252  263.85336304  253.52914429]]
500
[[   0.            1.          492.          316.        ]
 [ 211.78605652    0.          383.56057739  328.45166016]
 [  54.42770004    0.          297.51596069  328.45166016]
 ..., 
 [ 296.46331787  215.29656982  348.1361084   297.42349243]
 [ 252.99397278   53.84051895  290.03945923   94.15402985]
 [ 105.49359894  263.77859497  172.3366394   313.13397217]]
357
[[  31.          162.          356.          499.        ]
 [  15.          321.           61.          354.        ]
 [   0.          409.           10.          463.        ]
 ..., 
 [  33.52046204  174.7756958    54.99274826  199.95657349]
 [ 191.51260376  172.19789124  236.20449829  205.74342346]
 [ 213.48468018  185.53898621  277.27554321  250.36613464]]
375
[[  82.          244.          162.          332.        ]
 [  81.          319.          135.          497.        ]
 [ 143.          307.          306.          453.        ]
 ..., 
 [ 198.03727722  467.93173218  232.45832825  496.36923218]
 [ 143.03465271    0.          301.18682861  152.74328613]
 [ 110.10945892   67.97978973  196.41313171  111.18352509]]
346
[[  34.          139.          305.          497.        ]
 [  58.64890671  102.55295563  248.44657898  499.39334106]
 [  70.28943634    0.          283.37341309  499.39334106]
 ..., 
 [   0.          457.67196655  199.83677673  499.39334106]
 [ 159.08818054  144.35603333  200.47140503  169.92576599]
 [ 210.96708679  376.49237061  259.42755127  415.43948364]]
333
[[  72.          109.          256.          338.        ]
 [ 215.          129.          258.          163.        ]
 [ 117.36717224    0.          286.9536438   376.40814209]
 ..., 
 [  11.85806274   61.62470245   39.38412476  102.46546936]
 [ 110.14412689  368.92001343  210.20845032  418.73886108]
 [ 111.18424225   92.91558075  158.84992981  129.59968567]]
342
[[  80.            3.          223.          495.        ]
 [  77.98069       0.          227.26165771  411.6635437 ]
 [  41.56110382    0.          231.07775879  499.32000732]
 ..., 
 [  38.47521591  127.46173096  145.14707947  475.71588135]
 [ 187.96090698  334.00796509  341.42999268  438.13296509]
 [   0.          132.19876099   25.61635208  168.18843079]]
284
[[ 180.          118.          189.          124.        ]
 [ 189.          133.          195.          148.        ]
 [ 198.          149.          202.          159.        ]
 ..., 
 [  85.75779724  174.44862366  156.06848145  208.79576111]
 [   4.05659676  205.89768982   31.78396034  257.01177979]
 [ 161.55560303  397.91986084  177.19525146  415.94012451]]
333
[[ 165.           15.          320.          228.        ]
 [ 161.96585083    0.          331.03399658  318.57867432]
 [ 172.72198486   49.53783035  332.44500732  227.43110657]
 ..., 
 [ 105.66156006  230.18757629  274.44207764  370.18325806]
 [  89.14315796  294.94442749  152.30966187  325.24163818]
 [ 224.87405396  301.97836304  252.81913757  333.05056763]]
333
[[  87.          110.          229.          418.        ]
 [ 139.          444.          170.          468.        ]
 [  80.2684021   165.7301178   251.61689758  499.5       ]
 ..., 
 [ 105.39730835   68.70614624  132.66438293   95.26115417]
 [ 284.98199463  147.03381348  300.8354187   171.48069763]
 [ 187.31622314   97.1109848   321.31411743  247.96308899]]
333
[[   0.            0.          330.          381.        ]
 [  26.          345.          279.          498.        ]
 [ 140.82063293    0.          332.44500732  299.03729248]
 ..., 
 [ 130.67134094  297.05422974  226.72840881  359.1138916 ]
 [ 213.80677795  234.08782959  260.80004883  320.03521729]
 [  42.51374817   55.85448074   73.16340637   97.37407684]]
500
[[ 285.          142.          469.          324.        ]
 [ 340.45510864    8.76142216  499.5         332.44500732]
 [ 283.94003296   80.10436249  499.5         332.44500732]
 ..., 
 [ 391.10177612   65.23480988  425.77294922  101.75164795]
 [ 444.92810059   58.46397781  461.30276489  119.92441559]
 [   9.42035103  101.72411346   31.55578995  140.84741211]]
375
[[  84.          138.          287.          345.        ]
 [  97.3963089   165.56567383  294.62008667  352.69613647]
 [  69.641716    192.95881653  245.94306946  374.58776855]
 ..., 
 [ 214.80569458  298.73529053  247.60346985  332.43823242]
 [   0.          144.28085327  180.42527771  276.62142944]
 [ 215.72753906   81.2638092   349.6468811   313.79031372]]
333
[[ 132.          271.          259.          475.        ]
 [  95.25823975  127.5710907   294.08374023  499.5       ]
 [  48.44109344  177.00172424  332.44500732  498.54156494]
 ..., 
 [ 140.36695862  232.54872131  202.32423401  272.72210693]
 [   0.          315.11785889   27.4889164   360.42303467]
 [ 291.27163696  136.17344666  309.56930542  170.52241516]]
477
[[  93.           25.          354.          302.        ]
 [  28.          165.          114.          298.        ]
 [  73.          322.          241.          484.        ]
 ..., 
 [  30.75812149   69.07041931   54.88082886  105.21984863]
 [ 314.09030151  116.5965271   359.46173096  164.79705811]
 [ 338.0569458   167.14855957  449.33740234  251.18952942]]
500
[[  97.          127.          206.          278.        ]
 [  83.           26.          161.          114.        ]
 [ 321.          116.          383.          181.        ]
 ..., 
 [ 457.89962769  147.30671692  499.36499023  212.35443115]
 [  79.34789276  218.82646179  105.77654266  240.40141296]
 [   6.30510235   41.27710342  271.50366211  163.77696228]]
220
[[  56.           23.          163.          197.        ]
 [  45.85385895    0.          191.77183533  228.80000305]
 [  90.16268921    0.          194.26301575  228.80000305]
 ..., 
 [ 195.4185791     0.          219.6333313    74.63804626]
 [ 167.89657593  213.05044556  191.16181946  228.80000305]
 [ 105.45383453    0.          171.75862122   73.89920807]]
500
[[   7.           57.          425.          267.        ]
 [ 432.           89.          467.          218.        ]
 [  12.70319939    0.          269.56326294  374.375     ]
 ..., 
 [ 456.17755127   56.49671173  478.10748291  101.68531036]
 [ 166.23049927  349.64129639  187.04310608  369.18118286]
 [ 271.34036255  309.96438599  293.90777588  332.7545166 ]]
500
[[  23.           23.          459.          257.        ]
 [ 263.76797485    0.          453.59057617  374.375     ]
 [ 192.69262695    0.          499.375       295.36810303]
 ..., 
 [  78.56860352  227.30371094  122.30918884  252.92146301]
 [ 113.45552063  128.49478149  194.19378662  193.10163879]
 [ 404.53155518  325.16708374  443.26467896  353.89526367]]
500
[[  64.            1.          454.          385.        ]
 [ 138.34483337    0.          376.56802368  386.35498047]
 [  51.71432114   24.12027931  239.29681396  386.35498047]
 ..., 
 [ 439.55599976  148.2580719   456.77069092  166.33535767]
 [ 379.14974976   61.91759872  418.58947754   97.39831543]
 [ 395.69393921  145.38148499  426.89660645  181.25170898]]
500
[[  13.          111.          196.          302.        ]
 [ 306.           92.          482.          299.        ]
 [ 230.30596924    0.          495.15270996  374.375     ]
 ..., 
 [ 355.93777466  274.56222534  413.51443481  304.4810791 ]
 [ 443.99731445  235.58242798  470.60595703  274.80426025]
 [ 455.58239746  265.94277954  473.26992798  300.81463623]]
333
[[  21.           47.          281.          467.        ]
 [ 101.98001099  100.81488037  330.87789917  499.5       ]
 [ 115.71648407    0.          316.15579224  414.12634277]
 ..., 
 [  42.20856094  193.13238525   59.65454483  210.65318298]
 [ 182.16104126   72.58738708  250.20191956  136.67546082]
 [ 145.10021973  275.24588013  240.95542908  334.89532471]]
333
[[  1.00000000e+00   1.73000000e+02   2.55000000e+02   4.01000000e+02]
 [  0.00000000e+00   3.97000000e+02   2.01000000e+02   4.69000000e+02]
 [  6.40000000e+01   2.53000000e+02   1.35000000e+02   3.29000000e+02]
 ..., 
 [  1.62293655e+02   2.84266442e-01   2.16410141e+02   6.92014160e+01]
 [  1.12060862e+01   8.82499847e+01   2.79820061e+01   1.18073837e+02]
 [  2.08465714e+02   3.45789368e+02   2.52608673e+02   4.04986206e+02]]
333
[[ 100.          232.          240.          407.        ]
 [ 141.          185.          180.          232.        ]
 [ 108.62248993  250.36245728  263.31231689  400.13516235]
 ..., 
 [  36.51841736  480.01727295   59.36198044  499.5       ]
 [ 122.58961487  457.66986084  142.15455627  480.13604736]
 [ 310.25646973  311.29855347  332.44500732  499.5       ]]
500
[[  84.          131.          176.          156.        ]
 [   8.32923412    0.          314.82983398  332.44500732]
 [  72.05284882  125.3589325   179.91291809  147.88986206]
 ..., 
 [ 455.72119141  159.03359985  499.5         247.4888916 ]
 [ 211.36047363  291.71908569  351.70568848  332.44500732]
 [ 283.22390747  122.6479187   342.54772949  164.87168884]]
500
[[  25.          236.           66.          310.        ]
 [  62.           47.          112.           86.        ]
 [   0.           47.           48.          168.        ]
 ..., 
 [ 114.25934601  312.85144043  196.86573792  333.44332886]
 [ 297.6741333   168.69534302  322.40957642  188.00695801]
 [ 464.86178589  254.41220093  483.36923218  286.48742676]]
500
[[  80.            5.          191.          211.        ]
 [ 290.           32.          403.          226.        ]
 [ 299.29293823   38.77653885  407.44955444  229.07969666]
 ..., 
 [ 146.3943634    24.75896454  168.49404907   97.95052338]
 [ 279.98492432   32.17062378  297.1295166    54.28508377]
 [ 149.79077148   49.58618164  176.77516174  122.11833954]]
433
[[  86.          299.          290.          486.        ]
 [  18.           12.          420.          280.        ]
 [  80.92353821  316.89059448  292.67245483  497.92904663]
 ..., 
 [ 334.54705811  436.63409424  353.98400879  460.89459229]
 [ 271.66784668   40.24639511  335.18170166   70.01725006]
 [ 347.02890015  176.51913452  421.68035889  235.959198  ]]
492
[[ 158.          298.          187.          335.        ]
 [ 156.3548584   308.44589233  186.63166809  351.86175537]
 [ 168.14498901  290.10778809  190.62315369  311.60702515]
 ..., 
 [ 408.43444824    9.77040195  451.78573608  233.72447205]
 [ 425.58956909  127.81082916  475.15567017  212.53718567]
 [ 203.18655396  153.83563232  427.84243774  363.16366577]]
375
[[   4.          114.          174.          483.        ]
 [ 137.          118.          223.          211.        ]
 [   0.          174.21682739  201.92462158  499.375     ]
 ..., 
 [  60.79763794  413.07293701  107.86203003  464.76992798]
 [ 119.8766861   388.17105103  150.78062439  442.2003479 ]
 [  13.12951088   93.73986816  100.10053253  147.31520081]]
335
[[  72.           17.          213.          310.        ]
 [ 185.          264.          260.          380.        ]
 [  22.80440903    0.          220.45237732  473.91174316]
 ..., 
 [ 267.53039551  203.46017456  319.1902771   321.93643188]
 [ 181.8658905   195.24368286  205.60745239  250.28445435]
 [ 221.45335388  101.74601746  291.92053223  157.50274658]]
335
[[ 142.          141.          215.          291.        ]
 [ 207.          183.          255.          361.        ]
 [ 180.          431.          218.          497.        ]
 ..., 
 [  87.39606476   14.95792866  161.24098206   51.22919846]
 [ 299.16186523  139.45701599  319.65670776  172.46681213]
 [  84.81800079  207.80226135  103.62711334  235.45680237]]
500
[[  5.40000000e+01   2.17000000e+02   4.72000000e+02   4.51000000e+02]
 [  0.00000000e+00   0.00000000e+00   3.11175629e+02   4.99166656e+02]
 [  8.70624619e+01   0.00000000e+00   3.95065857e+02   4.99166656e+02]
 ..., 
 [  2.52577866e+02   1.73578110e+02   3.64653046e+02   2.15547165e+02]
 [  1.83672516e+02   1.54129410e+02   2.27676941e+02   1.92620224e+02]
 [  7.79889297e+01   1.14827149e-01   1.23232292e+02   3.38131828e+01]]
500
[[ 319.          200.          478.          235.        ]
 [ 372.          156.          426.          199.        ]
 [ 211.24871826    0.          480.10595703  255.45762634]
 ..., 
 [ 338.81719971   69.94770813  421.45346069  116.60015869]
 [  73.55673981  112.64370728  308.55725098  178.69151306]
 [ 296.64230347    0.          376.43630981  148.40719604]]
410
[[ 143.           15.          348.          391.        ]
 [ 275.          446.          310.          475.        ]
 [ 220.          440.          262.          477.        ]
 ..., 
 [ 342.35238647  354.75079346  367.59967041  409.63891602]
 [  86.14667511  143.41140747  174.4777832   193.70472717]
 [ 317.07241821  182.67549133  345.58050537  214.19882202]]
500
[[ 215.          194.          323.          234.        ]
 [ 210.3926239   189.79086304  347.03604126  228.51571655]
 [   0.           68.16970825  184.42004395  374.375     ]
 ..., 
 [ 471.47949219  205.6464386   499.375       273.71960449]
 [ 431.8664856    73.71602631  496.36462402  125.2792511 ]
 [ 416.56842041   46.57058334  446.60458374   79.25914764]]
375
[[  93.          116.          294.          371.        ]
 [  94.4425354     0.          336.65863037  499.375     ]
 [ 114.22833252   67.81959534  374.375       499.375     ]
 ..., 
 [  46.40135574  242.11169434   64.36858368  265.50650024]
 [  92.77896118  171.16812134  127.33042908  199.67680359]
 [ 203.42837524  436.20455933  228.07383728  467.35577393]]
500
[[  83.          117.          206.          334.        ]
 [ 166.          199.          366.          347.        ]
 [ 204.          126.          246.          194.        ]
 ..., 
 [  19.63629341  312.74182129   44.77785492  345.08343506]
 [ 276.96279907  183.4037323   293.99551392  198.92082214]
 [ 249.51521301   89.03266144  269.58297729  111.97306824]]
500
[[ 119.           15.          400.          334.        ]
 [ 399.          115.          485.          270.        ]
 [ 405.           35.          436.          101.        ]
 ..., 
 [ 143.11968994  287.11315918  200.70251465  341.48748779]
 [  99.34791565  186.13098145  128.38394165  214.83248901]
 [ 240.07415771   30.79533577  269.02603149   62.82103348]]
333
[[  64.          160.          149.          278.        ]
 [  95.          279.          157.          344.        ]
 [ 281.          277.          307.          349.        ]
 ..., 
 [   0.          232.01130676   55.30578613  291.00753784]
 [ 297.29507446  352.62075806  314.23355103  383.05776978]
 [ 142.6539917   326.7517395   185.77024841  380.79452515]]
231
[[  30.           19.          177.          489.        ]
 [  22.0475769     5.72575378  204.027771    499.5       ]
 [  48.09938049   82.40042114  176.10588074  499.5       ]
 ..., 
 [ 145.58651733  240.87867737  168.68914795  261.61248779]
 [ 197.17558289  440.30212402  213.71006775  466.59979248]
 [  86.74845886  323.23300171  112.38853455  370.40481567]]
500
[[ 132.           55.          343.          283.        ]
 [  74.13214874    0.          295.82217407  332.44500732]
 [  63.94166565   58.51819229  282.62902832  223.11958313]
 ..., 
 [ 117.61959076   55.06201172  139.05493164   84.83323669]
 [ 124.8602066   116.62335205  153.65936279  153.79463196]
 [ 321.84402466  156.34390259  344.59658813  213.8525238 ]]
333
[[ 115.          165.          204.          373.        ]
 [ 117.0891037   180.24000549  218.46966553  390.69766235]
 [ 100.30644989  237.8679657   229.0378418   499.5       ]
 ..., 
 [ 130.78865051   26.1082592   158.19796753   51.12636566]
 [ 126.07645416  162.17858887  320.96420288  446.74087524]
 [ 223.37863159  247.35491943  247.41712952  281.02606201]]
375
[[  89.            1.          294.          463.        ]
 [ 107.63690186  175.29681396  332.30618286  499.375     ]
 [ 126.15054321   62.76229858  311.42504883  499.375     ]
 ..., 
 [  25.21856308  122.58365631   90.39482117  151.86567688]
 [  60.3141098    93.90209961  125.56351471  138.22784424]
 [ 296.75491333  430.05337524  323.49758911  455.85690308]]
500
[[ 161.          119.          407.          288.        ]
 [ 110.84218597    0.          347.00973511  374.375     ]
 [ 246.98728943    0.          454.19342041  374.375     ]
 ..., 
 [ 303.67550659  270.61431885  348.32156372  316.37356567]
 [  90.89756012  300.57232666  454.2041626   374.375     ]
 [ 313.34335327  318.98483276  355.77252197  369.91473389]]
500
[[   1.            5.          206.          351.        ]
 [ 208.            2.          498.          170.        ]
 [ 211.          179.          360.          352.        ]
 ..., 
 [ 272.76739502   35.83666992  375.27658081   97.33239746]
 [ 170.1499939   156.39607239  245.06256104  205.28231812]
 [ 229.66944885   78.546875    397.35601807  270.14440918]]
356
[[   0.           23.          333.          491.        ]
 [ 124.36456299   48.85569763  343.90664673  499.58666992]
 [  48.61813354   35.9468956   273.93734741  499.58666992]
 ..., 
 [ 122.25191498   11.44973564  153.59214783   28.41654205]
 [   0.          354.06643677  106.03857422  421.53491211]
 [   4.79039097  141.77505493   90.41358948  193.44134521]]
416
[[  49.            0.          173.          476.        ]
 [ 252.            0.          404.          498.        ]
 [ 216.35917664   54.40351868  415.30667114  499.19998169]
 ..., 
 [ 146.87294006  344.14822388  177.23352051  409.09133911]
 [ 208.13624573   50.86626434  226.78303528   74.60707855]
 [ 156.42889404  416.70535278  253.18515015  466.27453613]]
500
[[  80.           52.          434.          299.        ]
 [  58.16688919    0.          287.68734741  332.44500732]
 [  84.16947937   58.0155983   336.64517212  276.29589844]
 ..., 
 [ 453.58084106   83.53510284  487.97109985  115.99721527]
 [ 481.20272827  186.10258484  499.5         248.75132751]
 [  45.85906982  267.67666626  124.95566559  318.35641479]]
500
[[   0.            0.          499.          333.        ]
 [  39.7507019     0.          306.85916138  333.44332886]
 [ 319.73495483    0.          499.32998657  333.44332886]
 ..., 
 [ 219.6812439   255.42698669  308.40884399  297.93652344]
 [ 303.62051392  268.9319458   319.47540283  289.2192688 ]
 [ 103.01628876   51.81401062  132.25091553   80.42204285]]
433
[[ 195.          394.          288.          449.        ]
 [ 165.          394.          194.          430.        ]
 [ 119.          384.          182.          437.        ]
 ..., 
 [   0.          138.496521     18.15432739  200.39768982]
 [ 187.43505859    6.0610652   237.5110321    91.35636902]
 [ 256.24319458    5.8135457   292.97952271   42.83401489]]
500
[[ 197.          245.          260.          371.        ]
 [ 228.          269.          247.          305.        ]
 [ 191.          208.          244.          268.        ]
 ..., 
 [ 382.73825073  200.79788208  417.8425293   243.29930115]
 [ 124.40027618  101.61415863  202.2460022   159.5877533 ]
 [ 111.20069122  158.14985657  243.56103516  303.46417236]]
500
[[ 231.          186.          250.          216.        ]
 [  57.          218.           80.          241.        ]
 [ 342.           79.          359.          104.        ]
 ..., 
 [  57.38434219  127.63558197  283.91091919  249.81593323]
 [ 270.89709473  300.03070068  347.66571045  364.60180664]
 [  19.51305008  337.19104004   62.01497269  381.36334229]]
500
[[ 108.           88.          409.          327.        ]
 [ 194.9662323     0.          482.25552368  390.34832764]
 [ 165.16323853    0.          357.11523438  390.34832764]
 ..., 
 [ 419.98098755  369.59667969  445.25131226  390.34832764]
 [ 441.89559937  163.37365723  467.91253662  197.71379089]
 [  23.43098068   54.23422623  111.0317688   113.78512573]]
500
[[  56.           61.          430.          371.        ]
 [   1.1345768    39.37778473  245.0506897   374.375     ]
 [ 244.7494812     0.          498.56564331  374.375     ]
 ..., 
 [  53.57578278   57.48530197   80.89988708  110.15835571]
 [ 294.85272217  139.40155029  424.11038208  223.67294312]
 [  91.87795258   75.89624023  183.87315369  147.22906494]]
500
[[  36.          226.          399.          274.        ]
 [ 139.           88.          392.          151.        ]
 [ 133.6505127   108.13731384  423.79589844  146.35498047]
 ..., 
 [  63.6537056    57.54054642  139.31671143  111.57943726]
 [ 439.1100769    30.02074242  469.78182983   68.2338028 ]
 [ 369.29144287  142.99949646  395.45385742  161.80297852]]
500
[[ 124.            1.          443.          338.        ]
 [ 231.79277039    0.          426.19049072  340.43167114]
 [ 178.20925903    0.          474.25640869  340.43167114]
 ..., 
 [ 247.78990173  232.62013245  267.31362915  268.94113159]
 [ 208.40925598   54.40347672  416.35827637  175.13252258]
 [  87.81941223  211.52276611  120.72634888  239.66435242]]
500
[[ 290.          166.          416.          287.        ]
 [ 267.21035767   95.84766388  499.375       374.375     ]
 [ 290.89407349  179.48748779  420.07330322  295.21835327]
 ..., 
 [ 283.28536987  116.47689819  307.51358032  142.50265503]
 [ 301.96450806  242.76347351  338.98892212  288.34249878]
 [ 354.58340454  100.27370453  390.42324829  131.0874939 ]]
500
[[   0.            6.          216.          247.        ]
 [ 263.           57.          335.          194.        ]
 [ 334.           10.          494.          234.        ]
 ..., 
 [ 204.42449951   53.59655762  225.43884277   92.18980408]
 [ 461.38095093  207.73027039  495.26553345  248.89692688]
 [ 342.84017944  148.22122192  383.41964722  188.04321289]]
390
[[   6.            1.          126.          230.        ]
 [ 145.            1.          247.          229.        ]
 [ 284.            0.          335.          220.        ]
 ..., 
 [  35.09786987  346.80334473  109.52282715  421.05285645]
 [ 147.70043945  472.05551147  188.96742249  499.19998169]
 [ 355.05050659    2.28958726  389.3500061    75.17307281]]
500
[[  60.            0.          335.          315.        ]
 [   0.            0.           62.          374.        ]
 [ 335.            1.          465.          373.        ]
 ..., 
 [ 303.20059204  290.72317505  422.38571167  374.375     ]
 [ 233.16574097  285.89840698  270.63061523  327.3963623 ]
 [ 392.52868652    1.24344826  443.51980591   83.37336731]]
500
[[ 101.          109.          334.          295.        ]
 [ 320.           91.          497.          248.        ]
 [ 310.57974243  123.05313873  489.11392212  252.2041626 ]
 ..., 
 [ 481.56484985   18.51183319  499.375        82.53610229]
 [ 140.76438904  326.90655518  203.83609009  373.33630371]
 [  64.04497528    0.          176.33758545  216.21760559]]
500
[[ 345.           96.          374.          131.        ]
 [ 344.41244507  100.5721817   375.41497803  136.7900238 ]
 [ 340.11196899   96.7421875   372.10882568  129.73487854]
 ..., 
 [ 393.03256226  189.7996521   415.97583008  215.38226318]
 [   4.12712431   43.86449051   30.60481071   84.2935791 ]
 [   4.02545929   77.34057617   29.81318474  116.47642517]]
500
[[  30.           57.          266.          138.        ]
 [  54.          207.          334.          261.        ]
 [ 332.          177.          446.          403.        ]
 ..., 
 [ 440.79669189  137.18797302  457.48547363  155.68305969]
 [ 239.47692871   82.14614105  263.27902222  119.0933075 ]
 [ 416.5440979   103.04684448  432.890625    120.19386292]]
500
[[  1.40000000e+01   3.80000000e+01   3.34000000e+02   4.50000000e+02]
 [  3.34000000e+02   4.80000000e+01   4.97000000e+02   4.40000000e+02]
 [  5.72926064e+01   2.40146375e+00   3.32005890e+02   4.55239990e+02]
 ..., 
 [  3.21937683e+02   3.14887553e-01   4.41741669e+02   4.16389389e+01]
 [  2.89486084e+02   8.87440948e+01   3.64574127e+02   2.01965439e+02]
 [  9.84304733e+01   4.42036018e+01   1.28105453e+02   6.25464249e+01]]
375
[[   0.           35.          334.          497.        ]
 [ 334.           96.          373.          126.        ]
 [  89.39923859    0.          374.375       499.375     ]
 ..., 
 [ 198.6368103   325.77954102  269.91690063  416.49304199]
 [ 160.53407288  335.56622314  194.86940002  385.05307007]
 [  35.33607483   19.9061718   287.89797974  135.88381958]]
363
[[   4.            1.          334.          493.        ]
 [ 335.           19.          357.          122.        ]
 [  65.47335052    0.          276.78900146  434.1413269 ]
 ..., 
 [ 240.56851196  117.25492096  362.39498901  410.73605347]
 [ 185.23361206   25.03783798  211.08990479   55.12999725]
 [ 117.17403412  339.921875    362.39498901  499.125     ]]
500
[[  14.           59.          334.          319.        ]
 [ 334.          192.          348.          242.        ]
 [ 364.          238.          449.          314.        ]
 ..., 
 [  35.73378372  212.10267639   68.05567169  251.47935486]
 [   0.           94.22705078   72.50040436  251.62519836]
 [   0.          219.75361633   46.45449829  351.45309448]]
500
[[  22.            0.          334.          273.        ]
 [ 320.           96.          457.          358.        ]
 [ 121.67276001    0.          295.04672241  365.39001465]
 ..., 
 [ 286.24417114   55.65694809  328.32312012  108.991539  ]
 [ 292.81738281  210.50183105  363.95864868  281.15338135]
 [ 251.27337646   85.97444916  306.09887695  145.02430725]]
500
[[   3.           19.          121.          324.        ]
 [ 121.            7.          263.          328.        ]
 [ 263.            6.          402.          310.        ]
 ..., 
 [ 471.91921997  102.07380676  499.32998657  161.19371033]
 [ 168.75547791   77.6476593   302.50210571  260.48486328]
 [ 137.7069397    93.12915039  193.46022034  185.2979126 ]]
374
[[ 103.          109.          299.          310.        ]
 [ 243.           30.          350.          142.        ]
 [ 284.          185.          313.          264.        ]
 ..., 
 [ 329.86605835  283.56298828  352.56866455  316.46347046]
 [ 223.53123474  467.6178894   251.25019836  498.03292847]
 [ 155.99085999  386.12814331  245.13179016  452.68075562]]
500
[[   0.            0.          240.          331.        ]
 [ 260.            0.          313.          101.        ]
 [ 279.            0.          440.          166.        ]
 ..., 
 [ 458.63562012   15.37585449  478.3085022    64.29558563]
 [   2.264745     78.56562042   36.43696976  134.79441833]
 [ 450.50445557   64.82100677  482.25915527  102.96105957]]
500
[[ 230.          149.          394.          278.        ]
 [ 216.01719666   40.8367691   350.36184692  338.43499756]
 [ 224.91835022  158.66262817  397.98449707  284.99032593]
 ..., 
 [ 347.943573     37.83408737  379.20755005   67.27662659]
 [  23.58926773    0.          100.15779114   16.5579319 ]
 [ 239.83081055  195.48912048  340.30108643  271.1111145 ]]
500
[[ 285.           96.          413.          292.        ]
 [ 289.72381592   93.37968445  404.6991272   321.72595215]
 [ 241.95831299   99.1469574   441.0585022   333.44332886]
 ..., 
 [ 171.7615509    47.23845291  226.40284729   84.34736633]
 [   0.            0.           29.7996769    27.56350708]
 [ 245.30973816  297.6887207   304.34063721  333.44332886]]
500
[[ 151.          159.          226.          215.        ]
 [ 284.          145.          477.          259.        ]
 [ 463.          289.          479.  WARNING: Logging before InitGoogleLogging() is written to STDERR
I1025 23:43:32.098414 16980 solver.cpp:48] Initializing solver from parameters: 
train_net: "models/pascal_voc/ZF/faster_rcnn_alt_opt/stage1_fast_rcnn_train.pt"
base_lr: 0.001
display: 20
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 30000
snapshot: 0
snapshot_prefix: "zf_fast_rcnn"
average_loss: 100
I1025 23:43:32.098448 16980 solver.cpp:81] Creating training net from train_net file: models/pascal_voc/ZF/faster_rcnn_alt_opt/stage1_fast_rcnn_train.pt
I1025 23:43:32.105723 16980 net.cpp:49] Initializing net from parameters: 
name: "ZF"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Python"
  top: "data"
  top: "rois"
  top: "labels"
  top: "bbox_targets"
  top: "bbox_inside_weights"
  top: "bbox_outside_weights"
  python_param {
    module: "roi_data_layer.layer"
    layer: "RoIDataLayer"
    param_str: "\'num_classes\': 2"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 96
    pad: 3
    kernel_size: 7
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
    engine: CAFFE
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    stride: 2
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
    engine: CAFFE
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "roi_pool_conv5"
  type: "ROIPooling"
  bottom: "conv5"
  bottom: "rois"
  top: "roi_pool_conv5"
  roi_pooling_param {
    pooled_h: 6
    pooled_w: 6
    spatial_scale: 0.0625
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "roi_pool_conv5"
  top: "fc6"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
    scale_train: false
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
    scale_train: false
  }
}
layer {
  name: "cls_score"
  type: "InnerProduct"
  bottom: "fc7"
  top: "cls_score"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bbox_pred"
  type: "InnerProduct"
  bottom: "fc7"
  top: "bbox_pred"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss_cls"
  type: "SoftmaxWithLoss"
  bottom: "cls_score"
  bottom: "labels"
  top: "cls_loss"
  loss_weight: 1
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: -1
    normalize: true
  }
}
layer {
  name: "loss_bbox"
  type: "SmoothL1Loss"
  bottom: "bbox_pred"
  bottom: "bbox_targets"
  bottom: "bbox_inside_weights"
  bottom: "bbox_outside_weights"
  top: "bbox_loss"
  loss_weight: 1
}
layer {
  name: "rpn_conv1"
  type: "Convolution"
  bottom: "conv5"
  top: "rpn_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_relu1"
  type: "ReLU"
  bottom: "rpn_conv1"
  top: "rpn_conv1"
}
layer {
  name: "rpn_cls_score"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_cls_score"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 18
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_bbox_pred"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_bbox_pred"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 36
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "silence_rpn_cls_score"
  type: "Silence"
  bottom: "rpn_cls_score"
}
layer {
  name: "silence_rpn_bbox_pred"
  type: "Silence"
  bottom: "rpn_bbox_pred"
}
I1025 23:43:32.105851 16980 layer_factory.hpp:77] Creating layer data
I1025 23:43:32.106240 16980 net.cpp:106] Creating Layer data
I1025 23:43:32.106248 16980 net.cpp:411] data -> data
I1025 23:43:32.106256 16980 net.cpp:411] data -> rois
I1025 23:43:32.106259 16980 net.cpp:411] data -> labels
I1025 23:43:32.106263 16980 net.cpp:411] data -> bbox_targets
I1025 23:43:32.106266 16980 net.cpp:411] data -> bbox_inside_weights
I1025 23:43:32.106271 16980 net.cpp:411] data -> bbox_outside_weights
I1025 23:43:32.112805 16980 net.cpp:150] Setting up data
I1025 23:43:32.112828 16980 net.cpp:157] Top shape: 2 3 600 1000 (3600000)
I1025 23:43:32.112830 16980 net.cpp:157] Top shape: 1 5 (5)
I1025 23:43:32.112833 16980 net.cpp:157] Top shape: 1 (1)
I1025 23:43:32.112833 16980 net.cpp:157] Top shape: 1 8 (8)
I1025 23:43:32.112835 16980 net.cpp:157] Top shape: 1 8 (8)
I1025 23:43:32.112838 16980 net.cpp:157] Top shape: 1 8 (8)
I1025 23:43:32.112840 16980 net.cpp:165] Memory required for data: 14400120
I1025 23:43:32.112856 16980 layer_factory.hpp:77] Creating layer conv1
I1025 23:43:32.112874 16980 net.cpp:106] Creating Layer conv1
I1025 23:43:32.112889 16980 net.cpp:454] conv1 <- data
I1025 23:43:32.112892 16980 net.cpp:411] conv1 -> conv1
I1025 23:43:32.217070 16980 net.cpp:150] Setting up conv1
I1025 23:43:32.217092 16980 net.cpp:157] Top shape: 2 96 300 500 (28800000)
I1025 23:43:32.217094 16980 net.cpp:165] Memory required for data: 129600120
I1025 23:43:32.217106 16980 layer_factory.hpp:77] Creating layer relu1
I1025 23:43:32.217118 16980 net.cpp:106] Creating Layer relu1
I1025 23:43:32.217133 16980 net.cpp:454] relu1 <- conv1
I1025 23:43:32.217139 16980 net.cpp:397] relu1 -> conv1 (in-place)
I1025 23:43:32.217344 16980 net.cpp:150] Setting up relu1
I1025 23:43:32.217351 16980 net.cpp:157] Top shape: 2 96 300 500 (28800000)
I1025 23:43:32.217365 16980 net.cpp:165] Memory required for data: 244800120
I1025 23:43:32.217367 16980 layer_factory.hpp:77] Creating layer norm1
I1025 23:43:32.217387 16980 net.cpp:106] Creating Layer norm1
I1025 23:43:32.217391 16980 net.cpp:454] norm1 <- conv1
I1025 23:43:32.217394 16980 net.cpp:411] norm1 -> norm1
I1025 23:43:32.217486 16980 net.cpp:150] Setting up norm1
I1025 23:43:32.217491 16980 net.cpp:157] Top shape: 2 96 300 500 (28800000)
I1025 23:43:32.217492 16980 net.cpp:165] Memory required for data: 360000120
I1025 23:43:32.217504 16980 layer_factory.hpp:77] Creating layer pool1
I1025 23:43:32.217509 16980 net.cpp:106] Creating Layer pool1
I1025 23:43:32.217511 16980 net.cpp:454] pool1 <- norm1
I1025 23:43:32.217514 16980 net.cpp:411] pool1 -> pool1
I1025 23:43:32.217538 16980 net.cpp:150] Setting up pool1
I1025 23:43:32.217543 16980 net.cpp:157] Top shape: 2 96 151 251 (7276992)
I1025 23:43:32.217546 16980 net.cpp:165] Memory required for data: 389108088
I1025 23:43:32.217548 16980 layer_factory.hpp:77] Creating layer conv2
I1025 23:43:32.217558 16980 net.cpp:106] Creating Layer conv2
I1025 23:43:32.217561 16980 net.cpp:454] conv2 <- pool1
I1025 23:43:32.217566 16980 net.cpp:411] conv2 -> conv2
I1025 23:43:32.218992 16980 net.cpp:150] Setting up conv2
I1025 23:43:32.219000 16980 net.cpp:157] Top shape: 2 256 76 126 (4902912)
I1025 23:43:32.219002 16980 net.cpp:165] Memory required for data: 408719736
I1025 23:43:32.219008 16980 layer_factory.hpp:77] Creating layer relu2
I1025 23:43:32.219012 16980 net.cpp:106] Creating Layer relu2
I1025 23:43:32.219013 16980 net.cpp:454] relu2 <- conv2
I1025 23:43:32.219017 16980 net.cpp:397] relu2 -> conv2 (in-place)
I1025 23:43:32.219252 16980 net.cpp:150] Setting up relu2
I1025 23:43:32.219260 16980 net.cpp:157] Top shape: 2 256 76 126 (4902912)
I1025 23:43:32.219262 16980 net.cpp:165] Memory required for data: 428331384
I1025 23:43:32.219264 16980 layer_factory.hpp:77] Creating layer norm2
I1025 23:43:32.219270 16980 net.cpp:106] Creating Layer norm2
I1025 23:43:32.219274 16980 net.cpp:454] norm2 <- conv2
I1025 23:43:32.219279 16980 net.cpp:411] norm2 -> norm2
I1025 23:43:32.219355 16980 net.cpp:150] Setting up norm2
I1025 23:43:32.219362 16980 net.cpp:157] Top shape: 2 256 76 126 (4902912)
I1025 23:43:32.219372 16980 net.cpp:165] Memory required for data: 447943032
I1025 23:43:32.219374 16980 layer_factory.hpp:77] Creating layer pool2
I1025 23:43:32.219379 16980 net.cpp:106] Creating Layer pool2
I1025 23:43:32.219383 16980 net.cpp:454] pool2 <- norm2
I1025 23:43:32.219388 16980 net.cpp:411] pool2 -> pool2
I1025 23:43:32.219421 16980 net.cpp:150] Setting up pool2
I1025 23:43:32.219425 16980 net.cpp:157] Top shape: 2 256 39 64 (1277952)
I1025 23:43:32.219427 16980 net.cpp:165] Memory required for data: 453054840
I1025 23:43:32.219444 16980 layer_factory.hpp:77] Creating layer conv3
I1025 23:43:32.219454 16980 net.cpp:106] Creating Layer conv3
I1025 23:43:32.219456 16980 net.cpp:454] conv3 <- pool2
I1025 23:43:32.219460 16980 net.cpp:411] conv3 -> conv3
I1025 23:43:32.221727 16980 net.cpp:150] Setting up conv3
I1025 23:43:32.221746 16980 net.cpp:157] Top shape: 2 384 39 64 (1916928)
I1025 23:43:32.221750 16980 net.cpp:165] Memory required for data: 460722552
I1025 23:43:32.221771 16980 layer_factory.hpp:77] Creating layer relu3
I1025 23:43:32.221791 16980 net.cpp:106] Creating Layer relu3
I1025 23:43:32.221796 16980 net.cpp:454] relu3 <- conv3
I1025 23:43:32.221798 16980 net.cpp:397] relu3 -> conv3 (in-place)
I1025 23:43:32.221937 16980 net.cpp:150] Setting up relu3
I1025 23:43:32.221942 16980 net.cpp:157] Top shape: 2 384 39 64 (1916928)
I1025 23:43:32.221945 16980 net.cpp:165] Memory required for data: 468390264
I1025 23:43:32.221956 16980 layer_factory.hpp:77] Creating layer conv4
I1025 23:43:32.221963 16980 net.cpp:106] Creating Layer conv4
I1025 23:43:32.221966 16980 net.cpp:454] conv4 <- conv3
I1025 23:43:32.221971 16980 net.cpp:411] conv4 -> conv4
I1025 23:43:32.224303 16980 net.cpp:150] Setting up conv4
I1025 23:43:32.224318 16980 net.cpp:157] Top shape: 2 384 39 64 (1916928)
I1025 23:43:32.224319 16980 net.cpp:165] Memory required for data: 476057976
I1025 23:43:32.224324 16980 layer_factory.hpp:77] Creating layer relu4
I1025 23:43:32.224329 16980 net.cpp:106] Creating Layer relu4
I1025 23:43:32.224333 16980 net.cpp:454] relu4 <- conv4
I1025 23:43:32.224336 16980 net.cpp:397] relu4 -> conv4 (in-place)
I1025 23:43:32.224586 16980 net.cpp:150] Setting up relu4
I1025 23:43:32.224594 16980 net.cpp:157] Top shape: 2 384 39 64 (1916928)
I1025 23:43:32.224606 16980 net.cpp:165] Memory required for data: 483725688
I1025 23:43:32.224607 16980 layer_factory.hpp:77] Creating layer conv5
I1025 23:43:32.224623 16980 net.cpp:106] Creating Layer conv5
I1025 23:43:32.224625 16980 net.cpp:454] conv5 <- conv4
I1025 23:43:32.224632 16980 net.cpp:411] conv5 -> conv5
I1025 23:43:32.226330 16980 net.cpp:150] Setting up conv5
I1025 23:43:32.226341 16980 net.cpp:157] Top shape: 2 256 39 64 (1277952)
I1025 23:43:32.226341 16980 net.cpp:165] Memory required for data: 488837496
I1025 23:43:32.226349 16980 layer_factory.hpp:77] Creating layer relu5
I1025 23:43:32.226371 16980 net.cpp:106] Creating Layer relu5
I1025 23:43:32.226385 16980 net.cpp:454] relu5 <- conv5
I1025 23:43:32.226390 16980 net.cpp:397] relu5 -> conv5 (in-place)
I1025 23:43:32.226635 16980 net.cpp:150] Setting up relu5
I1025 23:43:32.226642 16980 net.cpp:157] Top shape: 2 256 39 64 (1277952)
I1025 23:43:32.226644 16980 net.cpp:165] Memory required for data: 493949304
I1025 23:43:32.226646 16980 layer_factory.hpp:77] Creating layer conv5_relu5_0_split
I1025 23:43:32.226650 16980 net.cpp:106] Creating Layer conv5_relu5_0_split
I1025 23:43:32.226652 16980 net.cpp:454] conv5_relu5_0_split <- conv5
I1025 23:43:32.226656 16980 net.cpp:411] conv5_relu5_0_split -> conv5_relu5_0_split_0
I1025 23:43:32.226671 16980 net.cpp:411] conv5_relu5_0_split -> conv5_relu5_0_split_1
I1025 23:43:32.226723 16980 net.cpp:150] Setting up conv5_relu5_0_split
I1025 23:43:32.226727 16980 net.cpp:157] Top shape: 2 256 39 64 (1277952)
I1025 23:43:32.226739 16980 net.cpp:157] Top shape: 2 256 39 64 (1277952)
I1025 23:43:32.226740 16980 net.cpp:165] Memory required for data: 504172920
I1025 23:43:32.226742 16980 layer_factory.hpp:77] Creating layer roi_pool_conv5
I1025 23:43:32.226763 16980 net.cpp:106] Creating Layer roi_pool_conv5
I1025 23:43:32.226765 16980 net.cpp:454] roi_pool_conv5 <- conv5_relu5_0_split_0
I1025 23:43:32.226783 16980 net.cpp:454] roi_pool_conv5 <- rois
I1025 23:43:32.226785 16980 net.cpp:411] roi_pool_conv5 -> roi_pool_conv5
I1025 23:43:32.226789 16980 roi_pooling_layer.cpp:30] Spatial scale: 0.0625
I1025 23:43:32.226843 16980 net.cpp:150] Setting up roi_pool_conv5
I1025 23:43:32.226846 16980 net.cpp:157] Top shape: 1 256 6 6 (9216)
I1025 23:43:32.226848 16980 net.cpp:165] Memory required for data: 504209784
I1025 23:43:32.226850 16980 layer_factory.hpp:77] Creating layer fc6
I1025 23:43:32.226855 16980 net.cpp:106] Creating Layer fc6
I1025 23:43:32.226858 16980 net.cpp:454] fc6 <- roi_pool_conv5
I1025 23:43:32.226861 16980 net.cpp:411] fc6 -> fc6
I1025 23:43:32.272431 16980 net.cpp:150] Setting up fc6
I1025 23:43:32.272455 16980 net.cpp:157] Top shape: 1 4096 (4096)
I1025 23:43:32.272457 16980 net.cpp:165] Memory required for data: 504226168
I1025 23:43:32.272465 16980 layer_factory.hpp:77] Creating layer relu6
I1025 23:43:32.272475 16980 net.cpp:106] Creating Layer relu6
I1025 23:43:32.272477 16980 net.cpp:454] relu6 <- fc6
I1025 23:43:32.272492 16980 net.cpp:397] relu6 -> fc6 (in-place)
I1025 23:43:32.272862 16980 net.cpp:150] Setting up relu6
I1025 23:43:32.272869 16980 net.cpp:157] Top shape: 1 4096 (4096)
I1025 23:43:32.272871 16980 net.cpp:165] Memory required for data: 504242552
I1025 23:43:32.272873 16980 layer_factory.hpp:77] Creating layer drop6
I1025 23:43:32.272878 16980 net.cpp:106] Creating Layer drop6
I1025 23:43:32.272881 16980 net.cpp:454] drop6 <- fc6
I1025 23:43:32.272883 16980 net.cpp:397] drop6 -> fc6 (in-place)
I1025 23:43:32.272925 16980 net.cpp:150] Setting up drop6
I1025 23:43:32.272929 16980 net.cpp:157] Top shape: 1 4096 (4096)
I1025 23:43:32.272930 16980 net.cpp:165] Memory required for data: 504258936
I1025 23:43:32.272943 16980 layer_factory.hpp:77] Creating layer fc7
I1025 23:43:32.272948 16980 net.cpp:106] Creating Layer fc7
I1025 23:43:32.272951 16980 net.cpp:454] fc7 <- fc6
I1025 23:43:32.272956 16980 net.cpp:411] fc7 -> fc7
I1025 23:43:32.293972 16980 net.cpp:150] Setting up fc7
I1025 23:43:32.294004 16980 net.cpp:157] Top shape: 1 4096 (4096)
I1025 23:43:32.294006 16980 net.cpp:165] Memory required for data: 504275320
I1025 23:43:32.294014 16980 layer_factory.hpp:77] Creating layer relu7
I1025 23:43:32.294033 16980 net.cpp:106] Creating Layer relu7
I1025 23:43:32.294036 16980 net.cpp:454] relu7 <- fc7
I1025 23:43:32.294044 16980 net.cpp:397] relu7 -> fc7 (in-place)
I1025 23:43:32.294236 16980 net.cpp:150] Setting up relu7
I1025 23:43:32.294242 16980 net.cpp:157] Top shape: 1 4096 (4096)
I1025 23:43:32.294245 16980 net.cpp:165] Memory required for data: 504291704
I1025 23:43:32.294256 16980 layer_factory.hpp:77] Creating layer drop7
I1025 23:43:32.294261 16980 net.cpp:106] Creating Layer drop7
I1025 23:43:32.294275 16980 net.cpp:454] drop7 <- fc7
I1025 23:43:32.294281 16980 net.cpp:397] drop7 -> fc7 (in-place)
I1025 23:43:32.294304 16980 net.cpp:150] Setting up drop7
I1025 23:43:32.294308 16980 net.cpp:157] Top shape: 1 4096 (4096)
I1025 23:43:32.294312 16980 net.cpp:165] Memory required for data: 504308088
I1025 23:43:32.294314 16980 layer_factory.hpp:77] Creating layer fc7_drop7_0_split
I1025 23:43:32.294320 16980 net.cpp:106] Creating Layer fc7_drop7_0_split
I1025 23:43:32.294323 16980 net.cpp:454] fc7_drop7_0_split <- fc7
I1025 23:43:32.294329 16980 net.cpp:411] fc7_drop7_0_split -> fc7_drop7_0_split_0
I1025 23:43:32.294335 16980 net.cpp:411] fc7_drop7_0_split -> fc7_drop7_0_split_1
I1025 23:43:32.294364 16980 net.cpp:150] Setting up fc7_drop7_0_split
I1025 23:43:32.294368 16980 net.cpp:157] Top shape: 1 4096 (4096)
I1025 23:43:32.294373 16980 net.cpp:157] Top shape: 1 4096 (4096)
I1025 23:43:32.294376 16980 net.cpp:165] Memory required for data: 504340856
I1025 23:43:32.294378 16980 layer_factory.hpp:77] Creating layer cls_score
I1025 23:43:32.294385 16980 net.cpp:106] Creating Layer cls_score
I1025 23:43:32.294389 16980 net.cpp:454] cls_score <- fc7_drop7_0_split_0
I1025 23:43:32.294394 16980 net.cpp:411] cls_score -> cls_score
I1025 23:43:32.294555 16980 net.cpp:150] Setting up cls_score
I1025 23:43:32.294560 16980 net.cpp:157] Top shape: 1 2 (2)
I1025 23:43:32.294564 16980 net.cpp:165] Memory required for data: 504340864
I1025 23:43:32.294569 16980 layer_factory.hpp:77] Creating layer bbox_pred
I1025 23:43:32.294574 16980 net.cpp:106] Creating Layer bbox_pred
I1025 23:43:32.294577 16980 net.cpp:454] bbox_pred <- fc7_drop7_0_split_1
I1025 23:43:32.294584 16980 net.cpp:411] bbox_pred -> bbox_pred
I1025 23:43:32.295260 16980 net.cpp:150] Setting up bbox_pred
I1025 23:43:32.295269 16980 net.cpp:157] Top shape: 1 8 (8)
I1025 23:43:32.295272 16980 net.cpp:165] Memory required for data: 504340896
I1025 23:43:32.295281 16980 layer_factory.hpp:77] Creating layer loss_cls
I1025 23:43:32.295289 16980 net.cpp:106] Creating Layer loss_cls
I1025 23:43:32.295291 16980 net.cpp:454] loss_cls <- cls_score
I1025 23:43:32.295295 16980 net.cpp:454] loss_cls <- labels
I1025 23:43:32.295311 16980 net.cpp:411] loss_cls -> cls_loss
I1025 23:43:32.295320 16980 layer_factory.hpp:77] Creating layer loss_cls
I1025 23:43:32.295632 16980 net.cpp:150] Setting up loss_cls
I1025 23:43:32.295639 16980 net.cpp:157] Top shape: (1)
I1025 23:43:32.295641 16980 net.cpp:160]     with loss weight 1
I1025 23:43:32.295660 16980 net.cpp:165] Memory required for data: 504340900
I1025 23:43:32.295661 16980 layer_factory.hpp:77] Creating layer loss_bbox
I1025 23:43:32.295666 16980 net.cpp:106] Creating Layer loss_bbox
I1025 23:43:32.295668 16980 net.cpp:454] loss_bbox <- bbox_pred
I1025 23:43:32.295671 16980 net.cpp:454] loss_bbox <- bbox_targets
I1025 23:43:32.295675 16980 net.cpp:454] loss_bbox <- bbox_inside_weights
I1025 23:43:32.295677 16980 net.cpp:454] loss_bbox <- bbox_outside_weights
I1025 23:43:32.295681 16980 net.cpp:411] loss_bbox -> bbox_loss
I1025 23:43:32.295745 16980 net.cpp:150] Setting up loss_bbox
I1025 23:43:32.295749 16980 net.cpp:157] Top shape: (1)
I1025 23:43:32.295753 16980 net.cpp:160]     with loss weight 1
I1025 23:43:32.295756 16980 net.cpp:165] Memory required for data: 504340904
I1025 23:43:32.295759 16980 layer_factory.hpp:77] Creating layer rpn_conv1
I1025 23:43:32.295778 16980 net.cpp:106] Creating Layer rpn_conv1
I1025 23:43:32.295781 16980 net.cpp:454] rpn_conv1 <- conv5_relu5_0_split_1
I1025 23:43:32.295785 16980 net.cpp:411] rpn_conv1 -> rpn_conv1
I1025 23:43:32.301293 16980 net.cpp:150] Setting up rpn_conv1
I1025 23:43:32.301302 16980 net.cpp:157] Top shape: 2 256 39 64 (1277952)
I1025 23:43:32.301306 16980 net.cpp:165] Memory required for data: 509452712
I1025 23:43:32.301319 16980 layer_factory.hpp:77] Creating layer rpn_relu1
I1025 23:43:32.301323 16980 net.cpp:106] Creating Layer rpn_relu1
I1025 23:43:32.301326 16980 net.cpp:454] rpn_relu1 <- rpn_conv1
I1025 23:43:32.301329 16980 net.cpp:397] rpn_relu1 -> rpn_conv1 (in-place)
I1025 23:43:32.301563 16980 net.cpp:150] Setting up rpn_relu1
I1025 23:43:32.301569 16980 net.cpp:157] Top shape: 2 256 39 64 (1277952)
I1025 23:43:32.301573 16980 net.cpp:165] Memory required for data: 514564520
I1025 23:43:32.301584 16980 layer_factory.hpp:77] Creating layer rpn_conv1_rpn_relu1_0_split
I1025 23:43:32.301589 16980 net.cpp:106] Creating Layer rpn_conv1_rpn_relu1_0_split
I1025 23:43:32.301592 16980 net.cpp:454] rpn_conv1_rpn_relu1_0_split <- rpn_conv1
I1025 23:43:32.301599 16980 net.cpp:411] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_0
I1025 23:43:32.301604 16980 net.cpp:411] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_1
I1025 23:43:32.301633 16980 net.cpp:150] Setting up rpn_conv1_rpn_relu1_0_split
I1025 23:43:32.301637 16980 net.cpp:157] Top shape: 2 256 39 64 (1277952)
I1025 23:43:32.301640 16980 net.cpp:157] Top shape: 2 256 39 64 (1277952)
I1025 23:43:32.301642 16980 net.cpp:165] Memory required for data: 524788136
I1025 23:43:32.301643 16980 layer_factory.hpp:77] Creating layer rpn_cls_score
I1025 23:43:32.301651 16980 net.cpp:106] Creating Layer rpn_cls_score
I1025 23:43:32.301653 16980 net.cpp:454] rpn_cls_score <- rpn_conv1_rpn_relu1_0_split_0
I1025 23:43:32.301657 16980 net.cpp:411] rpn_cls_score -> rpn_cls_score
I1025 23:43:32.302374 16980 net.cpp:150] Setting up rpn_cls_score
I1025 23:43:32.302382 16980 net.cpp:157] Top shape: 2 18 39 64 (89856)
I1025 23:43:32.302384 16980 net.cpp:165] Memory required for data: 525147560
I1025 23:43:32.302388 16980 layer_factory.hpp:77] Creating layer rpn_bbox_pred
I1025 23:43:32.302399 16980 net.cpp:106] Creating Layer rpn_bbox_pred
I1025 23:43:32.302402 16980 net.cpp:454] rpn_bbox_pred <- rpn_conv1_rpn_relu1_0_split_1
I1025 23:43:32.302407 16980 net.cpp:411] rpn_bbox_pred -> rpn_bbox_pred
I1025 23:43:32.303158 16980 net.cpp:150] Setting up rpn_bbox_pred
I1025 23:43:32.303167 16980 net.cpp:157] Top shape: 2 36 39 64 (179712)
I1025 23:43:32.303169 16980 net.cpp:165] Memory required for data: 525866408
I1025 23:43:32.303174 16980 layer_factory.hpp:77] Creating layer silence_rpn_cls_score
I1025 23:43:32.303179 16980 net.cpp:106] Creating Layer silence_rpn_cls_score
I1025 23:43:32.303182 16980 net.cpp:454] silence_rpn_cls_score <- rpn_cls_score
I1025 23:43:32.303187 16980 net.cpp:150] Setting up silence_rpn_cls_score
I1025 23:43:32.303200 16980 net.cpp:165] Memory required for data: 525866408
I1025 23:43:32.303202 16980 layer_factory.hpp:77] Creating layer silence_rpn_bbox_pred
I1025 23:43:32.303206 16980 net.cpp:106] Creating Layer silence_rpn_bbox_pred
I1025 23:43:32.303210 16980 net.cpp:454] silence_rpn_bbox_pred <- rpn_bbox_pred
I1025 23:43:32.303212 16980 net.cpp:150] Setting up silence_rpn_bbox_pred
I1025 23:43:32.303215 16980 net.cpp:165] Memory required for data: 525866408
I1025 23:43:32.303218 16980 net.cpp:228] silence_rpn_bbox_pred does not need backward computation.
I1025 23:43:32.303221 16980 net.cpp:228] silence_rpn_cls_score does not need backward computation.
I1025 23:43:32.303225 16980 net.cpp:228] rpn_bbox_pred does not need backward computation.
I1025 23:43:32.303228 16980 net.cpp:228] rpn_cls_score does not need backward computation.
I1025 23:43:32.303232 16980 net.cpp:228] rpn_conv1_rpn_relu1_0_split does not need backward computation.
I1025 23:43:32.303236 16980 net.cpp:228] rpn_relu1 does not need backward computation.
I1025 23:43:32.303237 16980 net.cpp:228] rpn_conv1 does not need backward computation.
I1025 23:43:32.303241 16980 net.cpp:226] loss_bbox needs backward computation.
I1025 23:43:32.303246 16980 net.cpp:226] loss_cls needs backward computation.
I1025 23:43:32.303248 16980 net.cpp:226] bbox_pred needs backward computation.
I1025 23:43:32.303251 16980 net.cpp:226] cls_score needs backward computation.
I1025 23:43:32.303253 16980 net.cpp:226] fc7_drop7_0_split needs backward computation.
I1025 23:43:32.303257 16980 net.cpp:226] drop7 needs backward computation.
I1025 23:43:32.303259 16980 net.cpp:226] relu7 needs backward computation.
I1025 23:43:32.303263 16980 net.cpp:226] fc7 needs backward computation.
I1025 23:43:32.303266 16980 net.cpp:226] drop6 needs backward computation.
I1025 23:43:32.303269 16980 net.cpp:226] relu6 needs backward computation.
I1025 23:43:32.303272 16980 net.cpp:226] fc6 needs backward computation.
I1025 23:43:32.303275 16980 net.cpp:226] roi_pool_conv5 needs backward computation.
I1025 23:43:32.303279 16980 net.cpp:226] conv5_relu5_0_split needs backward computation.
I1025 23:43:32.303282 16980 net.cpp:226] relu5 needs backward computation.
I1025 23:43:32.303285 16980 net.cpp:226] conv5 needs backward computation.
I1025 23:43:32.303289 16980 net.cpp:226] relu4 needs backward computation.
I1025 23:43:32.303292 16980 net.cpp:226] conv4 needs backward computation.
I1025 23:43:32.303295 16980 net.cpp:226] relu3 needs backward computation.
I1025 23:43:32.303298 16980 net.cpp:226] conv3 needs backward computation.
I1025 23:43:32.303302 16980 net.cpp:226] pool2 needs backward computation.
I1025 23:43:32.303305 16980 net.cpp:226] norm2 needs backward computation.
I1025 23:43:32.303308 16980 net.cpp:226] relu2 needs backward computation.
I1025 23:43:32.303311 16980 net.cpp:226] conv2 needs backward computation.
I1025 23:43:32.303314 16980 net.cpp:226] pool1 needs backward computation.
I1025 23:43:32.303318 16980 net.cpp:226] norm1 needs backward computation.
I1025 23:43:32.303320 16980 net.cpp:226] relu1 needs backward computation.
I1025 23:43:32.303323 16980 net.cpp:226] conv1 needs backward computation.
I1025 23:43:32.303328 16980 net.cpp:228] data does not need backward computation.
I1025 23:43:32.303330 16980 net.cpp:270] This network produces output bbox_loss
I1025 23:43:32.303333 16980 net.cpp:270] This network produces output cls_loss
I1025 23:43:32.303351 16980 net.cpp:283] Network initialization done.
I1025 23:43:32.303467 16980 solver.cpp:60] Solver scaffolding done.
I1025 23:43:32.383121 16980 net.cpp:816] Ignoring source layer pool5_spm6
I1025 23:43:32.383141 16980 net.cpp:816] Ignoring source layer pool5_spm6_flatten
I1025 23:43:32.415798 16980 net.cpp:816] Ignoring source layer fc8
I1025 23:43:32.415813 16980 net.cpp:816] Ignoring source layer prob
/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/roi_data_layer/minibatch.py:100: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future
  fg_inds, size=fg_rois_per_this_image, replace=False)
/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/roi_data_layer/minibatch.py:113: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future
  bg_inds, size=bg_rois_per_this_image, replace=False)
/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/roi_data_layer/minibatch.py:120: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future
  labels[fg_rois_per_this_image:] = 0
/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/roi_data_layer/minibatch.py:176: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future
  bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]
/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/roi_data_layer/minibatch.py:177: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future
  bbox_inside_weights[ind, start:end] = cfg.TRAIN.BBOX_INSIDE_WEIGHTS
I1025 23:43:32.539558 16980 solver.cpp:229] Iteration 0, loss = 1.20823
I1025 23:43:32.539589 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.386131 (* 1 = 0.386131 loss)
I1025 23:43:32.539594 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.822097 (* 1 = 0.822097 loss)
I1025 23:43:32.539604 16980 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I1025 23:43:36.057206 16980 solver.cpp:229] Iteration 20, loss = 0.920904
I1025 23:43:36.057235 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.400721 (* 1 = 0.400721 loss)
I1025 23:43:36.057240 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.520182 (* 1 = 0.520182 loss)
I1025 23:43:36.057243 16980 sgd_solver.cpp:106] Iteration 20, lr = 0.001
I1025 23:43:39.541427 16980 solver.cpp:229] Iteration 40, loss = 0.754029
I1025 23:43:39.541458 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.462404 (* 1 = 0.462404 loss)
I1025 23:43:39.541465 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.291625 (* 1 = 0.291625 loss)
I1025 23:43:39.541471 16980 sgd_solver.cpp:106] Iteration 40, lr = 0.001
I1025 23:43:43.090778 16980 solver.cpp:229] Iteration 60, loss = 0.737862
I1025 23:43:43.090809 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.457492 (* 1 = 0.457492 loss)
I1025 23:43:43.090813 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.28037 (* 1 = 0.28037 loss)
I1025 23:43:43.090817 16980 sgd_solver.cpp:106] Iteration 60, lr = 0.001
I1025 23:43:46.630432 16980 solver.cpp:229] Iteration 80, loss = 0.944459
I1025 23:43:46.630460 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.441022 (* 1 = 0.441022 loss)
I1025 23:43:46.630465 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.503437 (* 1 = 0.503437 loss)
I1025 23:43:46.630470 16980 sgd_solver.cpp:106] Iteration 80, lr = 0.001
I1025 23:43:50.060045 16980 solver.cpp:229] Iteration 100, loss = 0.781624
I1025 23:43:50.060080 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.326772 (* 1 = 0.326772 loss)
I1025 23:43:50.060087 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.454851 (* 1 = 0.454851 loss)
I1025 23:43:50.060093 16980 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I1025 23:43:53.616884 16980 solver.cpp:229] Iteration 120, loss = 0.784408
I1025 23:43:53.616916 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.342285 (* 1 = 0.342285 loss)
I1025 23:43:53.616921 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.442123 (* 1 = 0.442123 loss)
I1025 23:43:53.616925 16980 sgd_solver.cpp:106] Iteration 120, lr = 0.001
I1025 23:43:57.372146 16980 solver.cpp:229] Iteration 140, loss = 0.570536
I1025 23:43:57.372179 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.372782 (* 1 = 0.372782 loss)
I1025 23:43:57.372184 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.197754 (* 1 = 0.197754 loss)
I1025 23:43:57.372187 16980 sgd_solver.cpp:106] Iteration 140, lr = 0.001
I1025 23:44:01.212313 16980 solver.cpp:229] Iteration 160, loss = 0.561852
I1025 23:44:01.212344 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.214428 (* 1 = 0.214428 loss)
I1025 23:44:01.212349 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.347423 (* 1 = 0.347423 loss)
I1025 23:44:01.212355 16980 sgd_solver.cpp:106] Iteration 160, lr = 0.001
I1025 23:44:05.057258 16980 solver.cpp:229] Iteration 180, loss = 0.420493
I1025 23:44:05.057289 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.286272 (* 1 = 0.286272 loss)
I1025 23:44:05.057294 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.134221 (* 1 = 0.134221 loss)
I1025 23:44:05.057299 16980 sgd_solver.cpp:106] Iteration 180, lr = 0.001
I1025 23:44:08.812449 16980 solver.cpp:229] Iteration 200, loss = 0.588078
I1025 23:44:08.812481 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.367425 (* 1 = 0.367425 loss)
I1025 23:44:08.812486 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.220654 (* 1 = 0.220654 loss)
I1025 23:44:08.812492 16980 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I1025 23:44:12.731582 16980 solver.cpp:229] Iteration 220, loss = 0.510898
I1025 23:44:12.731609 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.205555 (* 1 = 0.205555 loss)
I1025 23:44:12.731614 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.305344 (* 1 = 0.305344 loss)
I1025 23:44:12.731618 16980 sgd_solver.cpp:106] Iteration 220, lr = 0.001
I1025 23:44:16.564693 16980 solver.cpp:229] Iteration 240, loss = 0.989672
I1025 23:44:16.564723 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.388965 (* 1 = 0.388965 loss)
I1025 23:44:16.564728 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.600707 (* 1 = 0.600707 loss)
I1025 23:44:16.564731 16980 sgd_solver.cpp:106] Iteration 240, lr = 0.001
I1025 23:44:20.373850 16980 solver.cpp:229] Iteration 260, loss = 0.518273
I1025 23:44:20.373884 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.254637 (* 1 = 0.254637 loss)
I1025 23:44:20.373889 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.263636 (* 1 = 0.263636 loss)
I1025 23:44:20.373894 16980 sgd_solver.cpp:106] Iteration 260, lr = 0.001
I1025 23:44:24.050282 16980 solver.cpp:229] Iteration 280, loss = 0.665119
I1025 23:44:24.050313 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.317612 (* 1 = 0.317612 loss)
I1025 23:44:24.050318 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.347507 (* 1 = 0.347507 loss)
I1025 23:44:24.050323 16980 sgd_solver.cpp:106] Iteration 280, lr = 0.001
I1025 23:44:27.795682 16980 solver.cpp:229] Iteration 300, loss = 0.560766
I1025 23:44:27.795714 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.319818 (* 1 = 0.319818 loss)
I1025 23:44:27.795720 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.240948 (* 1 = 0.240948 loss)
I1025 23:44:27.795724 16980 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I1025 23:44:31.503151 16980 solver.cpp:229] Iteration 320, loss = 0.546136
I1025 23:44:31.503193 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.278463 (* 1 = 0.278463 loss)
I1025 23:44:31.503199 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.267674 (* 1 = 0.267674 loss)
I1025 23:44:31.503204 16980 sgd_solver.cpp:106] Iteration 320, lr = 0.001
I1025 23:44:35.193542 16980 solver.cpp:229] Iteration 340, loss = 0.804313
I1025 23:44:35.193574 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.372237 (* 1 = 0.372237 loss)
I1025 23:44:35.193579 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.432076 (* 1 = 0.432076 loss)
I1025 23:44:35.193584 16980 sgd_solver.cpp:106] Iteration 340, lr = 0.001
I1025 23:44:38.896832 16980 solver.cpp:229] Iteration 360, loss = 0.483079
I1025 23:44:38.896863 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.324673 (* 1 = 0.324673 loss)
I1025 23:44:38.896868 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.158406 (* 1 = 0.158406 loss)
I1025 23:44:38.896873 16980 sgd_solver.cpp:106] Iteration 360, lr = 0.001
I1025 23:44:42.594825 16980 solver.cpp:229] Iteration 380, loss = 0.914588
I1025 23:44:42.594854 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.38581 (* 1 = 0.38581 loss)
I1025 23:44:42.594859 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.528777 (* 1 = 0.528777 loss)
I1025 23:44:42.594863 16980 sgd_solver.cpp:106] Iteration 380, lr = 0.001
I1025 23:44:46.367967 16980 solver.cpp:229] Iteration 400, loss = 0.557454
I1025 23:44:46.368000 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.257014 (* 1 = 0.257014 loss)
I1025 23:44:46.368005 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.300441 (* 1 = 0.300441 loss)
I1025 23:44:46.368010 16980 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I1025 23:44:50.173228 16980 solver.cpp:229] Iteration 420, loss = 0.368913
I1025 23:44:50.173259 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.259896 (* 1 = 0.259896 loss)
I1025 23:44:50.173264 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.109017 (* 1 = 0.109017 loss)
I1025 23:44:50.173269 16980 sgd_solver.cpp:106] Iteration 420, lr = 0.001
I1025 23:44:53.949666 16980 solver.cpp:229] Iteration 440, loss = 0.391487
I1025 23:44:53.949698 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.251864 (* 1 = 0.251864 loss)
I1025 23:44:53.949704 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.139623 (* 1 = 0.139623 loss)
I1025 23:44:53.949709 16980 sgd_solver.cpp:106] Iteration 440, lr = 0.001
I1025 23:44:57.685966 16980 solver.cpp:229] Iteration 460, loss = 0.318028
I1025 23:44:57.685993 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.168052 (* 1 = 0.168052 loss)
I1025 23:44:57.685997 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.149977 (* 1 = 0.149977 loss)
I1025 23:44:57.686002 16980 sgd_solver.cpp:106] Iteration 460, lr = 0.001
I1025 23:45:01.423364 16980 solver.cpp:229] Iteration 480, loss = 0.61402
I1025 23:45:01.423396 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.303125 (* 1 = 0.303125 loss)
I1025 23:45:01.423400 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.310895 (* 1 = 0.310895 loss)
I1025 23:45:01.423416 16980 sgd_solver.cpp:106] Iteration 480, lr = 0.001
I1025 23:45:05.093657 16980 solver.cpp:229] Iteration 500, loss = 0.638295
I1025 23:45:05.093689 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.248889 (* 1 = 0.248889 loss)
I1025 23:45:05.093694 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.389406 (* 1 = 0.389406 loss)
I1025 23:45:05.093699 16980 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I1025 23:45:08.812108 16980 solver.cpp:229] Iteration 520, loss = 0.415583
I1025 23:45:08.812150 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.271757 (* 1 = 0.271757 loss)
I1025 23:45:08.812155 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.143826 (* 1 = 0.143826 loss)
I1025 23:45:08.812160 16980 sgd_solver.cpp:106] Iteration 520, lr = 0.001
I1025 23:45:12.485999 16980 solver.cpp:229] Iteration 540, loss = 0.596827
I1025 23:45:12.486042 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.30097 (* 1 = 0.30097 loss)
I1025 23:45:12.486047 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.295857 (* 1 = 0.295857 loss)
I1025 23:45:12.486060 16980 sgd_solver.cpp:106] Iteration 540, lr = 0.001
I1025 23:45:16.212074 16980 solver.cpp:229] Iteration 560, loss = 0.450185
I1025 23:45:16.212105 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.211315 (* 1 = 0.211315 loss)
I1025 23:45:16.212110 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.23887 (* 1 = 0.23887 loss)
I1025 23:45:16.212115 16980 sgd_solver.cpp:106] Iteration 560, lr = 0.001
I1025 23:45:20.041179 16980 solver.cpp:229] Iteration 580, loss = 0.628515
I1025 23:45:20.041211 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.343358 (* 1 = 0.343358 loss)
I1025 23:45:20.041218 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.285156 (* 1 = 0.285156 loss)
I1025 23:45:20.041225 16980 sgd_solver.cpp:106] Iteration 580, lr = 0.001
I1025 23:45:23.898789 16980 solver.cpp:229] Iteration 600, loss = 0.623598
I1025 23:45:23.898823 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.314721 (* 1 = 0.314721 loss)
I1025 23:45:23.898831 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.308877 (* 1 = 0.308877 loss)
I1025 23:45:23.898838 16980 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I1025 23:45:27.702059 16980 solver.cpp:229] Iteration 620, loss = 0.387703
I1025 23:45:27.702090 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.22841 (* 1 = 0.22841 loss)
I1025 23:45:27.702096 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.159293 (* 1 = 0.159293 loss)
I1025 23:45:27.702101 16980 sgd_solver.cpp:106] Iteration 620, lr = 0.001
I1025 23:45:31.449359 16980 solver.cpp:229] Iteration 640, loss = 0.589092
I1025 23:45:31.449393 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.260452 (* 1 = 0.260452 loss)
I1025 23:45:31.449399 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.32864 (* 1 = 0.32864 loss)
I1025 23:45:31.449404 16980 sgd_solver.cpp:106] Iteration 640, lr = 0.001
I1025 23:45:35.204107 16980 solver.cpp:229] Iteration 660, loss = 0.400739
I1025 23:45:35.204139 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.227424 (* 1 = 0.227424 loss)
I1025 23:45:35.204146 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.173316 (* 1 = 0.173316 loss)
I1025 23:45:35.204152 16980 sgd_solver.cpp:106] Iteration 660, lr = 0.001
I1025 23:45:38.858031 16980 solver.cpp:229] Iteration 680, loss = 0.530539
I1025 23:45:38.858062 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.287908 (* 1 = 0.287908 loss)
I1025 23:45:38.858067 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.242631 (* 1 = 0.242631 loss)
I1025 23:45:38.858073 16980 sgd_solver.cpp:106] Iteration 680, lr = 0.001
I1025 23:45:42.676174 16980 solver.cpp:229] Iteration 700, loss = 0.660309
I1025 23:45:42.676205 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.376187 (* 1 = 0.376187 loss)
I1025 23:45:42.676213 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.284122 (* 1 = 0.284122 loss)
I1025 23:45:42.676230 16980 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I1025 23:45:46.414813 16980 solver.cpp:229] Iteration 720, loss = 0.590073
I1025 23:45:46.414846 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.308288 (* 1 = 0.308288 loss)
I1025 23:45:46.414855 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.281784 (* 1 = 0.281784 loss)
I1025 23:45:46.414861 16980 sgd_solver.cpp:106] Iteration 720, lr = 0.001
I1025 23:45:50.095333 16980 solver.cpp:229] Iteration 740, loss = 0.326444
I1025 23:45:50.095367 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.200346 (* 1 = 0.200346 loss)
I1025 23:45:50.095374 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.126098 (* 1 = 0.126098 loss)
I1025 23:45:50.095381 16980 sgd_solver.cpp:106] Iteration 740, lr = 0.001
I1025 23:45:53.855360 16980 solver.cpp:229] Iteration 760, loss = 0.612541
I1025 23:45:53.855392 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.333978 (* 1 = 0.333978 loss)
I1025 23:45:53.855399 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.278563 (* 1 = 0.278563 loss)
I1025 23:45:53.855406 16980 sgd_solver.cpp:106] Iteration 760, lr = 0.001
I1025 23:45:57.570941 16980 solver.cpp:229] Iteration 780, loss = 0.547413
I1025 23:45:57.570973 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.292854 (* 1 = 0.292854 loss)
I1025 23:45:57.570978 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.254559 (* 1 = 0.254559 loss)
I1025 23:45:57.570986 16980 sgd_solver.cpp:106] Iteration 780, lr = 0.001
I1025 23:46:01.331850 16980 solver.cpp:229] Iteration 800, loss = 0.453799
I1025 23:46:01.331879 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.273911 (* 1 = 0.273911 loss)
I1025 23:46:01.331884 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.179889 (* 1 = 0.179889 loss)
I1025 23:46:01.331888 16980 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I1025 23:46:05.036274 16980 solver.cpp:229] Iteration 820, loss = 0.552603
I1025 23:46:05.036317 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.276635 (* 1 = 0.276635 loss)
I1025 23:46:05.036322 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.275968 (* 1 = 0.275968 loss)
I1025 23:46:05.036327 16980 sgd_solver.cpp:106] Iteration 820, lr = 0.001
I1025 23:46:08.750751 16980 solver.cpp:229] Iteration 840, loss = 0.372427
I1025 23:46:08.750777 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.20321 (* 1 = 0.20321 loss)
I1025 23:46:08.750782 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.169216 (* 1 = 0.169216 loss)
I1025 23:46:08.750785 16980 sgd_solver.cpp:106] Iteration 840, lr = 0.001
I1025 23:46:12.506826 16980 solver.cpp:229] Iteration 860, loss = 0.723222
I1025 23:46:12.506858 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.291823 (* 1 = 0.291823 loss)
I1025 23:46:12.506863 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.431398 (* 1 = 0.431398 loss)
I1025 23:46:12.506870 16980 sgd_solver.cpp:106] Iteration 860, lr = 0.001
I1025 23:46:16.178369 16980 solver.cpp:229] Iteration 880, loss = 0.456944
I1025 23:46:16.178401 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.268805 (* 1 = 0.268805 loss)
I1025 23:46:16.178406 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.18814 (* 1 = 0.18814 loss)
I1025 23:46:16.178411 16980 sgd_solver.cpp:106] Iteration 880, lr = 0.001
I1025 23:46:19.905093 16980 solver.cpp:229] Iteration 900, loss = 0.310125
I1025 23:46:19.905125 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.169714 (* 1 = 0.169714 loss)
I1025 23:46:19.905130 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.14041 (* 1 = 0.14041 loss)
I1025 23:46:19.905135 16980 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I1025 23:46:23.703982 16980 solver.cpp:229] Iteration 920, loss = 0.324443
I1025 23:46:23.704013 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.192614 (* 1 = 0.192614 loss)
I1025 23:46:23.704018 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.131829 (* 1 = 0.131829 loss)
I1025 23:46:23.704023 16980 sgd_solver.cpp:106] Iteration 920, lr = 0.001
I1025 23:46:27.548605 16980 solver.cpp:229] Iteration 940, loss = 0.439148
I1025 23:46:27.548638 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.205861 (* 1 = 0.205861 loss)
I1025 23:46:27.548643 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.233287 (* 1 = 0.233287 loss)
I1025 23:46:27.548648 16980 sgd_solver.cpp:106] Iteration 940, lr = 0.001
I1025 23:46:31.281057 16980 solver.cpp:229] Iteration 960, loss = 0.572175
I1025 23:46:31.281088 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.302643 (* 1 = 0.302643 loss)
I1025 23:46:31.281093 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.269532 (* 1 = 0.269532 loss)
I1025 23:46:31.281098 16980 sgd_solver.cpp:106] Iteration 960, lr = 0.001
I1025 23:46:35.096809 16980 solver.cpp:229] Iteration 980, loss = 0.355501
I1025 23:46:35.096843 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.227359 (* 1 = 0.227359 loss)
I1025 23:46:35.096848 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.128142 (* 1 = 0.128142 loss)
I1025 23:46:35.096853 16980 sgd_solver.cpp:106] Iteration 980, lr = 0.001
I1025 23:46:38.786355 16980 solver.cpp:229] Iteration 1000, loss = 0.394237
I1025 23:46:38.786382 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.209445 (* 1 = 0.209445 loss)
I1025 23:46:38.786387 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.184792 (* 1 = 0.184792 loss)
I1025 23:46:38.786391 16980 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I1025 23:46:42.536315 16980 solver.cpp:229] Iteration 1020, loss = 0.593586
I1025 23:46:42.536350 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.316552 (* 1 = 0.316552 loss)
I1025 23:46:42.536355 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.277033 (* 1 = 0.277033 loss)
I1025 23:46:42.536360 16980 sgd_solver.cpp:106] Iteration 1020, lr = 0.001
I1025 23:46:46.378473 16980 solver.cpp:229] Iteration 1040, loss = 0.34536
I1025 23:46:46.378501 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.196007 (* 1 = 0.196007 loss)
I1025 23:46:46.378506 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.149353 (* 1 = 0.149353 loss)
I1025 23:46:46.378510 16980 sgd_solver.cpp:106] Iteration 1040, lr = 0.001
I1025 23:46:50.085932 16980 solver.cpp:229] Iteration 1060, loss = 0.412438
I1025 23:46:50.085963 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.206289 (* 1 = 0.206289 loss)
I1025 23:46:50.085969 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.206149 (* 1 = 0.206149 loss)
I1025 23:46:50.085973 16980 sgd_solver.cpp:106] Iteration 1060, lr = 0.001
I1025 23:46:53.909852 16980 solver.cpp:229] Iteration 1080, loss = 0.354001
I1025 23:46:53.909883 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.204392 (* 1 = 0.204392 loss)
I1025 23:46:53.909888 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.149609 (* 1 = 0.149609 loss)
I1025 23:46:53.909891 16980 sgd_solver.cpp:106] Iteration 1080, lr = 0.001
I1025 23:46:57.629575 16980 solver.cpp:229] Iteration 1100, loss = 0.472608
I1025 23:46:57.629606 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.295595 (* 1 = 0.295595 loss)
I1025 23:46:57.629611 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.177013 (* 1 = 0.177013 loss)
I1025 23:46:57.629616 16980 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I1025 23:47:01.335014 16980 solver.cpp:229] Iteration 1120, loss = 0.348998
I1025 23:47:01.335043 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.242775 (* 1 = 0.242775 loss)
I1025 23:47:01.335048 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.106223 (* 1 = 0.106223 loss)
I1025 23:47:01.335053 16980 sgd_solver.cpp:106] Iteration 1120, lr = 0.001
I1025 23:47:05.074082 16980 solver.cpp:229] Iteration 1140, loss = 0.548225
I1025 23:47:05.074112 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.25019 (* 1 = 0.25019 loss)
I1025 23:47:05.074117 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.298035 (* 1 = 0.298035 loss)
I1025 23:47:05.074121 16980 sgd_solver.cpp:106] Iteration 1140, lr = 0.001
I1025 23:47:08.847139 16980 solver.cpp:229] Iteration 1160, loss = 0.420917
I1025 23:47:08.847170 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.277662 (* 1 = 0.277662 loss)
I1025 23:47:08.847174 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.143254 (* 1 = 0.143254 loss)
I1025 23:47:08.847178 16980 sgd_solver.cpp:106] Iteration 1160, lr = 0.001
I1025 23:47:12.537588 16980 solver.cpp:229] Iteration 1180, loss = 0.488155
I1025 23:47:12.537621 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.306713 (* 1 = 0.306713 loss)
I1025 23:47:12.537626 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.181442 (* 1 = 0.181442 loss)
I1025 23:47:12.537631 16980 sgd_solver.cpp:106] Iteration 1180, lr = 0.001
I1025 23:47:16.237006 16980 solver.cpp:229] Iteration 1200, loss = 0.446002
I1025 23:47:16.237038 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.263095 (* 1 = 0.263095 loss)
I1025 23:47:16.237043 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.182907 (* 1 = 0.182907 loss)
I1025 23:47:16.237048 16980 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I1025 23:47:19.887667 16980 solver.cpp:229] Iteration 1220, loss = 0.57197
I1025 23:47:19.887698 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.374228 (* 1 = 0.374228 loss)
I1025 23:47:19.887703 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.197742 (* 1 = 0.197742 loss)
I1025 23:47:19.887708 16980 sgd_solver.cpp:106] Iteration 1220, lr = 0.001
I1025 23:47:23.615051 16980 solver.cpp:229] Iteration 1240, loss = 0.415089
I1025 23:47:23.615082 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.292236 (* 1 = 0.292236 loss)
I1025 23:47:23.615088 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.122853 (* 1 = 0.122853 loss)
I1025 23:47:23.615093 16980 sgd_solver.cpp:106] Iteration 1240, lr = 0.001
I1025 23:47:27.296679 16980 solver.cpp:229] Iteration 1260, loss = 0.459393
I1025 23:47:27.296710 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.276976 (* 1 = 0.276976 loss)
I1025 23:47:27.296715 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.182418 (* 1 = 0.182418 loss)
I1025 23:47:27.296720 16980 sgd_solver.cpp:106] Iteration 1260, lr = 0.001
I1025 23:47:31.109092 16980 solver.cpp:229] Iteration 1280, loss = 0.542744
I1025 23:47:31.109134 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.231157 (* 1 = 0.231157 loss)
I1025 23:47:31.109139 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.311588 (* 1 = 0.311588 loss)
I1025 23:47:31.109143 16980 sgd_solver.cpp:106] Iteration 1280, lr = 0.001
I1025 23:47:34.876168 16980 solver.cpp:229] Iteration 1300, loss = 0.443568
I1025 23:47:34.876199 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.250849 (* 1 = 0.250849 loss)
I1025 23:47:34.876202 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.192718 (* 1 = 0.192718 loss)
I1025 23:47:34.876206 16980 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I1025 23:47:38.646941 16980 solver.cpp:229] Iteration 1320, loss = 0.47525
I1025 23:47:38.646970 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.242854 (* 1 = 0.242854 loss)
I1025 23:47:38.646975 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.232395 (* 1 = 0.232395 loss)
I1025 23:47:38.646978 16980 sgd_solver.cpp:106] Iteration 1320, lr = 0.001
I1025 23:47:42.325381 16980 solver.cpp:229] Iteration 1340, loss = 0.327631
I1025 23:47:42.325412 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.181319 (* 1 = 0.181319 loss)
I1025 23:47:42.325417 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.146313 (* 1 = 0.146313 loss)
I1025 23:47:42.325423 16980 sgd_solver.cpp:106] Iteration 1340, lr = 0.001
I1025 23:47:46.162251 16980 solver.cpp:229] Iteration 1360, loss = 0.464373
I1025 23:47:46.162279 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.256497 (* 1 = 0.256497 loss)
I1025 23:47:46.162283 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.207876 (* 1 = 0.207876 loss)
I1025 23:47:46.162287 16980 sgd_solver.cpp:106] Iteration 1360, lr = 0.001
I1025 23:47:49.780009 16980 solver.cpp:229] Iteration 1380, loss = 0.360029
I1025 23:47:49.780038 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.224649 (* 1 = 0.224649 loss)
I1025 23:47:49.780043 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.13538 (* 1 = 0.13538 loss)
I1025 23:47:49.780048 16980 sgd_solver.cpp:106] Iteration 1380, lr = 0.001
I1025 23:47:53.505587 16980 solver.cpp:229] Iteration 1400, loss = 0.39364
I1025 23:47:53.505622 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.208589 (* 1 = 0.208589 loss)
I1025 23:47:53.505630 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.185051 (* 1 = 0.185051 loss)
I1025 23:47:53.505636 16980 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I1025 23:47:57.301507 16980 solver.cpp:229] Iteration 1420, loss = 0.515832
I1025 23:47:57.301540 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.239706 (* 1 = 0.239706 loss)
I1025 23:47:57.301548 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.276126 (* 1 = 0.276126 loss)
I1025 23:47:57.301554 16980 sgd_solver.cpp:106] Iteration 1420, lr = 0.001
I1025 23:48:01.045294 16980 solver.cpp:229] Iteration 1440, loss = 0.701548
I1025 23:48:01.045330 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.269791 (* 1 = 0.269791 loss)
I1025 23:48:01.045337 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.431758 (* 1 = 0.431758 loss)
I1025 23:48:01.045344 16980 sgd_solver.cpp:106] Iteration 1440, lr = 0.001
I1025 23:48:04.801236 16980 solver.cpp:229] Iteration 1460, loss = 0.82714
I1025 23:48:04.801273 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.351862 (* 1 = 0.351862 loss)
I1025 23:48:04.801281 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.475278 (* 1 = 0.475278 loss)
I1025 23:48:04.801287 16980 sgd_solver.cpp:106] Iteration 1460, lr = 0.001
I1025 23:48:08.458886 16980 solver.cpp:229] Iteration 1480, loss = 0.388813
I1025 23:48:08.458919 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.203572 (* 1 = 0.203572 loss)
I1025 23:48:08.458926 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.185241 (* 1 = 0.185241 loss)
I1025 23:48:08.458932 16980 sgd_solver.cpp:106] Iteration 1480, lr = 0.001
I1025 23:48:12.162051 16980 solver.cpp:229] Iteration 1500, loss = 0.77762
I1025 23:48:12.162084 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.445856 (* 1 = 0.445856 loss)
I1025 23:48:12.162092 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.331764 (* 1 = 0.331764 loss)
I1025 23:48:12.162099 16980 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I1025 23:48:15.922163 16980 solver.cpp:229] Iteration 1520, loss = 0.247403
I1025 23:48:15.922195 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.141574 (* 1 = 0.141574 loss)
I1025 23:48:15.922204 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.105829 (* 1 = 0.105829 loss)
I1025 23:48:15.922210 16980 sgd_solver.cpp:106] Iteration 1520, lr = 0.001
I1025 23:48:19.727908 16980 solver.cpp:229] Iteration 1540, loss = 0.221556
I1025 23:48:19.727939 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.119668 (* 1 = 0.119668 loss)
I1025 23:48:19.727944 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.101887 (* 1 = 0.101887 loss)
I1025 23:48:19.727949 16980 sgd_solver.cpp:106] Iteration 1540, lr = 0.001
I1025 23:48:23.498173 16980 solver.cpp:229] Iteration 1560, loss = 0.384389
I1025 23:48:23.498224 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.253448 (* 1 = 0.253448 loss)
I1025 23:48:23.498229 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.130942 (* 1 = 0.130942 loss)
I1025 23:48:23.498244 16980 sgd_solver.cpp:106] Iteration 1560, lr = 0.001
I1025 23:48:27.317885 16980 solver.cpp:229] Iteration 1580, loss = 0.386349
I1025 23:48:27.317915 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.208646 (* 1 = 0.208646 loss)
I1025 23:48:27.317920 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.177703 (* 1 = 0.177703 loss)
I1025 23:48:27.317924 16980 sgd_solver.cpp:106] Iteration 1580, lr = 0.001
I1025 23:48:31.097301 16980 solver.cpp:229] Iteration 1600, loss = 0.249406
I1025 23:48:31.097333 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.176872 (* 1 = 0.176872 loss)
I1025 23:48:31.097338 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0725336 (* 1 = 0.0725336 loss)
I1025 23:48:31.097342 16980 sgd_solver.cpp:106] Iteration 1600, lr = 0.001
I1025 23:48:34.766978 16980 solver.cpp:229] Iteration 1620, loss = 0.493697
I1025 23:48:34.767006 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.268761 (* 1 = 0.268761 loss)
I1025 23:48:34.767026 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.224936 (* 1 = 0.224936 loss)
I1025 23:48:34.767030 16980 sgd_solver.cpp:106] Iteration 1620, lr = 0.001
I1025 23:48:38.401896 16980 solver.cpp:229] Iteration 1640, loss = 0.235272
I1025 23:48:38.401926 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.154362 (* 1 = 0.154362 loss)
I1025 23:48:38.401932 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0809101 (* 1 = 0.0809101 loss)
I1025 23:48:38.401937 16980 sgd_solver.cpp:106] Iteration 1640, lr = 0.001
I1025 23:48:42.092157 16980 solver.cpp:229] Iteration 1660, loss = 0.42291
I1025 23:48:42.092190 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.252622 (* 1 = 0.252622 loss)
I1025 23:48:42.092195 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.170288 (* 1 = 0.170288 loss)
I1025 23:48:42.092200 16980 sgd_solver.cpp:106] Iteration 1660, lr = 0.001
I1025 23:48:45.826737 16980 solver.cpp:229] Iteration 1680, loss = 0.537888
I1025 23:48:45.826767 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.272457 (* 1 = 0.272457 loss)
I1025 23:48:45.826772 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.265431 (* 1 = 0.265431 loss)
I1025 23:48:45.826777 16980 sgd_solver.cpp:106] Iteration 1680, lr = 0.001
I1025 23:48:49.452723 16980 solver.cpp:229] Iteration 1700, loss = 0.586485
I1025 23:48:49.452754 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.32149 (* 1 = 0.32149 loss)
I1025 23:48:49.452759 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.264995 (* 1 = 0.264995 loss)
I1025 23:48:49.452764 16980 sgd_solver.cpp:106] Iteration 1700, lr = 0.001
I1025 23:48:53.153910 16980 solver.cpp:229] Iteration 1720, loss = 0.449995
I1025 23:48:53.153939 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.254888 (* 1 = 0.254888 loss)
I1025 23:48:53.153942 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.195106 (* 1 = 0.195106 loss)
I1025 23:48:53.153947 16980 sgd_solver.cpp:106] Iteration 1720, lr = 0.001
I1025 23:48:56.806895 16980 solver.cpp:229] Iteration 1740, loss = 0.415804
I1025 23:48:56.806926 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.208372 (* 1 = 0.208372 loss)
I1025 23:48:56.806931 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.207431 (* 1 = 0.207431 loss)
I1025 23:48:56.806936 16980 sgd_solver.cpp:106] Iteration 1740, lr = 0.001
I1025 23:49:00.560384 16980 solver.cpp:229] Iteration 1760, loss = 0.391313
I1025 23:49:00.560415 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.209457 (* 1 = 0.209457 loss)
I1025 23:49:00.560420 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.181856 (* 1 = 0.181856 loss)
I1025 23:49:00.560426 16980 sgd_solver.cpp:106] Iteration 1760, lr = 0.001
I1025 23:49:04.381398 16980 solver.cpp:229] Iteration 1780, loss = 0.474185
I1025 23:49:04.381427 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.305692 (* 1 = 0.305692 loss)
I1025 23:49:04.381433 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.168494 (* 1 = 0.168494 loss)
I1025 23:49:04.381438 16980 sgd_solver.cpp:106] Iteration 1780, lr = 0.001
I1025 23:49:08.036571 16980 solver.cpp:229] Iteration 1800, loss = 0.556701
I1025 23:49:08.036602 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.30797 (* 1 = 0.30797 loss)
I1025 23:49:08.036607 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.248731 (* 1 = 0.248731 loss)
I1025 23:49:08.036612 16980 sgd_solver.cpp:106] Iteration 1800, lr = 0.001
I1025 23:49:11.789860 16980 solver.cpp:229] Iteration 1820, loss = 0.307553
I1025 23:49:11.789891 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.15444 (* 1 = 0.15444 loss)
I1025 23:49:11.789896 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.153112 (* 1 = 0.153112 loss)
I1025 23:49:11.789901 16980 sgd_solver.cpp:106] Iteration 1820, lr = 0.001
I1025 23:49:15.487962 16980 solver.cpp:229] Iteration 1840, loss = 0.283787
I1025 23:49:15.487989 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.182854 (* 1 = 0.182854 loss)
I1025 23:49:15.487994 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.100933 (* 1 = 0.100933 loss)
I1025 23:49:15.487998 16980 sgd_solver.cpp:106] Iteration 1840, lr = 0.001
I1025 23:49:19.175623 16980 solver.cpp:229] Iteration 1860, loss = 0.573319
I1025 23:49:19.175655 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.292469 (* 1 = 0.292469 loss)
I1025 23:49:19.175660 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.28085 (* 1 = 0.28085 loss)
I1025 23:49:19.175664 16980 sgd_solver.cpp:106] Iteration 1860, lr = 0.001
I1025 23:49:22.945212 16980 solver.cpp:229] Iteration 1880, loss = 0.165774
I1025 23:49:22.945245 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0101866 (* 1 = 0.0101866 loss)
I1025 23:49:22.945250 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.155587 (* 1 = 0.155587 loss)
I1025 23:49:22.945255 16980 sgd_solver.cpp:106] Iteration 1880, lr = 0.001
I1025 23:49:26.769356 16980 solver.cpp:229] Iteration 1900, loss = 0.268349
I1025 23:49:26.769402 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.16937 (* 1 = 0.16937 loss)
I1025 23:49:26.769408 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0989794 (* 1 = 0.0989794 loss)
I1025 23:49:26.769413 16980 sgd_solver.cpp:106] Iteration 1900, lr = 0.001
I1025 23:49:30.442891 16980 solver.cpp:229] Iteration 1920, loss = 0.512223
I1025 23:49:30.442922 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.234267 (* 1 = 0.234267 loss)
I1025 23:49:30.442927 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.277956 (* 1 = 0.277956 loss)
I1025 23:49:30.442934 16980 sgd_solver.cpp:106] Iteration 1920, lr = 0.001
I1025 23:49:34.103751 16980 solver.cpp:229] Iteration 1940, loss = 0.324188
I1025 23:49:34.103780 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.184018 (* 1 = 0.184018 loss)
I1025 23:49:34.103785 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.14017 (* 1 = 0.14017 loss)
I1025 23:49:34.103790 16980 sgd_solver.cpp:106] Iteration 1940, lr = 0.001
I1025 23:49:37.914294 16980 solver.cpp:229] Iteration 1960, loss = 0.337011
I1025 23:49:37.914326 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.173624 (* 1 = 0.173624 loss)
I1025 23:49:37.914331 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.163387 (* 1 = 0.163387 loss)
I1025 23:49:37.914336 16980 sgd_solver.cpp:106] Iteration 1960, lr = 0.001
I1025 23:49:41.536221 16980 solver.cpp:229] Iteration 1980, loss = 0.428676
I1025 23:49:41.536252 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.224589 (* 1 = 0.224589 loss)
I1025 23:49:41.536258 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.204086 (* 1 = 0.204086 loss)
I1025 23:49:41.536273 16980 sgd_solver.cpp:106] Iteration 1980, lr = 0.001
I1025 23:49:45.263789 16980 solver.cpp:229] Iteration 2000, loss = 0.438094
I1025 23:49:45.263815 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.249168 (* 1 = 0.249168 loss)
I1025 23:49:45.263820 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.188927 (* 1 = 0.188927 loss)
I1025 23:49:45.263824 16980 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I1025 23:49:49.068053 16980 solver.cpp:229] Iteration 2020, loss = 0.38795
I1025 23:49:49.068084 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.281831 (* 1 = 0.281831 loss)
I1025 23:49:49.068089 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.106119 (* 1 = 0.106119 loss)
I1025 23:49:49.068092 16980 sgd_solver.cpp:106] Iteration 2020, lr = 0.001
I1025 23:49:52.683316 16980 solver.cpp:229] Iteration 2040, loss = 0.331243
I1025 23:49:52.683344 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.166414 (* 1 = 0.166414 loss)
I1025 23:49:52.683348 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.16483 (* 1 = 0.16483 loss)
I1025 23:49:52.683353 16980 sgd_solver.cpp:106] Iteration 2040, lr = 0.001
I1025 23:49:56.433295 16980 solver.cpp:229] Iteration 2060, loss = 0.32964
I1025 23:49:56.433326 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.174652 (* 1 = 0.174652 loss)
I1025 23:49:56.433329 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.154988 (* 1 = 0.154988 loss)
I1025 23:49:56.433333 16980 sgd_solver.cpp:106] Iteration 2060, lr = 0.001
I1025 23:50:00.226835 16980 solver.cpp:229] Iteration 2080, loss = 0.511362
I1025 23:50:00.226874 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.295401 (* 1 = 0.295401 loss)
I1025 23:50:00.226879 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.215961 (* 1 = 0.215961 loss)
I1025 23:50:00.226883 16980 sgd_solver.cpp:106] Iteration 2080, lr = 0.001
I1025 23:50:03.862731 16980 solver.cpp:229] Iteration 2100, loss = 0.345089
I1025 23:50:03.862759 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.198887 (* 1 = 0.198887 loss)
I1025 23:50:03.862763 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.146202 (* 1 = 0.146202 loss)
I1025 23:50:03.862767 16980 sgd_solver.cpp:106] Iteration 2100, lr = 0.001
I1025 23:50:07.481137 16980 solver.cpp:229] Iteration 2120, loss = 0.615893
I1025 23:50:07.481168 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.325657 (* 1 = 0.325657 loss)
I1025 23:50:07.481173 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.290237 (* 1 = 0.290237 loss)
I1025 23:50:07.481178 16980 sgd_solver.cpp:106] Iteration 2120, lr = 0.001
I1025 23:50:11.217005 16980 solver.cpp:229] Iteration 2140, loss = 0.523206
I1025 23:50:11.217036 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.284974 (* 1 = 0.284974 loss)
I1025 23:50:11.217041 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.238232 (* 1 = 0.238232 loss)
I1025 23:50:11.217046 16980 sgd_solver.cpp:106] Iteration 2140, lr = 0.001
I1025 23:50:14.919812 16980 solver.cpp:229] Iteration 2160, loss = 0.298034
I1025 23:50:14.919844 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.153334 (* 1 = 0.153334 loss)
I1025 23:50:14.919849 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.1447 (* 1 = 0.1447 loss)
I1025 23:50:14.919853 16980 sgd_solver.cpp:106] Iteration 2160, lr = 0.001
I1025 23:50:18.693137 16980 solver.cpp:229] Iteration 2180, loss = 0.363934
I1025 23:50:18.693166 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.167275 (* 1 = 0.167275 loss)
I1025 23:50:18.693169 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.196659 (* 1 = 0.196659 loss)
I1025 23:50:18.693173 16980 sgd_solver.cpp:106] Iteration 2180, lr = 0.001
I1025 23:50:22.360952 16980 solver.cpp:229] Iteration 2200, loss = 0.401332
I1025 23:50:22.360983 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.226317 (* 1 = 0.226317 loss)
I1025 23:50:22.361004 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.175015 (* 1 = 0.175015 loss)
I1025 23:50:22.361008 16980 sgd_solver.cpp:106] Iteration 2200, lr = 0.001
I1025 23:50:26.121157 16980 solver.cpp:229] Iteration 2220, loss = 0.42373
I1025 23:50:26.121186 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.185908 (* 1 = 0.185908 loss)
I1025 23:50:26.121191 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.237822 (* 1 = 0.237822 loss)
I1025 23:50:26.121196 16980 sgd_solver.cpp:106] Iteration 2220, lr = 0.001
I1025 23:50:29.787318 16980 solver.cpp:229] Iteration 2240, loss = 0.370621
I1025 23:50:29.787350 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.181746 (* 1 = 0.181746 loss)
I1025 23:50:29.787355 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.188875 (* 1 = 0.188875 loss)
I1025 23:50:29.787360 16980 sgd_solver.cpp:106] Iteration 2240, lr = 0.001
I1025 23:50:33.587613 16980 solver.cpp:229] Iteration 2260, loss = 0.540809
I1025 23:50:33.587643 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.24195 (* 1 = 0.24195 loss)
I1025 23:50:33.587648 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.298859 (* 1 = 0.298859 loss)
I1025 23:50:33.587652 16980 sgd_solver.cpp:106] Iteration 2260, lr = 0.001
I1025 23:50:37.371330 16980 solver.cpp:229] Iteration 2280, loss = 0.573684
I1025 23:50:37.371361 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.344649 (* 1 = 0.344649 loss)
I1025 23:50:37.371366 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.229035 (* 1 = 0.229035 loss)
I1025 23:50:37.371371 16980 sgd_solver.cpp:106] Iteration 2280, lr = 0.001
I1025 23:50:41.109598 16980 solver.cpp:229] Iteration 2300, loss = 0.41067
I1025 23:50:41.109628 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.170101 (* 1 = 0.170101 loss)
I1025 23:50:41.109633 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.240569 (* 1 = 0.240569 loss)
I1025 23:50:41.109637 16980 sgd_solver.cpp:106] Iteration 2300, lr = 0.001
I1025 23:50:44.856714 16980 solver.cpp:229] Iteration 2320, loss = 0.679647
I1025 23:50:44.856747 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.355508 (* 1 = 0.355508 loss)
I1025 23:50:44.856752 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.324139 (* 1 = 0.324139 loss)
I1025 23:50:44.856757 16980 sgd_solver.cpp:106] Iteration 2320, lr = 0.001
I1025 23:50:48.572576 16980 solver.cpp:229] Iteration 2340, loss = 0.397189
I1025 23:50:48.572607 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.24334 (* 1 = 0.24334 loss)
I1025 23:50:48.572612 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.153849 (* 1 = 0.153849 loss)
I1025 23:50:48.572615 16980 sgd_solver.cpp:106] Iteration 2340, lr = 0.001
I1025 23:50:52.258322 16980 solver.cpp:229] Iteration 2360, loss = 0.436626
I1025 23:50:52.258354 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.21628 (* 1 = 0.21628 loss)
I1025 23:50:52.258359 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.220347 (* 1 = 0.220347 loss)
I1025 23:50:52.258364 16980 sgd_solver.cpp:106] Iteration 2360, lr = 0.001
I1025 23:50:55.905097 16980 solver.cpp:229] Iteration 2380, loss = 0.25963
I1025 23:50:55.905127 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.150127 (* 1 = 0.150127 loss)
I1025 23:50:55.905131 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.109503 (* 1 = 0.109503 loss)
I1025 23:50:55.905136 16980 sgd_solver.cpp:106] Iteration 2380, lr = 0.001
I1025 23:50:59.587862 16980 solver.cpp:229] Iteration 2400, loss = 0.287196
I1025 23:50:59.587893 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.157636 (* 1 = 0.157636 loss)
I1025 23:50:59.587898 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.12956 (* 1 = 0.12956 loss)
I1025 23:50:59.587903 16980 sgd_solver.cpp:106] Iteration 2400, lr = 0.001
I1025 23:51:03.267585 16980 solver.cpp:229] Iteration 2420, loss = 0.31545
I1025 23:51:03.267618 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.193947 (* 1 = 0.193947 loss)
I1025 23:51:03.267622 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.121503 (* 1 = 0.121503 loss)
I1025 23:51:03.267627 16980 sgd_solver.cpp:106] Iteration 2420, lr = 0.001
I1025 23:51:06.909157 16980 solver.cpp:229] Iteration 2440, loss = 0.238268
I1025 23:51:06.909186 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.152708 (* 1 = 0.152708 loss)
I1025 23:51:06.909191 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0855601 (* 1 = 0.0855601 loss)
I1025 23:51:06.909195 16980 sgd_solver.cpp:106] Iteration 2440, lr = 0.001
I1025 23:51:10.605890 16980 solver.cpp:229] Iteration 2460, loss = 0.351608
I1025 23:51:10.605931 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.176357 (* 1 = 0.176357 loss)
I1025 23:51:10.605936 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.175251 (* 1 = 0.175251 loss)
I1025 23:51:10.605949 16980 sgd_solver.cpp:106] Iteration 2460, lr = 0.001
I1025 23:51:14.406455 16980 solver.cpp:229] Iteration 2480, loss = 0.412569
I1025 23:51:14.406486 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.207546 (* 1 = 0.207546 loss)
I1025 23:51:14.406491 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.205023 (* 1 = 0.205023 loss)
I1025 23:51:14.406496 16980 sgd_solver.cpp:106] Iteration 2480, lr = 0.001
I1025 23:51:18.237000 16980 solver.cpp:229] Iteration 2500, loss = 0.352148
I1025 23:51:18.237041 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.227433 (* 1 = 0.227433 loss)
I1025 23:51:18.237046 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.124715 (* 1 = 0.124715 loss)
I1025 23:51:18.237051 16980 sgd_solver.cpp:106] Iteration 2500, lr = 0.001
I1025 23:51:21.844739 16980 solver.cpp:229] Iteration 2520, loss = 0.273802
I1025 23:51:21.844771 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.175711 (* 1 = 0.175711 loss)
I1025 23:51:21.844776 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0980903 (* 1 = 0.0980903 loss)
I1025 23:51:21.844781 16980 sgd_solver.cpp:106] Iteration 2520, lr = 0.001
I1025 23:51:25.513876 16980 solver.cpp:229] Iteration 2540, loss = 0.368597
I1025 23:51:25.513907 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.246258 (* 1 = 0.246258 loss)
I1025 23:51:25.513911 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.122338 (* 1 = 0.122338 loss)
I1025 23:51:25.513916 16980 sgd_solver.cpp:106] Iteration 2540, lr = 0.001
I1025 23:51:29.223161 16980 solver.cpp:229] Iteration 2560, loss = 0.38673
I1025 23:51:29.223192 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.187937 (* 1 = 0.187937 loss)
I1025 23:51:29.223196 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.198793 (* 1 = 0.198793 loss)
I1025 23:51:29.223201 16980 sgd_solver.cpp:106] Iteration 2560, lr = 0.001
I1025 23:51:32.930008 16980 solver.cpp:229] Iteration 2580, loss = 0.359207
I1025 23:51:32.930042 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.174434 (* 1 = 0.174434 loss)
I1025 23:51:32.930047 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.184773 (* 1 = 0.184773 loss)
I1025 23:51:32.930050 16980 sgd_solver.cpp:106] Iteration 2580, lr = 0.001
I1025 23:51:36.701789 16980 solver.cpp:229] Iteration 2600, loss = 0.415376
I1025 23:51:36.701817 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.243735 (* 1 = 0.243735 loss)
I1025 23:51:36.701822 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.171641 (* 1 = 0.171641 loss)
I1025 23:51:36.701827 16980 sgd_solver.cpp:106] Iteration 2600, lr = 0.001
I1025 23:51:40.435371 16980 solver.cpp:229] Iteration 2620, loss = 0.51422
I1025 23:51:40.435398 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.22989 (* 1 = 0.22989 loss)
I1025 23:51:40.435405 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.284329 (* 1 = 0.284329 loss)
I1025 23:51:40.435410 16980 sgd_solver.cpp:106] Iteration 2620, lr = 0.001
I1025 23:51:44.177096 16980 solver.cpp:229] Iteration 2640, loss = 0.363277
I1025 23:51:44.177127 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.257719 (* 1 = 0.257719 loss)
I1025 23:51:44.177131 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.105559 (* 1 = 0.105559 loss)
I1025 23:51:44.177136 16980 sgd_solver.cpp:106] Iteration 2640, lr = 0.001
I1025 23:51:47.905457 16980 solver.cpp:229] Iteration 2660, loss = 0.227682
I1025 23:51:47.905485 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.140431 (* 1 = 0.140431 loss)
I1025 23:51:47.905490 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0872509 (* 1 = 0.0872509 loss)
I1025 23:51:47.905494 16980 sgd_solver.cpp:106] Iteration 2660, lr = 0.001
I1025 23:51:51.614471 16980 solver.cpp:229] Iteration 2680, loss = 0.346423
I1025 23:51:51.614503 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.20314 (* 1 = 0.20314 loss)
I1025 23:51:51.614508 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.143283 (* 1 = 0.143283 loss)
I1025 23:51:51.614512 16980 sgd_solver.cpp:106] Iteration 2680, lr = 0.001
I1025 23:51:55.299379 16980 solver.cpp:229] Iteration 2700, loss = 0.386442
I1025 23:51:55.299412 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.276925 (* 1 = 0.276925 loss)
I1025 23:51:55.299417 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.109517 (* 1 = 0.109517 loss)
I1025 23:51:55.299435 16980 sgd_solver.cpp:106] Iteration 2700, lr = 0.001
I1025 23:51:59.083660 16980 solver.cpp:229] Iteration 2720, loss = 0.324104
I1025 23:51:59.083691 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.144673 (* 1 = 0.144673 loss)
I1025 23:51:59.083696 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.179431 (* 1 = 0.179431 loss)
I1025 23:51:59.083700 16980 sgd_solver.cpp:106] Iteration 2720, lr = 0.001
I1025 23:52:02.685566 16980 solver.cpp:229] Iteration 2740, loss = 0.283035
I1025 23:52:02.685595 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.155226 (* 1 = 0.155226 loss)
I1025 23:52:02.685600 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.12781 (* 1 = 0.12781 loss)
I1025 23:52:02.685603 16980 sgd_solver.cpp:106] Iteration 2740, lr = 0.001
I1025 23:52:06.294972 16980 solver.cpp:229] Iteration 2760, loss = 0.31935
I1025 23:52:06.295003 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.208512 (* 1 = 0.208512 loss)
I1025 23:52:06.295008 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.110838 (* 1 = 0.110838 loss)
I1025 23:52:06.295027 16980 sgd_solver.cpp:106] Iteration 2760, lr = 0.001
I1025 23:52:09.988812 16980 solver.cpp:229] Iteration 2780, loss = 0.357694
I1025 23:52:09.988844 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.195288 (* 1 = 0.195288 loss)
I1025 23:52:09.988848 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.162406 (* 1 = 0.162406 loss)
I1025 23:52:09.988853 16980 sgd_solver.cpp:106] Iteration 2780, lr = 0.001
I1025 23:52:13.694684 16980 solver.cpp:229] Iteration 2800, loss = 0.369391
I1025 23:52:13.694715 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.236208 (* 1 = 0.236208 loss)
I1025 23:52:13.694718 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.133183 (* 1 = 0.133183 loss)
I1025 23:52:13.694722 16980 sgd_solver.cpp:106] Iteration 2800, lr = 0.001
I1025 23:52:17.456635 16980 solver.cpp:229] Iteration 2820, loss = 0.364804
I1025 23:52:17.456665 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.152718 (* 1 = 0.152718 loss)
I1025 23:52:17.456670 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.212086 (* 1 = 0.212086 loss)
I1025 23:52:17.456674 16980 sgd_solver.cpp:106] Iteration 2820, lr = 0.001
I1025 23:52:21.321310 16980 solver.cpp:229] Iteration 2840, loss = 0.322209
I1025 23:52:21.321341 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.204131 (* 1 = 0.204131 loss)
I1025 23:52:21.321344 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.118078 (* 1 = 0.118078 loss)
I1025 23:52:21.321349 16980 sgd_solver.cpp:106] Iteration 2840, lr = 0.001
I1025 23:52:25.000448 16980 solver.cpp:229] Iteration 2860, loss = 0.308179
I1025 23:52:25.000483 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.172118 (* 1 = 0.172118 loss)
I1025 23:52:25.000488 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.136061 (* 1 = 0.136061 loss)
I1025 23:52:25.000490 16980 sgd_solver.cpp:106] Iteration 2860, lr = 0.001
I1025 23:52:28.738178 16980 solver.cpp:229] Iteration 2880, loss = 0.399315
I1025 23:52:28.738205 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.242578 (* 1 = 0.242578 loss)
I1025 23:52:28.738209 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.156737 (* 1 = 0.156737 loss)
I1025 23:52:28.738214 16980 sgd_solver.cpp:106] Iteration 2880, lr = 0.001
I1025 23:52:32.485833 16980 solver.cpp:229] Iteration 2900, loss = 0.311409
I1025 23:52:32.485865 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.203031 (* 1 = 0.203031 loss)
I1025 23:52:32.485870 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.108378 (* 1 = 0.108378 loss)
I1025 23:52:32.485875 16980 sgd_solver.cpp:106] Iteration 2900, lr = 0.001
I1025 23:52:36.218976 16980 solver.cpp:229] Iteration 2920, loss = 0.304982
I1025 23:52:36.219000 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.209866 (* 1 = 0.209866 loss)
I1025 23:52:36.219005 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0951161 (* 1 = 0.0951161 loss)
I1025 23:52:36.219010 16980 sgd_solver.cpp:106] Iteration 2920, lr = 0.001
I1025 23:52:39.982676 16980 solver.cpp:229] Iteration 2940, loss = 0.329408
I1025 23:52:39.982705 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.192974 (* 1 = 0.192974 loss)
I1025 23:52:39.982710 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.136434 (* 1 = 0.136434 loss)
I1025 23:52:39.982714 16980 sgd_solver.cpp:106] Iteration 2940, lr = 0.001
I1025 23:52:43.742902 16980 solver.cpp:229] Iteration 2960, loss = 0.300857
I1025 23:52:43.742931 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.189945 (* 1 = 0.189945 loss)
I1025 23:52:43.742936 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.110912 (* 1 = 0.110912 loss)
I1025 23:52:43.742940 16980 sgd_solver.cpp:106] Iteration 2960, lr = 0.001
I1025 23:52:47.392510 16980 solver.cpp:229] Iteration 2980, loss = 0.28517
I1025 23:52:47.392541 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.225791 (* 1 = 0.225791 loss)
I1025 23:52:47.392546 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0593791 (* 1 = 0.0593791 loss)
I1025 23:52:47.392551 16980 sgd_solver.cpp:106] Iteration 2980, lr = 0.001
I1025 23:52:51.078975 16980 solver.cpp:229] Iteration 3000, loss = 0.470493
I1025 23:52:51.079007 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.274192 (* 1 = 0.274192 loss)
I1025 23:52:51.079012 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.196301 (* 1 = 0.196301 loss)
I1025 23:52:51.079017 16980 sgd_solver.cpp:106] Iteration 3000, lr = 0.001
I1025 23:52:54.705822 16980 solver.cpp:229] Iteration 3020, loss = 0.583257
I1025 23:52:54.705845 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.2682 (* 1 = 0.2682 loss)
I1025 23:52:54.705850 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.315058 (* 1 = 0.315058 loss)
I1025 23:52:54.705854 16980 sgd_solver.cpp:106] Iteration 3020, lr = 0.001
I1025 23:52:58.442163 16980 solver.cpp:229] Iteration 3040, loss = 0.281193
I1025 23:52:58.442191 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.188814 (* 1 = 0.188814 loss)
I1025 23:52:58.442196 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0923789 (* 1 = 0.0923789 loss)
I1025 23:52:58.442200 16980 sgd_solver.cpp:106] Iteration 3040, lr = 0.001
I1025 23:53:02.188693 16980 solver.cpp:229] Iteration 3060, loss = 0.298005
I1025 23:53:02.188725 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.175987 (* 1 = 0.175987 loss)
I1025 23:53:02.188730 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.122017 (* 1 = 0.122017 loss)
I1025 23:53:02.188735 16980 sgd_solver.cpp:106] Iteration 3060, lr = 0.001
I1025 23:53:05.828727 16980 solver.cpp:229] Iteration 3080, loss = 0.274989
I1025 23:53:05.828758 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.16214 (* 1 = 0.16214 loss)
I1025 23:53:05.828761 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.112848 (* 1 = 0.112848 loss)
I1025 23:53:05.828766 16980 sgd_solver.cpp:106] Iteration 3080, lr = 0.001
I1025 23:53:09.428184 16980 solver.cpp:229] Iteration 3100, loss = 0.246036
I1025 23:53:09.428215 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.1677 (* 1 = 0.1677 loss)
I1025 23:53:09.428220 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0783356 (* 1 = 0.0783356 loss)
I1025 23:53:09.428225 16980 sgd_solver.cpp:106] Iteration 3100, lr = 0.001
I1025 23:53:13.186712 16980 solver.cpp:229] Iteration 3120, loss = 0.402907
I1025 23:53:13.186753 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.278944 (* 1 = 0.278944 loss)
I1025 23:53:13.186758 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.123963 (* 1 = 0.123963 loss)
I1025 23:53:13.186763 16980 sgd_solver.cpp:106] Iteration 3120, lr = 0.001
I1025 23:53:16.880640 16980 solver.cpp:229] Iteration 3140, loss = 0.243841
I1025 23:53:16.880671 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.137315 (* 1 = 0.137315 loss)
I1025 23:53:16.880676 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.106526 (* 1 = 0.106526 loss)
I1025 23:53:16.880681 16980 sgd_solver.cpp:106] Iteration 3140, lr = 0.001
I1025 23:53:20.452306 16980 solver.cpp:229] Iteration 3160, loss = 0.301114
I1025 23:53:20.452332 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.122575 (* 1 = 0.122575 loss)
I1025 23:53:20.452337 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.178538 (* 1 = 0.178538 loss)
I1025 23:53:20.452342 16980 sgd_solver.cpp:106] Iteration 3160, lr = 0.001
I1025 23:53:24.160243 16980 solver.cpp:229] Iteration 3180, loss = 0.441149
I1025 23:53:24.160274 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.233997 (* 1 = 0.233997 loss)
I1025 23:53:24.160279 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.207152 (* 1 = 0.207152 loss)
I1025 23:53:24.160282 16980 sgd_solver.cpp:106] Iteration 3180, lr = 0.001
I1025 23:53:27.887908 16980 solver.cpp:229] Iteration 3200, loss = 0.236953
I1025 23:53:27.887938 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.131719 (* 1 = 0.131719 loss)
I1025 23:53:27.887943 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.105235 (* 1 = 0.105235 loss)
I1025 23:53:27.887948 16980 sgd_solver.cpp:106] Iteration 3200, lr = 0.001
I1025 23:53:31.621253 16980 solver.cpp:229] Iteration 3220, loss = 0.323939
I1025 23:53:31.621282 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.197879 (* 1 = 0.197879 loss)
I1025 23:53:31.621286 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.12606 (* 1 = 0.12606 loss)
I1025 23:53:31.621290 16980 sgd_solver.cpp:106] Iteration 3220, lr = 0.001
I1025 23:53:35.426303 16980 solver.cpp:229] Iteration 3240, loss = 0.158927
I1025 23:53:35.426336 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.115488 (* 1 = 0.115488 loss)
I1025 23:53:35.426339 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0434397 (* 1 = 0.0434397 loss)
I1025 23:53:35.426344 16980 sgd_solver.cpp:106] Iteration 3240, lr = 0.001
I1025 23:53:39.175441 16980 solver.cpp:229] Iteration 3260, loss = 0.473449
I1025 23:53:39.175472 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.194215 (* 1 = 0.194215 loss)
I1025 23:53:39.175477 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.279234 (* 1 = 0.279234 loss)
I1025 23:53:39.175482 16980 sgd_solver.cpp:106] Iteration 3260, lr = 0.001
I1025 23:53:42.978047 16980 solver.cpp:229] Iteration 3280, loss = 0.309581
I1025 23:53:42.978078 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.200041 (* 1 = 0.200041 loss)
I1025 23:53:42.978082 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.10954 (* 1 = 0.10954 loss)
I1025 23:53:42.978087 16980 sgd_solver.cpp:106] Iteration 3280, lr = 0.001
I1025 23:53:46.760275 16980 solver.cpp:229] Iteration 3300, loss = 0.437712
I1025 23:53:46.760303 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.28185 (* 1 = 0.28185 loss)
I1025 23:53:46.760308 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.155862 (* 1 = 0.155862 loss)
I1025 23:53:46.760311 16980 sgd_solver.cpp:106] Iteration 3300, lr = 0.001
I1025 23:53:50.510555 16980 solver.cpp:229] Iteration 3320, loss = 0.196845
I1025 23:53:50.510586 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.119581 (* 1 = 0.119581 loss)
I1025 23:53:50.510591 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.077264 (* 1 = 0.077264 loss)
I1025 23:53:50.510594 16980 sgd_solver.cpp:106] Iteration 3320, lr = 0.001
I1025 23:53:54.209007 16980 solver.cpp:229] Iteration 3340, loss = 0.220678
I1025 23:53:54.209034 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.116944 (* 1 = 0.116944 loss)
I1025 23:53:54.209039 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.103734 (* 1 = 0.103734 loss)
I1025 23:53:54.209043 16980 sgd_solver.cpp:106] Iteration 3340, lr = 0.001
I1025 23:53:58.000267 16980 solver.cpp:229] Iteration 3360, loss = 0.49928
I1025 23:53:58.000298 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.249566 (* 1 = 0.249566 loss)
I1025 23:53:58.000301 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.249714 (* 1 = 0.249714 loss)
I1025 23:53:58.000305 16980 sgd_solver.cpp:106] Iteration 3360, lr = 0.001
I1025 23:54:01.660272 16980 solver.cpp:229] Iteration 3380, loss = 0.30933
I1025 23:54:01.660300 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.179883 (* 1 = 0.179883 loss)
I1025 23:54:01.660305 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.129447 (* 1 = 0.129447 loss)
I1025 23:54:01.660308 16980 sgd_solver.cpp:106] Iteration 3380, lr = 0.001
I1025 23:54:05.340312 16980 solver.cpp:229] Iteration 3400, loss = 0.489702
I1025 23:54:05.340342 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.271625 (* 1 = 0.271625 loss)
I1025 23:54:05.340345 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.218077 (* 1 = 0.218077 loss)
I1025 23:54:05.340349 16980 sgd_solver.cpp:106] Iteration 3400, lr = 0.001
I1025 23:54:09.080202 16980 solver.cpp:229] Iteration 3420, loss = 0.240695
I1025 23:54:09.080235 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.113197 (* 1 = 0.113197 loss)
I1025 23:54:09.080240 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.127498 (* 1 = 0.127498 loss)
I1025 23:54:09.080245 16980 sgd_solver.cpp:106] Iteration 3420, lr = 0.001
I1025 23:54:12.740622 16980 solver.cpp:229] Iteration 3440, loss = 0.221901
I1025 23:54:12.740649 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.154874 (* 1 = 0.154874 loss)
I1025 23:54:12.740654 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0670273 (* 1 = 0.0670273 loss)
I1025 23:54:12.740658 16980 sgd_solver.cpp:106] Iteration 3440, lr = 0.001
I1025 23:54:16.393663 16980 solver.cpp:229] Iteration 3460, loss = 0.524041
I1025 23:54:16.393692 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.334351 (* 1 = 0.334351 loss)
I1025 23:54:16.393697 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.18969 (* 1 = 0.18969 loss)
I1025 23:54:16.393702 16980 sgd_solver.cpp:106] Iteration 3460, lr = 0.001
I1025 23:54:20.167320 16980 solver.cpp:229] Iteration 3480, loss = 0.466902
I1025 23:54:20.167351 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.324572 (* 1 = 0.324572 loss)
I1025 23:54:20.167356 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.14233 (* 1 = 0.14233 loss)
I1025 23:54:20.167359 16980 sgd_solver.cpp:106] Iteration 3480, lr = 0.001
I1025 23:54:24.036608 16980 solver.cpp:229] Iteration 3500, loss = 0.32973
I1025 23:54:24.036639 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.160181 (* 1 = 0.160181 loss)
I1025 23:54:24.036644 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.169548 (* 1 = 0.169548 loss)
I1025 23:54:24.036648 16980 sgd_solver.cpp:106] Iteration 3500, lr = 0.001
I1025 23:54:27.746404 16980 solver.cpp:229] Iteration 3520, loss = 0.45221
I1025 23:54:27.746436 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.238812 (* 1 = 0.238812 loss)
I1025 23:54:27.746440 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.213398 (* 1 = 0.213398 loss)
I1025 23:54:27.746445 16980 sgd_solver.cpp:106] Iteration 3520, lr = 0.001
I1025 23:54:31.427723 16980 solver.cpp:229] Iteration 3540, loss = 0.371259
I1025 23:54:31.427752 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.19906 (* 1 = 0.19906 loss)
I1025 23:54:31.427757 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.172199 (* 1 = 0.172199 loss)
I1025 23:54:31.427760 16980 sgd_solver.cpp:106] Iteration 3540, lr = 0.001
I1025 23:54:35.073160 16980 solver.cpp:229] Iteration 3560, loss = 0.263463
I1025 23:54:35.073191 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.169258 (* 1 = 0.169258 loss)
I1025 23:54:35.073196 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0942054 (* 1 = 0.0942054 loss)
I1025 23:54:35.073201 16980 sgd_solver.cpp:106] Iteration 3560, lr = 0.001
I1025 23:54:38.786101 16980 solver.cpp:229] Iteration 3580, loss = 0.418183
I1025 23:54:38.786129 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.201358 (* 1 = 0.201358 loss)
I1025 23:54:38.786134 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.216825 (* 1 = 0.216825 loss)
I1025 23:54:38.786139 16980 sgd_solver.cpp:106] Iteration 3580, lr = 0.001
I1025 23:54:42.521209 16980 solver.cpp:229] Iteration 3600, loss = 0.299967
I1025 23:54:42.521235 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.162752 (* 1 = 0.162752 loss)
I1025 23:54:42.521239 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.137215 (* 1 = 0.137215 loss)
I1025 23:54:42.521244 16980 sgd_solver.cpp:106] Iteration 3600, lr = 0.001
I1025 23:54:46.261641 16980 solver.cpp:229] Iteration 3620, loss = 0.234893
I1025 23:54:46.261672 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.102617 (* 1 = 0.102617 loss)
I1025 23:54:46.261677 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.132276 (* 1 = 0.132276 loss)
I1025 23:54:46.261682 16980 sgd_solver.cpp:106] Iteration 3620, lr = 0.001
I1025 23:54:49.951776 16980 solver.cpp:229] Iteration 3640, loss = 0.167883
I1025 23:54:49.951807 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0984615 (* 1 = 0.0984615 loss)
I1025 23:54:49.951812 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0694217 (* 1 = 0.0694217 loss)
I1025 23:54:49.951817 16980 sgd_solver.cpp:106] Iteration 3640, lr = 0.001
I1025 23:54:53.600875 16980 solver.cpp:229] Iteration 3660, loss = 0.250418
I1025 23:54:53.600906 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.154634 (* 1 = 0.154634 loss)
I1025 23:54:53.600911 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0957839 (* 1 = 0.0957839 loss)
I1025 23:54:53.600916 16980 sgd_solver.cpp:106] Iteration 3660, lr = 0.001
I1025 23:54:57.313707 16980 solver.cpp:229] Iteration 3680, loss = 0.404838
I1025 23:54:57.313738 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.163581 (* 1 = 0.163581 loss)
I1025 23:54:57.313743 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.241256 (* 1 = 0.241256 loss)
I1025 23:54:57.313747 16980 sgd_solver.cpp:106] Iteration 3680, lr = 0.001
I1025 23:55:00.994984 16980 solver.cpp:229] Iteration 3700, loss = 0.41273
I1025 23:55:00.995014 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.197774 (* 1 = 0.197774 loss)
I1025 23:55:00.995019 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.214956 (* 1 = 0.214956 loss)
I1025 23:55:00.995023 16980 sgd_solver.cpp:106] Iteration 3700, lr = 0.001
I1025 23:55:04.782901 16980 solver.cpp:229] Iteration 3720, loss = 0.272984
I1025 23:55:04.782927 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.152402 (* 1 = 0.152402 loss)
I1025 23:55:04.782932 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.120582 (* 1 = 0.120582 loss)
I1025 23:55:04.782935 16980 sgd_solver.cpp:106] Iteration 3720, lr = 0.001
I1025 23:55:08.563884 16980 solver.cpp:229] Iteration 3740, loss = 0.289279
I1025 23:55:08.563915 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.184638 (* 1 = 0.184638 loss)
I1025 23:55:08.563920 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.104641 (* 1 = 0.104641 loss)
I1025 23:55:08.563925 16980 sgd_solver.cpp:106] Iteration 3740, lr = 0.001
I1025 23:55:12.281476 16980 solver.cpp:229] Iteration 3760, loss = 0.28273
I1025 23:55:12.281507 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.115953 (* 1 = 0.115953 loss)
I1025 23:55:12.281512 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.166777 (* 1 = 0.166777 loss)
I1025 23:55:12.281517 16980 sgd_solver.cpp:106] Iteration 3760, lr = 0.001
I1025 23:55:16.006366 16980 solver.cpp:229] Iteration 3780, loss = 0.64057
I1025 23:55:16.006392 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.290711 (* 1 = 0.290711 loss)
I1025 23:55:16.006397 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.34986 (* 1 = 0.34986 loss)
I1025 23:55:16.006400 16980 sgd_solver.cpp:106] Iteration 3780, lr = 0.001
I1025 23:55:19.768183 16980 solver.cpp:229] Iteration 3800, loss = 0.280519
I1025 23:55:19.768214 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.173907 (* 1 = 0.173907 loss)
I1025 23:55:19.768219 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.106612 (* 1 = 0.106612 loss)
I1025 23:55:19.768224 16980 sgd_solver.cpp:106] Iteration 3800, lr = 0.001
I1025 23:55:23.533080 16980 solver.cpp:229] Iteration 3820, loss = 0.173006
I1025 23:55:23.533113 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0878017 (* 1 = 0.0878017 loss)
I1025 23:55:23.533116 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0852048 (* 1 = 0.0852048 loss)
I1025 23:55:23.533121 16980 sgd_solver.cpp:106] Iteration 3820, lr = 0.001
I1025 23:55:27.238739 16980 solver.cpp:229] Iteration 3840, loss = 0.216278
I1025 23:55:27.238771 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.150463 (* 1 = 0.150463 loss)
I1025 23:55:27.238776 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0658148 (* 1 = 0.0658148 loss)
I1025 23:55:27.238780 16980 sgd_solver.cpp:106] Iteration 3840, lr = 0.001
I1025 23:55:30.916920 16980 solver.cpp:229] Iteration 3860, loss = 0.332432
I1025 23:55:30.916949 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.169423 (* 1 = 0.169423 loss)
I1025 23:55:30.916954 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.163009 (* 1 = 0.163009 loss)
I1025 23:55:30.916959 16980 sgd_solver.cpp:106] Iteration 3860, lr = 0.001
I1025 23:55:34.596338 16980 solver.cpp:229] Iteration 3880, loss = 0.204586
I1025 23:55:34.596369 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.141346 (* 1 = 0.141346 loss)
I1025 23:55:34.596374 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0632405 (* 1 = 0.0632405 loss)
I1025 23:55:34.596379 16980 sgd_solver.cpp:106] Iteration 3880, lr = 0.001
I1025 23:55:38.283004 16980 solver.cpp:229] Iteration 3900, loss = 0.154751
I1025 23:55:38.283035 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.111624 (* 1 = 0.111624 loss)
I1025 23:55:38.283040 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0431275 (* 1 = 0.0431275 loss)
I1025 23:55:38.283044 16980 sgd_solver.cpp:106] Iteration 3900, lr = 0.001
I1025 23:55:42.029137 16980 solver.cpp:229] Iteration 3920, loss = 0.414155
I1025 23:55:42.029166 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.228635 (* 1 = 0.228635 loss)
I1025 23:55:42.029171 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.18552 (* 1 = 0.18552 loss)
I1025 23:55:42.029175 16980 sgd_solver.cpp:106] Iteration 3920, lr = 0.001
I1025 23:55:45.701197 16980 solver.cpp:229] Iteration 3940, loss = 0.349862
I1025 23:55:45.701230 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.253778 (* 1 = 0.253778 loss)
I1025 23:55:45.701234 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0960844 (* 1 = 0.0960844 loss)
I1025 23:55:45.701238 16980 sgd_solver.cpp:106] Iteration 3940, lr = 0.001
I1025 23:55:49.495422 16980 solver.cpp:229] Iteration 3960, loss = 0.325695
I1025 23:55:49.495466 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.201052 (* 1 = 0.201052 loss)
I1025 23:55:49.495471 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.124643 (* 1 = 0.124643 loss)
I1025 23:55:49.495486 16980 sgd_solver.cpp:106] Iteration 3960, lr = 0.001
I1025 23:55:53.183877 16980 solver.cpp:229] Iteration 3980, loss = 0.241741
I1025 23:55:53.183907 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.142009 (* 1 = 0.142009 loss)
I1025 23:55:53.183910 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0997316 (* 1 = 0.0997316 loss)
I1025 23:55:53.183914 16980 sgd_solver.cpp:106] Iteration 3980, lr = 0.001
I1025 23:55:56.993095 16980 solver.cpp:229] Iteration 4000, loss = 0.246958
I1025 23:55:56.993124 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.170769 (* 1 = 0.170769 loss)
I1025 23:55:56.993129 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0761889 (* 1 = 0.0761889 loss)
I1025 23:55:56.993134 16980 sgd_solver.cpp:106] Iteration 4000, lr = 0.001
I1025 23:56:00.757697 16980 solver.cpp:229] Iteration 4020, loss = 0.330689
I1025 23:56:00.757727 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.224267 (* 1 = 0.224267 loss)
I1025 23:56:00.757731 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.106422 (* 1 = 0.106422 loss)
I1025 23:56:00.757736 16980 sgd_solver.cpp:106] Iteration 4020, lr = 0.001
I1025 23:56:04.483237 16980 solver.cpp:229] Iteration 4040, loss = 0.232312
I1025 23:56:04.483268 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.165251 (* 1 = 0.165251 loss)
I1025 23:56:04.483271 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0670611 (* 1 = 0.0670611 loss)
I1025 23:56:04.483276 16980 sgd_solver.cpp:106] Iteration 4040, lr = 0.001
I1025 23:56:08.178804 16980 solver.cpp:229] Iteration 4060, loss = 0.162775
I1025 23:56:08.178834 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0845202 (* 1 = 0.0845202 loss)
I1025 23:56:08.178839 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0782548 (* 1 = 0.0782548 loss)
I1025 23:56:08.178844 16980 sgd_solver.cpp:106] Iteration 4060, lr = 0.001
I1025 23:56:11.788249 16980 solver.cpp:229] Iteration 4080, loss = 0.393949
I1025 23:56:11.788280 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.224955 (* 1 = 0.224955 loss)
I1025 23:56:11.788285 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.168993 (* 1 = 0.168993 loss)
I1025 23:56:11.788290 16980 sgd_solver.cpp:106] Iteration 4080, lr = 0.001
I1025 23:56:15.503356 16980 solver.cpp:229] Iteration 4100, loss = 0.220606
I1025 23:56:15.503386 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.112516 (* 1 = 0.112516 loss)
I1025 23:56:15.503391 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.10809 (* 1 = 0.10809 loss)
I1025 23:56:15.503396 16980 sgd_solver.cpp:106] Iteration 4100, lr = 0.001
I1025 23:56:19.111433 16980 solver.cpp:229] Iteration 4120, loss = 0.328645
I1025 23:56:19.111464 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.221967 (* 1 = 0.221967 loss)
I1025 23:56:19.111469 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.106678 (* 1 = 0.106678 loss)
I1025 23:56:19.111472 16980 sgd_solver.cpp:106] Iteration 4120, lr = 0.001
I1025 23:56:22.827752 16980 solver.cpp:229] Iteration 4140, loss = 0.23166
I1025 23:56:22.827785 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.13284 (* 1 = 0.13284 loss)
I1025 23:56:22.827790 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0988202 (* 1 = 0.0988202 loss)
I1025 23:56:22.827795 16980 sgd_solver.cpp:106] Iteration 4140, lr = 0.001
I1025 23:56:26.528089 16980 solver.cpp:229] Iteration 4160, loss = 0.287906
I1025 23:56:26.528128 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.155956 (* 1 = 0.155956 loss)
I1025 23:56:26.528133 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.13195 (* 1 = 0.13195 loss)
I1025 23:56:26.528137 16980 sgd_solver.cpp:106] Iteration 4160, lr = 0.001
I1025 23:56:30.357448 16980 solver.cpp:229] Iteration 4180, loss = 0.3333
I1025 23:56:30.357476 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.227639 (* 1 = 0.227639 loss)
I1025 23:56:30.357481 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.105661 (* 1 = 0.105661 loss)
I1025 23:56:30.357486 16980 sgd_solver.cpp:106] Iteration 4180, lr = 0.001
I1025 23:56:34.048650 16980 solver.cpp:229] Iteration 4200, loss = 0.237099
I1025 23:56:34.048682 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.140679 (* 1 = 0.140679 loss)
I1025 23:56:34.048687 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0964196 (* 1 = 0.0964196 loss)
I1025 23:56:34.048692 16980 sgd_solver.cpp:106] Iteration 4200, lr = 0.001
I1025 23:56:37.782960 16980 solver.cpp:229] Iteration 4220, loss = 0.245793
I1025 23:56:37.782992 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.131737 (* 1 = 0.131737 loss)
I1025 23:56:37.782996 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.114057 (* 1 = 0.114057 loss)
I1025 23:56:37.783002 16980 sgd_solver.cpp:106] Iteration 4220, lr = 0.001
I1025 23:56:41.475009 16980 solver.cpp:229] Iteration 4240, loss = 0.209896
I1025 23:56:41.475039 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.127633 (* 1 = 0.127633 loss)
I1025 23:56:41.475044 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0822635 (* 1 = 0.0822635 loss)
I1025 23:56:41.475049 16980 sgd_solver.cpp:106] Iteration 4240, lr = 0.001
I1025 23:56:45.165431 16980 solver.cpp:229] Iteration 4260, loss = 0.0962529
I1025 23:56:45.165458 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0665039 (* 1 = 0.0665039 loss)
I1025 23:56:45.165462 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.029749 (* 1 = 0.029749 loss)
I1025 23:56:45.165467 16980 sgd_solver.cpp:106] Iteration 4260, lr = 0.001
I1025 23:56:48.850234 16980 solver.cpp:229] Iteration 4280, loss = 0.317077
I1025 23:56:48.850265 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.190947 (* 1 = 0.190947 loss)
I1025 23:56:48.850270 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.12613 (* 1 = 0.12613 loss)
I1025 23:56:48.850273 16980 sgd_solver.cpp:106] Iteration 4280, lr = 0.001
I1025 23:56:52.543694 16980 solver.cpp:229] Iteration 4300, loss = 0.377664
I1025 23:56:52.543725 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.161028 (* 1 = 0.161028 loss)
I1025 23:56:52.543730 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.216636 (* 1 = 0.216636 loss)
I1025 23:56:52.543733 16980 sgd_solver.cpp:106] Iteration 4300, lr = 0.001
I1025 23:56:56.223917 16980 solver.cpp:229] Iteration 4320, loss = 0.273073
I1025 23:56:56.223945 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.136621 (* 1 = 0.136621 loss)
I1025 23:56:56.223950 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.136451 (* 1 = 0.136451 loss)
I1025 23:56:56.223964 16980 sgd_solver.cpp:106] Iteration 4320, lr = 0.001
I1025 23:56:59.892369 16980 solver.cpp:229] Iteration 4340, loss = 0.410971
I1025 23:56:59.892401 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.252205 (* 1 = 0.252205 loss)
I1025 23:56:59.892405 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.158765 (* 1 = 0.158765 loss)
I1025 23:56:59.892410 16980 sgd_solver.cpp:106] Iteration 4340, lr = 0.001
I1025 23:57:03.700307 16980 solver.cpp:229] Iteration 4360, loss = 0.188638
I1025 23:57:03.700340 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0955333 (* 1 = 0.0955333 loss)
I1025 23:57:03.700345 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0931048 (* 1 = 0.0931048 loss)
I1025 23:57:03.700350 16980 sgd_solver.cpp:106] Iteration 4360, lr = 0.001
I1025 23:57:07.389397 16980 solver.cpp:229] Iteration 4380, loss = 0.184687
I1025 23:57:07.389430 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.117257 (* 1 = 0.117257 loss)
I1025 23:57:07.389434 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0674299 (* 1 = 0.0674299 loss)
I1025 23:57:07.389438 16980 sgd_solver.cpp:106] Iteration 4380, lr = 0.001
I1025 23:57:11.135941 16980 solver.cpp:229] Iteration 4400, loss = 0.424109
I1025 23:57:11.135970 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.244057 (* 1 = 0.244057 loss)
I1025 23:57:11.135974 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.180052 (* 1 = 0.180052 loss)
I1025 23:57:11.135978 16980 sgd_solver.cpp:106] Iteration 4400, lr = 0.001
I1025 23:57:14.823910 16980 solver.cpp:229] Iteration 4420, loss = 0.432654
I1025 23:57:14.823942 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.262773 (* 1 = 0.262773 loss)
I1025 23:57:14.823946 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.169882 (* 1 = 0.169882 loss)
I1025 23:57:14.823951 16980 sgd_solver.cpp:106] Iteration 4420, lr = 0.001
I1025 23:57:18.506984 16980 solver.cpp:229] Iteration 4440, loss = 0.198325
I1025 23:57:18.507014 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.118497 (* 1 = 0.118497 loss)
I1025 23:57:18.507019 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0798288 (* 1 = 0.0798288 loss)
I1025 23:57:18.507024 16980 sgd_solver.cpp:106] Iteration 4440, lr = 0.001
I1025 23:57:22.169855 16980 solver.cpp:229] Iteration 4460, loss = 0.260966
I1025 23:57:22.169888 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.174711 (* 1 = 0.174711 loss)
I1025 23:57:22.169891 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0862548 (* 1 = 0.0862548 loss)
I1025 23:57:22.169896 16980 sgd_solver.cpp:106] Iteration 4460, lr = 0.001
I1025 23:57:25.857967 16980 solver.cpp:229] Iteration 4480, loss = 0.273658
I1025 23:57:25.858000 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.15429 (* 1 = 0.15429 loss)
I1025 23:57:25.858003 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.119368 (* 1 = 0.119368 loss)
I1025 23:57:25.858008 16980 sgd_solver.cpp:106] Iteration 4480, lr = 0.001
I1025 23:57:29.618132 16980 solver.cpp:229] Iteration 4500, loss = 0.212842
I1025 23:57:29.618161 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.150095 (* 1 = 0.150095 loss)
I1025 23:57:29.618166 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0627469 (* 1 = 0.0627469 loss)
I1025 23:57:29.618170 16980 sgd_solver.cpp:106] Iteration 4500, lr = 0.001
I1025 23:57:33.372584 16980 solver.cpp:229] Iteration 4520, loss = 0.301052
I1025 23:57:33.372617 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.192523 (* 1 = 0.192523 loss)
I1025 23:57:33.372622 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.108529 (* 1 = 0.108529 loss)
I1025 23:57:33.372625 16980 sgd_solver.cpp:106] Iteration 4520, lr = 0.001
I1025 23:57:37.107445 16980 solver.cpp:229] Iteration 4540, loss = 0.142529
I1025 23:57:37.107476 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0913081 (* 1 = 0.0913081 loss)
I1025 23:57:37.107481 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0512207 (* 1 = 0.0512207 loss)
I1025 23:57:37.107486 16980 sgd_solver.cpp:106] Iteration 4540, lr = 0.001
I1025 23:57:40.782687 16980 solver.cpp:229] Iteration 4560, loss = 0.278902
I1025 23:57:40.782718 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.141405 (* 1 = 0.141405 loss)
I1025 23:57:40.782722 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.137496 (* 1 = 0.137496 loss)
I1025 23:57:40.782727 16980 sgd_solver.cpp:106] Iteration 4560, lr = 0.001
I1025 23:57:44.468729 16980 solver.cpp:229] Iteration 4580, loss = 0.36296
I1025 23:57:44.468758 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.180358 (* 1 = 0.180358 loss)
I1025 23:57:44.468762 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.182602 (* 1 = 0.182602 loss)
I1025 23:57:44.468767 16980 sgd_solver.cpp:106] Iteration 4580, lr = 0.001
I1025 23:57:48.273685 16980 solver.cpp:229] Iteration 4600, loss = 0.441855
I1025 23:57:48.273715 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.218093 (* 1 = 0.218093 loss)
I1025 23:57:48.273720 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.223762 (* 1 = 0.223762 loss)
I1025 23:57:48.273725 16980 sgd_solver.cpp:106] Iteration 4600, lr = 0.001
I1025 23:57:51.932135 16980 solver.cpp:229] Iteration 4620, loss = 0.280496
I1025 23:57:51.932168 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.185862 (* 1 = 0.185862 loss)
I1025 23:57:51.932173 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0946342 (* 1 = 0.0946342 loss)
I1025 23:57:51.932178 16980 sgd_solver.cpp:106] Iteration 4620, lr = 0.001
I1025 23:57:55.635988 16980 solver.cpp:229] Iteration 4640, loss = 0.217374
I1025 23:57:55.636030 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.122003 (* 1 = 0.122003 loss)
I1025 23:57:55.636036 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0953717 (* 1 = 0.0953717 loss)
I1025 23:57:55.636040 16980 sgd_solver.cpp:106] Iteration 4640, lr = 0.001
I1025 23:57:59.372386 16980 solver.cpp:229] Iteration 4660, loss = 0.325243
I1025 23:57:59.372419 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.171562 (* 1 = 0.171562 loss)
I1025 23:57:59.372423 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.153681 (* 1 = 0.153681 loss)
I1025 23:57:59.372428 16980 sgd_solver.cpp:106] Iteration 4660, lr = 0.001
I1025 23:58:02.985680 16980 solver.cpp:229] Iteration 4680, loss = 0.216339
I1025 23:58:02.985713 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.128196 (* 1 = 0.128196 loss)
I1025 23:58:02.985716 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0881424 (* 1 = 0.0881424 loss)
I1025 23:58:02.985723 16980 sgd_solver.cpp:106] Iteration 4680, lr = 0.001
I1025 23:58:06.668143 16980 solver.cpp:229] Iteration 4700, loss = 0.0976502
I1025 23:58:06.668172 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0829365 (* 1 = 0.0829365 loss)
I1025 23:58:06.668177 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0147138 (* 1 = 0.0147138 loss)
I1025 23:58:06.668182 16980 sgd_solver.cpp:106] Iteration 4700, lr = 0.001
I1025 23:58:10.242110 16980 solver.cpp:229] Iteration 4720, loss = 0.267362
I1025 23:58:10.242143 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.170057 (* 1 = 0.170057 loss)
I1025 23:58:10.242148 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0973054 (* 1 = 0.0973054 loss)
I1025 23:58:10.242153 16980 sgd_solver.cpp:106] Iteration 4720, lr = 0.001
I1025 23:58:14.025842 16980 solver.cpp:229] Iteration 4740, loss = 0.259941
I1025 23:58:14.025874 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.15421 (* 1 = 0.15421 loss)
I1025 23:58:14.025881 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.105731 (* 1 = 0.105731 loss)
I1025 23:58:14.025885 16980 sgd_solver.cpp:106] Iteration 4740, lr = 0.001
I1025 23:58:17.595481 16980 solver.cpp:229] Iteration 4760, loss = 0.339171
I1025 23:58:17.595511 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.202334 (* 1 = 0.202334 loss)
I1025 23:58:17.595515 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.136837 (* 1 = 0.136837 loss)
I1025 23:58:17.595520 16980 sgd_solver.cpp:106] Iteration 4760, lr = 0.001
I1025 23:58:21.317277 16980 solver.cpp:229] Iteration 4780, loss = 0.431772
I1025 23:58:21.317309 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.2278 (* 1 = 0.2278 loss)
I1025 23:58:21.317314 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.203973 (* 1 = 0.203973 loss)
I1025 23:58:21.317319 16980 sgd_solver.cpp:106] Iteration 4780, lr = 0.001
I1025 23:58:25.068747 16980 solver.cpp:229] Iteration 4800, loss = 0.329762
I1025 23:58:25.068778 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.211385 (* 1 = 0.211385 loss)
I1025 23:58:25.068783 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.118377 (* 1 = 0.118377 loss)
I1025 23:58:25.068788 16980 sgd_solver.cpp:106] Iteration 4800, lr = 0.001
I1025 23:58:28.816952 16980 solver.cpp:229] Iteration 4820, loss = 0.229524
I1025 23:58:28.816985 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.161401 (* 1 = 0.161401 loss)
I1025 23:58:28.816990 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0681236 (* 1 = 0.0681236 loss)
I1025 23:58:28.816995 16980 sgd_solver.cpp:106] Iteration 4820, lr = 0.001
I1025 23:58:32.512264 16980 solver.cpp:229] Iteration 4840, loss = 0.360363
I1025 23:58:32.512296 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.194129 (* 1 = 0.194129 loss)
I1025 23:58:32.512301 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.166234 (* 1 = 0.166234 loss)
I1025 23:58:32.512306 16980 sgd_solver.cpp:106] Iteration 4840, lr = 0.001
I1025 23:58:36.280040 16980 solver.cpp:229] Iteration 4860, loss = 0.211459
I1025 23:58:36.280069 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.150691 (* 1 = 0.150691 loss)
I1025 23:58:36.280074 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0607676 (* 1 = 0.0607676 loss)
I1025 23:58:36.280079 16980 sgd_solver.cpp:106] Iteration 4860, lr = 0.001
I1025 23:58:39.984336 16980 solver.cpp:229] Iteration 4880, loss = 0.2341
I1025 23:58:39.984369 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.144787 (* 1 = 0.144787 loss)
I1025 23:58:39.984375 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0893137 (* 1 = 0.0893137 loss)
I1025 23:58:39.984380 16980 sgd_solver.cpp:106] Iteration 4880, lr = 0.001
I1025 23:58:43.770947 16980 solver.cpp:229] Iteration 4900, loss = 0.445951
I1025 23:58:43.770978 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.215423 (* 1 = 0.215423 loss)
I1025 23:58:43.770983 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.230528 (* 1 = 0.230528 loss)
I1025 23:58:43.770988 16980 sgd_solver.cpp:106] Iteration 4900, lr = 0.001
I1025 23:58:47.564685 16980 solver.cpp:229] Iteration 4920, loss = 0.295763
I1025 23:58:47.564713 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.213716 (* 1 = 0.213716 loss)
I1025 23:58:47.564718 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.082047 (* 1 = 0.082047 loss)
I1025 23:58:47.564723 16980 sgd_solver.cpp:106] Iteration 4920, lr = 0.001
I1025 23:58:51.250212 16980 solver.cpp:229] Iteration 4940, loss = 0.297093
I1025 23:58:51.250243 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.204781 (* 1 = 0.204781 loss)
I1025 23:58:51.250247 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0923115 (* 1 = 0.0923115 loss)
I1025 23:58:51.250252 16980 sgd_solver.cpp:106] Iteration 4940, lr = 0.001
I1025 23:58:55.018419 16980 solver.cpp:229] Iteration 4960, loss = 0.256069
I1025 23:58:55.018450 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.106614 (* 1 = 0.106614 loss)
I1025 23:58:55.018455 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.149455 (* 1 = 0.149455 loss)
I1025 23:58:55.018460 16980 sgd_solver.cpp:106] Iteration 4960, lr = 0.001
I1025 23:58:58.716732 16980 solver.cpp:229] Iteration 4980, loss = 0.266449
I1025 23:58:58.716760 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.163994 (* 1 = 0.163994 loss)
I1025 23:58:58.716765 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.102455 (* 1 = 0.102455 loss)
I1025 23:58:58.716769 16980 sgd_solver.cpp:106] Iteration 4980, lr = 0.001
I1025 23:59:02.459210 16980 solver.cpp:229] Iteration 5000, loss = 0.322677
I1025 23:59:02.459242 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.201667 (* 1 = 0.201667 loss)
I1025 23:59:02.459246 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.12101 (* 1 = 0.12101 loss)
I1025 23:59:02.459251 16980 sgd_solver.cpp:106] Iteration 5000, lr = 0.001
I1025 23:59:06.209558 16980 solver.cpp:229] Iteration 5020, loss = 0.368483
I1025 23:59:06.209588 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.236022 (* 1 = 0.236022 loss)
I1025 23:59:06.209591 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.132461 (* 1 = 0.132461 loss)
I1025 23:59:06.209595 16980 sgd_solver.cpp:106] Iteration 5020, lr = 0.001
I1025 23:59:09.979508 16980 solver.cpp:229] Iteration 5040, loss = 0.328063
I1025 23:59:09.979540 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.157547 (* 1 = 0.157547 loss)
I1025 23:59:09.979544 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.170516 (* 1 = 0.170516 loss)
I1025 23:59:09.979548 16980 sgd_solver.cpp:106] Iteration 5040, lr = 0.001
I1025 23:59:13.731418 16980 solver.cpp:229] Iteration 5060, loss = 0.216058
I1025 23:59:13.731462 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.133764 (* 1 = 0.133764 loss)
I1025 23:59:13.731468 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0822937 (* 1 = 0.0822937 loss)
I1025 23:59:13.731473 16980 sgd_solver.cpp:106] Iteration 5060, lr = 0.001
I1025 23:59:17.522343 16980 solver.cpp:229] Iteration 5080, loss = 0.246753
I1025 23:59:17.522372 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.147095 (* 1 = 0.147095 loss)
I1025 23:59:17.522377 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0996578 (* 1 = 0.0996578 loss)
I1025 23:59:17.522382 16980 sgd_solver.cpp:106] Iteration 5080, lr = 0.001
I1025 23:59:21.273119 16980 solver.cpp:229] Iteration 5100, loss = 0.270963
I1025 23:59:21.273149 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.167082 (* 1 = 0.167082 loss)
I1025 23:59:21.273154 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.103881 (* 1 = 0.103881 loss)
I1025 23:59:21.273157 16980 sgd_solver.cpp:106] Iteration 5100, lr = 0.001
I1025 23:59:24.929365 16980 solver.cpp:229] Iteration 5120, loss = 0.24747
I1025 23:59:24.929397 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.12297 (* 1 = 0.12297 loss)
I1025 23:59:24.929402 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.124499 (* 1 = 0.124499 loss)
I1025 23:59:24.929406 16980 sgd_solver.cpp:106] Iteration 5120, lr = 0.001
I1025 23:59:28.724758 16980 solver.cpp:229] Iteration 5140, loss = 0.342685
I1025 23:59:28.724786 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.122071 (* 1 = 0.122071 loss)
I1025 23:59:28.724789 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.220615 (* 1 = 0.220615 loss)
I1025 23:59:28.724793 16980 sgd_solver.cpp:106] Iteration 5140, lr = 0.001
I1025 23:59:32.403039 16980 solver.cpp:229] Iteration 5160, loss = 0.461546
I1025 23:59:32.403066 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.313421 (* 1 = 0.313421 loss)
I1025 23:59:32.403071 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.148124 (* 1 = 0.148124 loss)
I1025 23:59:32.403075 16980 sgd_solver.cpp:106] Iteration 5160, lr = 0.001
I1025 23:59:36.204633 16980 solver.cpp:229] Iteration 5180, loss = 0.297965
I1025 23:59:36.204660 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.165976 (* 1 = 0.165976 loss)
I1025 23:59:36.204665 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.131989 (* 1 = 0.131989 loss)
I1025 23:59:36.204669 16980 sgd_solver.cpp:106] Iteration 5180, lr = 0.001
I1025 23:59:39.880467 16980 solver.cpp:229] Iteration 5200, loss = 0.534233
I1025 23:59:39.880499 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.233028 (* 1 = 0.233028 loss)
I1025 23:59:39.880503 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.301205 (* 1 = 0.301205 loss)
I1025 23:59:39.880508 16980 sgd_solver.cpp:106] Iteration 5200, lr = 0.001
I1025 23:59:43.675179 16980 solver.cpp:229] Iteration 5220, loss = 0.346628
I1025 23:59:43.675209 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.220648 (* 1 = 0.220648 loss)
I1025 23:59:43.675214 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.12598 (* 1 = 0.12598 loss)
I1025 23:59:43.675218 16980 sgd_solver.cpp:106] Iteration 5220, lr = 0.001
I1025 23:59:47.477355 16980 solver.cpp:229] Iteration 5240, loss = 0.180683
I1025 23:59:47.477387 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.136983 (* 1 = 0.136983 loss)
I1025 23:59:47.477392 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0437002 (* 1 = 0.0437002 loss)
I1025 23:59:47.477397 16980 sgd_solver.cpp:106] Iteration 5240, lr = 0.001
I1025 23:59:51.105000 16980 solver.cpp:229] Iteration 5260, loss = 0.509275
I1025 23:59:51.105031 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.193252 (* 1 = 0.193252 loss)
I1025 23:59:51.105036 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.316023 (* 1 = 0.316023 loss)
I1025 23:59:51.105041 16980 sgd_solver.cpp:106] Iteration 5260, lr = 0.001
I1025 23:59:54.736248 16980 solver.cpp:229] Iteration 5280, loss = 0.372995
I1025 23:59:54.736274 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.187918 (* 1 = 0.187918 loss)
I1025 23:59:54.736279 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.185076 (* 1 = 0.185076 loss)
I1025 23:59:54.736282 16980 sgd_solver.cpp:106] Iteration 5280, lr = 0.001
I1025 23:59:58.344755 16980 solver.cpp:229] Iteration 5300, loss = 0.175775
I1025 23:59:58.344786 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0953307 (* 1 = 0.0953307 loss)
I1025 23:59:58.344790 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0804444 (* 1 = 0.0804444 loss)
I1025 23:59:58.344795 16980 sgd_solver.cpp:106] Iteration 5300, lr = 0.001
I1026 00:00:01.975978 16980 solver.cpp:229] Iteration 5320, loss = 0.268352
I1026 00:00:01.976011 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.154946 (* 1 = 0.154946 loss)
I1026 00:00:01.976016 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.113406 (* 1 = 0.113406 loss)
I1026 00:00:01.976019 16980 sgd_solver.cpp:106] Iteration 5320, lr = 0.001
I1026 00:00:05.718997 16980 solver.cpp:229] Iteration 5340, loss = 0.191619
I1026 00:00:05.719028 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.13258 (* 1 = 0.13258 loss)
I1026 00:00:05.719033 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0590395 (* 1 = 0.0590395 loss)
I1026 00:00:05.719038 16980 sgd_solver.cpp:106] Iteration 5340, lr = 0.001
I1026 00:00:09.445355 16980 solver.cpp:229] Iteration 5360, loss = 0.22824
I1026 00:00:09.445387 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.142496 (* 1 = 0.142496 loss)
I1026 00:00:09.445392 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0857447 (* 1 = 0.0857447 loss)
I1026 00:00:09.445396 16980 sgd_solver.cpp:106] Iteration 5360, lr = 0.001
I1026 00:00:13.237532 16980 solver.cpp:229] Iteration 5380, loss = 0.198954
I1026 00:00:13.237565 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.123268 (* 1 = 0.123268 loss)
I1026 00:00:13.237568 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0756856 (* 1 = 0.0756856 loss)
I1026 00:00:13.237573 16980 sgd_solver.cpp:106] Iteration 5380, lr = 0.001
I1026 00:00:17.010545 16980 solver.cpp:229] Iteration 5400, loss = 0.250005
I1026 00:00:17.010576 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.170322 (* 1 = 0.170322 loss)
I1026 00:00:17.010579 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0796834 (* 1 = 0.0796834 loss)
I1026 00:00:17.010584 16980 sgd_solver.cpp:106] Iteration 5400, lr = 0.001
I1026 00:00:20.677433 16980 solver.cpp:229] Iteration 5420, loss = 0.161553
I1026 00:00:20.677460 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.097149 (* 1 = 0.097149 loss)
I1026 00:00:20.677464 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.064404 (* 1 = 0.064404 loss)
I1026 00:00:20.677469 16980 sgd_solver.cpp:106] Iteration 5420, lr = 0.001
I1026 00:00:24.418300 16980 solver.cpp:229] Iteration 5440, loss = 0.252963
I1026 00:00:24.418330 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.16085 (* 1 = 0.16085 loss)
I1026 00:00:24.418335 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0921125 (* 1 = 0.0921125 loss)
I1026 00:00:24.418340 16980 sgd_solver.cpp:106] Iteration 5440, lr = 0.001
I1026 00:00:28.168920 16980 solver.cpp:229] Iteration 5460, loss = 0.211932
I1026 00:00:28.168951 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.133582 (* 1 = 0.133582 loss)
I1026 00:00:28.168956 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0783496 (* 1 = 0.0783496 loss)
I1026 00:00:28.168961 16980 sgd_solver.cpp:106] Iteration 5460, lr = 0.001
I1026 00:00:31.848218 16980 solver.cpp:229] Iteration 5480, loss = 0.255311
I1026 00:00:31.848248 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.152502 (* 1 = 0.152502 loss)
I1026 00:00:31.848253 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.102809 (* 1 = 0.102809 loss)
I1026 00:00:31.848258 16980 sgd_solver.cpp:106] Iteration 5480, lr = 0.001
I1026 00:00:35.500952 16980 solver.cpp:229] Iteration 5500, loss = 0.322827
I1026 00:00:35.500982 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.180382 (* 1 = 0.180382 loss)
I1026 00:00:35.500986 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.142445 (* 1 = 0.142445 loss)
I1026 00:00:35.500991 16980 sgd_solver.cpp:106] Iteration 5500, lr = 0.001
I1026 00:00:39.267866 16980 solver.cpp:229] Iteration 5520, loss = 0.259771
I1026 00:00:39.267895 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.146201 (* 1 = 0.146201 loss)
I1026 00:00:39.267899 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.11357 (* 1 = 0.11357 loss)
I1026 00:00:39.267904 16980 sgd_solver.cpp:106] Iteration 5520, lr = 0.001
I1026 00:00:42.950464 16980 solver.cpp:229] Iteration 5540, loss = 0.215945
I1026 00:00:42.950495 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.131944 (* 1 = 0.131944 loss)
I1026 00:00:42.950500 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0840012 (* 1 = 0.0840012 loss)
I1026 00:00:42.950505 16980 sgd_solver.cpp:106] Iteration 5540, lr = 0.001
I1026 00:00:46.668824 16980 solver.cpp:229] Iteration 5560, loss = 0.22691
I1026 00:00:46.668854 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.132855 (* 1 = 0.132855 loss)
I1026 00:00:46.668860 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0940554 (* 1 = 0.0940554 loss)
I1026 00:00:46.668864 16980 sgd_solver.cpp:106] Iteration 5560, lr = 0.001
I1026 00:00:50.362295 16980 solver.cpp:229] Iteration 5580, loss = 0.192335
I1026 00:00:50.362326 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.12015 (* 1 = 0.12015 loss)
I1026 00:00:50.362331 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0721856 (* 1 = 0.0721856 loss)
I1026 00:00:50.362336 16980 sgd_solver.cpp:106] Iteration 5580, lr = 0.001
I1026 00:00:54.041348 16980 solver.cpp:229] Iteration 5600, loss = 0.269975
I1026 00:00:54.041381 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.210178 (* 1 = 0.210178 loss)
I1026 00:00:54.041385 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0597969 (* 1 = 0.0597969 loss)
I1026 00:00:54.041390 16980 sgd_solver.cpp:106] Iteration 5600, lr = 0.001
I1026 00:00:57.684077 16980 solver.cpp:229] Iteration 5620, loss = 0.367348
I1026 00:00:57.684108 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.234116 (* 1 = 0.234116 loss)
I1026 00:00:57.684113 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.133232 (* 1 = 0.133232 loss)
I1026 00:00:57.684118 16980 sgd_solver.cpp:106] Iteration 5620, lr = 0.001
I1026 00:01:01.325577 16980 solver.cpp:229] Iteration 5640, loss = 0.259586
I1026 00:01:01.325608 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.125639 (* 1 = 0.125639 loss)
I1026 00:01:01.325611 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.133947 (* 1 = 0.133947 loss)
I1026 00:01:01.325615 16980 sgd_solver.cpp:106] Iteration 5640, lr = 0.001
I1026 00:01:05.018100 16980 solver.cpp:229] Iteration 5660, loss = 0.336592
I1026 00:01:05.018148 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.241168 (* 1 = 0.241168 loss)
I1026 00:01:05.018154 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.095425 (* 1 = 0.095425 loss)
I1026 00:01:05.018158 16980 sgd_solver.cpp:106] Iteration 5660, lr = 0.001
I1026 00:01:08.678429 16980 solver.cpp:229] Iteration 5680, loss = 0.361234
I1026 00:01:08.678457 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.233535 (* 1 = 0.233535 loss)
I1026 00:01:08.678462 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.127699 (* 1 = 0.127699 loss)
I1026 00:01:08.678467 16980 sgd_solver.cpp:106] Iteration 5680, lr = 0.001
I1026 00:01:12.492784 16980 solver.cpp:229] Iteration 5700, loss = 0.263551
I1026 00:01:12.492811 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.142846 (* 1 = 0.142846 loss)
I1026 00:01:12.492815 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.120705 (* 1 = 0.120705 loss)
I1026 00:01:12.492820 16980 sgd_solver.cpp:106] Iteration 5700, lr = 0.001
I1026 00:01:16.146740 16980 solver.cpp:229] Iteration 5720, loss = 0.151961
I1026 00:01:16.146770 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0828342 (* 1 = 0.0828342 loss)
I1026 00:01:16.146773 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0691264 (* 1 = 0.0691264 loss)
I1026 00:01:16.146778 16980 sgd_solver.cpp:106] Iteration 5720, lr = 0.001
I1026 00:01:19.933185 16980 solver.cpp:229] Iteration 5740, loss = 0.1108
I1026 00:01:19.933218 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0791437 (* 1 = 0.0791437 loss)
I1026 00:01:19.933223 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0316568 (* 1 = 0.0316568 loss)
I1026 00:01:19.933228 16980 sgd_solver.cpp:106] Iteration 5740, lr = 0.001
I1026 00:01:23.555420 16980 solver.cpp:229] Iteration 5760, loss = 0.182189
I1026 00:01:23.555457 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0937987 (* 1 = 0.0937987 loss)
I1026 00:01:23.555462 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0883899 (* 1 = 0.0883899 loss)
I1026 00:01:23.555467 16980 sgd_solver.cpp:106] Iteration 5760, lr = 0.001
I1026 00:01:27.166616 16980 solver.cpp:229] Iteration 5780, loss = 0.302394
I1026 00:01:27.166651 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.111245 (* 1 = 0.111245 loss)
I1026 00:01:27.166656 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.191149 (* 1 = 0.191149 loss)
I1026 00:01:27.166661 16980 sgd_solver.cpp:106] Iteration 5780, lr = 0.001
I1026 00:01:30.879345 16980 solver.cpp:229] Iteration 5800, loss = 0.27808
I1026 00:01:30.879376 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.132936 (* 1 = 0.132936 loss)
I1026 00:01:30.879381 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.145144 (* 1 = 0.145144 loss)
I1026 00:01:30.879386 16980 sgd_solver.cpp:106] Iteration 5800, lr = 0.001
I1026 00:01:34.594882 16980 solver.cpp:229] Iteration 5820, loss = 0.296641
I1026 00:01:34.594915 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.201084 (* 1 = 0.201084 loss)
I1026 00:01:34.594921 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0955574 (* 1 = 0.0955574 loss)
I1026 00:01:34.594936 16980 sgd_solver.cpp:106] Iteration 5820, lr = 0.001
I1026 00:01:38.310155 16980 solver.cpp:229] Iteration 5840, loss = 0.233605
I1026 00:01:38.310187 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.118731 (* 1 = 0.118731 loss)
I1026 00:01:38.310194 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.114875 (* 1 = 0.114875 loss)
I1026 00:01:38.310199 16980 sgd_solver.cpp:106] Iteration 5840, lr = 0.001
I1026 00:01:42.037291 16980 solver.cpp:229] Iteration 5860, loss = 0.374048
I1026 00:01:42.037323 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.185792 (* 1 = 0.185792 loss)
I1026 00:01:42.037328 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.188255 (* 1 = 0.188255 loss)
I1026 00:01:42.037335 16980 sgd_solver.cpp:106] Iteration 5860, lr = 0.001
I1026 00:01:45.675305 16980 solver.cpp:229] Iteration 5880, loss = 0.166925
I1026 00:01:45.675339 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0889913 (* 1 = 0.0889913 loss)
I1026 00:01:45.675344 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0779336 (* 1 = 0.0779336 loss)
I1026 00:01:45.675349 16980 sgd_solver.cpp:106] Iteration 5880, lr = 0.001
I1026 00:01:49.362779 16980 solver.cpp:229] Iteration 5900, loss = 0.243092
I1026 00:01:49.362821 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.158506 (* 1 = 0.158506 loss)
I1026 00:01:49.362828 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0845865 (* 1 = 0.0845865 loss)
I1026 00:01:49.362833 16980 sgd_solver.cpp:106] Iteration 5900, lr = 0.001
I1026 00:01:53.098358 16980 solver.cpp:229] Iteration 5920, loss = 0.277806
I1026 00:01:53.098392 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.138906 (* 1 = 0.138906 loss)
I1026 00:01:53.098397 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.1389 (* 1 = 0.1389 loss)
I1026 00:01:53.098402 16980 sgd_solver.cpp:106] Iteration 5920, lr = 0.001
I1026 00:01:56.853078 16980 solver.cpp:229] Iteration 5940, loss = 0.270671
I1026 00:01:56.853127 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.140452 (* 1 = 0.140452 loss)
I1026 00:01:56.853132 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.130219 (* 1 = 0.130219 loss)
I1026 00:01:56.853137 16980 sgd_solver.cpp:106] Iteration 5940, lr = 0.001
I1026 00:02:00.598042 16980 solver.cpp:229] Iteration 5960, loss = 0.336321
I1026 00:02:00.598074 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.2049 (* 1 = 0.2049 loss)
I1026 00:02:00.598079 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.131422 (* 1 = 0.131422 loss)
I1026 00:02:00.598084 16980 sgd_solver.cpp:106] Iteration 5960, lr = 0.001
I1026 00:02:04.242007 16980 solver.cpp:229] Iteration 5980, loss = 0.160994
I1026 00:02:04.242039 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.113419 (* 1 = 0.113419 loss)
I1026 00:02:04.242044 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0475757 (* 1 = 0.0475757 loss)
I1026 00:02:04.242049 16980 sgd_solver.cpp:106] Iteration 5980, lr = 0.001
I1026 00:02:07.942548 16980 solver.cpp:229] Iteration 6000, loss = 0.116187
I1026 00:02:07.942580 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0746662 (* 1 = 0.0746662 loss)
I1026 00:02:07.942585 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0415204 (* 1 = 0.0415204 loss)
I1026 00:02:07.942590 16980 sgd_solver.cpp:106] Iteration 6000, lr = 0.001
I1026 00:02:11.641628 16980 solver.cpp:229] Iteration 6020, loss = 0.194819
I1026 00:02:11.641672 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.114392 (* 1 = 0.114392 loss)
I1026 00:02:11.641677 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.080427 (* 1 = 0.080427 loss)
I1026 00:02:11.641692 16980 sgd_solver.cpp:106] Iteration 6020, lr = 0.001
I1026 00:02:15.371495 16980 solver.cpp:229] Iteration 6040, loss = 0.280723
I1026 00:02:15.371534 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.181072 (* 1 = 0.181072 loss)
I1026 00:02:15.371539 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0996513 (* 1 = 0.0996513 loss)
I1026 00:02:15.371546 16980 sgd_solver.cpp:106] Iteration 6040, lr = 0.001
I1026 00:02:19.155179 16980 solver.cpp:229] Iteration 6060, loss = 0.367807
I1026 00:02:19.155221 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.210929 (* 1 = 0.210929 loss)
I1026 00:02:19.155227 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.156878 (* 1 = 0.156878 loss)
I1026 00:02:19.155232 16980 sgd_solver.cpp:106] Iteration 6060, lr = 0.001
I1026 00:02:22.962359 16980 solver.cpp:229] Iteration 6080, loss = 0.253941
I1026 00:02:22.962391 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.157583 (* 1 = 0.157583 loss)
I1026 00:02:22.962396 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0963582 (* 1 = 0.0963582 loss)
I1026 00:02:22.962401 16980 sgd_solver.cpp:106] Iteration 6080, lr = 0.001
I1026 00:02:26.814752 16980 solver.cpp:229] Iteration 6100, loss = 0.236998
I1026 00:02:26.814785 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.095606 (* 1 = 0.095606 loss)
I1026 00:02:26.814790 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.141392 (* 1 = 0.141392 loss)
I1026 00:02:26.814796 16980 sgd_solver.cpp:106] Iteration 6100, lr = 0.001
I1026 00:02:30.522732 16980 solver.cpp:229] Iteration 6120, loss = 0.242911
I1026 00:02:30.522763 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.151974 (* 1 = 0.151974 loss)
I1026 00:02:30.522768 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0909363 (* 1 = 0.0909363 loss)
I1026 00:02:30.522773 16980 sgd_solver.cpp:106] Iteration 6120, lr = 0.001
I1026 00:02:34.262334 16980 solver.cpp:229] Iteration 6140, loss = 0.193585
I1026 00:02:34.262367 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.108951 (* 1 = 0.108951 loss)
I1026 00:02:34.262372 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0846335 (* 1 = 0.0846335 loss)
I1026 00:02:34.262377 16980 sgd_solver.cpp:106] Iteration 6140, lr = 0.001
I1026 00:02:37.999850 16980 solver.cpp:229] Iteration 6160, loss = 0.121981
I1026 00:02:37.999878 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0909085 (* 1 = 0.0909085 loss)
I1026 00:02:37.999883 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0310725 (* 1 = 0.0310725 loss)
I1026 00:02:37.999888 16980 sgd_solver.cpp:106] Iteration 6160, lr = 0.001
I1026 00:02:41.629977 16980 solver.cpp:229] Iteration 6180, loss = 0.298654
I1026 00:02:41.630010 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.163492 (* 1 = 0.163492 loss)
I1026 00:02:41.630015 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.135162 (* 1 = 0.135162 loss)
I1026 00:02:41.630020 16980 sgd_solver.cpp:106] Iteration 6180, lr = 0.001
I1026 00:02:45.271709 16980 solver.cpp:229] Iteration 6200, loss = 0.23757
I1026 00:02:45.271737 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.147408 (* 1 = 0.147408 loss)
I1026 00:02:45.271742 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.090162 (* 1 = 0.090162 loss)
I1026 00:02:45.271746 16980 sgd_solver.cpp:106] Iteration 6200, lr = 0.001
I1026 00:02:48.917349 16980 solver.cpp:229] Iteration 6220, loss = 0.145495
I1026 00:02:48.917379 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0981274 (* 1 = 0.0981274 loss)
I1026 00:02:48.917384 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0473674 (* 1 = 0.0473674 loss)
I1026 00:02:48.917388 16980 sgd_solver.cpp:106] Iteration 6220, lr = 0.001
I1026 00:02:52.602202 16980 solver.cpp:229] Iteration 6240, loss = 0.168982
I1026 00:02:52.602234 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0954402 (* 1 = 0.0954402 loss)
I1026 00:02:52.602239 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0735417 (* 1 = 0.0735417 loss)
I1026 00:02:52.602244 16980 sgd_solver.cpp:106] Iteration 6240, lr = 0.001
I1026 00:02:56.221493 16980 solver.cpp:229] Iteration 6260, loss = 0.113731
I1026 00:02:56.221524 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0732234 (* 1 = 0.0732234 loss)
I1026 00:02:56.221529 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0405078 (* 1 = 0.0405078 loss)
I1026 00:02:56.221532 16980 sgd_solver.cpp:106] Iteration 6260, lr = 0.001
I1026 00:02:59.955202 16980 solver.cpp:229] Iteration 6280, loss = 0.368001
I1026 00:02:59.955232 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.150122 (* 1 = 0.150122 loss)
I1026 00:02:59.955237 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.217879 (* 1 = 0.217879 loss)
I1026 00:02:59.955241 16980 sgd_solver.cpp:106] Iteration 6280, lr = 0.001
I1026 00:03:03.623695 16980 solver.cpp:229] Iteration 6300, loss = 0.246164
I1026 00:03:03.623728 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.130035 (* 1 = 0.130035 loss)
I1026 00:03:03.623733 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.116128 (* 1 = 0.116128 loss)
I1026 00:03:03.623738 16980 sgd_solver.cpp:106] Iteration 6300, lr = 0.001
I1026 00:03:07.356000 16980 solver.cpp:229] Iteration 6320, loss = 0.236538
I1026 00:03:07.356034 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.121514 (* 1 = 0.121514 loss)
I1026 00:03:07.356037 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.115024 (* 1 = 0.115024 loss)
I1026 00:03:07.356042 16980 sgd_solver.cpp:106] Iteration 6320, lr = 0.001
I1026 00:03:11.031401 16980 solver.cpp:229] Iteration 6340, loss = 0.253845
I1026 00:03:11.031436 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.168637 (* 1 = 0.168637 loss)
I1026 00:03:11.031453 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0852082 (* 1 = 0.0852082 loss)
I1026 00:03:11.031458 16980 sgd_solver.cpp:106] Iteration 6340, lr = 0.001
I1026 00:03:14.775817 16980 solver.cpp:229] Iteration 6360, loss = 0.374312
I1026 00:03:14.775846 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.128783 (* 1 = 0.128783 loss)
I1026 00:03:14.775851 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.245529 (* 1 = 0.245529 loss)
I1026 00:03:14.775856 16980 sgd_solver.cpp:106] Iteration 6360, lr = 0.001
I1026 00:03:18.354909 16980 solver.cpp:229] Iteration 6380, loss = 0.266613
I1026 00:03:18.354941 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.169586 (* 1 = 0.169586 loss)
I1026 00:03:18.354946 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0970274 (* 1 = 0.0970274 loss)
I1026 00:03:18.354950 16980 sgd_solver.cpp:106] Iteration 6380, lr = 0.001
I1026 00:03:22.179718 16980 solver.cpp:229] Iteration 6400, loss = 0.188227
I1026 00:03:22.179765 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.149658 (* 1 = 0.149658 loss)
I1026 00:03:22.179771 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0385692 (* 1 = 0.0385692 loss)
I1026 00:03:22.179775 16980 sgd_solver.cpp:106] Iteration 6400, lr = 0.001
I1026 00:03:25.884667 16980 solver.cpp:229] Iteration 6420, loss = 0.347221
I1026 00:03:25.884698 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.148786 (* 1 = 0.148786 loss)
I1026 00:03:25.884703 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.198436 (* 1 = 0.198436 loss)
I1026 00:03:25.884708 16980 sgd_solver.cpp:106] Iteration 6420, lr = 0.001
I1026 00:03:29.614810 16980 solver.cpp:229] Iteration 6440, loss = 0.209044
I1026 00:03:29.614852 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.12845 (* 1 = 0.12845 loss)
I1026 00:03:29.614858 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0805948 (* 1 = 0.0805948 loss)
I1026 00:03:29.614871 16980 sgd_solver.cpp:106] Iteration 6440, lr = 0.001
I1026 00:03:33.316165 16980 solver.cpp:229] Iteration 6460, loss = 0.296933
I1026 00:03:33.316197 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.14347 (* 1 = 0.14347 loss)
I1026 00:03:33.316201 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.153464 (* 1 = 0.153464 loss)
I1026 00:03:33.316207 16980 sgd_solver.cpp:106] Iteration 6460, lr = 0.001
I1026 00:03:37.015310 16980 solver.cpp:229] Iteration 6480, loss = 0.24533
I1026 00:03:37.015339 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.132994 (* 1 = 0.132994 loss)
I1026 00:03:37.015344 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.112337 (* 1 = 0.112337 loss)
I1026 00:03:37.015348 16980 sgd_solver.cpp:106] Iteration 6480, lr = 0.001
I1026 00:03:40.642127 16980 solver.cpp:229] Iteration 6500, loss = 0.185511
I1026 00:03:40.642155 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.097839 (* 1 = 0.097839 loss)
I1026 00:03:40.642160 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0876717 (* 1 = 0.0876717 loss)
I1026 00:03:40.642164 16980 sgd_solver.cpp:106] Iteration 6500, lr = 0.001
I1026 00:03:44.368160 16980 solver.cpp:229] Iteration 6520, loss = 0.185503
I1026 00:03:44.368201 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0901519 (* 1 = 0.0901519 loss)
I1026 00:03:44.368206 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.095351 (* 1 = 0.095351 loss)
I1026 00:03:44.368211 16980 sgd_solver.cpp:106] Iteration 6520, lr = 0.001
I1026 00:03:48.070415 16980 solver.cpp:229] Iteration 6540, loss = 0.169481
I1026 00:03:48.070446 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.136144 (* 1 = 0.136144 loss)
I1026 00:03:48.070451 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0333376 (* 1 = 0.0333376 loss)
I1026 00:03:48.070454 16980 sgd_solver.cpp:106] Iteration 6540, lr = 0.001
I1026 00:03:51.820808 16980 solver.cpp:229] Iteration 6560, loss = 0.205359
I1026 00:03:51.820837 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.131318 (* 1 = 0.131318 loss)
I1026 00:03:51.820840 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0740417 (* 1 = 0.0740417 loss)
I1026 00:03:51.820845 16980 sgd_solver.cpp:106] Iteration 6560, lr = 0.001
I1026 00:03:55.684109 16980 solver.cpp:229] Iteration 6580, loss = 0.209818
I1026 00:03:55.684141 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.140556 (* 1 = 0.140556 loss)
I1026 00:03:55.684145 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0692622 (* 1 = 0.0692622 loss)
I1026 00:03:55.684150 16980 sgd_solver.cpp:106] Iteration 6580, lr = 0.001
I1026 00:03:59.347146 16980 solver.cpp:229] Iteration 6600, loss = 0.167784
I1026 00:03:59.347179 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.119305 (* 1 = 0.119305 loss)
I1026 00:03:59.347183 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0484794 (* 1 = 0.0484794 loss)
I1026 00:03:59.347188 16980 sgd_solver.cpp:106] Iteration 6600, lr = 0.001
I1026 00:04:03.157744 16980 solver.cpp:229] Iteration 6620, loss = 0.215166
I1026 00:04:03.157776 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.158684 (* 1 = 0.158684 loss)
I1026 00:04:03.157781 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0564823 (* 1 = 0.0564823 loss)
I1026 00:04:03.157786 16980 sgd_solver.cpp:106] Iteration 6620, lr = 0.001
I1026 00:04:06.847003 16980 solver.cpp:229] Iteration 6640, loss = 0.146958
I1026 00:04:06.847034 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.112704 (* 1 = 0.112704 loss)
I1026 00:04:06.847039 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0342533 (* 1 = 0.0342533 loss)
I1026 00:04:06.847043 16980 sgd_solver.cpp:106] Iteration 6640, lr = 0.001
I1026 00:04:10.469251 16980 solver.cpp:229] Iteration 6660, loss = 0.295479
I1026 00:04:10.469282 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.161929 (* 1 = 0.161929 loss)
I1026 00:04:10.469286 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.13355 (* 1 = 0.13355 loss)
I1026 00:04:10.469291 16980 sgd_solver.cpp:106] Iteration 6660, lr = 0.001
I1026 00:04:14.242861 16980 solver.cpp:229] Iteration 6680, loss = 0.355905
I1026 00:04:14.242890 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.167025 (* 1 = 0.167025 loss)
I1026 00:04:14.242894 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.18888 (* 1 = 0.18888 loss)
I1026 00:04:14.242899 16980 sgd_solver.cpp:106] Iteration 6680, lr = 0.001
I1026 00:04:17.990278 16980 solver.cpp:229] Iteration 6700, loss = 0.117899
I1026 00:04:17.990310 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0711501 (* 1 = 0.0711501 loss)
I1026 00:04:17.990315 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0467489 (* 1 = 0.0467489 loss)
I1026 00:04:17.990320 16980 sgd_solver.cpp:106] Iteration 6700, lr = 0.001
I1026 00:04:21.737489 16980 solver.cpp:229] Iteration 6720, loss = 0.193101
I1026 00:04:21.737521 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.112251 (* 1 = 0.112251 loss)
I1026 00:04:21.737526 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0808498 (* 1 = 0.0808498 loss)
I1026 00:04:21.737532 16980 sgd_solver.cpp:106] Iteration 6720, lr = 0.001
I1026 00:04:25.402792 16980 solver.cpp:229] Iteration 6740, loss = 0.280923
I1026 00:04:25.402823 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.169328 (* 1 = 0.169328 loss)
I1026 00:04:25.402828 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.111595 (* 1 = 0.111595 loss)
I1026 00:04:25.402832 16980 sgd_solver.cpp:106] Iteration 6740, lr = 0.001
I1026 00:04:29.058084 16980 solver.cpp:229] Iteration 6760, loss = 0.256627
I1026 00:04:29.058112 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.100111 (* 1 = 0.100111 loss)
I1026 00:04:29.058117 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.156516 (* 1 = 0.156516 loss)
I1026 00:04:29.058121 16980 sgd_solver.cpp:106] Iteration 6760, lr = 0.001
I1026 00:04:32.738508 16980 solver.cpp:229] Iteration 6780, loss = 0.256551
I1026 00:04:32.738538 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.130768 (* 1 = 0.130768 loss)
I1026 00:04:32.738541 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.125783 (* 1 = 0.125783 loss)
I1026 00:04:32.738546 16980 sgd_solver.cpp:106] Iteration 6780, lr = 0.001
I1026 00:04:36.407181 16980 solver.cpp:229] Iteration 6800, loss = 0.127257
I1026 00:04:36.407207 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0955837 (* 1 = 0.0955837 loss)
I1026 00:04:36.407212 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.031673 (* 1 = 0.031673 loss)
I1026 00:04:36.407217 16980 sgd_solver.cpp:106] Iteration 6800, lr = 0.001
I1026 00:04:40.116746 16980 solver.cpp:229] Iteration 6820, loss = 0.188381
I1026 00:04:40.116776 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.105903 (* 1 = 0.105903 loss)
I1026 00:04:40.116781 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0824773 (* 1 = 0.0824773 loss)
I1026 00:04:40.116786 16980 sgd_solver.cpp:106] Iteration 6820, lr = 0.001
I1026 00:04:43.840826 16980 solver.cpp:229] Iteration 6840, loss = 0.393896
I1026 00:04:43.840855 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.222174 (* 1 = 0.222174 loss)
I1026 00:04:43.840859 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.171722 (* 1 = 0.171722 loss)
I1026 00:04:43.840863 16980 sgd_solver.cpp:106] Iteration 6840, lr = 0.001
I1026 00:04:47.505904 16980 solver.cpp:229] Iteration 6860, loss = 0.168008
I1026 00:04:47.505935 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.109748 (* 1 = 0.109748 loss)
I1026 00:04:47.505940 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0582598 (* 1 = 0.0582598 loss)
I1026 00:04:47.505945 16980 sgd_solver.cpp:106] Iteration 6860, lr = 0.001
I1026 00:04:51.154362 16980 solver.cpp:229] Iteration 6880, loss = 0.332297
I1026 00:04:51.154393 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.194425 (* 1 = 0.194425 loss)
I1026 00:04:51.154397 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.137872 (* 1 = 0.137872 loss)
I1026 00:04:51.154402 16980 sgd_solver.cpp:106] Iteration 6880, lr = 0.001
I1026 00:04:54.896117 16980 solver.cpp:229] Iteration 6900, loss = 0.157056
I1026 00:04:54.896147 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.112557 (* 1 = 0.112557 loss)
I1026 00:04:54.896152 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0444991 (* 1 = 0.0444991 loss)
I1026 00:04:54.896155 16980 sgd_solver.cpp:106] Iteration 6900, lr = 0.001
I1026 00:04:58.568325 16980 solver.cpp:229] Iteration 6920, loss = 0.234132
I1026 00:04:58.568356 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.169002 (* 1 = 0.169002 loss)
I1026 00:04:58.568359 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0651297 (* 1 = 0.0651297 loss)
I1026 00:04:58.568364 16980 sgd_solver.cpp:106] Iteration 6920, lr = 0.001
I1026 00:05:02.346817 16980 solver.cpp:229] Iteration 6940, loss = 0.21857
I1026 00:05:02.346848 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.153518 (* 1 = 0.153518 loss)
I1026 00:05:02.346853 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0650519 (* 1 = 0.0650519 loss)
I1026 00:05:02.346858 16980 sgd_solver.cpp:106] Iteration 6940, lr = 0.001
I1026 00:05:06.117765 16980 solver.cpp:229] Iteration 6960, loss = 0.15201
I1026 00:05:06.117794 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0652213 (* 1 = 0.0652213 loss)
I1026 00:05:06.117799 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0867886 (* 1 = 0.0867886 loss)
I1026 00:05:06.117802 16980 sgd_solver.cpp:106] Iteration 6960, lr = 0.001
I1026 00:05:09.850927 16980 solver.cpp:229] Iteration 6980, loss = 0.0980202
I1026 00:05:09.850960 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.076563 (* 1 = 0.076563 loss)
I1026 00:05:09.850965 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0214572 (* 1 = 0.0214572 loss)
I1026 00:05:09.850970 16980 sgd_solver.cpp:106] Iteration 6980, lr = 0.001
I1026 00:05:13.553395 16980 solver.cpp:229] Iteration 7000, loss = 0.216732
I1026 00:05:13.553427 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.134305 (* 1 = 0.134305 loss)
I1026 00:05:13.553433 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0824272 (* 1 = 0.0824272 loss)
I1026 00:05:13.553439 16980 sgd_solver.cpp:106] Iteration 7000, lr = 0.001
I1026 00:05:17.217365 16980 solver.cpp:229] Iteration 7020, loss = 0.185841
I1026 00:05:17.217396 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.101658 (* 1 = 0.101658 loss)
I1026 00:05:17.217401 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0841824 (* 1 = 0.0841824 loss)
I1026 00:05:17.217404 16980 sgd_solver.cpp:106] Iteration 7020, lr = 0.001
I1026 00:05:20.862370 16980 solver.cpp:229] Iteration 7040, loss = 0.16364
I1026 00:05:20.862414 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.103725 (* 1 = 0.103725 loss)
I1026 00:05:20.862421 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0599154 (* 1 = 0.0599154 loss)
I1026 00:05:20.862426 16980 sgd_solver.cpp:106] Iteration 7040, lr = 0.001
I1026 00:05:24.610055 16980 solver.cpp:229] Iteration 7060, loss = 0.173643
I1026 00:05:24.610087 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.134569 (* 1 = 0.134569 loss)
I1026 00:05:24.610092 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0390734 (* 1 = 0.0390734 loss)
I1026 00:05:24.610096 16980 sgd_solver.cpp:106] Iteration 7060, lr = 0.001
I1026 00:05:28.334408 16980 solver.cpp:229] Iteration 7080, loss = 0.289029
I1026 00:05:28.334442 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.119626 (* 1 = 0.119626 loss)
I1026 00:05:28.334447 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.169403 (* 1 = 0.169403 loss)
I1026 00:05:28.334452 16980 sgd_solver.cpp:106] Iteration 7080, lr = 0.001
I1026 00:05:32.078706 16980 solver.cpp:229] Iteration 7100, loss = 0.251644
I1026 00:05:32.078739 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.119585 (* 1 = 0.119585 loss)
I1026 00:05:32.078743 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.132058 (* 1 = 0.132058 loss)
I1026 00:05:32.078748 16980 sgd_solver.cpp:106] Iteration 7100, lr = 0.001
I1026 00:05:35.802847 16980 solver.cpp:229] Iteration 7120, loss = 0.198179
I1026 00:05:35.802880 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.104849 (* 1 = 0.104849 loss)
I1026 00:05:35.802884 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0933302 (* 1 = 0.0933302 loss)
I1026 00:05:35.802889 16980 sgd_solver.cpp:106] Iteration 7120, lr = 0.001
I1026 00:05:39.512441 16980 solver.cpp:229] Iteration 7140, loss = 0.152258
I1026 00:05:39.512475 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.096402 (* 1 = 0.096402 loss)
I1026 00:05:39.512480 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0558559 (* 1 = 0.0558559 loss)
I1026 00:05:39.512483 16980 sgd_solver.cpp:106] Iteration 7140, lr = 0.001
I1026 00:05:43.241850 16980 solver.cpp:229] Iteration 7160, loss = 0.203539
I1026 00:05:43.241883 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0875808 (* 1 = 0.0875808 loss)
I1026 00:05:43.241888 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.115958 (* 1 = 0.115958 loss)
I1026 00:05:43.241894 16980 sgd_solver.cpp:106] Iteration 7160, lr = 0.001
I1026 00:05:46.891856 16980 solver.cpp:229] Iteration 7180, loss = 0.114123
I1026 00:05:46.891891 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.060968 (* 1 = 0.060968 loss)
I1026 00:05:46.891896 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0531551 (* 1 = 0.0531551 loss)
I1026 00:05:46.891901 16980 sgd_solver.cpp:106] Iteration 7180, lr = 0.001
I1026 00:05:50.502888 16980 solver.cpp:229] Iteration 7200, loss = 0.123226
I1026 00:05:50.502920 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0881274 (* 1 = 0.0881274 loss)
I1026 00:05:50.502924 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0350986 (* 1 = 0.0350986 loss)
I1026 00:05:50.502929 16980 sgd_solver.cpp:106] Iteration 7200, lr = 0.001
I1026 00:05:54.205788 16980 solver.cpp:229] Iteration 7220, loss = 0.171084
I1026 00:05:54.205819 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.104554 (* 1 = 0.104554 loss)
I1026 00:05:54.205824 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0665309 (* 1 = 0.0665309 loss)
I1026 00:05:54.205828 16980 sgd_solver.cpp:106] Iteration 7220, lr = 0.001
I1026 00:05:57.788411 16980 solver.cpp:229] Iteration 7240, loss = 0.212132
I1026 00:05:57.788444 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0913986 (* 1 = 0.0913986 loss)
I1026 00:05:57.788449 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.120733 (* 1 = 0.120733 loss)
I1026 00:05:57.788452 16980 sgd_solver.cpp:106] Iteration 7240, lr = 0.001
I1026 00:06:01.518435 16980 solver.cpp:229] Iteration 7260, loss = 0.173857
I1026 00:06:01.518467 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.106874 (* 1 = 0.106874 loss)
I1026 00:06:01.518472 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0669828 (* 1 = 0.0669828 loss)
I1026 00:06:01.518477 16980 sgd_solver.cpp:106] Iteration 7260, lr = 0.001
I1026 00:06:05.237644 16980 solver.cpp:229] Iteration 7280, loss = 0.312261
I1026 00:06:05.237674 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.185589 (* 1 = 0.185589 loss)
I1026 00:06:05.237679 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.126672 (* 1 = 0.126672 loss)
I1026 00:06:05.237684 16980 sgd_solver.cpp:106] Iteration 7280, lr = 0.001
I1026 00:06:08.887259 16980 solver.cpp:229] Iteration 7300, loss = 0.172835
I1026 00:06:08.887289 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.105038 (* 1 = 0.105038 loss)
I1026 00:06:08.887295 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0677968 (* 1 = 0.0677968 loss)
I1026 00:06:08.887300 16980 sgd_solver.cpp:106] Iteration 7300, lr = 0.001
I1026 00:06:12.588271 16980 solver.cpp:229] Iteration 7320, loss = 0.13236
I1026 00:06:12.588300 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0835093 (* 1 = 0.0835093 loss)
I1026 00:06:12.588304 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.048851 (* 1 = 0.048851 loss)
I1026 00:06:12.588310 16980 sgd_solver.cpp:106] Iteration 7320, lr = 0.001
I1026 00:06:16.221105 16980 solver.cpp:229] Iteration 7340, loss = 0.282241
I1026 00:06:16.221137 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.212001 (* 1 = 0.212001 loss)
I1026 00:06:16.221143 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0702398 (* 1 = 0.0702398 loss)
I1026 00:06:16.221146 16980 sgd_solver.cpp:106] Iteration 7340, lr = 0.001
I1026 00:06:20.016499 16980 solver.cpp:229] Iteration 7360, loss = 0.315903
I1026 00:06:20.016530 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.154857 (* 1 = 0.154857 loss)
I1026 00:06:20.016535 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.161046 (* 1 = 0.161046 loss)
I1026 00:06:20.016541 16980 sgd_solver.cpp:106] Iteration 7360, lr = 0.001
I1026 00:06:23.709604 16980 solver.cpp:229] Iteration 7380, loss = 0.177634
I1026 00:06:23.709637 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.135743 (* 1 = 0.135743 loss)
I1026 00:06:23.709643 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0418913 (* 1 = 0.0418913 loss)
I1026 00:06:23.709647 16980 sgd_solver.cpp:106] Iteration 7380, lr = 0.001
I1026 00:06:27.508092 16980 solver.cpp:229] Iteration 7400, loss = 0.284661
I1026 00:06:27.508123 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.124207 (* 1 = 0.124207 loss)
I1026 00:06:27.508128 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.160454 (* 1 = 0.160454 loss)
I1026 00:06:27.508133 16980 sgd_solver.cpp:106] Iteration 7400, lr = 0.001
I1026 00:06:31.194329 16980 solver.cpp:229] Iteration 7420, loss = 0.273448
I1026 00:06:31.194360 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.201272 (* 1 = 0.201272 loss)
I1026 00:06:31.194366 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0721763 (* 1 = 0.0721763 loss)
I1026 00:06:31.194371 16980 sgd_solver.cpp:106] Iteration 7420, lr = 0.001
I1026 00:06:34.827270 16980 solver.cpp:229] Iteration 7440, loss = 0.147046
I1026 00:06:34.827302 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0972284 (* 1 = 0.0972284 loss)
I1026 00:06:34.827308 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0498179 (* 1 = 0.0498179 loss)
I1026 00:06:34.827312 16980 sgd_solver.cpp:106] Iteration 7440, lr = 0.001
I1026 00:06:38.642573 16980 solver.cpp:229] Iteration 7460, loss = 0.1486
I1026 00:06:38.642603 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0949923 (* 1 = 0.0949923 loss)
I1026 00:06:38.642608 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0536078 (* 1 = 0.0536078 loss)
I1026 00:06:38.642613 16980 sgd_solver.cpp:106] Iteration 7460, lr = 0.001
I1026 00:06:42.398430 16980 solver.cpp:229] Iteration 7480, loss = 0.162636
I1026 00:06:42.398463 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0888674 (* 1 = 0.0888674 loss)
I1026 00:06:42.398469 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0737684 (* 1 = 0.0737684 loss)
I1026 00:06:42.398473 16980 sgd_solver.cpp:106] Iteration 7480, lr = 0.001
I1026 00:06:46.182659 16980 solver.cpp:229] Iteration 7500, loss = 0.18332
I1026 00:06:46.182690 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.1465 (* 1 = 0.1465 loss)
I1026 00:06:46.182696 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0368198 (* 1 = 0.0368198 loss)
I1026 00:06:46.182701 16980 sgd_solver.cpp:106] Iteration 7500, lr = 0.001
I1026 00:06:49.799773 16980 solver.cpp:229] Iteration 7520, loss = 0.192103
I1026 00:06:49.799804 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.100577 (* 1 = 0.100577 loss)
I1026 00:06:49.799810 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0915256 (* 1 = 0.0915256 loss)
I1026 00:06:49.799815 16980 sgd_solver.cpp:106] Iteration 7520, lr = 0.001
I1026 00:06:53.529770 16980 solver.cpp:229] Iteration 7540, loss = 0.347984
I1026 00:06:53.529803 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.227571 (* 1 = 0.227571 loss)
I1026 00:06:53.529808 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.120413 (* 1 = 0.120413 loss)
I1026 00:06:53.529813 16980 sgd_solver.cpp:106] Iteration 7540, lr = 0.001
I1026 00:06:57.209136 16980 solver.cpp:229] Iteration 7560, loss = 0.359692
I1026 00:06:57.209167 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.211204 (* 1 = 0.211204 loss)
I1026 00:06:57.209172 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.148488 (* 1 = 0.148488 loss)
I1026 00:06:57.209177 16980 sgd_solver.cpp:106] Iteration 7560, lr = 0.001
I1026 00:07:00.890122 16980 solver.cpp:229] Iteration 7580, loss = 0.535731
I1026 00:07:00.890156 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.294179 (* 1 = 0.294179 loss)
I1026 00:07:00.890162 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.241552 (* 1 = 0.241552 loss)
I1026 00:07:00.890166 16980 sgd_solver.cpp:106] Iteration 7580, lr = 0.001
I1026 00:07:04.708941 16980 solver.cpp:229] Iteration 7600, loss = 0.193959
I1026 00:07:04.708968 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0952529 (* 1 = 0.0952529 loss)
I1026 00:07:04.708974 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0987059 (* 1 = 0.0987059 loss)
I1026 00:07:04.708979 16980 sgd_solver.cpp:106] Iteration 7600, lr = 0.001
I1026 00:07:08.398540 16980 solver.cpp:229] Iteration 7620, loss = 0.236748
I1026 00:07:08.398572 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.197309 (* 1 = 0.197309 loss)
I1026 00:07:08.398577 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0394395 (* 1 = 0.0394395 loss)
I1026 00:07:08.398583 16980 sgd_solver.cpp:106] Iteration 7620, lr = 0.001
I1026 00:07:12.022944 16980 solver.cpp:229] Iteration 7640, loss = 0.20436
I1026 00:07:12.022976 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.125716 (* 1 = 0.125716 loss)
I1026 00:07:12.022982 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0786435 (* 1 = 0.0786435 loss)
I1026 00:07:12.022987 16980 sgd_solver.cpp:106] Iteration 7640, lr = 0.001
I1026 00:07:15.653539 16980 solver.cpp:229] Iteration 7660, loss = 0.26034
I1026 00:07:15.653583 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.103151 (* 1 = 0.103151 loss)
I1026 00:07:15.653589 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.157189 (* 1 = 0.157189 loss)
I1026 00:07:15.653594 16980 sgd_solver.cpp:106] Iteration 7660, lr = 0.001
I1026 00:07:19.279151 16980 solver.cpp:229] Iteration 7680, loss = 0.160938
I1026 00:07:19.279180 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.115623 (* 1 = 0.115623 loss)
I1026 00:07:19.279184 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0453142 (* 1 = 0.0453142 loss)
I1026 00:07:19.279189 16980 sgd_solver.cpp:106] Iteration 7680, lr = 0.001
I1026 00:07:23.031042 16980 solver.cpp:229] Iteration 7700, loss = 0.137546
I1026 00:07:23.031076 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0922339 (* 1 = 0.0922339 loss)
I1026 00:07:23.031081 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0453119 (* 1 = 0.0453119 loss)
I1026 00:07:23.031086 16980 sgd_solver.cpp:106] Iteration 7700, lr = 0.001
I1026 00:07:26.595831 16980 solver.cpp:229] Iteration 7720, loss = 0.131812
I1026 00:07:26.595865 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.079564 (* 1 = 0.079564 loss)
I1026 00:07:26.595870 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0522476 (* 1 = 0.0522476 loss)
I1026 00:07:26.595875 16980 sgd_solver.cpp:106] Iteration 7720, lr = 0.001
I1026 00:07:30.403784 16980 solver.cpp:229] Iteration 7740, loss = 0.284707
I1026 00:07:30.403812 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.153327 (* 1 = 0.153327 loss)
I1026 00:07:30.403817 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.13138 (* 1 = 0.13138 loss)
I1026 00:07:30.403822 16980 sgd_solver.cpp:106] Iteration 7740, lr = 0.001
I1026 00:07:34.184590 16980 solver.cpp:229] Iteration 7760, loss = 0.0943084
I1026 00:07:34.184623 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0627516 (* 1 = 0.0627516 loss)
I1026 00:07:34.184628 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0315568 (* 1 = 0.0315568 loss)
I1026 00:07:34.184633 16980 sgd_solver.cpp:106] Iteration 7760, lr = 0.001
I1026 00:07:37.910857 16980 solver.cpp:229] Iteration 7780, loss = 0.233532
I1026 00:07:37.910898 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.130899 (* 1 = 0.130899 loss)
I1026 00:07:37.910912 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.102633 (* 1 = 0.102633 loss)
I1026 00:07:37.910917 16980 sgd_solver.cpp:106] Iteration 7780, lr = 0.001
I1026 00:07:41.716917 16980 solver.cpp:229] Iteration 7800, loss = 0.301495
I1026 00:07:41.716964 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.20076 (* 1 = 0.20076 loss)
I1026 00:07:41.716969 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.100735 (* 1 = 0.100735 loss)
I1026 00:07:41.716974 16980 sgd_solver.cpp:106] Iteration 7800, lr = 0.001
I1026 00:07:45.387902 16980 solver.cpp:229] Iteration 7820, loss = 0.163949
I1026 00:07:45.387934 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.113515 (* 1 = 0.113515 loss)
I1026 00:07:45.387939 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0504339 (* 1 = 0.0504339 loss)
I1026 00:07:45.387944 16980 sgd_solver.cpp:106] Iteration 7820, lr = 0.001
I1026 00:07:49.099211 16980 solver.cpp:229] Iteration 7840, loss = 0.13723
I1026 00:07:49.099244 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0719192 (* 1 = 0.0719192 loss)
I1026 00:07:49.099248 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0653105 (* 1 = 0.0653105 loss)
I1026 00:07:49.099254 16980 sgd_solver.cpp:106] Iteration 7840, lr = 0.001
I1026 00:07:52.853272 16980 solver.cpp:229] Iteration 7860, loss = 0.0873188
I1026 00:07:52.853303 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0588808 (* 1 = 0.0588808 loss)
I1026 00:07:52.853307 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.028438 (* 1 = 0.028438 loss)
I1026 00:07:52.853312 16980 sgd_solver.cpp:106] Iteration 7860, lr = 0.001
I1026 00:07:56.608063 16980 solver.cpp:229] Iteration 7880, loss = 0.28833
I1026 00:07:56.608094 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.19102 (* 1 = 0.19102 loss)
I1026 00:07:56.608099 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0973098 (* 1 = 0.0973098 loss)
I1026 00:07:56.608103 16980 sgd_solver.cpp:106] Iteration 7880, lr = 0.001
I1026 00:08:00.224005 16980 solver.cpp:229] Iteration 7900, loss = 0.153284
I1026 00:08:00.224035 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.114457 (* 1 = 0.114457 loss)
I1026 00:08:00.224040 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0388268 (* 1 = 0.0388268 loss)
I1026 00:08:00.224043 16980 sgd_solver.cpp:106] Iteration 7900, lr = 0.001
I1026 00:08:03.921707 16980 solver.cpp:229] Iteration 7920, loss = 0.125462
I1026 00:08:03.921741 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.082295 (* 1 = 0.082295 loss)
I1026 00:08:03.921746 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0431672 (* 1 = 0.0431672 loss)
I1026 00:08:03.921749 16980 sgd_solver.cpp:106] Iteration 7920, lr = 0.001
I1026 00:08:07.676190 16980 solver.cpp:229] Iteration 7940, loss = 0.359278
I1026 00:08:07.676221 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.182798 (* 1 = 0.182798 loss)
I1026 00:08:07.676225 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.17648 (* 1 = 0.17648 loss)
I1026 00:08:07.676230 16980 sgd_solver.cpp:106] Iteration 7940, lr = 0.001
I1026 00:08:11.388936 16980 solver.cpp:229] Iteration 7960, loss = 0.233558
I1026 00:08:11.388967 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.127886 (* 1 = 0.127886 loss)
I1026 00:08:11.388972 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.105672 (* 1 = 0.105672 loss)
I1026 00:08:11.388977 16980 sgd_solver.cpp:106] Iteration 7960, lr = 0.001
I1026 00:08:15.156795 16980 solver.cpp:229] Iteration 7980, loss = 0.364574
I1026 00:08:15.156828 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.181828 (* 1 = 0.181828 loss)
I1026 00:08:15.156832 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.182746 (* 1 = 0.182746 loss)
I1026 00:08:15.156837 16980 sgd_solver.cpp:106] Iteration 7980, lr = 0.001
I1026 00:08:18.789335 16980 solver.cpp:229] Iteration 8000, loss = 0.20363
I1026 00:08:18.789361 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.119206 (* 1 = 0.119206 loss)
I1026 00:08:18.789366 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0844236 (* 1 = 0.0844236 loss)
I1026 00:08:18.789371 16980 sgd_solver.cpp:106] Iteration 8000, lr = 0.001
I1026 00:08:22.451208 16980 solver.cpp:229] Iteration 8020, loss = 0.143418
I1026 00:08:22.451239 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0931877 (* 1 = 0.0931877 loss)
I1026 00:08:22.451244 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0502307 (* 1 = 0.0502307 loss)
I1026 00:08:22.451249 16980 sgd_solver.cpp:106] Iteration 8020, lr = 0.001
I1026 00:08:26.223537 16980 solver.cpp:229] Iteration 8040, loss = 0.113112
I1026 00:08:26.223567 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0873495 (* 1 = 0.0873495 loss)
I1026 00:08:26.223572 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0257628 (* 1 = 0.0257628 loss)
I1026 00:08:26.223577 16980 sgd_solver.cpp:106] Iteration 8040, lr = 0.001
I1026 00:08:29.989778 16980 solver.cpp:229] Iteration 8060, loss = 0.116846
I1026 00:08:29.989811 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0900389 (* 1 = 0.0900389 loss)
I1026 00:08:29.989816 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0268072 (* 1 = 0.0268072 loss)
I1026 00:08:29.989821 16980 sgd_solver.cpp:106] Iteration 8060, lr = 0.001
I1026 00:08:33.627954 16980 solver.cpp:229] Iteration 8080, loss = 0.286919
I1026 00:08:33.627985 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.171807 (* 1 = 0.171807 loss)
I1026 00:08:33.627990 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.115112 (* 1 = 0.115112 loss)
I1026 00:08:33.627995 16980 sgd_solver.cpp:106] Iteration 8080, lr = 0.001
I1026 00:08:37.267238 16980 solver.cpp:229] Iteration 8100, loss = 0.422209
I1026 00:08:37.267271 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.231372 (* 1 = 0.231372 loss)
I1026 00:08:37.267277 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.190837 (* 1 = 0.190837 loss)
I1026 00:08:37.267280 16980 sgd_solver.cpp:106] Iteration 8100, lr = 0.001
I1026 00:08:41.008788 16980 solver.cpp:229] Iteration 8120, loss = 0.139654
I1026 00:08:41.008827 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0854398 (* 1 = 0.0854398 loss)
I1026 00:08:41.008832 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0542142 (* 1 = 0.0542142 loss)
I1026 00:08:41.008837 16980 sgd_solver.cpp:106] Iteration 8120, lr = 0.001
I1026 00:08:44.656846 16980 solver.cpp:229] Iteration 8140, loss = 0.325149
I1026 00:08:44.656873 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.161927 (* 1 = 0.161927 loss)
I1026 00:08:44.656878 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.163222 (* 1 = 0.163222 loss)
I1026 00:08:44.656884 16980 sgd_solver.cpp:106] Iteration 8140, lr = 0.001
I1026 00:08:48.335304 16980 solver.cpp:229] Iteration 8160, loss = 0.204962
I1026 00:08:48.335336 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.102235 (* 1 = 0.102235 loss)
I1026 00:08:48.335341 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.102727 (* 1 = 0.102727 loss)
I1026 00:08:48.335346 16980 sgd_solver.cpp:106] Iteration 8160, lr = 0.001
I1026 00:08:52.125154 16980 solver.cpp:229] Iteration 8180, loss = 0.498001
I1026 00:08:52.125185 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.222877 (* 1 = 0.222877 loss)
I1026 00:08:52.125190 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.275123 (* 1 = 0.275123 loss)
I1026 00:08:52.125195 16980 sgd_solver.cpp:106] Iteration 8180, lr = 0.001
I1026 00:08:55.856362 16980 solver.cpp:229] Iteration 8200, loss = 0.245383
I1026 00:08:55.856395 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.108295 (* 1 = 0.108295 loss)
I1026 00:08:55.856416 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.137088 (* 1 = 0.137088 loss)
I1026 00:08:55.856421 16980 sgd_solver.cpp:106] Iteration 8200, lr = 0.001
I1026 00:08:59.556099 16980 solver.cpp:229] Iteration 8220, loss = 0.155462
I1026 00:08:59.556133 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.12741 (* 1 = 0.12741 loss)
I1026 00:08:59.556138 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0280516 (* 1 = 0.0280516 loss)
I1026 00:08:59.556143 16980 sgd_solver.cpp:106] Iteration 8220, lr = 0.001
I1026 00:09:03.300660 16980 solver.cpp:229] Iteration 8240, loss = 0.321885
I1026 00:09:03.300693 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.187124 (* 1 = 0.187124 loss)
I1026 00:09:03.300701 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.13476 (* 1 = 0.13476 loss)
I1026 00:09:03.300706 16980 sgd_solver.cpp:106] Iteration 8240, lr = 0.001
I1026 00:09:07.060966 16980 solver.cpp:229] Iteration 8260, loss = 0.171348
I1026 00:09:07.060997 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.101012 (* 1 = 0.101012 loss)
I1026 00:09:07.061003 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0703364 (* 1 = 0.0703364 loss)
I1026 00:09:07.061008 16980 sgd_solver.cpp:106] Iteration 8260, lr = 0.001
I1026 00:09:10.791715 16980 solver.cpp:229] Iteration 8280, loss = 0.228453
I1026 00:09:10.791749 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.125419 (* 1 = 0.125419 loss)
I1026 00:09:10.791754 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.103034 (* 1 = 0.103034 loss)
I1026 00:09:10.791759 16980 sgd_solver.cpp:106] Iteration 8280, lr = 0.001
I1026 00:09:14.518882 16980 solver.cpp:229] Iteration 8300, loss = 0.0960934
I1026 00:09:14.518914 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0545856 (* 1 = 0.0545856 loss)
I1026 00:09:14.518919 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0415077 (* 1 = 0.0415077 loss)
I1026 00:09:14.518925 16980 sgd_solver.cpp:106] Iteration 8300, lr = 0.001
I1026 00:09:18.289928 16980 solver.cpp:229] Iteration 8320, loss = 0.182849
I1026 00:09:18.289963 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.127382 (* 1 = 0.127382 loss)
I1026 00:09:18.289968 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0554671 (* 1 = 0.0554671 loss)
I1026 00:09:18.289973 16980 sgd_solver.cpp:106] Iteration 8320, lr = 0.001
I1026 00:09:22.025162 16980 solver.cpp:229] Iteration 8340, loss = 0.102681
I1026 00:09:22.025197 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0522306 (* 1 = 0.0522306 loss)
I1026 00:09:22.025202 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0504509 (* 1 = 0.0504509 loss)
I1026 00:09:22.025207 16980 sgd_solver.cpp:106] Iteration 8340, lr = 0.001
I1026 00:09:25.689167 16980 solver.cpp:229] Iteration 8360, loss = 0.240672
I1026 00:09:25.689200 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.168058 (* 1 = 0.168058 loss)
I1026 00:09:25.689206 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0726144 (* 1 = 0.0726144 loss)
I1026 00:09:25.689210 16980 sgd_solver.cpp:106] Iteration 8360, lr = 0.001
I1026 00:09:29.326115 16980 solver.cpp:229] Iteration 8380, loss = 0.263383
I1026 00:09:29.326148 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.144027 (* 1 = 0.144027 loss)
I1026 00:09:29.326153 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.119356 (* 1 = 0.119356 loss)
I1026 00:09:29.326159 16980 sgd_solver.cpp:106] Iteration 8380, lr = 0.001
I1026 00:09:32.972465 16980 solver.cpp:229] Iteration 8400, loss = 0.284273
I1026 00:09:32.972497 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.128994 (* 1 = 0.128994 loss)
I1026 00:09:32.972502 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.155279 (* 1 = 0.155279 loss)
I1026 00:09:32.972506 16980 sgd_solver.cpp:106] Iteration 8400, lr = 0.001
I1026 00:09:36.704216 16980 solver.cpp:229] Iteration 8420, loss = 0.295008
I1026 00:09:36.704242 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.131113 (* 1 = 0.131113 loss)
I1026 00:09:36.704247 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.163894 (* 1 = 0.163894 loss)
I1026 00:09:36.704252 16980 sgd_solver.cpp:106] Iteration 8420, lr = 0.001
I1026 00:09:40.325191 16980 solver.cpp:229] Iteration 8440, loss = 0.143931
I1026 00:09:40.325222 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0793126 (* 1 = 0.0793126 loss)
I1026 00:09:40.325227 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0646179 (* 1 = 0.0646179 loss)
I1026 00:09:40.325232 16980 sgd_solver.cpp:106] Iteration 8440, lr = 0.001
I1026 00:09:44.029180 16980 solver.cpp:229] Iteration 8460, loss = 0.143358
I1026 00:09:44.029211 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0831821 (* 1 = 0.0831821 loss)
I1026 00:09:44.029217 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0601756 (* 1 = 0.0601756 loss)
I1026 00:09:44.029220 16980 sgd_solver.cpp:106] Iteration 8460, lr = 0.001
I1026 00:09:47.738991 16980 solver.cpp:229] Iteration 8480, loss = 0.170869
I1026 00:09:47.739024 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0895152 (* 1 = 0.0895152 loss)
I1026 00:09:47.739028 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0813533 (* 1 = 0.0813533 loss)
I1026 00:09:47.739033 16980 sgd_solver.cpp:106] Iteration 8480, lr = 0.001
I1026 00:09:51.481528 16980 solver.cpp:229] Iteration 8500, loss = 0.174158
I1026 00:09:51.481559 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0945396 (* 1 = 0.0945396 loss)
I1026 00:09:51.481564 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0796181 (* 1 = 0.0796181 loss)
I1026 00:09:51.481570 16980 sgd_solver.cpp:106] Iteration 8500, lr = 0.001
I1026 00:09:55.264427 16980 solver.cpp:229] Iteration 8520, loss = 0.0997224
I1026 00:09:55.264461 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.063283 (* 1 = 0.063283 loss)
I1026 00:09:55.264467 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0364394 (* 1 = 0.0364394 loss)
I1026 00:09:55.264472 16980 sgd_solver.cpp:106] Iteration 8520, lr = 0.001
I1026 00:09:58.989843 16980 solver.cpp:229] Iteration 8540, loss = 0.212466
I1026 00:09:58.989874 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.110596 (* 1 = 0.110596 loss)
I1026 00:09:58.989878 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.10187 (* 1 = 0.10187 loss)
I1026 00:09:58.989883 16980 sgd_solver.cpp:106] Iteration 8540, lr = 0.001
I1026 00:10:02.801024 16980 solver.cpp:229] Iteration 8560, loss = 0.277323
I1026 00:10:02.801057 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.183463 (* 1 = 0.183463 loss)
I1026 00:10:02.801062 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0938604 (* 1 = 0.0938604 loss)
I1026 00:10:02.801067 16980 sgd_solver.cpp:106] Iteration 8560, lr = 0.001
I1026 00:10:06.570066 16980 solver.cpp:229] Iteration 8580, loss = 0.263856
I1026 00:10:06.570098 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.154273 (* 1 = 0.154273 loss)
I1026 00:10:06.570103 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.109583 (* 1 = 0.109583 loss)
I1026 00:10:06.570108 16980 sgd_solver.cpp:106] Iteration 8580, lr = 0.001
I1026 00:10:10.319360 16980 solver.cpp:229] Iteration 8600, loss = 0.276135
I1026 00:10:10.319391 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.148339 (* 1 = 0.148339 loss)
I1026 00:10:10.319396 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.127796 (* 1 = 0.127796 loss)
I1026 00:10:10.319401 16980 sgd_solver.cpp:106] Iteration 8600, lr = 0.001
I1026 00:10:13.975229 16980 solver.cpp:229] Iteration 8620, loss = 0.135904
I1026 00:10:13.975260 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0809444 (* 1 = 0.0809444 loss)
I1026 00:10:13.975265 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0549595 (* 1 = 0.0549595 loss)
I1026 00:10:13.975268 16980 sgd_solver.cpp:106] Iteration 8620, lr = 0.001
I1026 00:10:17.756475 16980 solver.cpp:229] Iteration 8640, loss = 0.203902
I1026 00:10:17.756507 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.118801 (* 1 = 0.118801 loss)
I1026 00:10:17.756512 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0851013 (* 1 = 0.0851013 loss)
I1026 00:10:17.756517 16980 sgd_solver.cpp:106] Iteration 8640, lr = 0.001
I1026 00:10:21.570165 16980 solver.cpp:229] Iteration 8660, loss = 0.155237
I1026 00:10:21.570195 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0940613 (* 1 = 0.0940613 loss)
I1026 00:10:21.570200 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0611752 (* 1 = 0.0611752 loss)
I1026 00:10:21.570204 16980 sgd_solver.cpp:106] Iteration 8660, lr = 0.001
I1026 00:10:25.232283 16980 solver.cpp:229] Iteration 8680, loss = 0.203443
I1026 00:10:25.232311 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.137486 (* 1 = 0.137486 loss)
I1026 00:10:25.232316 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0659564 (* 1 = 0.0659564 loss)
I1026 00:10:25.232321 16980 sgd_solver.cpp:106] Iteration 8680, lr = 0.001
I1026 00:10:28.969143 16980 solver.cpp:229] Iteration 8700, loss = 0.184025
I1026 00:10:28.969174 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.086575 (* 1 = 0.086575 loss)
I1026 00:10:28.969179 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0974499 (* 1 = 0.0974499 loss)
I1026 00:10:28.969184 16980 sgd_solver.cpp:106] Iteration 8700, lr = 0.001
I1026 00:10:32.648084 16980 solver.cpp:229] Iteration 8720, loss = 0.269081
I1026 00:10:32.648109 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.143143 (* 1 = 0.143143 loss)
I1026 00:10:32.648113 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.125938 (* 1 = 0.125938 loss)
I1026 00:10:32.648118 16980 sgd_solver.cpp:106] Iteration 8720, lr = 0.001
I1026 00:10:36.399793 16980 solver.cpp:229] Iteration 8740, loss = 0.129146
I1026 00:10:36.399827 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0977054 (* 1 = 0.0977054 loss)
I1026 00:10:36.399832 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0314411 (* 1 = 0.0314411 loss)
I1026 00:10:36.399835 16980 sgd_solver.cpp:106] Iteration 8740, lr = 0.001
I1026 00:10:40.134135 16980 solver.cpp:229] Iteration 8760, loss = 0.116293
I1026 00:10:40.134178 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0544091 (* 1 = 0.0544091 loss)
I1026 00:10:40.134182 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0618839 (* 1 = 0.0618839 loss)
I1026 00:10:40.134186 16980 sgd_solver.cpp:106] Iteration 8760, lr = 0.001
I1026 00:10:43.807672 16980 solver.cpp:229] Iteration 8780, loss = 0.259289
I1026 00:10:43.807703 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.157984 (* 1 = 0.157984 loss)
I1026 00:10:43.807708 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.101305 (* 1 = 0.101305 loss)
I1026 00:10:43.807711 16980 sgd_solver.cpp:106] Iteration 8780, lr = 0.001
I1026 00:10:47.511147 16980 solver.cpp:229] Iteration 8800, loss = 0.118174
I1026 00:10:47.511178 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0825823 (* 1 = 0.0825823 loss)
I1026 00:10:47.511183 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0355917 (* 1 = 0.0355917 loss)
I1026 00:10:47.511199 16980 sgd_solver.cpp:106] Iteration 8800, lr = 0.001
I1026 00:10:51.139941 16980 solver.cpp:229] Iteration 8820, loss = 0.121264
I1026 00:10:51.139974 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.088847 (* 1 = 0.088847 loss)
I1026 00:10:51.139979 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0324165 (* 1 = 0.0324165 loss)
I1026 00:10:51.139986 16980 sgd_solver.cpp:106] Iteration 8820, lr = 0.001
I1026 00:10:54.962280 16980 solver.cpp:229] Iteration 8840, loss = 0.121166
I1026 00:10:54.962313 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.067746 (* 1 = 0.067746 loss)
I1026 00:10:54.962318 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0534197 (* 1 = 0.0534197 loss)
I1026 00:10:54.962323 16980 sgd_solver.cpp:106] Iteration 8840, lr = 0.001
I1026 00:10:58.650365 16980 solver.cpp:229] Iteration 8860, loss = 0.140339
I1026 00:10:58.650395 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0532964 (* 1 = 0.0532964 loss)
I1026 00:10:58.650400 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0870422 (* 1 = 0.0870422 loss)
I1026 00:10:58.650404 16980 sgd_solver.cpp:106] Iteration 8860, lr = 0.001
I1026 00:11:02.390597 16980 solver.cpp:229] Iteration 8880, loss = 0.216346
I1026 00:11:02.390630 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.119201 (* 1 = 0.119201 loss)
I1026 00:11:02.390652 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0971451 (* 1 = 0.0971451 loss)
I1026 00:11:02.390657 16980 sgd_solver.cpp:106] Iteration 8880, lr = 0.001
I1026 00:11:06.122117 16980 solver.cpp:229] Iteration 8900, loss = 0.149081
I1026 00:11:06.122146 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0919452 (* 1 = 0.0919452 loss)
I1026 00:11:06.122153 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.057136 (* 1 = 0.057136 loss)
I1026 00:11:06.122158 16980 sgd_solver.cpp:106] Iteration 8900, lr = 0.001
I1026 00:11:09.789476 16980 solver.cpp:229] Iteration 8920, loss = 0.138655
I1026 00:11:09.789508 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.115243 (* 1 = 0.115243 loss)
I1026 00:11:09.789515 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0234121 (* 1 = 0.0234121 loss)
I1026 00:11:09.789520 16980 sgd_solver.cpp:106] Iteration 8920, lr = 0.001
I1026 00:11:13.472757 16980 solver.cpp:229] Iteration 8940, loss = 0.226057
I1026 00:11:13.472790 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.144605 (* 1 = 0.144605 loss)
I1026 00:11:13.472795 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.081452 (* 1 = 0.081452 loss)
I1026 00:11:13.472800 16980 sgd_solver.cpp:106] Iteration 8940, lr = 0.001
I1026 00:11:17.172140 16980 solver.cpp:229] Iteration 8960, loss = 0.160718
I1026 00:11:17.172173 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.101941 (* 1 = 0.101941 loss)
I1026 00:11:17.172178 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0587769 (* 1 = 0.0587769 loss)
I1026 00:11:17.172183 16980 sgd_solver.cpp:106] Iteration 8960, lr = 0.001
I1026 00:11:20.900290 16980 solver.cpp:229] Iteration 8980, loss = 0.16558
I1026 00:11:20.900321 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0964732 (* 1 = 0.0964732 loss)
I1026 00:11:20.900326 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0691065 (* 1 = 0.0691065 loss)
I1026 00:11:20.900331 16980 sgd_solver.cpp:106] Iteration 8980, lr = 0.001
I1026 00:11:24.557888 16980 solver.cpp:229] Iteration 9000, loss = 0.147261
I1026 00:11:24.557919 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0620184 (* 1 = 0.0620184 loss)
I1026 00:11:24.557924 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0852428 (* 1 = 0.0852428 loss)
I1026 00:11:24.557927 16980 sgd_solver.cpp:106] Iteration 9000, lr = 0.001
I1026 00:11:28.313762 16980 solver.cpp:229] Iteration 9020, loss = 0.102413
I1026 00:11:28.313794 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.065425 (* 1 = 0.065425 loss)
I1026 00:11:28.313799 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0369885 (* 1 = 0.0369885 loss)
I1026 00:11:28.313804 16980 sgd_solver.cpp:106] Iteration 9020, lr = 0.001
I1026 00:11:31.851534 16980 solver.cpp:229] Iteration 9040, loss = 0.193241
I1026 00:11:31.851567 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.104074 (* 1 = 0.104074 loss)
I1026 00:11:31.851572 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0891671 (* 1 = 0.0891671 loss)
I1026 00:11:31.851575 16980 sgd_solver.cpp:106] Iteration 9040, lr = 0.001
I1026 00:11:35.484041 16980 solver.cpp:229] Iteration 9060, loss = 0.239596
I1026 00:11:35.484073 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.100182 (* 1 = 0.100182 loss)
I1026 00:11:35.484078 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.139413 (* 1 = 0.139413 loss)
I1026 00:11:35.484082 16980 sgd_solver.cpp:106] Iteration 9060, lr = 0.001
I1026 00:11:39.249512 16980 solver.cpp:229] Iteration 9080, loss = 0.181667
I1026 00:11:39.249546 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.108129 (* 1 = 0.108129 loss)
I1026 00:11:39.249550 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0735388 (* 1 = 0.0735388 loss)
I1026 00:11:39.249555 16980 sgd_solver.cpp:106] Iteration 9080, lr = 0.001
I1026 00:11:43.012257 16980 solver.cpp:229] Iteration 9100, loss = 0.317675
I1026 00:11:43.012290 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.170369 (* 1 = 0.170369 loss)
I1026 00:11:43.012296 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.147306 (* 1 = 0.147306 loss)
I1026 00:11:43.012301 16980 sgd_solver.cpp:106] Iteration 9100, lr = 0.001
I1026 00:11:46.640810 16980 solver.cpp:229] Iteration 9120, loss = 0.10238
I1026 00:11:46.640839 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0837621 (* 1 = 0.0837621 loss)
I1026 00:11:46.640843 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0186182 (* 1 = 0.0186182 loss)
I1026 00:11:46.640848 16980 sgd_solver.cpp:106] Iteration 9120, lr = 0.001
I1026 00:11:50.355128 16980 solver.cpp:229] Iteration 9140, loss = 0.196397
I1026 00:11:50.355159 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.145638 (* 1 = 0.145638 loss)
I1026 00:11:50.355165 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0507587 (* 1 = 0.0507587 loss)
I1026 00:11:50.355170 16980 sgd_solver.cpp:106] Iteration 9140, lr = 0.001
I1026 00:11:54.156651 16980 solver.cpp:229] Iteration 9160, loss = 0.113064
I1026 00:11:54.156682 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0561578 (* 1 = 0.0561578 loss)
I1026 00:11:54.156687 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0569063 (* 1 = 0.0569063 loss)
I1026 00:11:54.156692 16980 sgd_solver.cpp:106] Iteration 9160, lr = 0.001
I1026 00:11:57.822368 16980 solver.cpp:229] Iteration 9180, loss = 0.369041
I1026 00:11:57.822402 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.208581 (* 1 = 0.208581 loss)
I1026 00:11:57.822408 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.16046 (* 1 = 0.16046 loss)
I1026 00:11:57.822413 16980 sgd_solver.cpp:106] Iteration 9180, lr = 0.001
I1026 00:12:01.548858 16980 solver.cpp:229] Iteration 9200, loss = 0.176284
I1026 00:12:01.548890 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.110193 (* 1 = 0.110193 loss)
I1026 00:12:01.548895 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0660909 (* 1 = 0.0660909 loss)
I1026 00:12:01.548900 16980 sgd_solver.cpp:106] Iteration 9200, lr = 0.001
I1026 00:12:05.298671 16980 solver.cpp:229] Iteration 9220, loss = 0.165506
I1026 00:12:05.298702 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0872929 (* 1 = 0.0872929 loss)
I1026 00:12:05.298707 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.078213 (* 1 = 0.078213 loss)
I1026 00:12:05.298710 16980 sgd_solver.cpp:106] Iteration 9220, lr = 0.001
I1026 00:12:08.898113 16980 solver.cpp:229] Iteration 9240, loss = 0.143696
I1026 00:12:08.898144 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0970129 (* 1 = 0.0970129 loss)
I1026 00:12:08.898149 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0466826 (* 1 = 0.0466826 loss)
I1026 00:12:08.898154 16980 sgd_solver.cpp:106] Iteration 9240, lr = 0.001
I1026 00:12:12.585810 16980 solver.cpp:229] Iteration 9260, loss = 0.20537
I1026 00:12:12.585840 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.152296 (* 1 = 0.152296 loss)
I1026 00:12:12.585845 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0530746 (* 1 = 0.0530746 loss)
I1026 00:12:12.585850 16980 sgd_solver.cpp:106] Iteration 9260, lr = 0.001
I1026 00:12:16.255395 16980 solver.cpp:229] Iteration 9280, loss = 0.107984
I1026 00:12:16.255426 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0757028 (* 1 = 0.0757028 loss)
I1026 00:12:16.255434 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0322811 (* 1 = 0.0322811 loss)
I1026 00:12:16.255439 16980 sgd_solver.cpp:106] Iteration 9280, lr = 0.001
I1026 00:12:19.849758 16980 solver.cpp:229] Iteration 9300, loss = 0.151668
I1026 00:12:19.849789 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.10775 (* 1 = 0.10775 loss)
I1026 00:12:19.849795 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0439177 (* 1 = 0.0439177 loss)
I1026 00:12:19.849800 16980 sgd_solver.cpp:106] Iteration 9300, lr = 0.001
I1026 00:12:23.515981 16980 solver.cpp:229] Iteration 9320, loss = 0.12352
I1026 00:12:23.516026 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0981429 (* 1 = 0.0981429 loss)
I1026 00:12:23.516041 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.025377 (* 1 = 0.025377 loss)
I1026 00:12:23.516044 16980 sgd_solver.cpp:106] Iteration 9320, lr = 0.001
I1026 00:12:27.286988 16980 solver.cpp:229] Iteration 9340, loss = 0.133054
I1026 00:12:27.287020 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0705337 (* 1 = 0.0705337 loss)
I1026 00:12:27.287025 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0625203 (* 1 = 0.0625203 loss)
I1026 00:12:27.287030 16980 sgd_solver.cpp:106] Iteration 9340, lr = 0.001
I1026 00:12:31.018602 16980 solver.cpp:229] Iteration 9360, loss = 0.146073
I1026 00:12:31.018635 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0979684 (* 1 = 0.0979684 loss)
I1026 00:12:31.018640 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0481047 (* 1 = 0.0481047 loss)
I1026 00:12:31.018643 16980 sgd_solver.cpp:106] Iteration 9360, lr = 0.001
I1026 00:12:34.732563 16980 solver.cpp:229] Iteration 9380, loss = 0.152253
I1026 00:12:34.732591 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0980437 (* 1 = 0.0980437 loss)
I1026 00:12:34.732595 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0542095 (* 1 = 0.0542095 loss)
I1026 00:12:34.732600 16980 sgd_solver.cpp:106] Iteration 9380, lr = 0.001
I1026 00:12:38.426960 16980 solver.cpp:229] Iteration 9400, loss = 0.294843
I1026 00:12:38.426993 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.146208 (* 1 = 0.146208 loss)
I1026 00:12:38.426997 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.148635 (* 1 = 0.148635 loss)
I1026 00:12:38.427001 16980 sgd_solver.cpp:106] Iteration 9400, lr = 0.001
I1026 00:12:42.066448 16980 solver.cpp:229] Iteration 9420, loss = 0.0821618
I1026 00:12:42.066483 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0508781 (* 1 = 0.0508781 loss)
I1026 00:12:42.066488 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0312838 (* 1 = 0.0312838 loss)
I1026 00:12:42.066493 16980 sgd_solver.cpp:106] Iteration 9420, lr = 0.001
I1026 00:12:45.866183 16980 solver.cpp:229] Iteration 9440, loss = 0.116405
I1026 00:12:45.866214 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0891624 (* 1 = 0.0891624 loss)
I1026 00:12:45.866219 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0272429 (* 1 = 0.0272429 loss)
I1026 00:12:45.866225 16980 sgd_solver.cpp:106] Iteration 9440, lr = 0.001
I1026 00:12:49.561220 16980 solver.cpp:229] Iteration 9460, loss = 0.0877721
I1026 00:12:49.561254 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0592474 (* 1 = 0.0592474 loss)
I1026 00:12:49.561259 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0285246 (* 1 = 0.0285246 loss)
I1026 00:12:49.561264 16980 sgd_solver.cpp:106] Iteration 9460, lr = 0.001
I1026 00:12:53.226918 16980 solver.cpp:229] Iteration 9480, loss = 0.188989
I1026 00:12:53.226948 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0935994 (* 1 = 0.0935994 loss)
I1026 00:12:53.226953 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0953893 (* 1 = 0.0953893 loss)
I1026 00:12:53.226974 16980 sgd_solver.cpp:106] Iteration 9480, lr = 0.001
I1026 00:12:56.976061 16980 solver.cpp:229] Iteration 9500, loss = 0.275828
I1026 00:12:56.976094 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.134312 (* 1 = 0.134312 loss)
I1026 00:12:56.976100 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.141516 (* 1 = 0.141516 loss)
I1026 00:12:56.976105 16980 sgd_solver.cpp:106] Iteration 9500, lr = 0.001
I1026 00:13:00.866675 16980 solver.cpp:229] Iteration 9520, loss = 0.199582
I1026 00:13:00.866709 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.143555 (* 1 = 0.143555 loss)
I1026 00:13:00.866716 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0560271 (* 1 = 0.0560271 loss)
I1026 00:13:00.866720 16980 sgd_solver.cpp:106] Iteration 9520, lr = 0.001
I1026 00:13:04.548558 16980 solver.cpp:229] Iteration 9540, loss = 0.112918
I1026 00:13:04.548593 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0742781 (* 1 = 0.0742781 loss)
I1026 00:13:04.548598 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0386398 (* 1 = 0.0386398 loss)
I1026 00:13:04.548602 16980 sgd_solver.cpp:106] Iteration 9540, lr = 0.001
I1026 00:13:08.268887 16980 solver.cpp:229] Iteration 9560, loss = 0.253798
I1026 00:13:08.268918 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.148788 (* 1 = 0.148788 loss)
I1026 00:13:08.268923 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.105009 (* 1 = 0.105009 loss)
I1026 00:13:08.268929 16980 sgd_solver.cpp:106] Iteration 9560, lr = 0.001
I1026 00:13:11.944669 16980 solver.cpp:229] Iteration 9580, loss = 0.209673
I1026 00:13:11.944701 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0987156 (* 1 = 0.0987156 loss)
I1026 00:13:11.944707 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.110958 (* 1 = 0.110958 loss)
I1026 00:13:11.944711 16980 sgd_solver.cpp:106] Iteration 9580, lr = 0.001
I1026 00:13:15.627542 16980 solver.cpp:229] Iteration 9600, loss = 0.115578
I1026 00:13:15.627573 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.107828 (* 1 = 0.107828 loss)
I1026 00:13:15.627578 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.00774915 (* 1 = 0.00774915 loss)
I1026 00:13:15.627583 16980 sgd_solver.cpp:106] Iteration 9600, lr = 0.001
I1026 00:13:19.292594 16980 solver.cpp:229] Iteration 9620, loss = 0.266889
I1026 00:13:19.292625 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.112744 (* 1 = 0.112744 loss)
I1026 00:13:19.292630 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.154145 (* 1 = 0.154145 loss)
I1026 00:13:19.292635 16980 sgd_solver.cpp:106] Iteration 9620, lr = 0.001
I1026 00:13:22.968444 16980 solver.cpp:229] Iteration 9640, loss = 0.176406
I1026 00:13:22.968477 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.118136 (* 1 = 0.118136 loss)
I1026 00:13:22.968482 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0582703 (* 1 = 0.0582703 loss)
I1026 00:13:22.968487 16980 sgd_solver.cpp:106] Iteration 9640, lr = 0.001
I1026 00:13:26.811216 16980 solver.cpp:229] Iteration 9660, loss = 0.11842
I1026 00:13:26.811259 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0805404 (* 1 = 0.0805404 loss)
I1026 00:13:26.811264 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0378796 (* 1 = 0.0378796 loss)
I1026 00:13:26.811269 16980 sgd_solver.cpp:106] Iteration 9660, lr = 0.001
I1026 00:13:30.527864 16980 solver.cpp:229] Iteration 9680, loss = 0.170184
I1026 00:13:30.527894 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0952312 (* 1 = 0.0952312 loss)
I1026 00:13:30.527899 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0749528 (* 1 = 0.0749528 loss)
I1026 00:13:30.527902 16980 sgd_solver.cpp:106] Iteration 9680, lr = 0.001
I1026 00:13:34.131377 16980 solver.cpp:229] Iteration 9700, loss = 0.17664
I1026 00:13:34.131410 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.110704 (* 1 = 0.110704 loss)
I1026 00:13:34.131415 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.065936 (* 1 = 0.065936 loss)
I1026 00:13:34.131419 16980 sgd_solver.cpp:106] Iteration 9700, lr = 0.001
I1026 00:13:37.711180 16980 solver.cpp:229] Iteration 9720, loss = 0.124964
I1026 00:13:37.711211 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0589136 (* 1 = 0.0589136 loss)
I1026 00:13:37.711215 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0660506 (* 1 = 0.0660506 loss)
I1026 00:13:37.711220 16980 sgd_solver.cpp:106] Iteration 9720, lr = 0.001
I1026 00:13:41.443174 16980 solver.cpp:229] Iteration 9740, loss = 0.246674
I1026 00:13:41.443202 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.138067 (* 1 = 0.138067 loss)
I1026 00:13:41.443207 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.108607 (* 1 = 0.108607 loss)
I1026 00:13:41.443212 16980 sgd_solver.cpp:106] Iteration 9740, lr = 0.001
I1026 00:13:45.139606 16980 solver.cpp:229] Iteration 9760, loss = 0.174316
I1026 00:13:45.139638 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0994155 (* 1 = 0.0994155 loss)
I1026 00:13:45.139643 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0749007 (* 1 = 0.0749007 loss)
I1026 00:13:45.139648 16980 sgd_solver.cpp:106] Iteration 9760, lr = 0.001
I1026 00:13:48.794970 16980 solver.cpp:229] Iteration 9780, loss = 0.131979
I1026 00:13:48.794998 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0683535 (* 1 = 0.0683535 loss)
I1026 00:13:48.795002 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0636258 (* 1 = 0.0636258 loss)
I1026 00:13:48.795007 16980 sgd_solver.cpp:106] Iteration 9780, lr = 0.001
I1026 00:13:52.550998 16980 solver.cpp:229] Iteration 9800, loss = 0.180456
I1026 00:13:52.551029 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.108066 (* 1 = 0.108066 loss)
I1026 00:13:52.551034 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0723893 (* 1 = 0.0723893 loss)
I1026 00:13:52.551039 16980 sgd_solver.cpp:106] Iteration 9800, lr = 0.001
I1026 00:13:56.326149 16980 solver.cpp:229] Iteration 9820, loss = 0.271101
I1026 00:13:56.326179 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.153818 (* 1 = 0.153818 loss)
I1026 00:13:56.326184 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.117283 (* 1 = 0.117283 loss)
I1026 00:13:56.326189 16980 sgd_solver.cpp:106] Iteration 9820, lr = 0.001
I1026 00:14:00.058940 16980 solver.cpp:229] Iteration 9840, loss = 0.305463
I1026 00:14:00.058969 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.14739 (* 1 = 0.14739 loss)
I1026 00:14:00.058974 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.158074 (* 1 = 0.158074 loss)
I1026 00:14:00.058979 16980 sgd_solver.cpp:106] Iteration 9840, lr = 0.001
I1026 00:14:03.869396 16980 solver.cpp:229] Iteration 9860, loss = 0.197747
I1026 00:14:03.869426 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.139937 (* 1 = 0.139937 loss)
I1026 00:14:03.869431 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0578108 (* 1 = 0.0578108 loss)
I1026 00:14:03.869436 16980 sgd_solver.cpp:106] Iteration 9860, lr = 0.001
I1026 00:14:07.542716 16980 solver.cpp:229] Iteration 9880, loss = 0.188478
I1026 00:14:07.542747 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.114951 (* 1 = 0.114951 loss)
I1026 00:14:07.542752 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.073527 (* 1 = 0.073527 loss)
I1026 00:14:07.542757 16980 sgd_solver.cpp:106] Iteration 9880, lr = 0.001
I1026 00:14:11.295754 16980 solver.cpp:229] Iteration 9900, loss = 0.176919
I1026 00:14:11.295784 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.116736 (* 1 = 0.116736 loss)
I1026 00:14:11.295789 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0601826 (* 1 = 0.0601826 loss)
I1026 00:14:11.295794 16980 sgd_solver.cpp:106] Iteration 9900, lr = 0.001
I1026 00:14:15.015662 16980 solver.cpp:229] Iteration 9920, loss = 0.0819261
I1026 00:14:15.015708 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0508112 (* 1 = 0.0508112 loss)
I1026 00:14:15.015713 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0311148 (* 1 = 0.0311148 loss)
I1026 00:14:15.015717 16980 sgd_solver.cpp:106] Iteration 9920, lr = 0.001
I1026 00:14:18.837505 16980 solver.cpp:229] Iteration 9940, loss = 0.15983
I1026 00:14:18.837535 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0793176 (* 1 = 0.0793176 loss)
I1026 00:14:18.837540 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0805121 (* 1 = 0.0805121 loss)
I1026 00:14:18.837544 16980 sgd_solver.cpp:106] Iteration 9940, lr = 0.001
I1026 00:14:22.463842 16980 solver.cpp:229] Iteration 9960, loss = 0.305521
I1026 00:14:22.463872 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.189265 (* 1 = 0.189265 loss)
I1026 00:14:22.463877 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.116257 (* 1 = 0.116257 loss)
I1026 00:14:22.463881 16980 sgd_solver.cpp:106] Iteration 9960, lr = 0.001
I1026 00:14:26.114557 16980 solver.cpp:229] Iteration 9980, loss = 0.203122
I1026 00:14:26.114585 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.127905 (* 1 = 0.127905 loss)
I1026 00:14:26.114590 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0752173 (* 1 = 0.0752173 loss)
I1026 00:14:26.114595 16980 sgd_solver.cpp:106] Iteration 9980, lr = 0.001
I1026 00:14:30.363742 16980 solver.cpp:229] Iteration 10000, loss = 0.221171
I1026 00:14:30.363775 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.133935 (* 1 = 0.133935 loss)
I1026 00:14:30.363780 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0872366 (* 1 = 0.0872366 loss)
I1026 00:14:30.363785 16980 sgd_solver.cpp:106] Iteration 10000, lr = 0.001
I1026 00:14:34.065654 16980 solver.cpp:229] Iteration 10020, loss = 0.215696
I1026 00:14:34.065685 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0900658 (* 1 = 0.0900658 loss)
I1026 00:14:34.065690 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.12563 (* 1 = 0.12563 loss)
I1026 00:14:34.065695 16980 sgd_solver.cpp:106] Iteration 10020, lr = 0.001
I1026 00:14:37.640216 16980 solver.cpp:229] Iteration 10040, loss = 0.190714
I1026 00:14:37.640249 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.119852 (* 1 = 0.119852 loss)
I1026 00:14:37.640254 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0708624 (* 1 = 0.0708624 loss)
I1026 00:14:37.640259 16980 sgd_solver.cpp:106] Iteration 10040, lr = 0.001
I1026 00:14:41.276974 16980 solver.cpp:229] Iteration 10060, loss = 0.120689
I1026 00:14:41.277014 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0867749 (* 1 = 0.0867749 loss)
I1026 00:14:41.277019 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0339145 (* 1 = 0.0339145 loss)
I1026 00:14:41.277024 16980 sgd_solver.cpp:106] Iteration 10060, lr = 0.001
I1026 00:14:44.978890 16980 solver.cpp:229] Iteration 10080, loss = 0.138437
I1026 00:14:44.978922 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0702238 (* 1 = 0.0702238 loss)
I1026 00:14:44.978927 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0682128 (* 1 = 0.0682128 loss)
I1026 00:14:44.978931 16980 sgd_solver.cpp:106] Iteration 10080, lr = 0.001
I1026 00:14:48.730984 16980 solver.cpp:229] Iteration 10100, loss = 0.14279
I1026 00:14:48.731014 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0731739 (* 1 = 0.0731739 loss)
I1026 00:14:48.731017 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0696163 (* 1 = 0.0696163 loss)
I1026 00:14:48.731022 16980 sgd_solver.cpp:106] Iteration 10100, lr = 0.001
I1026 00:14:52.469159 16980 solver.cpp:229] Iteration 10120, loss = 0.277293
I1026 00:14:52.469189 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.112792 (* 1 = 0.112792 loss)
I1026 00:14:52.469193 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.164501 (* 1 = 0.164501 loss)
I1026 00:14:52.469198 16980 sgd_solver.cpp:106] Iteration 10120, lr = 0.001
I1026 00:14:56.070009 16980 solver.cpp:229] Iteration 10140, loss = 0.305919
I1026 00:14:56.070039 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.18678 (* 1 = 0.18678 loss)
I1026 00:14:56.070044 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.119139 (* 1 = 0.119139 loss)
I1026 00:14:56.070049 16980 sgd_solver.cpp:106] Iteration 10140, lr = 0.001
I1026 00:14:59.720238 16980 solver.cpp:229] Iteration 10160, loss = 0.103376
I1026 00:14:59.720270 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0713572 (* 1 = 0.0713572 loss)
I1026 00:14:59.720274 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0320189 (* 1 = 0.0320189 loss)
I1026 00:14:59.720278 16980 sgd_solver.cpp:106] Iteration 10160, lr = 0.001
I1026 00:15:03.385413 16980 solver.cpp:229] Iteration 10180, loss = 0.278734
I1026 00:15:03.385442 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.198257 (* 1 = 0.198257 loss)
I1026 00:15:03.385447 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0804773 (* 1 = 0.0804773 loss)
I1026 00:15:03.385452 16980 sgd_solver.cpp:106] Iteration 10180, lr = 0.001
I1026 00:15:07.133994 16980 solver.cpp:229] Iteration 10200, loss = 0.167425
I1026 00:15:07.134027 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.090444 (* 1 = 0.090444 loss)
I1026 00:15:07.134032 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0769814 (* 1 = 0.0769814 loss)
I1026 00:15:07.134037 16980 sgd_solver.cpp:106] Iteration 10200, lr = 0.001
I1026 00:15:10.873271 16980 solver.cpp:229] Iteration 10220, loss = 0.194822
I1026 00:15:10.873302 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.087375 (* 1 = 0.087375 loss)
I1026 00:15:10.873306 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.107447 (* 1 = 0.107447 loss)
I1026 00:15:10.873311 16980 sgd_solver.cpp:106] Iteration 10220, lr = 0.001
I1026 00:15:14.645865 16980 solver.cpp:229] Iteration 10240, loss = 0.117385
I1026 00:15:14.645907 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0489108 (* 1 = 0.0489108 loss)
I1026 00:15:14.645912 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0684741 (* 1 = 0.0684741 loss)
I1026 00:15:14.645916 16980 sgd_solver.cpp:106] Iteration 10240, lr = 0.001
I1026 00:15:18.388252 16980 solver.cpp:229] Iteration 10260, loss = 0.184724
I1026 00:15:18.388283 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.117487 (* 1 = 0.117487 loss)
I1026 00:15:18.388289 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0672372 (* 1 = 0.0672372 loss)
I1026 00:15:18.388293 16980 sgd_solver.cpp:106] Iteration 10260, lr = 0.001
I1026 00:15:22.143991 16980 solver.cpp:229] Iteration 10280, loss = 0.194868
I1026 00:15:22.144034 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.121961 (* 1 = 0.121961 loss)
I1026 00:15:22.144038 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0729075 (* 1 = 0.0729075 loss)
I1026 00:15:22.144043 16980 sgd_solver.cpp:106] Iteration 10280, lr = 0.001
I1026 00:15:25.817618 16980 solver.cpp:229] Iteration 10300, loss = 0.105939
I1026 00:15:25.817649 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0731273 (* 1 = 0.0731273 loss)
I1026 00:15:25.817654 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.032812 (* 1 = 0.032812 loss)
I1026 00:15:25.817661 16980 sgd_solver.cpp:106] Iteration 10300, lr = 0.001
I1026 00:15:29.587541 16980 solver.cpp:229] Iteration 10320, loss = 0.326677
I1026 00:15:29.587574 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.220343 (* 1 = 0.220343 loss)
I1026 00:15:29.587579 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.106334 (* 1 = 0.106334 loss)
I1026 00:15:29.587584 16980 sgd_solver.cpp:106] Iteration 10320, lr = 0.001
I1026 00:15:33.339963 16980 solver.cpp:229] Iteration 10340, loss = 0.190942
I1026 00:15:33.339993 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.104279 (* 1 = 0.104279 loss)
I1026 00:15:33.339998 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0866631 (* 1 = 0.0866631 loss)
I1026 00:15:33.340003 16980 sgd_solver.cpp:106] Iteration 10340, lr = 0.001
I1026 00:15:37.112133 16980 solver.cpp:229] Iteration 10360, loss = 0.124535
I1026 00:15:37.112164 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0785112 (* 1 = 0.0785112 loss)
I1026 00:15:37.112169 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0460236 (* 1 = 0.0460236 loss)
I1026 00:15:37.112174 16980 sgd_solver.cpp:106] Iteration 10360, lr = 0.001
I1026 00:15:40.844519 16980 solver.cpp:229] Iteration 10380, loss = 0.125848
I1026 00:15:40.844549 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0967427 (* 1 = 0.0967427 loss)
I1026 00:15:40.844554 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0291052 (* 1 = 0.0291052 loss)
I1026 00:15:40.844559 16980 sgd_solver.cpp:106] Iteration 10380, lr = 0.001
I1026 00:15:44.442940 16980 solver.cpp:229] Iteration 10400, loss = 0.177377
I1026 00:15:44.442972 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.119834 (* 1 = 0.119834 loss)
I1026 00:15:44.442977 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0575434 (* 1 = 0.0575434 loss)
I1026 00:15:44.442982 16980 sgd_solver.cpp:106] Iteration 10400, lr = 0.001
I1026 00:15:48.149694 16980 solver.cpp:229] Iteration 10420, loss = 0.193739
I1026 00:15:48.149737 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.102348 (* 1 = 0.102348 loss)
I1026 00:15:48.149742 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0913914 (* 1 = 0.0913914 loss)
I1026 00:15:48.149746 16980 sgd_solver.cpp:106] Iteration 10420, lr = 0.001
I1026 00:15:51.826448 16980 solver.cpp:229] Iteration 10440, loss = 0.105228
I1026 00:15:51.826475 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0755688 (* 1 = 0.0755688 loss)
I1026 00:15:51.826480 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0296594 (* 1 = 0.0296594 loss)
I1026 00:15:51.826484 16980 sgd_solver.cpp:106] Iteration 10440, lr = 0.001
I1026 00:15:55.521615 16980 solver.cpp:229] Iteration 10460, loss = 0.143872
I1026 00:15:55.521656 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.076524 (* 1 = 0.076524 loss)
I1026 00:15:55.521661 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0673478 (* 1 = 0.0673478 loss)
I1026 00:15:55.521666 16980 sgd_solver.cpp:106] Iteration 10460, lr = 0.001
I1026 00:15:59.201732 16980 solver.cpp:229] Iteration 10480, loss = 0.238358
I1026 00:15:59.201764 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.141597 (* 1 = 0.141597 loss)
I1026 00:15:59.201769 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0967602 (* 1 = 0.0967602 loss)
I1026 00:15:59.201774 16980 sgd_solver.cpp:106] Iteration 10480, lr = 0.001
I1026 00:16:02.981827 16980 solver.cpp:229] Iteration 10500, loss = 0.289005
I1026 00:16:02.981873 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.157419 (* 1 = 0.157419 loss)
I1026 00:16:02.981878 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.131586 (* 1 = 0.131586 loss)
I1026 00:16:02.981884 16980 sgd_solver.cpp:106] Iteration 10500, lr = 0.001
I1026 00:16:06.665467 16980 solver.cpp:229] Iteration 10520, loss = 0.2345
I1026 00:16:06.665496 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.138199 (* 1 = 0.138199 loss)
I1026 00:16:06.665503 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0963015 (* 1 = 0.0963015 loss)
I1026 00:16:06.665506 16980 sgd_solver.cpp:106] Iteration 10520, lr = 0.001
I1026 00:16:10.389792 16980 solver.cpp:229] Iteration 10540, loss = 0.272435
I1026 00:16:10.389824 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.121286 (* 1 = 0.121286 loss)
I1026 00:16:10.389829 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.151149 (* 1 = 0.151149 loss)
I1026 00:16:10.389834 16980 sgd_solver.cpp:106] Iteration 10540, lr = 0.001
I1026 00:16:14.134362 16980 solver.cpp:229] Iteration 10560, loss = 0.109585
I1026 00:16:14.134394 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0663145 (* 1 = 0.0663145 loss)
I1026 00:16:14.134398 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0432703 (* 1 = 0.0432703 loss)
I1026 00:16:14.134404 16980 sgd_solver.cpp:106] Iteration 10560, lr = 0.001
I1026 00:16:17.841506 16980 solver.cpp:229] Iteration 10580, loss = 0.0914138
I1026 00:16:17.841539 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0717363 (* 1 = 0.0717363 loss)
I1026 00:16:17.841544 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0196775 (* 1 = 0.0196775 loss)
I1026 00:16:17.841549 16980 sgd_solver.cpp:106] Iteration 10580, lr = 0.001
I1026 00:16:21.620246 16980 solver.cpp:229] Iteration 10600, loss = 0.139807
I1026 00:16:21.620280 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0694613 (* 1 = 0.0694613 loss)
I1026 00:16:21.620285 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0703455 (* 1 = 0.0703455 loss)
I1026 00:16:21.620290 16980 sgd_solver.cpp:106] Iteration 10600, lr = 0.001
I1026 00:16:25.351547 16980 solver.cpp:229] Iteration 10620, loss = 0.176274
I1026 00:16:25.351578 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0777714 (* 1 = 0.0777714 loss)
I1026 00:16:25.351583 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0985027 (* 1 = 0.0985027 loss)
I1026 00:16:25.351588 16980 sgd_solver.cpp:106] Iteration 10620, lr = 0.001
I1026 00:16:28.996079 16980 solver.cpp:229] Iteration 10640, loss = 0.271438
I1026 00:16:28.996109 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.132186 (* 1 = 0.132186 loss)
I1026 00:16:28.996114 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.139252 (* 1 = 0.139252 loss)
I1026 00:16:28.996117 16980 sgd_solver.cpp:106] Iteration 10640, lr = 0.001
I1026 00:16:32.713523 16980 solver.cpp:229] Iteration 10660, loss = 0.188982
I1026 00:16:32.713552 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.111794 (* 1 = 0.111794 loss)
I1026 00:16:32.713558 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0771883 (* 1 = 0.0771883 loss)
I1026 00:16:32.713563 16980 sgd_solver.cpp:106] Iteration 10660, lr = 0.001
I1026 00:16:36.434356 16980 solver.cpp:229] Iteration 10680, loss = 0.109042
I1026 00:16:36.434387 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0931148 (* 1 = 0.0931148 loss)
I1026 00:16:36.434392 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0159272 (* 1 = 0.0159272 loss)
I1026 00:16:36.434397 16980 sgd_solver.cpp:106] Iteration 10680, lr = 0.001
I1026 00:16:40.168419 16980 solver.cpp:229] Iteration 10700, loss = 0.0703188
I1026 00:16:40.168463 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0581292 (* 1 = 0.0581292 loss)
I1026 00:16:40.168467 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0121896 (* 1 = 0.0121896 loss)
I1026 00:16:40.168473 16980 sgd_solver.cpp:106] Iteration 10700, lr = 0.001
I1026 00:16:43.756062 16980 solver.cpp:229] Iteration 10720, loss = 0.0960134
I1026 00:16:43.756106 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0596442 (* 1 = 0.0596442 loss)
I1026 00:16:43.756110 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0363692 (* 1 = 0.0363692 loss)
I1026 00:16:43.756115 16980 sgd_solver.cpp:106] Iteration 10720, lr = 0.001
I1026 00:16:47.484211 16980 solver.cpp:229] Iteration 10740, loss = 0.161403
I1026 00:16:47.484242 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.11329 (* 1 = 0.11329 loss)
I1026 00:16:47.484247 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0481131 (* 1 = 0.0481131 loss)
I1026 00:16:47.484251 16980 sgd_solver.cpp:106] Iteration 10740, lr = 0.001
I1026 00:16:51.140986 16980 solver.cpp:229] Iteration 10760, loss = 0.123659
I1026 00:16:51.141029 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0613961 (* 1 = 0.0613961 loss)
I1026 00:16:51.141033 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0622627 (* 1 = 0.0622627 loss)
I1026 00:16:51.141038 16980 sgd_solver.cpp:106] Iteration 10760, lr = 0.001
I1026 00:16:54.783459 16980 solver.cpp:229] Iteration 10780, loss = 0.153521
I1026 00:16:54.783488 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.122051 (* 1 = 0.122051 loss)
I1026 00:16:54.783491 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0314694 (* 1 = 0.0314694 loss)
I1026 00:16:54.783496 16980 sgd_solver.cpp:106] Iteration 10780, lr = 0.001
I1026 00:16:58.431291 16980 solver.cpp:229] Iteration 10800, loss = 0.23675
I1026 00:16:58.431321 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.135959 (* 1 = 0.135959 loss)
I1026 00:16:58.431326 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.100791 (* 1 = 0.100791 loss)
I1026 00:16:58.431331 16980 sgd_solver.cpp:106] Iteration 10800, lr = 0.001
I1026 00:17:02.162325 16980 solver.cpp:229] Iteration 10820, loss = 0.165152
I1026 00:17:02.162358 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.131965 (* 1 = 0.131965 loss)
I1026 00:17:02.162362 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0331864 (* 1 = 0.0331864 loss)
I1026 00:17:02.162367 16980 sgd_solver.cpp:106] Iteration 10820, lr = 0.001
I1026 00:17:05.921604 16980 solver.cpp:229] Iteration 10840, loss = 0.121464
I1026 00:17:05.921638 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0591484 (* 1 = 0.0591484 loss)
I1026 00:17:05.921643 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0623159 (* 1 = 0.0623159 loss)
I1026 00:17:05.921646 16980 sgd_solver.cpp:106] Iteration 10840, lr = 0.001
I1026 00:17:09.753147 16980 solver.cpp:229] Iteration 10860, loss = 0.529573
I1026 00:17:09.753180 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.242244 (* 1 = 0.242244 loss)
I1026 00:17:09.753185 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.287329 (* 1 = 0.287329 loss)
I1026 00:17:09.753188 16980 sgd_solver.cpp:106] Iteration 10860, lr = 0.001
I1026 00:17:13.552348 16980 solver.cpp:229] Iteration 10880, loss = 0.105181
I1026 00:17:13.552379 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0587403 (* 1 = 0.0587403 loss)
I1026 00:17:13.552384 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0464402 (* 1 = 0.0464402 loss)
I1026 00:17:13.552388 16980 sgd_solver.cpp:106] Iteration 10880, lr = 0.001
I1026 00:17:17.301257 16980 solver.cpp:229] Iteration 10900, loss = 0.131834
I1026 00:17:17.301301 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0844108 (* 1 = 0.0844108 loss)
I1026 00:17:17.301306 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0474235 (* 1 = 0.0474235 loss)
I1026 00:17:17.301311 16980 sgd_solver.cpp:106] Iteration 10900, lr = 0.001
I1026 00:17:21.102895 16980 solver.cpp:229] Iteration 10920, loss = 0.172496
I1026 00:17:21.102926 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0876363 (* 1 = 0.0876363 loss)
I1026 00:17:21.102931 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0848599 (* 1 = 0.0848599 loss)
I1026 00:17:21.102934 16980 sgd_solver.cpp:106] Iteration 10920, lr = 0.001
I1026 00:17:24.888465 16980 solver.cpp:229] Iteration 10940, loss = 0.107691
I1026 00:17:24.888509 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0577715 (* 1 = 0.0577715 loss)
I1026 00:17:24.888514 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0499198 (* 1 = 0.0499198 loss)
I1026 00:17:24.888517 16980 sgd_solver.cpp:106] Iteration 10940, lr = 0.001
I1026 00:17:28.654857 16980 solver.cpp:229] Iteration 10960, loss = 0.34477
I1026 00:17:28.654887 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.206798 (* 1 = 0.206798 loss)
I1026 00:17:28.654896 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.137972 (* 1 = 0.137972 loss)
I1026 00:17:28.654901 16980 sgd_solver.cpp:106] Iteration 10960, lr = 0.001
I1026 00:17:32.439690 16980 solver.cpp:229] Iteration 10980, loss = 0.137923
I1026 00:17:32.439721 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0716707 (* 1 = 0.0716707 loss)
I1026 00:17:32.439725 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0662524 (* 1 = 0.0662524 loss)
I1026 00:17:32.439730 16980 sgd_solver.cpp:106] Iteration 10980, lr = 0.001
I1026 00:17:36.182410 16980 solver.cpp:229] Iteration 11000, loss = 0.114567
I1026 00:17:36.182453 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0742382 (* 1 = 0.0742382 loss)
I1026 00:17:36.182458 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0403293 (* 1 = 0.0403293 loss)
I1026 00:17:36.182472 16980 sgd_solver.cpp:106] Iteration 11000, lr = 0.001
I1026 00:17:39.982270 16980 solver.cpp:229] Iteration 11020, loss = 0.239088
I1026 00:17:39.982301 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.102714 (* 1 = 0.102714 loss)
I1026 00:17:39.982306 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.136375 (* 1 = 0.136375 loss)
I1026 00:17:39.982312 16980 sgd_solver.cpp:106] Iteration 11020, lr = 0.001
I1026 00:17:43.636711 16980 solver.cpp:229] Iteration 11040, loss = 0.118857
I1026 00:17:43.636754 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0906994 (* 1 = 0.0906994 loss)
I1026 00:17:43.636759 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0281572 (* 1 = 0.0281572 loss)
I1026 00:17:43.636764 16980 sgd_solver.cpp:106] Iteration 11040, lr = 0.001
I1026 00:17:47.343138 16980 solver.cpp:229] Iteration 11060, loss = 0.158417
I1026 00:17:47.343170 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0786848 (* 1 = 0.0786848 loss)
I1026 00:17:47.343175 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0797324 (* 1 = 0.0797324 loss)
I1026 00:17:47.343180 16980 sgd_solver.cpp:106] Iteration 11060, lr = 0.001
I1026 00:17:50.901232 16980 solver.cpp:229] Iteration 11080, loss = 0.124085
I1026 00:17:50.901264 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0749311 (* 1 = 0.0749311 loss)
I1026 00:17:50.901269 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0491542 (* 1 = 0.0491542 loss)
I1026 00:17:50.901274 16980 sgd_solver.cpp:106] Iteration 11080, lr = 0.001
I1026 00:17:54.415935 16980 solver.cpp:229] Iteration 11100, loss = 0.141875
I1026 00:17:54.415968 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.113823 (* 1 = 0.113823 loss)
I1026 00:17:54.415973 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0280521 (* 1 = 0.0280521 loss)
I1026 00:17:54.415977 16980 sgd_solver.cpp:106] Iteration 11100, lr = 0.001
I1026 00:17:58.035665 16980 solver.cpp:229] Iteration 11120, loss = 0.163803
I1026 00:17:58.035697 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.076202 (* 1 = 0.076202 loss)
I1026 00:17:58.035701 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0876013 (* 1 = 0.0876013 loss)
I1026 00:17:58.035706 16980 sgd_solver.cpp:106] Iteration 11120, lr = 0.001
I1026 00:18:01.740844 16980 solver.cpp:229] Iteration 11140, loss = 0.169532
I1026 00:18:01.740872 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.135867 (* 1 = 0.135867 loss)
I1026 00:18:01.740877 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0336654 (* 1 = 0.0336654 loss)
I1026 00:18:01.740882 16980 sgd_solver.cpp:106] Iteration 11140, lr = 0.001
I1026 00:18:05.474045 16980 solver.cpp:229] Iteration 11160, loss = 0.239751
I1026 00:18:05.474077 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.168934 (* 1 = 0.168934 loss)
I1026 00:18:05.474086 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0708166 (* 1 = 0.0708166 loss)
I1026 00:18:05.474093 16980 sgd_solver.cpp:106] Iteration 11160, lr = 0.001
I1026 00:18:09.245930 16980 solver.cpp:229] Iteration 11180, loss = 0.230175
I1026 00:18:09.245965 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.117072 (* 1 = 0.117072 loss)
I1026 00:18:09.245972 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.113104 (* 1 = 0.113104 loss)
I1026 00:18:09.245978 16980 sgd_solver.cpp:106] Iteration 11180, lr = 0.001
I1026 00:18:12.946137 16980 solver.cpp:229] Iteration 11200, loss = 0.168983
I1026 00:18:12.946168 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0923834 (* 1 = 0.0923834 loss)
I1026 00:18:12.946176 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0766 (* 1 = 0.0766 loss)
I1026 00:18:12.946192 16980 sgd_solver.cpp:106] Iteration 11200, lr = 0.001
I1026 00:18:16.612673 16980 solver.cpp:229] Iteration 11220, loss = 0.188533
I1026 00:18:16.612704 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.122482 (* 1 = 0.122482 loss)
I1026 00:18:16.612711 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0660505 (* 1 = 0.0660505 loss)
I1026 00:18:16.612718 16980 sgd_solver.cpp:106] Iteration 11220, lr = 0.001
I1026 00:18:20.324920 16980 solver.cpp:229] Iteration 11240, loss = 0.263031
I1026 00:18:20.324952 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.142174 (* 1 = 0.142174 loss)
I1026 00:18:20.324957 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.120857 (* 1 = 0.120857 loss)
I1026 00:18:20.324962 16980 sgd_solver.cpp:106] Iteration 11240, lr = 0.001
I1026 00:18:24.037767 16980 solver.cpp:229] Iteration 11260, loss = 0.127096
I1026 00:18:24.037793 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.082433 (* 1 = 0.082433 loss)
I1026 00:18:24.037798 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0446628 (* 1 = 0.0446628 loss)
I1026 00:18:24.037802 16980 sgd_solver.cpp:106] Iteration 11260, lr = 0.001
I1026 00:18:27.689966 16980 solver.cpp:229] Iteration 11280, loss = 0.105811
I1026 00:18:27.690001 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0567501 (* 1 = 0.0567501 loss)
I1026 00:18:27.690006 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0490608 (* 1 = 0.0490608 loss)
I1026 00:18:27.690009 16980 sgd_solver.cpp:106] Iteration 11280, lr = 0.001
I1026 00:18:31.451206 16980 solver.cpp:229] Iteration 11300, loss = 0.105215
I1026 00:18:31.451241 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0864719 (* 1 = 0.0864719 loss)
I1026 00:18:31.451246 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0187436 (* 1 = 0.0187436 loss)
I1026 00:18:31.451251 16980 sgd_solver.cpp:106] Iteration 11300, lr = 0.001
I1026 00:18:35.128430 16980 solver.cpp:229] Iteration 11320, loss = 0.172113
I1026 00:18:35.128463 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.112468 (* 1 = 0.112468 loss)
I1026 00:18:35.128468 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0596442 (* 1 = 0.0596442 loss)
I1026 00:18:35.128471 16980 sgd_solver.cpp:106] Iteration 11320, lr = 0.001
I1026 00:18:38.815379 16980 solver.cpp:229] Iteration 11340, loss = 0.102542
I1026 00:18:38.815409 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0902494 (* 1 = 0.0902494 loss)
I1026 00:18:38.815415 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0122926 (* 1 = 0.0122926 loss)
I1026 00:18:38.815420 16980 sgd_solver.cpp:106] Iteration 11340, lr = 0.001
I1026 00:18:42.451545 16980 solver.cpp:229] Iteration 11360, loss = 0.191316
I1026 00:18:42.451577 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.122404 (* 1 = 0.122404 loss)
I1026 00:18:42.451582 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0689119 (* 1 = 0.0689119 loss)
I1026 00:18:42.451588 16980 sgd_solver.cpp:106] Iteration 11360, lr = 0.001
I1026 00:18:46.021188 16980 solver.cpp:229] Iteration 11380, loss = 0.143599
I1026 00:18:46.021215 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.073612 (* 1 = 0.073612 loss)
I1026 00:18:46.021219 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0699869 (* 1 = 0.0699869 loss)
I1026 00:18:46.021224 16980 sgd_solver.cpp:106] Iteration 11380, lr = 0.001
I1026 00:18:49.635916 16980 solver.cpp:229] Iteration 11400, loss = 0.0780885
I1026 00:18:49.635946 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0488609 (* 1 = 0.0488609 loss)
I1026 00:18:49.635951 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0292277 (* 1 = 0.0292277 loss)
I1026 00:18:49.635956 16980 sgd_solver.cpp:106] Iteration 11400, lr = 0.001
I1026 00:18:53.310431 16980 solver.cpp:229] Iteration 11420, loss = 0.143547
I1026 00:18:53.310466 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.081023 (* 1 = 0.081023 loss)
I1026 00:18:53.310470 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0625238 (* 1 = 0.0625238 loss)
I1026 00:18:53.310475 16980 sgd_solver.cpp:106] Iteration 11420, lr = 0.001
I1026 00:18:57.001943 16980 solver.cpp:229] Iteration 11440, loss = 0.202415
I1026 00:18:57.001971 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.127135 (* 1 = 0.127135 loss)
I1026 00:18:57.001976 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0752792 (* 1 = 0.0752792 loss)
I1026 00:18:57.001981 16980 sgd_solver.cpp:106] Iteration 11440, lr = 0.001
I1026 00:19:00.679519 16980 solver.cpp:229] Iteration 11460, loss = 0.183552
I1026 00:19:00.679546 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0814762 (* 1 = 0.0814762 loss)
I1026 00:19:00.679550 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.102076 (* 1 = 0.102076 loss)
I1026 00:19:00.679555 16980 sgd_solver.cpp:106] Iteration 11460, lr = 0.001
I1026 00:19:04.350261 16980 solver.cpp:229] Iteration 11480, loss = 0.16969
I1026 00:19:04.350291 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.100455 (* 1 = 0.100455 loss)
I1026 00:19:04.350296 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0692342 (* 1 = 0.0692342 loss)
I1026 00:19:04.350301 16980 sgd_solver.cpp:106] Iteration 11480, lr = 0.001
I1026 00:19:08.018385 16980 solver.cpp:229] Iteration 11500, loss = 0.281109
I1026 00:19:08.018419 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0912231 (* 1 = 0.0912231 loss)
I1026 00:19:08.018424 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.189886 (* 1 = 0.189886 loss)
I1026 00:19:08.018427 16980 sgd_solver.cpp:106] Iteration 11500, lr = 0.001
I1026 00:19:11.725002 16980 solver.cpp:229] Iteration 11520, loss = 0.17204
I1026 00:19:11.725033 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0856389 (* 1 = 0.0856389 loss)
I1026 00:19:11.725038 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0864015 (* 1 = 0.0864015 loss)
I1026 00:19:11.725041 16980 sgd_solver.cpp:106] Iteration 11520, lr = 0.001
I1026 00:19:15.500336 16980 solver.cpp:229] Iteration 11540, loss = 0.167658
I1026 00:19:15.500365 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.122379 (* 1 = 0.122379 loss)
I1026 00:19:15.500370 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0452787 (* 1 = 0.0452787 loss)
I1026 00:19:15.500375 16980 sgd_solver.cpp:106] Iteration 11540, lr = 0.001
I1026 00:19:19.177965 16980 solver.cpp:229] Iteration 11560, loss = 0.0810034
I1026 00:19:19.177997 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0496561 (* 1 = 0.0496561 loss)
I1026 00:19:19.178002 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0313473 (* 1 = 0.0313473 loss)
I1026 00:19:19.178007 16980 sgd_solver.cpp:106] Iteration 11560, lr = 0.001
I1026 00:19:23.004066 16980 solver.cpp:229] Iteration 11580, loss = 0.17713
I1026 00:19:23.004099 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.131315 (* 1 = 0.131315 loss)
I1026 00:19:23.004106 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0458152 (* 1 = 0.0458152 loss)
I1026 00:19:23.004109 16980 sgd_solver.cpp:106] Iteration 11580, lr = 0.001
I1026 00:19:26.771849 16980 solver.cpp:229] Iteration 11600, loss = 0.112845
I1026 00:19:26.771878 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0587363 (* 1 = 0.0587363 loss)
I1026 00:19:26.771883 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0541086 (* 1 = 0.0541086 loss)
I1026 00:19:26.771888 16980 sgd_solver.cpp:106] Iteration 11600, lr = 0.001
I1026 00:19:30.454566 16980 solver.cpp:229] Iteration 11620, loss = 0.188525
I1026 00:19:30.454598 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.153738 (* 1 = 0.153738 loss)
I1026 00:19:30.454603 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0347872 (* 1 = 0.0347872 loss)
I1026 00:19:30.454608 16980 sgd_solver.cpp:106] Iteration 11620, lr = 0.001
I1026 00:19:34.089061 16980 solver.cpp:229] Iteration 11640, loss = 0.248274
I1026 00:19:34.089090 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.145737 (* 1 = 0.145737 loss)
I1026 00:19:34.089095 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.102537 (* 1 = 0.102537 loss)
I1026 00:19:34.089100 16980 sgd_solver.cpp:106] Iteration 11640, lr = 0.001
I1026 00:19:37.814748 16980 solver.cpp:229] Iteration 11660, loss = 0.132112
I1026 00:19:37.814780 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0709131 (* 1 = 0.0709131 loss)
I1026 00:19:37.814785 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0611993 (* 1 = 0.0611993 loss)
I1026 00:19:37.814790 16980 sgd_solver.cpp:106] Iteration 11660, lr = 0.001
I1026 00:19:41.466245 16980 solver.cpp:229] Iteration 11680, loss = 0.190505
I1026 00:19:41.466277 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.113765 (* 1 = 0.113765 loss)
I1026 00:19:41.466282 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0767396 (* 1 = 0.0767396 loss)
I1026 00:19:41.466289 16980 sgd_solver.cpp:106] Iteration 11680, lr = 0.001
I1026 00:19:45.160338 16980 solver.cpp:229] Iteration 11700, loss = 0.147148
I1026 00:19:45.160369 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0710663 (* 1 = 0.0710663 loss)
I1026 00:19:45.160374 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.076082 (* 1 = 0.076082 loss)
I1026 00:19:45.160378 16980 sgd_solver.cpp:106] Iteration 11700, lr = 0.001
I1026 00:19:48.820194 16980 solver.cpp:229] Iteration 11720, loss = 0.119554
I1026 00:19:48.820225 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0696357 (* 1 = 0.0696357 loss)
I1026 00:19:48.820230 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0499183 (* 1 = 0.0499183 loss)
I1026 00:19:48.820233 16980 sgd_solver.cpp:106] Iteration 11720, lr = 0.001
I1026 00:19:52.517078 16980 solver.cpp:229] Iteration 11740, loss = 0.116953
I1026 00:19:52.517107 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0641645 (* 1 = 0.0641645 loss)
I1026 00:19:52.517112 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0527888 (* 1 = 0.0527888 loss)
I1026 00:19:52.517115 16980 sgd_solver.cpp:106] Iteration 11740, lr = 0.001
I1026 00:19:56.198513 16980 solver.cpp:229] Iteration 11760, loss = 0.151576
I1026 00:19:56.198542 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.101082 (* 1 = 0.101082 loss)
I1026 00:19:56.198547 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0504939 (* 1 = 0.0504939 loss)
I1026 00:19:56.198551 16980 sgd_solver.cpp:106] Iteration 11760, lr = 0.001
I1026 00:19:59.919015 16980 solver.cpp:229] Iteration 11780, loss = 0.107146
I1026 00:19:59.919047 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0607232 (* 1 = 0.0607232 loss)
I1026 00:19:59.919054 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0464232 (* 1 = 0.0464232 loss)
I1026 00:19:59.919060 16980 sgd_solver.cpp:106] Iteration 11780, lr = 0.001
I1026 00:20:03.621083 16980 solver.cpp:229] Iteration 11800, loss = 0.119085
I1026 00:20:03.621114 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0700274 (* 1 = 0.0700274 loss)
I1026 00:20:03.621119 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0490577 (* 1 = 0.0490577 loss)
I1026 00:20:03.621124 16980 sgd_solver.cpp:106] Iteration 11800, lr = 0.001
I1026 00:20:07.307384 16980 solver.cpp:229] Iteration 11820, loss = 0.221463
I1026 00:20:07.307417 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.100368 (* 1 = 0.100368 loss)
I1026 00:20:07.307421 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.121095 (* 1 = 0.121095 loss)
I1026 00:20:07.307426 16980 sgd_solver.cpp:106] Iteration 11820, lr = 0.001
I1026 00:20:11.022878 16980 solver.cpp:229] Iteration 11840, loss = 0.188931
I1026 00:20:11.022928 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.112888 (* 1 = 0.112888 loss)
I1026 00:20:11.022933 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0760434 (* 1 = 0.0760434 loss)
I1026 00:20:11.022938 16980 sgd_solver.cpp:106] Iteration 11840, lr = 0.001
I1026 00:20:14.802903 16980 solver.cpp:229] Iteration 11860, loss = 0.104858
I1026 00:20:14.802937 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0913287 (* 1 = 0.0913287 loss)
I1026 00:20:14.802942 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0135297 (* 1 = 0.0135297 loss)
I1026 00:20:14.802947 16980 sgd_solver.cpp:106] Iteration 11860, lr = 0.001
I1026 00:20:18.525007 16980 solver.cpp:229] Iteration 11880, loss = 0.328723
I1026 00:20:18.525039 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.159983 (* 1 = 0.159983 loss)
I1026 00:20:18.525044 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.168741 (* 1 = 0.168741 loss)
I1026 00:20:18.525049 16980 sgd_solver.cpp:106] Iteration 11880, lr = 0.001
I1026 00:20:22.230339 16980 solver.cpp:229] Iteration 11900, loss = 0.156452
I1026 00:20:22.230370 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0588034 (* 1 = 0.0588034 loss)
I1026 00:20:22.230373 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0976491 (* 1 = 0.0976491 loss)
I1026 00:20:22.230378 16980 sgd_solver.cpp:106] Iteration 11900, lr = 0.001
I1026 00:20:25.923449 16980 solver.cpp:229] Iteration 11920, loss = 0.190703
I1026 00:20:25.923493 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0982893 (* 1 = 0.0982893 loss)
I1026 00:20:25.923497 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0924136 (* 1 = 0.0924136 loss)
I1026 00:20:25.923502 16980 sgd_solver.cpp:106] Iteration 11920, lr = 0.001
I1026 00:20:29.598260 16980 solver.cpp:229] Iteration 11940, loss = 0.127997
I1026 00:20:29.598304 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0856636 (* 1 = 0.0856636 loss)
I1026 00:20:29.598307 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0423335 (* 1 = 0.0423335 loss)
I1026 00:20:29.598312 16980 sgd_solver.cpp:106] Iteration 11940, lr = 0.001
I1026 00:20:33.351598 16980 solver.cpp:229] Iteration 11960, loss = 0.23544
I1026 00:20:33.351630 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.160649 (* 1 = 0.160649 loss)
I1026 00:20:33.351635 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0747909 (* 1 = 0.0747909 loss)
I1026 00:20:33.351640 16980 sgd_solver.cpp:106] Iteration 11960, lr = 0.001
I1026 00:20:37.043056 16980 solver.cpp:229] Iteration 11980, loss = 0.1054
I1026 00:20:37.043089 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.076854 (* 1 = 0.076854 loss)
I1026 00:20:37.043094 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0285455 (* 1 = 0.0285455 loss)
I1026 00:20:37.043099 16980 sgd_solver.cpp:106] Iteration 11980, lr = 0.001
I1026 00:20:40.734045 16980 solver.cpp:229] Iteration 12000, loss = 0.083449
I1026 00:20:40.734072 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0572309 (* 1 = 0.0572309 loss)
I1026 00:20:40.734077 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0262181 (* 1 = 0.0262181 loss)
I1026 00:20:40.734082 16980 sgd_solver.cpp:106] Iteration 12000, lr = 0.001
I1026 00:20:44.429642 16980 solver.cpp:229] Iteration 12020, loss = 0.0993525
I1026 00:20:44.429675 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0817462 (* 1 = 0.0817462 loss)
I1026 00:20:44.429680 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0176064 (* 1 = 0.0176064 loss)
I1026 00:20:44.429684 16980 sgd_solver.cpp:106] Iteration 12020, lr = 0.001
I1026 00:20:48.227207 16980 solver.cpp:229] Iteration 12040, loss = 0.0899328
I1026 00:20:48.227238 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0582424 (* 1 = 0.0582424 loss)
I1026 00:20:48.227242 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0316904 (* 1 = 0.0316904 loss)
I1026 00:20:48.227247 16980 sgd_solver.cpp:106] Iteration 12040, lr = 0.001
I1026 00:20:51.968428 16980 solver.cpp:229] Iteration 12060, loss = 0.146532
I1026 00:20:51.968467 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.116181 (* 1 = 0.116181 loss)
I1026 00:20:51.968472 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0303505 (* 1 = 0.0303505 loss)
I1026 00:20:51.968485 16980 sgd_solver.cpp:106] Iteration 12060, lr = 0.001
I1026 00:20:55.691568 16980 solver.cpp:229] Iteration 12080, loss = 0.0735944
I1026 00:20:55.691598 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0391472 (* 1 = 0.0391472 loss)
I1026 00:20:55.691602 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0344471 (* 1 = 0.0344471 loss)
I1026 00:20:55.691606 16980 sgd_solver.cpp:106] Iteration 12080, lr = 0.001
I1026 00:20:59.500321 16980 solver.cpp:229] Iteration 12100, loss = 0.27396
I1026 00:20:59.500355 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.154366 (* 1 = 0.154366 loss)
I1026 00:20:59.500360 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.119594 (* 1 = 0.119594 loss)
I1026 00:20:59.500365 16980 sgd_solver.cpp:106] Iteration 12100, lr = 0.001
I1026 00:21:03.287307 16980 solver.cpp:229] Iteration 12120, loss = 0.306201
I1026 00:21:03.287338 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0989154 (* 1 = 0.0989154 loss)
I1026 00:21:03.287343 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.207286 (* 1 = 0.207286 loss)
I1026 00:21:03.287346 16980 sgd_solver.cpp:106] Iteration 12120, lr = 0.001
I1026 00:21:06.879946 16980 solver.cpp:229] Iteration 12140, loss = 0.149684
I1026 00:21:06.879989 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0549866 (* 1 = 0.0549866 loss)
I1026 00:21:06.879993 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.094697 (* 1 = 0.094697 loss)
I1026 00:21:06.879998 16980 sgd_solver.cpp:106] Iteration 12140, lr = 0.001
I1026 00:21:10.524410 16980 solver.cpp:229] Iteration 12160, loss = 0.293254
I1026 00:21:10.524442 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.193664 (* 1 = 0.193664 loss)
I1026 00:21:10.524447 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0995892 (* 1 = 0.0995892 loss)
I1026 00:21:10.524452 16980 sgd_solver.cpp:106] Iteration 12160, lr = 0.001
I1026 00:21:14.303541 16980 solver.cpp:229] Iteration 12180, loss = 0.10659
I1026 00:21:14.303572 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.055281 (* 1 = 0.055281 loss)
I1026 00:21:14.303577 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0513092 (* 1 = 0.0513092 loss)
I1026 00:21:14.303582 16980 sgd_solver.cpp:106] Iteration 12180, lr = 0.001
I1026 00:21:18.056733 16980 solver.cpp:229] Iteration 12200, loss = 0.1895
I1026 00:21:18.056767 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.123534 (* 1 = 0.123534 loss)
I1026 00:21:18.056771 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0659654 (* 1 = 0.0659654 loss)
I1026 00:21:18.056777 16980 sgd_solver.cpp:106] Iteration 12200, lr = 0.001
I1026 00:21:21.706799 16980 solver.cpp:229] Iteration 12220, loss = 0.221019
I1026 00:21:21.706830 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0608984 (* 1 = 0.0608984 loss)
I1026 00:21:21.706835 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.160121 (* 1 = 0.160121 loss)
I1026 00:21:21.706840 16980 sgd_solver.cpp:106] Iteration 12220, lr = 0.001
I1026 00:21:25.494529 16980 solver.cpp:229] Iteration 12240, loss = 0.223763
I1026 00:21:25.494561 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.125177 (* 1 = 0.125177 loss)
I1026 00:21:25.494566 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0985862 (* 1 = 0.0985862 loss)
I1026 00:21:25.494570 16980 sgd_solver.cpp:106] Iteration 12240, lr = 0.001
I1026 00:21:29.218492 16980 solver.cpp:229] Iteration 12260, loss = 0.152601
I1026 00:21:29.218523 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0927772 (* 1 = 0.0927772 loss)
I1026 00:21:29.218528 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0598237 (* 1 = 0.0598237 loss)
I1026 00:21:29.218533 16980 sgd_solver.cpp:106] Iteration 12260, lr = 0.001
I1026 00:21:32.959286 16980 solver.cpp:229] Iteration 12280, loss = 0.162087
I1026 00:21:32.959322 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0865963 (* 1 = 0.0865963 loss)
I1026 00:21:32.959342 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0754903 (* 1 = 0.0754903 loss)
I1026 00:21:32.959347 16980 sgd_solver.cpp:106] Iteration 12280, lr = 0.001
I1026 00:21:36.665120 16980 solver.cpp:229] Iteration 12300, loss = 0.336331
I1026 00:21:36.665150 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.201472 (* 1 = 0.201472 loss)
I1026 00:21:36.665158 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.134859 (* 1 = 0.134859 loss)
I1026 00:21:36.665163 16980 sgd_solver.cpp:106] Iteration 12300, lr = 0.001
I1026 00:21:40.431653 16980 solver.cpp:229] Iteration 12320, loss = 0.386658
I1026 00:21:40.431684 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.240822 (* 1 = 0.240822 loss)
I1026 00:21:40.431691 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.145835 (* 1 = 0.145835 loss)
I1026 00:21:40.431697 16980 sgd_solver.cpp:106] Iteration 12320, lr = 0.001
I1026 00:21:44.086045 16980 solver.cpp:229] Iteration 12340, loss = 0.149016
I1026 00:21:44.086079 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0928601 (* 1 = 0.0928601 loss)
I1026 00:21:44.086086 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0561561 (* 1 = 0.0561561 loss)
I1026 00:21:44.086102 16980 sgd_solver.cpp:106] Iteration 12340, lr = 0.001
I1026 00:21:47.826447 16980 solver.cpp:229] Iteration 12360, loss = 0.283636
I1026 00:21:47.826478 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0928127 (* 1 = 0.0928127 loss)
I1026 00:21:47.826483 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.190824 (* 1 = 0.190824 loss)
I1026 00:21:47.826486 16980 sgd_solver.cpp:106] Iteration 12360, lr = 0.001
I1026 00:21:51.536959 16980 solver.cpp:229] Iteration 12380, loss = 0.137925
I1026 00:21:51.537009 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0805887 (* 1 = 0.0805887 loss)
I1026 00:21:51.537014 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0573365 (* 1 = 0.0573365 loss)
I1026 00:21:51.537019 16980 sgd_solver.cpp:106] Iteration 12380, lr = 0.001
I1026 00:21:55.244410 16980 solver.cpp:229] Iteration 12400, loss = 0.235331
I1026 00:21:55.244441 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.124917 (* 1 = 0.124917 loss)
I1026 00:21:55.244446 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.110414 (* 1 = 0.110414 loss)
I1026 00:21:55.244449 16980 sgd_solver.cpp:106] Iteration 12400, lr = 0.001
I1026 00:21:58.938474 16980 solver.cpp:229] Iteration 12420, loss = 0.0816874
I1026 00:21:58.938504 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.047472 (* 1 = 0.047472 loss)
I1026 00:21:58.938508 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0342154 (* 1 = 0.0342154 loss)
I1026 00:21:58.938513 16980 sgd_solver.cpp:106] Iteration 12420, lr = 0.001
I1026 00:22:02.672060 16980 solver.cpp:229] Iteration 12440, loss = 0.14443
I1026 00:22:02.672089 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0614146 (* 1 = 0.0614146 loss)
I1026 00:22:02.672094 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0830149 (* 1 = 0.0830149 loss)
I1026 00:22:02.672098 16980 sgd_solver.cpp:106] Iteration 12440, lr = 0.001
I1026 00:22:06.401118 16980 solver.cpp:229] Iteration 12460, loss = 0.0818609
I1026 00:22:06.401147 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0719583 (* 1 = 0.0719583 loss)
I1026 00:22:06.401151 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.00990256 (* 1 = 0.00990256 loss)
I1026 00:22:06.401156 16980 sgd_solver.cpp:106] Iteration 12460, lr = 0.001
I1026 00:22:10.219732 16980 solver.cpp:229] Iteration 12480, loss = 0.193981
I1026 00:22:10.219764 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0732588 (* 1 = 0.0732588 loss)
I1026 00:22:10.219769 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.120722 (* 1 = 0.120722 loss)
I1026 00:22:10.219774 16980 sgd_solver.cpp:106] Iteration 12480, lr = 0.001
I1026 00:22:13.937070 16980 solver.cpp:229] Iteration 12500, loss = 0.0731393
I1026 00:22:13.937101 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0497351 (* 1 = 0.0497351 loss)
I1026 00:22:13.937106 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0234043 (* 1 = 0.0234043 loss)
I1026 00:22:13.937110 16980 sgd_solver.cpp:106] Iteration 12500, lr = 0.001
I1026 00:22:17.705016 16980 solver.cpp:229] Iteration 12520, loss = 0.158807
I1026 00:22:17.705049 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0707809 (* 1 = 0.0707809 loss)
I1026 00:22:17.705054 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0880262 (* 1 = 0.0880262 loss)
I1026 00:22:17.705059 16980 sgd_solver.cpp:106] Iteration 12520, lr = 0.001
I1026 00:22:21.422802 16980 solver.cpp:229] Iteration 12540, loss = 0.103373
I1026 00:22:21.422833 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0784511 (* 1 = 0.0784511 loss)
I1026 00:22:21.422838 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0249219 (* 1 = 0.0249219 loss)
I1026 00:22:21.422842 16980 sgd_solver.cpp:106] Iteration 12540, lr = 0.001
I1026 00:22:25.117485 16980 solver.cpp:229] Iteration 12560, loss = 0.166162
I1026 00:22:25.117513 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0953503 (* 1 = 0.0953503 loss)
I1026 00:22:25.117518 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0708121 (* 1 = 0.0708121 loss)
I1026 00:22:25.117522 16980 sgd_solver.cpp:106] Iteration 12560, lr = 0.001
I1026 00:22:28.900897 16980 solver.cpp:229] Iteration 12580, loss = 0.102148
I1026 00:22:28.900940 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0891696 (* 1 = 0.0891696 loss)
I1026 00:22:28.900944 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0129788 (* 1 = 0.0129788 loss)
I1026 00:22:28.900959 16980 sgd_solver.cpp:106] Iteration 12580, lr = 0.001
I1026 00:22:32.639500 16980 solver.cpp:229] Iteration 12600, loss = 0.241725
I1026 00:22:32.639530 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.101266 (* 1 = 0.101266 loss)
I1026 00:22:32.639535 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.140459 (* 1 = 0.140459 loss)
I1026 00:22:32.639539 16980 sgd_solver.cpp:106] Iteration 12600, lr = 0.001
I1026 00:22:36.334288 16980 solver.cpp:229] Iteration 12620, loss = 0.179754
I1026 00:22:36.334318 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.106439 (* 1 = 0.106439 loss)
I1026 00:22:36.334323 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0733152 (* 1 = 0.0733152 loss)
I1026 00:22:36.334327 16980 sgd_solver.cpp:106] Iteration 12620, lr = 0.001
I1026 00:22:40.018267 16980 solver.cpp:229] Iteration 12640, loss = 0.136404
I1026 00:22:40.018298 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0900779 (* 1 = 0.0900779 loss)
I1026 00:22:40.018303 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0463263 (* 1 = 0.0463263 loss)
I1026 00:22:40.018307 16980 sgd_solver.cpp:106] Iteration 12640, lr = 0.001
I1026 00:22:43.693064 16980 solver.cpp:229] Iteration 12660, loss = 0.0755695
I1026 00:22:43.693099 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0501061 (* 1 = 0.0501061 loss)
I1026 00:22:43.693105 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0254634 (* 1 = 0.0254634 loss)
I1026 00:22:43.693110 16980 sgd_solver.cpp:106] Iteration 12660, lr = 0.001
I1026 00:22:47.392619 16980 solver.cpp:229] Iteration 12680, loss = 0.093141
I1026 00:22:47.392652 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0615752 (* 1 = 0.0615752 loss)
I1026 00:22:47.392659 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0315658 (* 1 = 0.0315658 loss)
I1026 00:22:47.392665 16980 sgd_solver.cpp:106] Iteration 12680, lr = 0.001
I1026 00:22:51.001145 16980 solver.cpp:229] Iteration 12700, loss = 0.161453
I1026 00:22:51.001181 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0826753 (* 1 = 0.0826753 loss)
I1026 00:22:51.001189 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0787772 (* 1 = 0.0787772 loss)
I1026 00:22:51.001196 16980 sgd_solver.cpp:106] Iteration 12700, lr = 0.001
I1026 00:22:54.610894 16980 solver.cpp:229] Iteration 12720, loss = 0.147143
I1026 00:22:54.610929 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0855552 (* 1 = 0.0855552 loss)
I1026 00:22:54.610937 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0615879 (* 1 = 0.0615879 loss)
I1026 00:22:54.610944 16980 sgd_solver.cpp:106] Iteration 12720, lr = 0.001
I1026 00:22:58.231505 16980 solver.cpp:229] Iteration 12740, loss = 0.14881
I1026 00:22:58.231539 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0654063 (* 1 = 0.0654063 loss)
I1026 00:22:58.231545 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0834035 (* 1 = 0.0834035 loss)
I1026 00:22:58.231551 16980 sgd_solver.cpp:106] Iteration 12740, lr = 0.001
I1026 00:23:01.863272 16980 solver.cpp:229] Iteration 12760, loss = 0.148408
I1026 00:23:01.863302 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.110181 (* 1 = 0.110181 loss)
I1026 00:23:01.863307 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0382275 (* 1 = 0.0382275 loss)
I1026 00:23:01.863312 16980 sgd_solver.cpp:106] Iteration 12760, lr = 0.001
I1026 00:23:05.529888 16980 solver.cpp:229] Iteration 12780, loss = 0.15753
I1026 00:23:05.529928 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0679131 (* 1 = 0.0679131 loss)
I1026 00:23:05.529933 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0896164 (* 1 = 0.0896164 loss)
I1026 00:23:05.529937 16980 sgd_solver.cpp:106] Iteration 12780, lr = 0.001
I1026 00:23:09.222422 16980 solver.cpp:229] Iteration 12800, loss = 0.0809595
I1026 00:23:09.222452 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.064375 (* 1 = 0.064375 loss)
I1026 00:23:09.222457 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0165846 (* 1 = 0.0165846 loss)
I1026 00:23:09.222462 16980 sgd_solver.cpp:106] Iteration 12800, lr = 0.001
I1026 00:23:12.923558 16980 solver.cpp:229] Iteration 12820, loss = 0.18619
I1026 00:23:12.923588 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.136817 (* 1 = 0.136817 loss)
I1026 00:23:12.923593 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0493729 (* 1 = 0.0493729 loss)
I1026 00:23:12.923598 16980 sgd_solver.cpp:106] Iteration 12820, lr = 0.001
I1026 00:23:16.610783 16980 solver.cpp:229] Iteration 12840, loss = 0.120135
I1026 00:23:16.610816 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0540305 (* 1 = 0.0540305 loss)
I1026 00:23:16.610821 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.066104 (* 1 = 0.066104 loss)
I1026 00:23:16.610826 16980 sgd_solver.cpp:106] Iteration 12840, lr = 0.001
I1026 00:23:20.271108 16980 solver.cpp:229] Iteration 12860, loss = 0.0943446
I1026 00:23:20.271149 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0559198 (* 1 = 0.0559198 loss)
I1026 00:23:20.271154 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0384248 (* 1 = 0.0384248 loss)
I1026 00:23:20.271158 16980 sgd_solver.cpp:106] Iteration 12860, lr = 0.001
I1026 00:23:23.992807 16980 solver.cpp:229] Iteration 12880, loss = 0.163327
I1026 00:23:23.992849 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.107388 (* 1 = 0.107388 loss)
I1026 00:23:23.992853 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0559389 (* 1 = 0.0559389 loss)
I1026 00:23:23.992858 16980 sgd_solver.cpp:106] Iteration 12880, lr = 0.001
I1026 00:23:27.722213 16980 solver.cpp:229] Iteration 12900, loss = 0.204611
I1026 00:23:27.722247 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.11405 (* 1 = 0.11405 loss)
I1026 00:23:27.722251 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0905604 (* 1 = 0.0905604 loss)
I1026 00:23:27.722256 16980 sgd_solver.cpp:106] Iteration 12900, lr = 0.001
I1026 00:23:31.483378 16980 solver.cpp:229] Iteration 12920, loss = 0.112388
I1026 00:23:31.483410 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.079576 (* 1 = 0.079576 loss)
I1026 00:23:31.483414 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0328124 (* 1 = 0.0328124 loss)
I1026 00:23:31.483419 16980 sgd_solver.cpp:106] Iteration 12920, lr = 0.001
I1026 00:23:35.182133 16980 solver.cpp:229] Iteration 12940, loss = 0.140164
I1026 00:23:35.182165 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0830491 (* 1 = 0.0830491 loss)
I1026 00:23:35.182169 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0571148 (* 1 = 0.0571148 loss)
I1026 00:23:35.182174 16980 sgd_solver.cpp:106] Iteration 12940, lr = 0.001
I1026 00:23:38.941849 16980 solver.cpp:229] Iteration 12960, loss = 0.102355
I1026 00:23:38.941880 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0756547 (* 1 = 0.0756547 loss)
I1026 00:23:38.941885 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0267005 (* 1 = 0.0267005 loss)
I1026 00:23:38.941890 16980 sgd_solver.cpp:106] Iteration 12960, lr = 0.001
I1026 00:23:42.544296 16980 solver.cpp:229] Iteration 12980, loss = 0.106459
I1026 00:23:42.544327 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0702628 (* 1 = 0.0702628 loss)
I1026 00:23:42.544332 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0361967 (* 1 = 0.0361967 loss)
I1026 00:23:42.544335 16980 sgd_solver.cpp:106] Iteration 12980, lr = 0.001
I1026 00:23:46.263571 16980 solver.cpp:229] Iteration 13000, loss = 0.202267
I1026 00:23:46.263602 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.148857 (* 1 = 0.148857 loss)
I1026 00:23:46.263607 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0534095 (* 1 = 0.0534095 loss)
I1026 00:23:46.263612 16980 sgd_solver.cpp:106] Iteration 13000, lr = 0.001
I1026 00:23:50.040366 16980 solver.cpp:229] Iteration 13020, loss = 0.100351
I1026 00:23:50.040397 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0859484 (* 1 = 0.0859484 loss)
I1026 00:23:50.040401 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0144026 (* 1 = 0.0144026 loss)
I1026 00:23:50.040406 16980 sgd_solver.cpp:106] Iteration 13020, lr = 0.001
I1026 00:23:53.740046 16980 solver.cpp:229] Iteration 13040, loss = 0.178238
I1026 00:23:53.740077 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.128345 (* 1 = 0.128345 loss)
I1026 00:23:53.740082 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0498936 (* 1 = 0.0498936 loss)
I1026 00:23:53.740087 16980 sgd_solver.cpp:106] Iteration 13040, lr = 0.001
I1026 00:23:57.350049 16980 solver.cpp:229] Iteration 13060, loss = 0.183921
I1026 00:23:57.350080 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.105918 (* 1 = 0.105918 loss)
I1026 00:23:57.350085 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0780031 (* 1 = 0.0780031 loss)
I1026 00:23:57.350090 16980 sgd_solver.cpp:106] Iteration 13060, lr = 0.001
I1026 00:24:01.075516 16980 solver.cpp:229] Iteration 13080, loss = 0.0680196
I1026 00:24:01.075551 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0502895 (* 1 = 0.0502895 loss)
I1026 00:24:01.075556 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0177301 (* 1 = 0.0177301 loss)
I1026 00:24:01.075562 16980 sgd_solver.cpp:106] Iteration 13080, lr = 0.001
I1026 00:24:04.812130 16980 solver.cpp:229] Iteration 13100, loss = 0.296531
I1026 00:24:04.812162 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.12451 (* 1 = 0.12451 loss)
I1026 00:24:04.812166 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.17202 (* 1 = 0.17202 loss)
I1026 00:24:04.812171 16980 sgd_solver.cpp:106] Iteration 13100, lr = 0.001
I1026 00:24:08.584221 16980 solver.cpp:229] Iteration 13120, loss = 0.110398
I1026 00:24:08.584251 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.054581 (* 1 = 0.054581 loss)
I1026 00:24:08.584256 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.055817 (* 1 = 0.055817 loss)
I1026 00:24:08.584260 16980 sgd_solver.cpp:106] Iteration 13120, lr = 0.001
I1026 00:24:12.297037 16980 solver.cpp:229] Iteration 13140, loss = 0.110403
I1026 00:24:12.297068 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0725651 (* 1 = 0.0725651 loss)
I1026 00:24:12.297073 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0378378 (* 1 = 0.0378378 loss)
I1026 00:24:12.297077 16980 sgd_solver.cpp:106] Iteration 13140, lr = 0.001
I1026 00:24:16.068766 16980 solver.cpp:229] Iteration 13160, loss = 0.158955
I1026 00:24:16.068795 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0923027 (* 1 = 0.0923027 loss)
I1026 00:24:16.068800 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0666528 (* 1 = 0.0666528 loss)
I1026 00:24:16.068805 16980 sgd_solver.cpp:106] Iteration 13160, lr = 0.001
I1026 00:24:19.828500 16980 solver.cpp:229] Iteration 13180, loss = 0.128854
I1026 00:24:19.828532 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0882923 (* 1 = 0.0882923 loss)
I1026 00:24:19.828537 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0405622 (* 1 = 0.0405622 loss)
I1026 00:24:19.828542 16980 sgd_solver.cpp:106] Iteration 13180, lr = 0.001
I1026 00:24:23.586779 16980 solver.cpp:229] Iteration 13200, loss = 0.136389
I1026 00:24:23.586812 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.075797 (* 1 = 0.075797 loss)
I1026 00:24:23.586817 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0605915 (* 1 = 0.0605915 loss)
I1026 00:24:23.586822 16980 sgd_solver.cpp:106] Iteration 13200, lr = 0.001
I1026 00:24:27.229321 16980 solver.cpp:229] Iteration 13220, loss = 0.151941
I1026 00:24:27.229353 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0987398 (* 1 = 0.0987398 loss)
I1026 00:24:27.229357 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0532009 (* 1 = 0.0532009 loss)
I1026 00:24:27.229362 16980 sgd_solver.cpp:106] Iteration 13220, lr = 0.001
I1026 00:24:30.825820 16980 solver.cpp:229] Iteration 13240, loss = 0.122547
I1026 00:24:30.825855 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0793885 (* 1 = 0.0793885 loss)
I1026 00:24:30.825858 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.043159 (* 1 = 0.043159 loss)
I1026 00:24:30.825863 16980 sgd_solver.cpp:106] Iteration 13240, lr = 0.001
I1026 00:24:34.459357 16980 solver.cpp:229] Iteration 13260, loss = 0.205555
I1026 00:24:34.459389 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0810135 (* 1 = 0.0810135 loss)
I1026 00:24:34.459393 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.124541 (* 1 = 0.124541 loss)
I1026 00:24:34.459398 16980 sgd_solver.cpp:106] Iteration 13260, lr = 0.001
I1026 00:24:38.227926 16980 solver.cpp:229] Iteration 13280, loss = 0.0932583
I1026 00:24:38.227957 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0587001 (* 1 = 0.0587001 loss)
I1026 00:24:38.227962 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0345582 (* 1 = 0.0345582 loss)
I1026 00:24:38.227967 16980 sgd_solver.cpp:106] Iteration 13280, lr = 0.001
I1026 00:24:41.981999 16980 solver.cpp:229] Iteration 13300, loss = 0.165972
I1026 00:24:41.982043 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.118693 (* 1 = 0.118693 loss)
I1026 00:24:41.982048 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.047279 (* 1 = 0.047279 loss)
I1026 00:24:41.982053 16980 sgd_solver.cpp:106] Iteration 13300, lr = 0.001
I1026 00:24:45.586904 16980 solver.cpp:229] Iteration 13320, loss = 0.0890307
I1026 00:24:45.586930 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0606988 (* 1 = 0.0606988 loss)
I1026 00:24:45.586935 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0283319 (* 1 = 0.0283319 loss)
I1026 00:24:45.586940 16980 sgd_solver.cpp:106] Iteration 13320, lr = 0.001
I1026 00:24:49.355526 16980 solver.cpp:229] Iteration 13340, loss = 0.282762
I1026 00:24:49.355576 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.178917 (* 1 = 0.178917 loss)
I1026 00:24:49.355581 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.103845 (* 1 = 0.103845 loss)
I1026 00:24:49.355586 16980 sgd_solver.cpp:106] Iteration 13340, lr = 0.001
I1026 00:24:53.069139 16980 solver.cpp:229] Iteration 13360, loss = 0.08179
I1026 00:24:53.069169 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0482231 (* 1 = 0.0482231 loss)
I1026 00:24:53.069172 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.033567 (* 1 = 0.033567 loss)
I1026 00:24:53.069177 16980 sgd_solver.cpp:106] Iteration 13360, lr = 0.001
I1026 00:24:56.753754 16980 solver.cpp:229] Iteration 13380, loss = 0.117705
I1026 00:24:56.753793 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0582008 (* 1 = 0.0582008 loss)
I1026 00:24:56.753798 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0595042 (* 1 = 0.0595042 loss)
I1026 00:24:56.753813 16980 sgd_solver.cpp:106] Iteration 13380, lr = 0.001
I1026 00:25:00.434052 16980 solver.cpp:229] Iteration 13400, loss = 0.123155
I1026 00:25:00.434080 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0743783 (* 1 = 0.0743783 loss)
I1026 00:25:00.434085 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0487767 (* 1 = 0.0487767 loss)
I1026 00:25:00.434090 16980 sgd_solver.cpp:106] Iteration 13400, lr = 0.001
I1026 00:25:04.255154 16980 solver.cpp:229] Iteration 13420, loss = 0.140311
I1026 00:25:04.255187 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.111286 (* 1 = 0.111286 loss)
I1026 00:25:04.255193 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0290253 (* 1 = 0.0290253 loss)
I1026 00:25:04.255199 16980 sgd_solver.cpp:106] Iteration 13420, lr = 0.001
I1026 00:25:07.976568 16980 solver.cpp:229] Iteration 13440, loss = 0.184831
I1026 00:25:07.976600 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.124722 (* 1 = 0.124722 loss)
I1026 00:25:07.976605 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0601093 (* 1 = 0.0601093 loss)
I1026 00:25:07.976610 16980 sgd_solver.cpp:106] Iteration 13440, lr = 0.001
I1026 00:25:11.759676 16980 solver.cpp:229] Iteration 13460, loss = 0.145592
I1026 00:25:11.759711 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0763735 (* 1 = 0.0763735 loss)
I1026 00:25:11.759714 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0692187 (* 1 = 0.0692187 loss)
I1026 00:25:11.759721 16980 sgd_solver.cpp:106] Iteration 13460, lr = 0.001
I1026 00:25:15.470516 16980 solver.cpp:229] Iteration 13480, loss = 0.131242
I1026 00:25:15.470547 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.103438 (* 1 = 0.103438 loss)
I1026 00:25:15.470552 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0278038 (* 1 = 0.0278038 loss)
I1026 00:25:15.470557 16980 sgd_solver.cpp:106] Iteration 13480, lr = 0.001
I1026 00:25:19.091665 16980 solver.cpp:229] Iteration 13500, loss = 0.156456
I1026 00:25:19.091696 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0783883 (* 1 = 0.0783883 loss)
I1026 00:25:19.091701 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0780672 (* 1 = 0.0780672 loss)
I1026 00:25:19.091704 16980 sgd_solver.cpp:106] Iteration 13500, lr = 0.001
I1026 00:25:22.712571 16980 solver.cpp:229] Iteration 13520, loss = 0.281081
I1026 00:25:22.712600 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.126416 (* 1 = 0.126416 loss)
I1026 00:25:22.712604 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.154666 (* 1 = 0.154666 loss)
I1026 00:25:22.712610 16980 sgd_solver.cpp:106] Iteration 13520, lr = 0.001
I1026 00:25:26.417026 16980 solver.cpp:229] Iteration 13540, loss = 0.0350933
I1026 00:25:26.417057 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0275903 (* 1 = 0.0275903 loss)
I1026 00:25:26.417062 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.00750297 (* 1 = 0.00750297 loss)
I1026 00:25:26.417065 16980 sgd_solver.cpp:106] Iteration 13540, lr = 0.001
I1026 00:25:30.175865 16980 solver.cpp:229] Iteration 13560, loss = 0.0871994
I1026 00:25:30.175897 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0522668 (* 1 = 0.0522668 loss)
I1026 00:25:30.175902 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0349327 (* 1 = 0.0349327 loss)
I1026 00:25:30.175906 16980 sgd_solver.cpp:106] Iteration 13560, lr = 0.001
I1026 00:25:33.849442 16980 solver.cpp:229] Iteration 13580, loss = 0.127511
I1026 00:25:33.849468 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0706159 (* 1 = 0.0706159 loss)
I1026 00:25:33.849472 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0568948 (* 1 = 0.0568948 loss)
I1026 00:25:33.849488 16980 sgd_solver.cpp:106] Iteration 13580, lr = 0.001
I1026 00:25:37.509246 16980 solver.cpp:229] Iteration 13600, loss = 0.152919
I1026 00:25:37.509276 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.109927 (* 1 = 0.109927 loss)
I1026 00:25:37.509281 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0429922 (* 1 = 0.0429922 loss)
I1026 00:25:37.509285 16980 sgd_solver.cpp:106] Iteration 13600, lr = 0.001
I1026 00:25:41.262253 16980 solver.cpp:229] Iteration 13620, loss = 0.227563
I1026 00:25:41.262281 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.117429 (* 1 = 0.117429 loss)
I1026 00:25:41.262286 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.110134 (* 1 = 0.110134 loss)
I1026 00:25:41.262290 16980 sgd_solver.cpp:106] Iteration 13620, lr = 0.001
I1026 00:25:45.028645 16980 solver.cpp:229] Iteration 13640, loss = 0.213755
I1026 00:25:45.028677 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.122607 (* 1 = 0.122607 loss)
I1026 00:25:45.028682 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0911479 (* 1 = 0.0911479 loss)
I1026 00:25:45.028688 16980 sgd_solver.cpp:106] Iteration 13640, lr = 0.001
I1026 00:25:48.757094 16980 solver.cpp:229] Iteration 13660, loss = 0.195421
I1026 00:25:48.757122 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0841149 (* 1 = 0.0841149 loss)
I1026 00:25:48.757127 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.111306 (* 1 = 0.111306 loss)
I1026 00:25:48.757130 16980 sgd_solver.cpp:106] Iteration 13660, lr = 0.001
I1026 00:25:52.460940 16980 solver.cpp:229] Iteration 13680, loss = 0.166301
I1026 00:25:52.460980 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.107097 (* 1 = 0.107097 loss)
I1026 00:25:52.460985 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0592044 (* 1 = 0.0592044 loss)
I1026 00:25:52.460990 16980 sgd_solver.cpp:106] Iteration 13680, lr = 0.001
I1026 00:25:56.063073 16980 solver.cpp:229] Iteration 13700, loss = 0.0893848
I1026 00:25:56.063102 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0492788 (* 1 = 0.0492788 loss)
I1026 00:25:56.063107 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.040106 (* 1 = 0.040106 loss)
I1026 00:25:56.063112 16980 sgd_solver.cpp:106] Iteration 13700, lr = 0.001
I1026 00:25:59.693683 16980 solver.cpp:229] Iteration 13720, loss = 0.143779
I1026 00:25:59.693717 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.101943 (* 1 = 0.101943 loss)
I1026 00:25:59.693722 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0418356 (* 1 = 0.0418356 loss)
I1026 00:25:59.693727 16980 sgd_solver.cpp:106] Iteration 13720, lr = 0.001
I1026 00:26:03.393401 16980 solver.cpp:229] Iteration 13740, loss = 0.158174
I1026 00:26:03.393434 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0852431 (* 1 = 0.0852431 loss)
I1026 00:26:03.393438 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0729312 (* 1 = 0.0729312 loss)
I1026 00:26:03.393443 16980 sgd_solver.cpp:106] Iteration 13740, lr = 0.001
I1026 00:26:07.084157 16980 solver.cpp:229] Iteration 13760, loss = 0.121289
I1026 00:26:07.084189 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0980677 (* 1 = 0.0980677 loss)
I1026 00:26:07.084197 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0232213 (* 1 = 0.0232213 loss)
I1026 00:26:07.084203 16980 sgd_solver.cpp:106] Iteration 13760, lr = 0.001
I1026 00:26:10.859251 16980 solver.cpp:229] Iteration 13780, loss = 0.140625
I1026 00:26:10.859285 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0754876 (* 1 = 0.0754876 loss)
I1026 00:26:10.859292 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.065137 (* 1 = 0.065137 loss)
I1026 00:26:10.859297 16980 sgd_solver.cpp:106] Iteration 13780, lr = 0.001
I1026 00:26:14.616724 16980 solver.cpp:229] Iteration 13800, loss = 0.213422
I1026 00:26:14.616755 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.11771 (* 1 = 0.11771 loss)
I1026 00:26:14.616760 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0957115 (* 1 = 0.0957115 loss)
I1026 00:26:14.616763 16980 sgd_solver.cpp:106] Iteration 13800, lr = 0.001
I1026 00:26:18.360088 16980 solver.cpp:229] Iteration 13820, loss = 0.0678821
I1026 00:26:18.360126 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0445641 (* 1 = 0.0445641 loss)
I1026 00:26:18.360133 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.023318 (* 1 = 0.023318 loss)
I1026 00:26:18.360141 16980 sgd_solver.cpp:106] Iteration 13820, lr = 0.001
I1026 00:26:21.949232 16980 solver.cpp:229] Iteration 13840, loss = 0.0790668
I1026 00:26:21.949266 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0573115 (* 1 = 0.0573115 loss)
I1026 00:26:21.949273 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0217554 (* 1 = 0.0217554 loss)
I1026 00:26:21.949280 16980 sgd_solver.cpp:106] Iteration 13840, lr = 0.001
I1026 00:26:25.661330 16980 solver.cpp:229] Iteration 13860, loss = 0.15629
I1026 00:26:25.661363 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.04342 (* 1 = 0.04342 loss)
I1026 00:26:25.661370 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.11287 (* 1 = 0.11287 loss)
I1026 00:26:25.661376 16980 sgd_solver.cpp:106] Iteration 13860, lr = 0.001
I1026 00:26:29.381834 16980 solver.cpp:229] Iteration 13880, loss = 0.141182
I1026 00:26:29.381867 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0873858 (* 1 = 0.0873858 loss)
I1026 00:26:29.381873 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0537964 (* 1 = 0.0537964 loss)
I1026 00:26:29.381880 16980 sgd_solver.cpp:106] Iteration 13880, lr = 0.001
I1026 00:26:33.031886 16980 solver.cpp:229] Iteration 13900, loss = 0.0538954
I1026 00:26:33.031921 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0303404 (* 1 = 0.0303404 loss)
I1026 00:26:33.031927 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0235551 (* 1 = 0.0235551 loss)
I1026 00:26:33.031934 16980 sgd_solver.cpp:106] Iteration 13900, lr = 0.001
I1026 00:26:36.737133 16980 solver.cpp:229] Iteration 13920, loss = 0.0791449
I1026 00:26:36.737164 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0237143 (* 1 = 0.0237143 loss)
I1026 00:26:36.737170 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0554306 (* 1 = 0.0554306 loss)
I1026 00:26:36.737176 16980 sgd_solver.cpp:106] Iteration 13920, lr = 0.001
I1026 00:26:40.482830 16980 solver.cpp:229] Iteration 13940, loss = 0.121254
I1026 00:26:40.482862 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.093344 (* 1 = 0.093344 loss)
I1026 00:26:40.482869 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0279101 (* 1 = 0.0279101 loss)
I1026 00:26:40.482875 16980 sgd_solver.cpp:106] Iteration 13940, lr = 0.001
I1026 00:26:44.276084 16980 solver.cpp:229] Iteration 13960, loss = 0.260966
I1026 00:26:44.276115 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.173042 (* 1 = 0.173042 loss)
I1026 00:26:44.276123 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0879243 (* 1 = 0.0879243 loss)
I1026 00:26:44.276129 16980 sgd_solver.cpp:106] Iteration 13960, lr = 0.001
I1026 00:26:47.929422 16980 solver.cpp:229] Iteration 13980, loss = 0.141229
I1026 00:26:47.929457 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0575156 (* 1 = 0.0575156 loss)
I1026 00:26:47.929466 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0837138 (* 1 = 0.0837138 loss)
I1026 00:26:47.929472 16980 sgd_solver.cpp:106] Iteration 13980, lr = 0.001
I1026 00:26:51.652004 16980 solver.cpp:229] Iteration 14000, loss = 0.254872
I1026 00:26:51.652037 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.101223 (* 1 = 0.101223 loss)
I1026 00:26:51.652043 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.153649 (* 1 = 0.153649 loss)
I1026 00:26:51.652050 16980 sgd_solver.cpp:106] Iteration 14000, lr = 0.001
I1026 00:26:55.314357 16980 solver.cpp:229] Iteration 14020, loss = 0.0813372
I1026 00:26:55.314393 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0494174 (* 1 = 0.0494174 loss)
I1026 00:26:55.314402 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0319199 (* 1 = 0.0319199 loss)
I1026 00:26:55.314409 16980 sgd_solver.cpp:106] Iteration 14020, lr = 0.001
I1026 00:26:59.043754 16980 solver.cpp:229] Iteration 14040, loss = 0.152299
I1026 00:26:59.043788 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.10261 (* 1 = 0.10261 loss)
I1026 00:26:59.043795 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0496888 (* 1 = 0.0496888 loss)
I1026 00:26:59.043802 16980 sgd_solver.cpp:106] Iteration 14040, lr = 0.001
I1026 00:27:02.803730 16980 solver.cpp:229] Iteration 14060, loss = 0.070447
I1026 00:27:02.803762 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.046443 (* 1 = 0.046443 loss)
I1026 00:27:02.803769 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.024004 (* 1 = 0.024004 loss)
I1026 00:27:02.803776 16980 sgd_solver.cpp:106] Iteration 14060, lr = 0.001
I1026 00:27:06.651836 16980 solver.cpp:229] Iteration 14080, loss = 0.079636
I1026 00:27:06.651867 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0525981 (* 1 = 0.0525981 loss)
I1026 00:27:06.651875 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0270379 (* 1 = 0.0270379 loss)
I1026 00:27:06.651880 16980 sgd_solver.cpp:106] Iteration 14080, lr = 0.001
I1026 00:27:10.390193 16980 solver.cpp:229] Iteration 14100, loss = 0.274455
I1026 00:27:10.390226 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.173377 (* 1 = 0.173377 loss)
I1026 00:27:10.390234 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.101078 (* 1 = 0.101078 loss)
I1026 00:27:10.390240 16980 sgd_solver.cpp:106] Iteration 14100, lr = 0.001
I1026 00:27:14.056105 16980 solver.cpp:229] Iteration 14120, loss = 0.222264
I1026 00:27:14.056138 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.142113 (* 1 = 0.142113 loss)
I1026 00:27:14.056143 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0801513 (* 1 = 0.0801513 loss)
I1026 00:27:14.056146 16980 sgd_solver.cpp:106] Iteration 14120, lr = 0.001
I1026 00:27:17.751451 16980 solver.cpp:229] Iteration 14140, loss = 0.0999479
I1026 00:27:17.751480 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0682266 (* 1 = 0.0682266 loss)
I1026 00:27:17.751485 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0317214 (* 1 = 0.0317214 loss)
I1026 00:27:17.751489 16980 sgd_solver.cpp:106] Iteration 14140, lr = 0.001
I1026 00:27:21.413753 16980 solver.cpp:229] Iteration 14160, loss = 0.101439
I1026 00:27:21.413786 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0707666 (* 1 = 0.0707666 loss)
I1026 00:27:21.413791 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0306722 (* 1 = 0.0306722 loss)
I1026 00:27:21.413796 16980 sgd_solver.cpp:106] Iteration 14160, lr = 0.001
I1026 00:27:25.084800 16980 solver.cpp:229] Iteration 14180, loss = 0.111922
I1026 00:27:25.084833 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0462731 (* 1 = 0.0462731 loss)
I1026 00:27:25.084838 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0656485 (* 1 = 0.0656485 loss)
I1026 00:27:25.084843 16980 sgd_solver.cpp:106] Iteration 14180, lr = 0.001
I1026 00:27:28.823645 16980 solver.cpp:229] Iteration 14200, loss = 0.0935161
I1026 00:27:28.823676 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0430958 (* 1 = 0.0430958 loss)
I1026 00:27:28.823680 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0504203 (* 1 = 0.0504203 loss)
I1026 00:27:28.823685 16980 sgd_solver.cpp:106] Iteration 14200, lr = 0.001
I1026 00:27:32.520737 16980 solver.cpp:229] Iteration 14220, loss = 0.0912314
I1026 00:27:32.520768 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0585304 (* 1 = 0.0585304 loss)
I1026 00:27:32.520773 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.032701 (* 1 = 0.032701 loss)
I1026 00:27:32.520777 16980 sgd_solver.cpp:106] Iteration 14220, lr = 0.001
I1026 00:27:36.246863 16980 solver.cpp:229] Iteration 14240, loss = 0.185207
I1026 00:27:36.246894 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.125769 (* 1 = 0.125769 loss)
I1026 00:27:36.246899 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0594378 (* 1 = 0.0594378 loss)
I1026 00:27:36.246903 16980 sgd_solver.cpp:106] Iteration 14240, lr = 0.001
I1026 00:27:39.984591 16980 solver.cpp:229] Iteration 14260, loss = 0.0614702
I1026 00:27:39.984624 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0423735 (* 1 = 0.0423735 loss)
I1026 00:27:39.984628 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0190967 (* 1 = 0.0190967 loss)
I1026 00:27:39.984634 16980 sgd_solver.cpp:106] Iteration 14260, lr = 0.001
I1026 00:27:43.683837 16980 solver.cpp:229] Iteration 14280, loss = 0.0924399
I1026 00:27:43.683871 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0597182 (* 1 = 0.0597182 loss)
I1026 00:27:43.683876 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0327217 (* 1 = 0.0327217 loss)
I1026 00:27:43.683881 16980 sgd_solver.cpp:106] Iteration 14280, lr = 0.001
I1026 00:27:47.415592 16980 solver.cpp:229] Iteration 14300, loss = 0.214487
I1026 00:27:47.415623 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.13218 (* 1 = 0.13218 loss)
I1026 00:27:47.415628 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.082307 (* 1 = 0.082307 loss)
I1026 00:27:47.415632 16980 sgd_solver.cpp:106] Iteration 14300, lr = 0.001
I1026 00:27:51.081212 16980 solver.cpp:229] Iteration 14320, loss = 0.124254
I1026 00:27:51.081254 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.105749 (* 1 = 0.105749 loss)
I1026 00:27:51.081259 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0185049 (* 1 = 0.0185049 loss)
I1026 00:27:51.081264 16980 sgd_solver.cpp:106] Iteration 14320, lr = 0.001
I1026 00:27:54.774109 16980 solver.cpp:229] Iteration 14340, loss = 0.122229
I1026 00:27:54.774137 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0909503 (* 1 = 0.0909503 loss)
I1026 00:27:54.774142 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0312782 (* 1 = 0.0312782 loss)
I1026 00:27:54.774147 16980 sgd_solver.cpp:106] Iteration 14340, lr = 0.001
I1026 00:27:58.430469 16980 solver.cpp:229] Iteration 14360, loss = 0.147866
I1026 00:27:58.430498 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.107488 (* 1 = 0.107488 loss)
I1026 00:27:58.430503 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0403772 (* 1 = 0.0403772 loss)
I1026 00:27:58.430507 16980 sgd_solver.cpp:106] Iteration 14360, lr = 0.001
I1026 00:28:02.121204 16980 solver.cpp:229] Iteration 14380, loss = 0.105871
I1026 00:28:02.121237 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0634998 (* 1 = 0.0634998 loss)
I1026 00:28:02.121243 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0423712 (* 1 = 0.0423712 loss)
I1026 00:28:02.121246 16980 sgd_solver.cpp:106] Iteration 14380, lr = 0.001
I1026 00:28:05.813992 16980 solver.cpp:229] Iteration 14400, loss = 0.070526
I1026 00:28:05.814024 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.045412 (* 1 = 0.045412 loss)
I1026 00:28:05.814029 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.025114 (* 1 = 0.025114 loss)
I1026 00:28:05.814034 16980 sgd_solver.cpp:106] Iteration 14400, lr = 0.001
I1026 00:28:09.546738 16980 solver.cpp:229] Iteration 14420, loss = 0.146356
I1026 00:28:09.546772 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0975315 (* 1 = 0.0975315 loss)
I1026 00:28:09.546777 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0488248 (* 1 = 0.0488248 loss)
I1026 00:28:09.546780 16980 sgd_solver.cpp:106] Iteration 14420, lr = 0.001
I1026 00:28:13.276046 16980 solver.cpp:229] Iteration 14440, loss = 0.0913672
I1026 00:28:13.276075 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0648963 (* 1 = 0.0648963 loss)
I1026 00:28:13.276079 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0264709 (* 1 = 0.0264709 loss)
I1026 00:28:13.276084 16980 sgd_solver.cpp:106] Iteration 14440, lr = 0.001
I1026 00:28:16.878772 16980 solver.cpp:229] Iteration 14460, loss = 0.13958
I1026 00:28:16.878803 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0850008 (* 1 = 0.0850008 loss)
I1026 00:28:16.878808 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0545791 (* 1 = 0.0545791 loss)
I1026 00:28:16.878813 16980 sgd_solver.cpp:106] Iteration 14460, lr = 0.001
I1026 00:28:20.576930 16980 solver.cpp:229] Iteration 14480, loss = 0.163144
I1026 00:28:20.576964 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0979587 (* 1 = 0.0979587 loss)
I1026 00:28:20.576968 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0651851 (* 1 = 0.0651851 loss)
I1026 00:28:20.576974 16980 sgd_solver.cpp:106] Iteration 14480, lr = 0.001
I1026 00:28:24.163862 16980 solver.cpp:229] Iteration 14500, loss = 0.254334
I1026 00:28:24.163895 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.142409 (* 1 = 0.142409 loss)
I1026 00:28:24.163900 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.111925 (* 1 = 0.111925 loss)
I1026 00:28:24.163905 16980 sgd_solver.cpp:106] Iteration 14500, lr = 0.001
I1026 00:28:27.876246 16980 solver.cpp:229] Iteration 14520, loss = 0.123161
I1026 00:28:27.876278 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0838256 (* 1 = 0.0838256 loss)
I1026 00:28:27.876283 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0393359 (* 1 = 0.0393359 loss)
I1026 00:28:27.876289 16980 sgd_solver.cpp:106] Iteration 14520, lr = 0.001
I1026 00:28:31.600905 16980 solver.cpp:229] Iteration 14540, loss = 0.178917
I1026 00:28:31.600939 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0929908 (* 1 = 0.0929908 loss)
I1026 00:28:31.600942 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0859263 (* 1 = 0.0859263 loss)
I1026 00:28:31.600947 16980 sgd_solver.cpp:106] Iteration 14540, lr = 0.001
I1026 00:28:35.300724 16980 solver.cpp:229] Iteration 14560, loss = 0.125641
I1026 00:28:35.300755 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0573211 (* 1 = 0.0573211 loss)
I1026 00:28:35.300760 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0683201 (* 1 = 0.0683201 loss)
I1026 00:28:35.300765 16980 sgd_solver.cpp:106] Iteration 14560, lr = 0.001
I1026 00:28:39.025974 16980 solver.cpp:229] Iteration 14580, loss = 0.105424
I1026 00:28:39.026007 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0819128 (* 1 = 0.0819128 loss)
I1026 00:28:39.026012 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0235111 (* 1 = 0.0235111 loss)
I1026 00:28:39.026017 16980 sgd_solver.cpp:106] Iteration 14580, lr = 0.001
I1026 00:28:42.728914 16980 solver.cpp:229] Iteration 14600, loss = 0.167413
I1026 00:28:42.728942 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.11723 (* 1 = 0.11723 loss)
I1026 00:28:42.728946 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0501835 (* 1 = 0.0501835 loss)
I1026 00:28:42.728951 16980 sgd_solver.cpp:106] Iteration 14600, lr = 0.001
I1026 00:28:46.372195 16980 solver.cpp:229] Iteration 14620, loss = 0.0964511
I1026 00:28:46.372225 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0397414 (* 1 = 0.0397414 loss)
I1026 00:28:46.372228 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0567097 (* 1 = 0.0567097 loss)
I1026 00:28:46.372232 16980 sgd_solver.cpp:106] Iteration 14620, lr = 0.001
I1026 00:28:50.006139 16980 solver.cpp:229] Iteration 14640, loss = 0.0946462
I1026 00:28:50.006170 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0680489 (* 1 = 0.0680489 loss)
I1026 00:28:50.006175 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0265972 (* 1 = 0.0265972 loss)
I1026 00:28:50.006180 16980 sgd_solver.cpp:106] Iteration 14640, lr = 0.001
I1026 00:28:53.706677 16980 solver.cpp:229] Iteration 14660, loss = 0.0959444
I1026 00:28:53.706708 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0405582 (* 1 = 0.0405582 loss)
I1026 00:28:53.706712 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0553862 (* 1 = 0.0553862 loss)
I1026 00:28:53.706717 16980 sgd_solver.cpp:106] Iteration 14660, lr = 0.001
I1026 00:28:57.401466 16980 solver.cpp:229] Iteration 14680, loss = 0.133322
I1026 00:28:57.401497 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.092779 (* 1 = 0.092779 loss)
I1026 00:28:57.401501 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0405432 (* 1 = 0.0405432 loss)
I1026 00:28:57.401506 16980 sgd_solver.cpp:106] Iteration 14680, lr = 0.001
I1026 00:29:01.107058 16980 solver.cpp:229] Iteration 14700, loss = 0.218924
I1026 00:29:01.107089 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.119342 (* 1 = 0.119342 loss)
I1026 00:29:01.107095 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0995821 (* 1 = 0.0995821 loss)
I1026 00:29:01.107100 16980 sgd_solver.cpp:106] Iteration 14700, lr = 0.001
I1026 00:29:04.664736 16980 solver.cpp:229] Iteration 14720, loss = 0.172045
I1026 00:29:04.664764 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.133755 (* 1 = 0.133755 loss)
I1026 00:29:04.664769 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0382899 (* 1 = 0.0382899 loss)
I1026 00:29:04.664773 16980 sgd_solver.cpp:106] Iteration 14720, lr = 0.001
I1026 00:29:08.329002 16980 solver.cpp:229] Iteration 14740, loss = 0.161485
I1026 00:29:08.329033 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.100182 (* 1 = 0.100182 loss)
I1026 00:29:08.329038 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0613032 (* 1 = 0.0613032 loss)
I1026 00:29:08.329042 16980 sgd_solver.cpp:106] Iteration 14740, lr = 0.001
I1026 00:29:12.071771 16980 solver.cpp:229] Iteration 14760, loss = 0.088917
I1026 00:29:12.071805 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0544887 (* 1 = 0.0544887 loss)
I1026 00:29:12.071808 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0344282 (* 1 = 0.0344282 loss)
I1026 00:29:12.071815 16980 sgd_solver.cpp:106] Iteration 14760, lr = 0.001
I1026 00:29:15.856142 16980 solver.cpp:229] Iteration 14780, loss = 0.184272
I1026 00:29:15.856173 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.113391 (* 1 = 0.113391 loss)
I1026 00:29:15.856178 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0708806 (* 1 = 0.0708806 loss)
I1026 00:29:15.856181 16980 sgd_solver.cpp:106] Iteration 14780, lr = 0.001
I1026 00:29:19.615834 16980 solver.cpp:229] Iteration 14800, loss = 0.228836
I1026 00:29:19.615869 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.117407 (* 1 = 0.117407 loss)
I1026 00:29:19.615874 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.11143 (* 1 = 0.11143 loss)
I1026 00:29:19.615878 16980 sgd_solver.cpp:106] Iteration 14800, lr = 0.001
I1026 00:29:23.374809 16980 solver.cpp:229] Iteration 14820, loss = 0.0987089
I1026 00:29:23.374840 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0417459 (* 1 = 0.0417459 loss)
I1026 00:29:23.374845 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.056963 (* 1 = 0.056963 loss)
I1026 00:29:23.374850 16980 sgd_solver.cpp:106] Iteration 14820, lr = 0.001
I1026 00:29:27.053267 16980 solver.cpp:229] Iteration 14840, loss = 0.0689467
I1026 00:29:27.053299 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0472608 (* 1 = 0.0472608 loss)
I1026 00:29:27.053304 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0216859 (* 1 = 0.0216859 loss)
I1026 00:29:27.053308 16980 sgd_solver.cpp:106] Iteration 14840, lr = 0.001
I1026 00:29:30.753412 16980 solver.cpp:229] Iteration 14860, loss = 0.13625
I1026 00:29:30.753439 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.100834 (* 1 = 0.100834 loss)
I1026 00:29:30.753444 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0354165 (* 1 = 0.0354165 loss)
I1026 00:29:30.753448 16980 sgd_solver.cpp:106] Iteration 14860, lr = 0.001
I1026 00:29:34.533268 16980 solver.cpp:229] Iteration 14880, loss = 0.121455
I1026 00:29:34.533300 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0748292 (* 1 = 0.0748292 loss)
I1026 00:29:34.533305 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0466257 (* 1 = 0.0466257 loss)
I1026 00:29:34.533311 16980 sgd_solver.cpp:106] Iteration 14880, lr = 0.001
I1026 00:29:38.283583 16980 solver.cpp:229] Iteration 14900, loss = 0.0857446
I1026 00:29:38.283632 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0447367 (* 1 = 0.0447367 loss)
I1026 00:29:38.283648 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0410079 (* 1 = 0.0410079 loss)
I1026 00:29:38.283654 16980 sgd_solver.cpp:106] Iteration 14900, lr = 0.001
I1026 00:29:42.059823 16980 solver.cpp:229] Iteration 14920, loss = 0.174813
I1026 00:29:42.059852 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.107615 (* 1 = 0.107615 loss)
I1026 00:29:42.059857 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0671982 (* 1 = 0.0671982 loss)
I1026 00:29:42.059860 16980 sgd_solver.cpp:106] Iteration 14920, lr = 0.001
I1026 00:29:45.778434 16980 solver.cpp:229] Iteration 14940, loss = 0.0787695
I1026 00:29:45.778466 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0709214 (* 1 = 0.0709214 loss)
I1026 00:29:45.778471 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.00784814 (* 1 = 0.00784814 loss)
I1026 00:29:45.778476 16980 sgd_solver.cpp:106] Iteration 14940, lr = 0.001
I1026 00:29:49.479933 16980 solver.cpp:229] Iteration 14960, loss = 0.193514
I1026 00:29:49.479957 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.106949 (* 1 = 0.106949 loss)
I1026 00:29:49.479962 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0865648 (* 1 = 0.0865648 loss)
I1026 00:29:49.479966 16980 sgd_solver.cpp:106] Iteration 14960, lr = 0.001
I1026 00:29:53.136745 16980 solver.cpp:229] Iteration 14980, loss = 0.168405
I1026 00:29:53.136790 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0882805 (* 1 = 0.0882805 loss)
I1026 00:29:53.136795 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.080125 (* 1 = 0.080125 loss)
I1026 00:29:53.136798 16980 sgd_solver.cpp:106] Iteration 14980, lr = 0.001
I1026 00:29:56.778483 16980 solver.cpp:229] Iteration 15000, loss = 0.0810807
I1026 00:29:56.778515 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0698483 (* 1 = 0.0698483 loss)
I1026 00:29:56.778519 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0112324 (* 1 = 0.0112324 loss)
I1026 00:29:56.778525 16980 sgd_solver.cpp:106] Iteration 15000, lr = 0.001
I1026 00:30:00.516402 16980 solver.cpp:229] Iteration 15020, loss = 0.165283
I1026 00:30:00.516432 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.107125 (* 1 = 0.107125 loss)
I1026 00:30:00.516435 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0581584 (* 1 = 0.0581584 loss)
I1026 00:30:00.516439 16980 sgd_solver.cpp:106] Iteration 15020, lr = 0.001
I1026 00:30:04.158048 16980 solver.cpp:229] Iteration 15040, loss = 0.10818
I1026 00:30:04.158082 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0644498 (* 1 = 0.0644498 loss)
I1026 00:30:04.158087 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0437299 (* 1 = 0.0437299 loss)
I1026 00:30:04.158092 16980 sgd_solver.cpp:106] Iteration 15040, lr = 0.001
I1026 00:30:07.804769 16980 solver.cpp:229] Iteration 15060, loss = 0.0599075
I1026 00:30:07.804801 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0449914 (* 1 = 0.0449914 loss)
I1026 00:30:07.804805 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0149162 (* 1 = 0.0149162 loss)
I1026 00:30:07.804811 16980 sgd_solver.cpp:106] Iteration 15060, lr = 0.001
I1026 00:30:11.561297 16980 solver.cpp:229] Iteration 15080, loss = 0.106305
I1026 00:30:11.561327 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0794626 (* 1 = 0.0794626 loss)
I1026 00:30:11.561332 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0268428 (* 1 = 0.0268428 loss)
I1026 00:30:11.561347 16980 sgd_solver.cpp:106] Iteration 15080, lr = 0.001
I1026 00:30:15.334758 16980 solver.cpp:229] Iteration 15100, loss = 0.296257
I1026 00:30:15.334785 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.111102 (* 1 = 0.111102 loss)
I1026 00:30:15.334790 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.185155 (* 1 = 0.185155 loss)
I1026 00:30:15.334795 16980 sgd_solver.cpp:106] Iteration 15100, lr = 0.001
I1026 00:30:19.044976 16980 solver.cpp:229] Iteration 15120, loss = 0.142664
I1026 00:30:19.045009 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0585349 (* 1 = 0.0585349 loss)
I1026 00:30:19.045014 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0841295 (* 1 = 0.0841295 loss)
I1026 00:30:19.045018 16980 sgd_solver.cpp:106] Iteration 15120, lr = 0.001
I1026 00:30:22.837724 16980 solver.cpp:229] Iteration 15140, loss = 0.21032
I1026 00:30:22.837756 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.146573 (* 1 = 0.146573 loss)
I1026 00:30:22.837761 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0637475 (* 1 = 0.0637475 loss)
I1026 00:30:22.837766 16980 sgd_solver.cpp:106] Iteration 15140, lr = 0.001
I1026 00:30:26.497856 16980 solver.cpp:229] Iteration 15160, loss = 0.223572
I1026 00:30:26.497889 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.115112 (* 1 = 0.115112 loss)
I1026 00:30:26.497892 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.10846 (* 1 = 0.10846 loss)
I1026 00:30:26.497897 16980 sgd_solver.cpp:106] Iteration 15160, lr = 0.001
I1026 00:30:30.122480 16980 solver.cpp:229] Iteration 15180, loss = 0.23955
I1026 00:30:30.122512 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.114448 (* 1 = 0.114448 loss)
I1026 00:30:30.122516 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.125102 (* 1 = 0.125102 loss)
I1026 00:30:30.122522 16980 sgd_solver.cpp:106] Iteration 15180, lr = 0.001
I1026 00:30:33.819813 16980 solver.cpp:229] Iteration 15200, loss = 0.0881669
I1026 00:30:33.819844 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0625127 (* 1 = 0.0625127 loss)
I1026 00:30:33.819849 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0256542 (* 1 = 0.0256542 loss)
I1026 00:30:33.819854 16980 sgd_solver.cpp:106] Iteration 15200, lr = 0.001
I1026 00:30:37.461854 16980 solver.cpp:229] Iteration 15220, loss = 0.0938102
I1026 00:30:37.461882 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0463892 (* 1 = 0.0463892 loss)
I1026 00:30:37.461887 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.047421 (* 1 = 0.047421 loss)
I1026 00:30:37.461892 16980 sgd_solver.cpp:106] Iteration 15220, lr = 0.001
I1026 00:30:41.189795 16980 solver.cpp:229] Iteration 15240, loss = 0.107625
I1026 00:30:41.189837 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.09434 (* 1 = 0.09434 loss)
I1026 00:30:41.189842 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0132851 (* 1 = 0.0132851 loss)
I1026 00:30:41.189847 16980 sgd_solver.cpp:106] Iteration 15240, lr = 0.001
I1026 00:30:44.868953 16980 solver.cpp:229] Iteration 15260, loss = 0.133512
I1026 00:30:44.868984 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0871429 (* 1 = 0.0871429 loss)
I1026 00:30:44.868989 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0463694 (* 1 = 0.0463694 loss)
I1026 00:30:44.868993 16980 sgd_solver.cpp:106] Iteration 15260, lr = 0.001
I1026 00:30:48.534682 16980 solver.cpp:229] Iteration 15280, loss = 0.160469
I1026 00:30:48.534713 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0858689 (* 1 = 0.0858689 loss)
I1026 00:30:48.534716 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0745998 (* 1 = 0.0745998 loss)
I1026 00:30:48.534720 16980 sgd_solver.cpp:106] Iteration 15280, lr = 0.001
I1026 00:30:52.233916 16980 solver.cpp:229] Iteration 15300, loss = 0.23728
I1026 00:30:52.233947 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.153557 (* 1 = 0.153557 loss)
I1026 00:30:52.233952 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0837231 (* 1 = 0.0837231 loss)
I1026 00:30:52.233957 16980 sgd_solver.cpp:106] Iteration 15300, lr = 0.001
I1026 00:30:56.055213 16980 solver.cpp:229] Iteration 15320, loss = 0.249134
I1026 00:30:56.055240 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.148467 (* 1 = 0.148467 loss)
I1026 00:30:56.055244 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.100668 (* 1 = 0.100668 loss)
I1026 00:30:56.055249 16980 sgd_solver.cpp:106] Iteration 15320, lr = 0.001
I1026 00:30:59.693406 16980 solver.cpp:229] Iteration 15340, loss = 0.135423
I1026 00:30:59.693439 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0816066 (* 1 = 0.0816066 loss)
I1026 00:30:59.693444 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0538167 (* 1 = 0.0538167 loss)
I1026 00:30:59.693449 16980 sgd_solver.cpp:106] Iteration 15340, lr = 0.001
I1026 00:31:03.495352 16980 solver.cpp:229] Iteration 15360, loss = 0.174941
I1026 00:31:03.495379 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0684746 (* 1 = 0.0684746 loss)
I1026 00:31:03.495384 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.106467 (* 1 = 0.106467 loss)
I1026 00:31:03.495389 16980 sgd_solver.cpp:106] Iteration 15360, lr = 0.001
I1026 00:31:07.185022 16980 solver.cpp:229] Iteration 15380, loss = 0.0868928
I1026 00:31:07.185055 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0484938 (* 1 = 0.0484938 loss)
I1026 00:31:07.185060 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.038399 (* 1 = 0.038399 loss)
I1026 00:31:07.185065 16980 sgd_solver.cpp:106] Iteration 15380, lr = 0.001
I1026 00:31:10.849521 16980 solver.cpp:229] Iteration 15400, loss = 0.113424
I1026 00:31:10.849555 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0509238 (* 1 = 0.0509238 loss)
I1026 00:31:10.849560 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0624998 (* 1 = 0.0624998 loss)
I1026 00:31:10.849566 16980 sgd_solver.cpp:106] Iteration 15400, lr = 0.001
I1026 00:31:14.393272 16980 solver.cpp:229] Iteration 15420, loss = 0.121011
I1026 00:31:14.393301 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0843356 (* 1 = 0.0843356 loss)
I1026 00:31:14.393306 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0366755 (* 1 = 0.0366755 loss)
I1026 00:31:14.393311 16980 sgd_solver.cpp:106] Iteration 15420, lr = 0.001
I1026 00:31:17.966198 16980 solver.cpp:229] Iteration 15440, loss = 0.136909
I1026 00:31:17.966228 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0713447 (* 1 = 0.0713447 loss)
I1026 00:31:17.966233 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0655648 (* 1 = 0.0655648 loss)
I1026 00:31:17.966236 16980 sgd_solver.cpp:106] Iteration 15440, lr = 0.001
I1026 00:31:21.680196 16980 solver.cpp:229] Iteration 15460, loss = 0.0806262
I1026 00:31:21.680227 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0482739 (* 1 = 0.0482739 loss)
I1026 00:31:21.680232 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0323524 (* 1 = 0.0323524 loss)
I1026 00:31:21.680236 16980 sgd_solver.cpp:106] Iteration 15460, lr = 0.001
I1026 00:31:25.388660 16980 solver.cpp:229] Iteration 15480, loss = 0.240166
I1026 00:31:25.388689 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.192788 (* 1 = 0.192788 loss)
I1026 00:31:25.388694 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0473783 (* 1 = 0.0473783 loss)
I1026 00:31:25.388698 16980 sgd_solver.cpp:106] Iteration 15480, lr = 0.001
I1026 00:31:29.021731 16980 solver.cpp:229] Iteration 15500, loss = 0.235029
I1026 00:31:29.021762 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.110935 (* 1 = 0.110935 loss)
I1026 00:31:29.021766 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.124094 (* 1 = 0.124094 loss)
I1026 00:31:29.021771 16980 sgd_solver.cpp:106] Iteration 15500, lr = 0.001
I1026 00:31:32.778421 16980 solver.cpp:229] Iteration 15520, loss = 0.0941265
I1026 00:31:32.778451 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0514077 (* 1 = 0.0514077 loss)
I1026 00:31:32.778455 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0427189 (* 1 = 0.0427189 loss)
I1026 00:31:32.778460 16980 sgd_solver.cpp:106] Iteration 15520, lr = 0.001
I1026 00:31:36.568367 16980 solver.cpp:229] Iteration 15540, loss = 0.136146
I1026 00:31:36.568397 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0721666 (* 1 = 0.0721666 loss)
I1026 00:31:36.568400 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0639794 (* 1 = 0.0639794 loss)
I1026 00:31:36.568404 16980 sgd_solver.cpp:106] Iteration 15540, lr = 0.001
I1026 00:31:40.278641 16980 solver.cpp:229] Iteration 15560, loss = 0.0940179
I1026 00:31:40.278673 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.056532 (* 1 = 0.056532 loss)
I1026 00:31:40.278677 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0374859 (* 1 = 0.0374859 loss)
I1026 00:31:40.278681 16980 sgd_solver.cpp:106] Iteration 15560, lr = 0.001
I1026 00:31:44.048161 16980 solver.cpp:229] Iteration 15580, loss = 0.102822
I1026 00:31:44.048190 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0553358 (* 1 = 0.0553358 loss)
I1026 00:31:44.048195 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0474862 (* 1 = 0.0474862 loss)
I1026 00:31:44.048200 16980 sgd_solver.cpp:106] Iteration 15580, lr = 0.001
I1026 00:31:47.759747 16980 solver.cpp:229] Iteration 15600, loss = 0.0821916
I1026 00:31:47.759789 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0521794 (* 1 = 0.0521794 loss)
I1026 00:31:47.759793 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0300122 (* 1 = 0.0300122 loss)
I1026 00:31:47.759799 16980 sgd_solver.cpp:106] Iteration 15600, lr = 0.001
I1026 00:31:51.512816 16980 solver.cpp:229] Iteration 15620, loss = 0.0940097
I1026 00:31:51.512847 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0828938 (* 1 = 0.0828938 loss)
I1026 00:31:51.512852 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0111159 (* 1 = 0.0111159 loss)
I1026 00:31:51.512856 16980 sgd_solver.cpp:106] Iteration 15620, lr = 0.001
I1026 00:31:55.233857 16980 solver.cpp:229] Iteration 15640, loss = 0.230594
I1026 00:31:55.233888 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.144149 (* 1 = 0.144149 loss)
I1026 00:31:55.233893 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.086445 (* 1 = 0.086445 loss)
I1026 00:31:55.233898 16980 sgd_solver.cpp:106] Iteration 15640, lr = 0.001
I1026 00:31:58.954237 16980 solver.cpp:229] Iteration 15660, loss = 0.141862
I1026 00:31:58.954268 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0498036 (* 1 = 0.0498036 loss)
I1026 00:31:58.954272 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0920582 (* 1 = 0.0920582 loss)
I1026 00:31:58.954277 16980 sgd_solver.cpp:106] Iteration 15660, lr = 0.001
I1026 00:32:02.593765 16980 solver.cpp:229] Iteration 15680, loss = 0.138903
I1026 00:32:02.593796 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0920777 (* 1 = 0.0920777 loss)
I1026 00:32:02.593799 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0468249 (* 1 = 0.0468249 loss)
I1026 00:32:02.593804 16980 sgd_solver.cpp:106] Iteration 15680, lr = 0.001
I1026 00:32:06.256603 16980 solver.cpp:229] Iteration 15700, loss = 0.0861439
I1026 00:32:06.256644 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0524193 (* 1 = 0.0524193 loss)
I1026 00:32:06.256649 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0337247 (* 1 = 0.0337247 loss)
I1026 00:32:06.256661 16980 sgd_solver.cpp:106] Iteration 15700, lr = 0.001
I1026 00:32:09.921170 16980 solver.cpp:229] Iteration 15720, loss = 0.0912436
I1026 00:32:09.921202 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0511296 (* 1 = 0.0511296 loss)
I1026 00:32:09.921207 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.040114 (* 1 = 0.040114 loss)
I1026 00:32:09.921211 16980 sgd_solver.cpp:106] Iteration 15720, lr = 0.001
I1026 00:32:13.597996 16980 solver.cpp:229] Iteration 15740, loss = 0.0607845
I1026 00:32:13.598027 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0534778 (* 1 = 0.0534778 loss)
I1026 00:32:13.598031 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.00730666 (* 1 = 0.00730666 loss)
I1026 00:32:13.598037 16980 sgd_solver.cpp:106] Iteration 15740, lr = 0.001
I1026 00:32:17.304286 16980 solver.cpp:229] Iteration 15760, loss = 0.152083
I1026 00:32:17.304319 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0675897 (* 1 = 0.0675897 loss)
I1026 00:32:17.304324 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0844938 (* 1 = 0.0844938 loss)
I1026 00:32:17.304329 16980 sgd_solver.cpp:106] Iteration 15760, lr = 0.001
I1026 00:32:20.960036 16980 solver.cpp:229] Iteration 15780, loss = 0.0476742
I1026 00:32:20.960067 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0287739 (* 1 = 0.0287739 loss)
I1026 00:32:20.960072 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0189003 (* 1 = 0.0189003 loss)
I1026 00:32:20.960078 16980 sgd_solver.cpp:106] Iteration 15780, lr = 0.001
I1026 00:32:24.721442 16980 solver.cpp:229] Iteration 15800, loss = 0.14331
I1026 00:32:24.721472 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0815725 (* 1 = 0.0815725 loss)
I1026 00:32:24.721475 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0617374 (* 1 = 0.0617374 loss)
I1026 00:32:24.721480 16980 sgd_solver.cpp:106] Iteration 15800, lr = 0.001
I1026 00:32:28.405354 16980 solver.cpp:229] Iteration 15820, loss = 0.188956
I1026 00:32:28.405383 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.127297 (* 1 = 0.127297 loss)
I1026 00:32:28.405405 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0616599 (* 1 = 0.0616599 loss)
I1026 00:32:28.405410 16980 sgd_solver.cpp:106] Iteration 15820, lr = 0.001
I1026 00:32:32.155562 16980 solver.cpp:229] Iteration 15840, loss = 0.0936297
I1026 00:32:32.155593 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0358873 (* 1 = 0.0358873 loss)
I1026 00:32:32.155597 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0577424 (* 1 = 0.0577424 loss)
I1026 00:32:32.155601 16980 sgd_solver.cpp:106] Iteration 15840, lr = 0.001
I1026 00:32:35.877512 16980 solver.cpp:229] Iteration 15860, loss = 0.076458
I1026 00:32:35.877544 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.065618 (* 1 = 0.065618 loss)
I1026 00:32:35.877549 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.01084 (* 1 = 0.01084 loss)
I1026 00:32:35.877554 16980 sgd_solver.cpp:106] Iteration 15860, lr = 0.001
I1026 00:32:39.535326 16980 solver.cpp:229] Iteration 15880, loss = 0.135075
I1026 00:32:39.535356 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0834027 (* 1 = 0.0834027 loss)
I1026 00:32:39.535362 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0516719 (* 1 = 0.0516719 loss)
I1026 00:32:39.535365 16980 sgd_solver.cpp:106] Iteration 15880, lr = 0.001
I1026 00:32:43.292197 16980 solver.cpp:229] Iteration 15900, loss = 0.0954287
I1026 00:32:43.292229 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0420361 (* 1 = 0.0420361 loss)
I1026 00:32:43.292234 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0533926 (* 1 = 0.0533926 loss)
I1026 00:32:43.292239 16980 sgd_solver.cpp:106] Iteration 15900, lr = 0.001
I1026 00:32:46.972931 16980 solver.cpp:229] Iteration 15920, loss = 0.114904
I1026 00:32:46.972965 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0579452 (* 1 = 0.0579452 loss)
I1026 00:32:46.972970 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0569586 (* 1 = 0.0569586 loss)
I1026 00:32:46.972985 16980 sgd_solver.cpp:106] Iteration 15920, lr = 0.001
I1026 00:32:50.733875 16980 solver.cpp:229] Iteration 15940, loss = 0.122182
I1026 00:32:50.733903 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0618552 (* 1 = 0.0618552 loss)
I1026 00:32:50.733907 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0603264 (* 1 = 0.0603264 loss)
I1026 00:32:50.733911 16980 sgd_solver.cpp:106] Iteration 15940, lr = 0.001
I1026 00:32:54.442003 16980 solver.cpp:229] Iteration 15960, loss = 0.0990845
I1026 00:32:54.442034 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0633143 (* 1 = 0.0633143 loss)
I1026 00:32:54.442039 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0357703 (* 1 = 0.0357703 loss)
I1026 00:32:54.442044 16980 sgd_solver.cpp:106] Iteration 15960, lr = 0.001
I1026 00:32:58.122792 16980 solver.cpp:229] Iteration 15980, loss = 0.298331
I1026 00:32:58.122823 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.14648 (* 1 = 0.14648 loss)
I1026 00:32:58.122828 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.151851 (* 1 = 0.151851 loss)
I1026 00:32:58.122833 16980 sgd_solver.cpp:106] Iteration 15980, lr = 0.001
I1026 00:33:01.838454 16980 solver.cpp:229] Iteration 16000, loss = 0.18718
I1026 00:33:01.838486 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.110747 (* 1 = 0.110747 loss)
I1026 00:33:01.838491 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0764331 (* 1 = 0.0764331 loss)
I1026 00:33:01.838496 16980 sgd_solver.cpp:106] Iteration 16000, lr = 0.001
I1026 00:33:05.512176 16980 solver.cpp:229] Iteration 16020, loss = 0.149717
I1026 00:33:05.512205 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.101276 (* 1 = 0.101276 loss)
I1026 00:33:05.512210 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0484413 (* 1 = 0.0484413 loss)
I1026 00:33:05.512215 16980 sgd_solver.cpp:106] Iteration 16020, lr = 0.001
I1026 00:33:09.199864 16980 solver.cpp:229] Iteration 16040, loss = 0.0762138
I1026 00:33:09.199898 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0483143 (* 1 = 0.0483143 loss)
I1026 00:33:09.199903 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0278995 (* 1 = 0.0278995 loss)
I1026 00:33:09.199908 16980 sgd_solver.cpp:106] Iteration 16040, lr = 0.001
I1026 00:33:12.891585 16980 solver.cpp:229] Iteration 16060, loss = 0.260379
I1026 00:33:12.891619 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.104727 (* 1 = 0.104727 loss)
I1026 00:33:12.891623 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.155652 (* 1 = 0.155652 loss)
I1026 00:33:12.891628 16980 sgd_solver.cpp:106] Iteration 16060, lr = 0.001
I1026 00:33:16.549566 16980 solver.cpp:229] Iteration 16080, loss = 0.0904522
I1026 00:33:16.549597 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0786419 (* 1 = 0.0786419 loss)
I1026 00:33:16.549602 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0118103 (* 1 = 0.0118103 loss)
I1026 00:33:16.549605 16980 sgd_solver.cpp:106] Iteration 16080, lr = 0.001
I1026 00:33:20.233937 16980 solver.cpp:229] Iteration 16100, loss = 0.0982704
I1026 00:33:20.233997 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0464487 (* 1 = 0.0464487 loss)
I1026 00:33:20.234002 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0518218 (* 1 = 0.0518218 loss)
I1026 00:33:20.234007 16980 sgd_solver.cpp:106] Iteration 16100, lr = 0.001
I1026 00:33:23.839577 16980 solver.cpp:229] Iteration 16120, loss = 0.241119
I1026 00:33:23.839607 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.157276 (* 1 = 0.157276 loss)
I1026 00:33:23.839612 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0838426 (* 1 = 0.0838426 loss)
I1026 00:33:23.839617 16980 sgd_solver.cpp:106] Iteration 16120, lr = 0.001
I1026 00:33:27.549707 16980 solver.cpp:229] Iteration 16140, loss = 0.107156
I1026 00:33:27.549762 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0650527 (* 1 = 0.0650527 loss)
I1026 00:33:27.549777 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0421031 (* 1 = 0.0421031 loss)
I1026 00:33:27.549782 16980 sgd_solver.cpp:106] Iteration 16140, lr = 0.001
I1026 00:33:31.262552 16980 solver.cpp:229] Iteration 16160, loss = 0.268764
I1026 00:33:31.262583 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.187786 (* 1 = 0.187786 loss)
I1026 00:33:31.262589 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0809772 (* 1 = 0.0809772 loss)
I1026 00:33:31.262594 16980 sgd_solver.cpp:106] Iteration 16160, lr = 0.001
I1026 00:33:35.021225 16980 solver.cpp:229] Iteration 16180, loss = 0.152966
I1026 00:33:35.021257 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0592224 (* 1 = 0.0592224 loss)
I1026 00:33:35.021262 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0937432 (* 1 = 0.0937432 loss)
I1026 00:33:35.021267 16980 sgd_solver.cpp:106] Iteration 16180, lr = 0.001
I1026 00:33:38.702476 16980 solver.cpp:229] Iteration 16200, loss = 0.120951
I1026 00:33:38.702503 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0923741 (* 1 = 0.0923741 loss)
I1026 00:33:38.702508 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0285766 (* 1 = 0.0285766 loss)
I1026 00:33:38.702512 16980 sgd_solver.cpp:106] Iteration 16200, lr = 0.001
I1026 00:33:42.383162 16980 solver.cpp:229] Iteration 16220, loss = 0.0449933
I1026 00:33:42.383194 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0318116 (* 1 = 0.0318116 loss)
I1026 00:33:42.383198 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0131816 (* 1 = 0.0131816 loss)
I1026 00:33:42.383204 16980 sgd_solver.cpp:106] Iteration 16220, lr = 0.001
I1026 00:33:46.071482 16980 solver.cpp:229] Iteration 16240, loss = 0.0645854
I1026 00:33:46.071511 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0556054 (* 1 = 0.0556054 loss)
I1026 00:33:46.071517 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.00897996 (* 1 = 0.00897996 loss)
I1026 00:33:46.071522 16980 sgd_solver.cpp:106] Iteration 16240, lr = 0.001
I1026 00:33:49.815568 16980 solver.cpp:229] Iteration 16260, loss = 0.18821
I1026 00:33:49.815601 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.113683 (* 1 = 0.113683 loss)
I1026 00:33:49.815606 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0745265 (* 1 = 0.0745265 loss)
I1026 00:33:49.815611 16980 sgd_solver.cpp:106] Iteration 16260, lr = 0.001
I1026 00:33:53.410562 16980 solver.cpp:229] Iteration 16280, loss = 0.0576754
I1026 00:33:53.410588 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0475014 (* 1 = 0.0475014 loss)
I1026 00:33:53.410593 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.010174 (* 1 = 0.010174 loss)
I1026 00:33:53.410598 16980 sgd_solver.cpp:106] Iteration 16280, lr = 0.001
I1026 00:33:56.995141 16980 solver.cpp:229] Iteration 16300, loss = 0.0921676
I1026 00:33:56.995172 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0442417 (* 1 = 0.0442417 loss)
I1026 00:33:56.995177 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0479259 (* 1 = 0.0479259 loss)
I1026 00:33:56.995182 16980 sgd_solver.cpp:106] Iteration 16300, lr = 0.001
I1026 00:34:00.554241 16980 solver.cpp:229] Iteration 16320, loss = 0.149255
I1026 00:34:00.554273 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0760477 (* 1 = 0.0760477 loss)
I1026 00:34:00.554278 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0732068 (* 1 = 0.0732068 loss)
I1026 00:34:00.554283 16980 sgd_solver.cpp:106] Iteration 16320, lr = 0.001
I1026 00:34:04.241416 16980 solver.cpp:229] Iteration 16340, loss = 0.1626
I1026 00:34:04.241447 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.11031 (* 1 = 0.11031 loss)
I1026 00:34:04.241451 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0522899 (* 1 = 0.0522899 loss)
I1026 00:34:04.241456 16980 sgd_solver.cpp:106] Iteration 16340, lr = 0.001
I1026 00:34:08.009243 16980 solver.cpp:229] Iteration 16360, loss = 0.110438
I1026 00:34:08.009275 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0541427 (* 1 = 0.0541427 loss)
I1026 00:34:08.009280 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0562952 (* 1 = 0.0562952 loss)
I1026 00:34:08.009285 16980 sgd_solver.cpp:106] Iteration 16360, lr = 0.001
I1026 00:34:11.705808 16980 solver.cpp:229] Iteration 16380, loss = 0.146606
I1026 00:34:11.705838 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0734067 (* 1 = 0.0734067 loss)
I1026 00:34:11.705844 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0731994 (* 1 = 0.0731994 loss)
I1026 00:34:11.705848 16980 sgd_solver.cpp:106] Iteration 16380, lr = 0.001
I1026 00:34:15.414428 16980 solver.cpp:229] Iteration 16400, loss = 0.114222
I1026 00:34:15.414463 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.069378 (* 1 = 0.069378 loss)
I1026 00:34:15.414466 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0448435 (* 1 = 0.0448435 loss)
I1026 00:34:15.414471 16980 sgd_solver.cpp:106] Iteration 16400, lr = 0.001
I1026 00:34:19.085471 16980 solver.cpp:229] Iteration 16420, loss = 0.125292
I1026 00:34:19.085503 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0759868 (* 1 = 0.0759868 loss)
I1026 00:34:19.085508 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0493055 (* 1 = 0.0493055 loss)
I1026 00:34:19.085512 16980 sgd_solver.cpp:106] Iteration 16420, lr = 0.001
I1026 00:34:22.803696 16980 solver.cpp:229] Iteration 16440, loss = 0.133605
I1026 00:34:22.803728 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0600549 (* 1 = 0.0600549 loss)
I1026 00:34:22.803733 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0735499 (* 1 = 0.0735499 loss)
I1026 00:34:22.803738 16980 sgd_solver.cpp:106] Iteration 16440, lr = 0.001
I1026 00:34:26.527367 16980 solver.cpp:229] Iteration 16460, loss = 0.0802901
I1026 00:34:26.527400 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0502883 (* 1 = 0.0502883 loss)
I1026 00:34:26.527405 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0300018 (* 1 = 0.0300018 loss)
I1026 00:34:26.527410 16980 sgd_solver.cpp:106] Iteration 16460, lr = 0.001
I1026 00:34:30.230459 16980 solver.cpp:229] Iteration 16480, loss = 0.0400427
I1026 00:34:30.230490 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0357265 (* 1 = 0.0357265 loss)
I1026 00:34:30.230494 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.00431623 (* 1 = 0.00431623 loss)
I1026 00:34:30.230499 16980 sgd_solver.cpp:106] Iteration 16480, lr = 0.001
I1026 00:34:33.957310 16980 solver.cpp:229] Iteration 16500, loss = 0.170301
I1026 00:34:33.957340 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.118674 (* 1 = 0.118674 loss)
I1026 00:34:33.957345 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0516272 (* 1 = 0.0516272 loss)
I1026 00:34:33.957350 16980 sgd_solver.cpp:106] Iteration 16500, lr = 0.001
I1026 00:34:37.642530 16980 solver.cpp:229] Iteration 16520, loss = 0.122959
I1026 00:34:37.642570 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0523639 (* 1 = 0.0523639 loss)
I1026 00:34:37.642575 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0705948 (* 1 = 0.0705948 loss)
I1026 00:34:37.642590 16980 sgd_solver.cpp:106] Iteration 16520, lr = 0.001
I1026 00:34:41.414712 16980 solver.cpp:229] Iteration 16540, loss = 0.101117
I1026 00:34:41.414746 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0728827 (* 1 = 0.0728827 loss)
I1026 00:34:41.414752 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0282342 (* 1 = 0.0282342 loss)
I1026 00:34:41.414755 16980 sgd_solver.cpp:106] Iteration 16540, lr = 0.001
I1026 00:34:45.112094 16980 solver.cpp:229] Iteration 16560, loss = 0.082304
I1026 00:34:45.112128 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0540522 (* 1 = 0.0540522 loss)
I1026 00:34:45.112131 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0282519 (* 1 = 0.0282519 loss)
I1026 00:34:45.112138 16980 sgd_solver.cpp:106] Iteration 16560, lr = 0.001
I1026 00:34:48.688740 16980 solver.cpp:229] Iteration 16580, loss = 0.160443
I1026 00:34:48.688767 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0886517 (* 1 = 0.0886517 loss)
I1026 00:34:48.688772 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0717918 (* 1 = 0.0717918 loss)
I1026 00:34:48.688777 16980 sgd_solver.cpp:106] Iteration 16580, lr = 0.001
I1026 00:34:52.352866 16980 solver.cpp:229] Iteration 16600, loss = 0.108863
I1026 00:34:52.352898 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0523654 (* 1 = 0.0523654 loss)
I1026 00:34:52.352902 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0564977 (* 1 = 0.0564977 loss)
I1026 00:34:52.352906 16980 sgd_solver.cpp:106] Iteration 16600, lr = 0.001
I1026 00:34:56.036862 16980 solver.cpp:229] Iteration 16620, loss = 0.245466
I1026 00:34:56.036895 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.132175 (* 1 = 0.132175 loss)
I1026 00:34:56.036900 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.113291 (* 1 = 0.113291 loss)
I1026 00:34:56.036903 16980 sgd_solver.cpp:106] Iteration 16620, lr = 0.001
I1026 00:34:59.748430 16980 solver.cpp:229] Iteration 16640, loss = 0.0686742
I1026 00:34:59.748461 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0429129 (* 1 = 0.0429129 loss)
I1026 00:34:59.748464 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0257613 (* 1 = 0.0257613 loss)
I1026 00:34:59.748469 16980 sgd_solver.cpp:106] Iteration 16640, lr = 0.001
I1026 00:35:03.421177 16980 solver.cpp:229] Iteration 16660, loss = 0.168804
I1026 00:35:03.421210 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.089594 (* 1 = 0.089594 loss)
I1026 00:35:03.421214 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0792097 (* 1 = 0.0792097 loss)
I1026 00:35:03.421219 16980 sgd_solver.cpp:106] Iteration 16660, lr = 0.001
I1026 00:35:07.015923 16980 solver.cpp:229] Iteration 16680, loss = 0.128054
I1026 00:35:07.015957 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.057729 (* 1 = 0.057729 loss)
I1026 00:35:07.015962 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0703247 (* 1 = 0.0703247 loss)
I1026 00:35:07.015967 16980 sgd_solver.cpp:106] Iteration 16680, lr = 0.001
I1026 00:35:10.642691 16980 solver.cpp:229] Iteration 16700, loss = 0.175453
I1026 00:35:10.642721 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.10346 (* 1 = 0.10346 loss)
I1026 00:35:10.642726 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0719928 (* 1 = 0.0719928 loss)
I1026 00:35:10.642731 16980 sgd_solver.cpp:106] Iteration 16700, lr = 0.001
I1026 00:35:14.309818 16980 solver.cpp:229] Iteration 16720, loss = 0.0721943
I1026 00:35:14.309849 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0521551 (* 1 = 0.0521551 loss)
I1026 00:35:14.309854 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0200392 (* 1 = 0.0200392 loss)
I1026 00:35:14.309857 16980 sgd_solver.cpp:106] Iteration 16720, lr = 0.001
I1026 00:35:18.077868 16980 solver.cpp:229] Iteration 16740, loss = 0.152423
I1026 00:35:18.077900 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.101483 (* 1 = 0.101483 loss)
I1026 00:35:18.077905 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0509398 (* 1 = 0.0509398 loss)
I1026 00:35:18.077910 16980 sgd_solver.cpp:106] Iteration 16740, lr = 0.001
I1026 00:35:21.776876 16980 solver.cpp:229] Iteration 16760, loss = 0.0740317
I1026 00:35:21.776907 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0435646 (* 1 = 0.0435646 loss)
I1026 00:35:21.776912 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0304671 (* 1 = 0.0304671 loss)
I1026 00:35:21.776916 16980 sgd_solver.cpp:106] Iteration 16760, lr = 0.001
I1026 00:35:25.443084 16980 solver.cpp:229] Iteration 16780, loss = 0.072712
I1026 00:35:25.443117 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0367705 (* 1 = 0.0367705 loss)
I1026 00:35:25.443122 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0359415 (* 1 = 0.0359415 loss)
I1026 00:35:25.443127 16980 sgd_solver.cpp:106] Iteration 16780, lr = 0.001
I1026 00:35:29.143923 16980 solver.cpp:229] Iteration 16800, loss = 0.0543705
I1026 00:35:29.143955 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0390261 (* 1 = 0.0390261 loss)
I1026 00:35:29.143960 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0153444 (* 1 = 0.0153444 loss)
I1026 00:35:29.143965 16980 sgd_solver.cpp:106] Iteration 16800, lr = 0.001
I1026 00:35:32.897892 16980 solver.cpp:229] Iteration 16820, loss = 0.183337
I1026 00:35:32.897925 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.100023 (* 1 = 0.100023 loss)
I1026 00:35:32.897930 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0833132 (* 1 = 0.0833132 loss)
I1026 00:35:32.897935 16980 sgd_solver.cpp:106] Iteration 16820, lr = 0.001
I1026 00:35:36.707628 16980 solver.cpp:229] Iteration 16840, loss = 0.152809
I1026 00:35:36.707655 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0773252 (* 1 = 0.0773252 loss)
I1026 00:35:36.707659 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0754837 (* 1 = 0.0754837 loss)
I1026 00:35:36.707664 16980 sgd_solver.cpp:106] Iteration 16840, lr = 0.001
I1026 00:35:40.417690 16980 solver.cpp:229] Iteration 16860, loss = 0.0558152
I1026 00:35:40.417719 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0458622 (* 1 = 0.0458622 loss)
I1026 00:35:40.417724 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.00995304 (* 1 = 0.00995304 loss)
I1026 00:35:40.417728 16980 sgd_solver.cpp:106] Iteration 16860, lr = 0.001
I1026 00:35:44.268409 16980 solver.cpp:229] Iteration 16880, loss = 0.160353
I1026 00:35:44.268440 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0666131 (* 1 = 0.0666131 loss)
I1026 00:35:44.268445 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0937399 (* 1 = 0.0937399 loss)
I1026 00:35:44.268448 16980 sgd_solver.cpp:106] Iteration 16880, lr = 0.001
I1026 00:35:47.928813 16980 solver.cpp:229] Iteration 16900, loss = 0.137724
I1026 00:35:47.928846 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.093803 (* 1 = 0.093803 loss)
I1026 00:35:47.928851 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0439208 (* 1 = 0.0439208 loss)
I1026 00:35:47.928858 16980 sgd_solver.cpp:106] Iteration 16900, lr = 0.001
I1026 00:35:51.665287 16980 solver.cpp:229] Iteration 16920, loss = 0.0785173
I1026 00:35:51.665318 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0392989 (* 1 = 0.0392989 loss)
I1026 00:35:51.665323 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0392184 (* 1 = 0.0392184 loss)
I1026 00:35:51.665328 16980 sgd_solver.cpp:106] Iteration 16920, lr = 0.001
I1026 00:35:55.315989 16980 solver.cpp:229] Iteration 16940, loss = 0.194822
I1026 00:35:55.316020 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0510169 (* 1 = 0.0510169 loss)
I1026 00:35:55.316025 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.143805 (* 1 = 0.143805 loss)
I1026 00:35:55.316030 16980 sgd_solver.cpp:106] Iteration 16940, lr = 0.001
I1026 00:35:59.008281 16980 solver.cpp:229] Iteration 16960, loss = 0.20672
I1026 00:35:59.008312 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.1201 (* 1 = 0.1201 loss)
I1026 00:35:59.008317 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.08662 (* 1 = 0.08662 loss)
I1026 00:35:59.008322 16980 sgd_solver.cpp:106] Iteration 16960, lr = 0.001
I1026 00:36:02.598044 16980 solver.cpp:229] Iteration 16980, loss = 0.0891594
I1026 00:36:02.598086 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0604221 (* 1 = 0.0604221 loss)
I1026 00:36:02.598091 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0287373 (* 1 = 0.0287373 loss)
I1026 00:36:02.598095 16980 sgd_solver.cpp:106] Iteration 16980, lr = 0.001
        330.        ]
 ..., 
 [  28.25951958  221.62142944   67.54941559  249.51475525]
 [ 108.27911377    0.          285.54663086   85.33618927]
 [ 195.06497192   18.53363609  214.78363037   34.50650787]]
500
[[  28.            2.          481.          467.        ]
 [ 108.71279144  137.17814636  388.13983154  476.20501709]
 [  44.8440361     0.          312.31393433  476.20501709]
 ..., 
 [  56.37346649    0.          162.84220886   37.83689117]
 [ 179.76046753  168.8067627   337.83355713  445.38867188]
 [ 199.53851318    0.          414.4864502   131.71176147]]
500
[[  39.           99.          140.          243.        ]
 [ 364.          107.          469.          233.        ]
 [ 361.21975708  104.84031677  479.72796631  252.11578369]
 ..., 
 [ 314.45843506   21.53587341  340.87789917   48.79504776]
 [ 127.83261871   86.07713318  331.47329712  163.42837524]
 [ 303.73272705    2.0851469   385.7802124    41.25251389]]
500
[[  91.          177.          334.          291.        ]
 [ 334.          175.          368.          196.        ]
 [  13.67590904    0.          246.62051392  374.375     ]
 ..., 
 [  19.79506302   61.88737869   49.85546875   90.57273865]
 [ 323.97909546  238.29188538  340.50613403  254.6159668 ]
 [ 322.66525269   95.01187897  354.40084839  129.86322021]]

done
Preparing training data...
done
Output will be saved to `/home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train`
Filtered 0 roidb entries: 1424 -> 1424
Computing bounding-box regression targets...
bbox target means:
[[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [ -1.32320502e-10  -2.11188709e-03   2.15428586e-02   6.05928410e-02]]
[ -1.32320502e-10  -2.11188709e-03   2.15428586e-02   6.05928410e-02]
bbox target stdevs:
[[ 0.          0.          0.          0.        ]
 [ 0.13021484  0.13133138  0.23925601  0.23313801]]
[ 0.13021484  0.13133138  0.23925601  0.23313801]
Normalizing targets
done
RoiDataLayer: name_to_top: {'bbox_inside_weights': 4, 'labels': 2, 'rois': 1, 'bbox_targets': 3, 'bbox_outside_weights': 5, 'data': 0}
Loading pretrained model weights from data/imagenet_models/ZF.v2.caffemodel
Solving...
speed: 0.181s / iter
speed: 0.184s / iter
speed: 0.185s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.187s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
Wrote snapshot to: /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_fast_rcnn_stage1_iter_10000.caffemodel
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.186s / iter
speed: 0.185s / iter
speed: 0.185s / iter
speed: 0.185s / iter
speed: 0.185I1026 00:36:06.206586 16980 solver.cpp:229] Iteration 17000, loss = 0.127031
I1026 00:36:06.206619 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0681593 (* 1 = 0.0681593 loss)
I1026 00:36:06.206624 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0588715 (* 1 = 0.0588715 loss)
I1026 00:36:06.206629 16980 sgd_solver.cpp:106] Iteration 17000, lr = 0.001
I1026 00:36:09.946645 16980 solver.cpp:229] Iteration 17020, loss = 0.138841
I1026 00:36:09.946677 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.101002 (* 1 = 0.101002 loss)
I1026 00:36:09.946682 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0378383 (* 1 = 0.0378383 loss)
I1026 00:36:09.946686 16980 sgd_solver.cpp:106] Iteration 17020, lr = 0.001
I1026 00:36:13.583166 16980 solver.cpp:229] Iteration 17040, loss = 0.189354
I1026 00:36:13.583194 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0918759 (* 1 = 0.0918759 loss)
I1026 00:36:13.583199 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0974778 (* 1 = 0.0974778 loss)
I1026 00:36:13.583204 16980 sgd_solver.cpp:106] Iteration 17040, lr = 0.001
I1026 00:36:17.322603 16980 solver.cpp:229] Iteration 17060, loss = 0.326538
I1026 00:36:17.322633 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.150444 (* 1 = 0.150444 loss)
I1026 00:36:17.322638 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.176095 (* 1 = 0.176095 loss)
I1026 00:36:17.322643 16980 sgd_solver.cpp:106] Iteration 17060, lr = 0.001
I1026 00:36:21.053963 16980 solver.cpp:229] Iteration 17080, loss = 0.0635219
I1026 00:36:21.053995 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0452179 (* 1 = 0.0452179 loss)
I1026 00:36:21.053999 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0183039 (* 1 = 0.0183039 loss)
I1026 00:36:21.054003 16980 sgd_solver.cpp:106] Iteration 17080, lr = 0.001
I1026 00:36:24.739537 16980 solver.cpp:229] Iteration 17100, loss = 0.209329
I1026 00:36:24.739578 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.093239 (* 1 = 0.093239 loss)
I1026 00:36:24.739583 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.11609 (* 1 = 0.11609 loss)
I1026 00:36:24.739586 16980 sgd_solver.cpp:106] Iteration 17100, lr = 0.001
I1026 00:36:28.470837 16980 solver.cpp:229] Iteration 17120, loss = 0.165372
I1026 00:36:28.470865 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.067825 (* 1 = 0.067825 loss)
I1026 00:36:28.470868 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0975472 (* 1 = 0.0975472 loss)
I1026 00:36:28.470873 16980 sgd_solver.cpp:106] Iteration 17120, lr = 0.001
I1026 00:36:32.092592 16980 solver.cpp:229] Iteration 17140, loss = 0.244858
I1026 00:36:32.092623 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.109003 (* 1 = 0.109003 loss)
I1026 00:36:32.092628 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.135855 (* 1 = 0.135855 loss)
I1026 00:36:32.092633 16980 sgd_solver.cpp:106] Iteration 17140, lr = 0.001
I1026 00:36:35.814672 16980 solver.cpp:229] Iteration 17160, loss = 0.0951517
I1026 00:36:35.814703 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0589746 (* 1 = 0.0589746 loss)
I1026 00:36:35.814707 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0361771 (* 1 = 0.0361771 loss)
I1026 00:36:35.814713 16980 sgd_solver.cpp:106] Iteration 17160, lr = 0.001
I1026 00:36:39.490051 16980 solver.cpp:229] Iteration 17180, loss = 0.180023
I1026 00:36:39.490082 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0735448 (* 1 = 0.0735448 loss)
I1026 00:36:39.490087 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.106478 (* 1 = 0.106478 loss)
I1026 00:36:39.490090 16980 sgd_solver.cpp:106] Iteration 17180, lr = 0.001
I1026 00:36:43.188328 16980 solver.cpp:229] Iteration 17200, loss = 0.0820253
I1026 00:36:43.188370 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0497408 (* 1 = 0.0497408 loss)
I1026 00:36:43.188385 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0322845 (* 1 = 0.0322845 loss)
I1026 00:36:43.188390 16980 sgd_solver.cpp:106] Iteration 17200, lr = 0.001
I1026 00:36:46.955833 16980 solver.cpp:229] Iteration 17220, loss = 0.0689567
I1026 00:36:46.955864 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0384718 (* 1 = 0.0384718 loss)
I1026 00:36:46.955869 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0304849 (* 1 = 0.0304849 loss)
I1026 00:36:46.955873 16980 sgd_solver.cpp:106] Iteration 17220, lr = 0.001
I1026 00:36:50.610597 16980 solver.cpp:229] Iteration 17240, loss = 0.053506
I1026 00:36:50.610628 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0454911 (* 1 = 0.0454911 loss)
I1026 00:36:50.610633 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.00801499 (* 1 = 0.00801499 loss)
I1026 00:36:50.610637 16980 sgd_solver.cpp:106] Iteration 17240, lr = 0.001
I1026 00:36:54.233886 16980 solver.cpp:229] Iteration 17260, loss = 0.0873678
I1026 00:36:54.233916 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0617329 (* 1 = 0.0617329 loss)
I1026 00:36:54.233922 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0256349 (* 1 = 0.0256349 loss)
I1026 00:36:54.233925 16980 sgd_solver.cpp:106] Iteration 17260, lr = 0.001
I1026 00:36:57.882063 16980 solver.cpp:229] Iteration 17280, loss = 0.194687
I1026 00:36:57.882103 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.136804 (* 1 = 0.136804 loss)
I1026 00:36:57.882108 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0578828 (* 1 = 0.0578828 loss)
I1026 00:36:57.882113 16980 sgd_solver.cpp:106] Iteration 17280, lr = 0.001
I1026 00:37:01.420274 16980 solver.cpp:229] Iteration 17300, loss = 0.171546
I1026 00:37:01.420305 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.095202 (* 1 = 0.095202 loss)
I1026 00:37:01.420310 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0763441 (* 1 = 0.0763441 loss)
I1026 00:37:01.420313 16980 sgd_solver.cpp:106] Iteration 17300, lr = 0.001
I1026 00:37:05.114398 16980 solver.cpp:229] Iteration 17320, loss = 0.147517
I1026 00:37:05.114431 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.111096 (* 1 = 0.111096 loss)
I1026 00:37:05.114435 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0364205 (* 1 = 0.0364205 loss)
I1026 00:37:05.114441 16980 sgd_solver.cpp:106] Iteration 17320, lr = 0.001
I1026 00:37:08.855509 16980 solver.cpp:229] Iteration 17340, loss = 0.104924
I1026 00:37:08.855540 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0402335 (* 1 = 0.0402335 loss)
I1026 00:37:08.855545 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0646906 (* 1 = 0.0646906 loss)
I1026 00:37:08.855550 16980 sgd_solver.cpp:106] Iteration 17340, lr = 0.001
I1026 00:37:12.607580 16980 solver.cpp:229] Iteration 17360, loss = 0.104898
I1026 00:37:12.607614 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0913209 (* 1 = 0.0913209 loss)
I1026 00:37:12.607619 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0135768 (* 1 = 0.0135768 loss)
I1026 00:37:12.607623 16980 sgd_solver.cpp:106] Iteration 17360, lr = 0.001
I1026 00:37:16.300472 16980 solver.cpp:229] Iteration 17380, loss = 0.119489
I1026 00:37:16.300501 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0688848 (* 1 = 0.0688848 loss)
I1026 00:37:16.300505 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0506044 (* 1 = 0.0506044 loss)
I1026 00:37:16.300510 16980 sgd_solver.cpp:106] Iteration 17380, lr = 0.001
I1026 00:37:20.068402 16980 solver.cpp:229] Iteration 17400, loss = 0.22951
I1026 00:37:20.068434 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.067955 (* 1 = 0.067955 loss)
I1026 00:37:20.068439 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.161555 (* 1 = 0.161555 loss)
I1026 00:37:20.068444 16980 sgd_solver.cpp:106] Iteration 17400, lr = 0.001
I1026 00:37:23.830487 16980 solver.cpp:229] Iteration 17420, loss = 0.0564974
I1026 00:37:23.830516 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0369817 (* 1 = 0.0369817 loss)
I1026 00:37:23.830521 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0195157 (* 1 = 0.0195157 loss)
I1026 00:37:23.830525 16980 sgd_solver.cpp:106] Iteration 17420, lr = 0.001
I1026 00:37:27.608882 16980 solver.cpp:229] Iteration 17440, loss = 0.0836017
I1026 00:37:27.608916 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0424125 (* 1 = 0.0424125 loss)
I1026 00:37:27.608919 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0411893 (* 1 = 0.0411893 loss)
I1026 00:37:27.608924 16980 sgd_solver.cpp:106] Iteration 17440, lr = 0.001
I1026 00:37:31.308727 16980 solver.cpp:229] Iteration 17460, loss = 0.141549
I1026 00:37:31.308759 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0770662 (* 1 = 0.0770662 loss)
I1026 00:37:31.308763 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0644828 (* 1 = 0.0644828 loss)
I1026 00:37:31.308768 16980 sgd_solver.cpp:106] Iteration 17460, lr = 0.001
I1026 00:37:34.909525 16980 solver.cpp:229] Iteration 17480, loss = 0.137781
I1026 00:37:34.909556 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0787766 (* 1 = 0.0787766 loss)
I1026 00:37:34.909561 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0590039 (* 1 = 0.0590039 loss)
I1026 00:37:34.909565 16980 sgd_solver.cpp:106] Iteration 17480, lr = 0.001
I1026 00:37:38.636399 16980 solver.cpp:229] Iteration 17500, loss = 0.0781463
I1026 00:37:38.636426 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.036942 (* 1 = 0.036942 loss)
I1026 00:37:38.636431 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0412043 (* 1 = 0.0412043 loss)
I1026 00:37:38.636436 16980 sgd_solver.cpp:106] Iteration 17500, lr = 0.001
I1026 00:37:42.263061 16980 solver.cpp:229] Iteration 17520, loss = 0.166718
I1026 00:37:42.263108 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.124596 (* 1 = 0.124596 loss)
I1026 00:37:42.263114 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.042122 (* 1 = 0.042122 loss)
I1026 00:37:42.263118 16980 sgd_solver.cpp:106] Iteration 17520, lr = 0.001
I1026 00:37:45.863895 16980 solver.cpp:229] Iteration 17540, loss = 0.221286
I1026 00:37:45.863926 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.118747 (* 1 = 0.118747 loss)
I1026 00:37:45.863931 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.102539 (* 1 = 0.102539 loss)
I1026 00:37:45.863935 16980 sgd_solver.cpp:106] Iteration 17540, lr = 0.001
I1026 00:37:49.519807 16980 solver.cpp:229] Iteration 17560, loss = 0.0695417
I1026 00:37:49.519834 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0572492 (* 1 = 0.0572492 loss)
I1026 00:37:49.519839 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0122925 (* 1 = 0.0122925 loss)
I1026 00:37:49.519842 16980 sgd_solver.cpp:106] Iteration 17560, lr = 0.001
I1026 00:37:53.163998 16980 solver.cpp:229] Iteration 17580, loss = 0.0807665
I1026 00:37:53.164031 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0672307 (* 1 = 0.0672307 loss)
I1026 00:37:53.164036 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0135358 (* 1 = 0.0135358 loss)
I1026 00:37:53.164041 16980 sgd_solver.cpp:106] Iteration 17580, lr = 0.001
I1026 00:37:56.936591 16980 solver.cpp:229] Iteration 17600, loss = 0.167366
I1026 00:37:56.936624 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.107957 (* 1 = 0.107957 loss)
I1026 00:37:56.936628 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0594091 (* 1 = 0.0594091 loss)
I1026 00:37:56.936632 16980 sgd_solver.cpp:106] Iteration 17600, lr = 0.001
I1026 00:38:00.637563 16980 solver.cpp:229] Iteration 17620, loss = 0.281708
I1026 00:38:00.637591 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.163522 (* 1 = 0.163522 loss)
I1026 00:38:00.637595 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.118186 (* 1 = 0.118186 loss)
I1026 00:38:00.637600 16980 sgd_solver.cpp:106] Iteration 17620, lr = 0.001
I1026 00:38:04.395557 16980 solver.cpp:229] Iteration 17640, loss = 0.0510102
I1026 00:38:04.395588 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0389988 (* 1 = 0.0389988 loss)
I1026 00:38:04.395592 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0120114 (* 1 = 0.0120114 loss)
I1026 00:38:04.395597 16980 sgd_solver.cpp:106] Iteration 17640, lr = 0.001
I1026 00:38:08.107101 16980 solver.cpp:229] Iteration 17660, loss = 0.16305
I1026 00:38:08.107134 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0791211 (* 1 = 0.0791211 loss)
I1026 00:38:08.107156 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0839293 (* 1 = 0.0839293 loss)
I1026 00:38:08.107159 16980 sgd_solver.cpp:106] Iteration 17660, lr = 0.001
I1026 00:38:11.838106 16980 solver.cpp:229] Iteration 17680, loss = 0.0959938
I1026 00:38:11.838135 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0631614 (* 1 = 0.0631614 loss)
I1026 00:38:11.838140 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0328324 (* 1 = 0.0328324 loss)
I1026 00:38:11.838145 16980 sgd_solver.cpp:106] Iteration 17680, lr = 0.001
I1026 00:38:15.504446 16980 solver.cpp:229] Iteration 17700, loss = 0.156832
I1026 00:38:15.504477 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0810789 (* 1 = 0.0810789 loss)
I1026 00:38:15.504482 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0757534 (* 1 = 0.0757534 loss)
I1026 00:38:15.504487 16980 sgd_solver.cpp:106] Iteration 17700, lr = 0.001
I1026 00:38:19.252005 16980 solver.cpp:229] Iteration 17720, loss = 0.0908303
I1026 00:38:19.252037 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0643553 (* 1 = 0.0643553 loss)
I1026 00:38:19.252041 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.026475 (* 1 = 0.026475 loss)
I1026 00:38:19.252045 16980 sgd_solver.cpp:106] Iteration 17720, lr = 0.001
I1026 00:38:22.939501 16980 solver.cpp:229] Iteration 17740, loss = 0.0835634
I1026 00:38:22.939532 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0688607 (* 1 = 0.0688607 loss)
I1026 00:38:22.939537 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0147027 (* 1 = 0.0147027 loss)
I1026 00:38:22.939540 16980 sgd_solver.cpp:106] Iteration 17740, lr = 0.001
I1026 00:38:26.552059 16980 solver.cpp:229] Iteration 17760, loss = 0.101466
I1026 00:38:26.552101 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0690945 (* 1 = 0.0690945 loss)
I1026 00:38:26.552106 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0323719 (* 1 = 0.0323719 loss)
I1026 00:38:26.552119 16980 sgd_solver.cpp:106] Iteration 17760, lr = 0.001
I1026 00:38:30.253283 16980 solver.cpp:229] Iteration 17780, loss = 0.0955016
I1026 00:38:30.253327 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0500047 (* 1 = 0.0500047 loss)
I1026 00:38:30.253331 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0454969 (* 1 = 0.0454969 loss)
I1026 00:38:30.253336 16980 sgd_solver.cpp:106] Iteration 17780, lr = 0.001
I1026 00:38:34.054291 16980 solver.cpp:229] Iteration 17800, loss = 0.142669
I1026 00:38:34.054322 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0479286 (* 1 = 0.0479286 loss)
I1026 00:38:34.054327 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0947404 (* 1 = 0.0947404 loss)
I1026 00:38:34.054332 16980 sgd_solver.cpp:106] Iteration 17800, lr = 0.001
I1026 00:38:37.746970 16980 solver.cpp:229] Iteration 17820, loss = 0.120389
I1026 00:38:37.747000 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0775049 (* 1 = 0.0775049 loss)
I1026 00:38:37.747004 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0428837 (* 1 = 0.0428837 loss)
I1026 00:38:37.747009 16980 sgd_solver.cpp:106] Iteration 17820, lr = 0.001
I1026 00:38:41.437038 16980 solver.cpp:229] Iteration 17840, loss = 0.180166
I1026 00:38:41.437070 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.112995 (* 1 = 0.112995 loss)
I1026 00:38:41.437074 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0671718 (* 1 = 0.0671718 loss)
I1026 00:38:41.437079 16980 sgd_solver.cpp:106] Iteration 17840, lr = 0.001
I1026 00:38:45.151713 16980 solver.cpp:229] Iteration 17860, loss = 0.141559
I1026 00:38:45.151746 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0995117 (* 1 = 0.0995117 loss)
I1026 00:38:45.151751 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0420472 (* 1 = 0.0420472 loss)
I1026 00:38:45.151756 16980 sgd_solver.cpp:106] Iteration 17860, lr = 0.001
I1026 00:38:48.784631 16980 solver.cpp:229] Iteration 17880, loss = 0.0763238
I1026 00:38:48.784658 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0380479 (* 1 = 0.0380479 loss)
I1026 00:38:48.784663 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0382759 (* 1 = 0.0382759 loss)
I1026 00:38:48.784667 16980 sgd_solver.cpp:106] Iteration 17880, lr = 0.001
I1026 00:38:52.464702 16980 solver.cpp:229] Iteration 17900, loss = 0.0974216
I1026 00:38:52.464732 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.057007 (* 1 = 0.057007 loss)
I1026 00:38:52.464736 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0404146 (* 1 = 0.0404146 loss)
I1026 00:38:52.464741 16980 sgd_solver.cpp:106] Iteration 17900, lr = 0.001
I1026 00:38:56.093034 16980 solver.cpp:229] Iteration 17920, loss = 0.455804
I1026 00:38:56.093061 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.215026 (* 1 = 0.215026 loss)
I1026 00:38:56.093065 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.240777 (* 1 = 0.240777 loss)
I1026 00:38:56.093070 16980 sgd_solver.cpp:106] Iteration 17920, lr = 0.001
I1026 00:38:59.797101 16980 solver.cpp:229] Iteration 17940, loss = 0.144722
I1026 00:38:59.797132 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0907604 (* 1 = 0.0907604 loss)
I1026 00:38:59.797137 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0539617 (* 1 = 0.0539617 loss)
I1026 00:38:59.797142 16980 sgd_solver.cpp:106] Iteration 17940, lr = 0.001
I1026 00:39:03.469977 16980 solver.cpp:229] Iteration 17960, loss = 0.152121
I1026 00:39:03.470008 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.105895 (* 1 = 0.105895 loss)
I1026 00:39:03.470012 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.046226 (* 1 = 0.046226 loss)
I1026 00:39:03.470016 16980 sgd_solver.cpp:106] Iteration 17960, lr = 0.001
I1026 00:39:07.152617 16980 solver.cpp:229] Iteration 17980, loss = 0.140892
I1026 00:39:07.152649 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0730486 (* 1 = 0.0730486 loss)
I1026 00:39:07.152653 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0678433 (* 1 = 0.0678433 loss)
I1026 00:39:07.152657 16980 sgd_solver.cpp:106] Iteration 17980, lr = 0.001
I1026 00:39:10.883714 16980 solver.cpp:229] Iteration 18000, loss = 0.0647891
I1026 00:39:10.883756 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0575585 (* 1 = 0.0575585 loss)
I1026 00:39:10.883762 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.00723058 (* 1 = 0.00723058 loss)
I1026 00:39:10.883766 16980 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I1026 00:39:14.593410 16980 solver.cpp:229] Iteration 18020, loss = 0.151917
I1026 00:39:14.593439 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0523932 (* 1 = 0.0523932 loss)
I1026 00:39:14.593443 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.099524 (* 1 = 0.099524 loss)
I1026 00:39:14.593448 16980 sgd_solver.cpp:106] Iteration 18020, lr = 0.001
I1026 00:39:18.305804 16980 solver.cpp:229] Iteration 18040, loss = 0.123267
I1026 00:39:18.305835 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0605634 (* 1 = 0.0605634 loss)
I1026 00:39:18.305840 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0627038 (* 1 = 0.0627038 loss)
I1026 00:39:18.305845 16980 sgd_solver.cpp:106] Iteration 18040, lr = 0.001
I1026 00:39:22.073073 16980 solver.cpp:229] Iteration 18060, loss = 0.14208
I1026 00:39:22.073106 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0792476 (* 1 = 0.0792476 loss)
I1026 00:39:22.073109 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0628322 (* 1 = 0.0628322 loss)
I1026 00:39:22.073114 16980 sgd_solver.cpp:106] Iteration 18060, lr = 0.001
I1026 00:39:25.723366 16980 solver.cpp:229] Iteration 18080, loss = 0.119525
I1026 00:39:25.723395 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0529139 (* 1 = 0.0529139 loss)
I1026 00:39:25.723400 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0666111 (* 1 = 0.0666111 loss)
I1026 00:39:25.723404 16980 sgd_solver.cpp:106] Iteration 18080, lr = 0.001
I1026 00:39:29.401751 16980 solver.cpp:229] Iteration 18100, loss = 0.105826
I1026 00:39:29.401783 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0737186 (* 1 = 0.0737186 loss)
I1026 00:39:29.401787 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0321079 (* 1 = 0.0321079 loss)
I1026 00:39:29.401793 16980 sgd_solver.cpp:106] Iteration 18100, lr = 0.001
I1026 00:39:33.055915 16980 solver.cpp:229] Iteration 18120, loss = 0.193983
I1026 00:39:33.055958 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0651083 (* 1 = 0.0651083 loss)
I1026 00:39:33.055963 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.128875 (* 1 = 0.128875 loss)
I1026 00:39:33.055976 16980 sgd_solver.cpp:106] Iteration 18120, lr = 0.001
I1026 00:39:36.660809 16980 solver.cpp:229] Iteration 18140, loss = 0.0685643
I1026 00:39:36.660837 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0457041 (* 1 = 0.0457041 loss)
I1026 00:39:36.660842 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0228602 (* 1 = 0.0228602 loss)
I1026 00:39:36.660846 16980 sgd_solver.cpp:106] Iteration 18140, lr = 0.001
I1026 00:39:40.424643 16980 solver.cpp:229] Iteration 18160, loss = 0.0947307
I1026 00:39:40.424671 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0653919 (* 1 = 0.0653919 loss)
I1026 00:39:40.424676 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0293389 (* 1 = 0.0293389 loss)
I1026 00:39:40.424680 16980 sgd_solver.cpp:106] Iteration 18160, lr = 0.001
I1026 00:39:44.091070 16980 solver.cpp:229] Iteration 18180, loss = 0.0798769
I1026 00:39:44.091102 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0490758 (* 1 = 0.0490758 loss)
I1026 00:39:44.091106 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0308011 (* 1 = 0.0308011 loss)
I1026 00:39:44.091110 16980 sgd_solver.cpp:106] Iteration 18180, lr = 0.001
I1026 00:39:47.782999 16980 solver.cpp:229] Iteration 18200, loss = 0.131016
I1026 00:39:47.783030 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0734525 (* 1 = 0.0734525 loss)
I1026 00:39:47.783035 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.057563 (* 1 = 0.057563 loss)
I1026 00:39:47.783041 16980 sgd_solver.cpp:106] Iteration 18200, lr = 0.001
I1026 00:39:51.555819 16980 solver.cpp:229] Iteration 18220, loss = 0.104872
I1026 00:39:51.555850 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0568207 (* 1 = 0.0568207 loss)
I1026 00:39:51.555855 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0480509 (* 1 = 0.0480509 loss)
I1026 00:39:51.555860 16980 sgd_solver.cpp:106] Iteration 18220, lr = 0.001
I1026 00:39:55.224174 16980 solver.cpp:229] Iteration 18240, loss = 0.103684
I1026 00:39:55.224205 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.069053 (* 1 = 0.069053 loss)
I1026 00:39:55.224210 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.034631 (* 1 = 0.034631 loss)
I1026 00:39:55.224215 16980 sgd_solver.cpp:106] Iteration 18240, lr = 0.001
I1026 00:39:58.939873 16980 solver.cpp:229] Iteration 18260, loss = 0.107495
I1026 00:39:58.939904 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0783102 (* 1 = 0.0783102 loss)
I1026 00:39:58.939909 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0291852 (* 1 = 0.0291852 loss)
I1026 00:39:58.939913 16980 sgd_solver.cpp:106] Iteration 18260, lr = 0.001
I1026 00:40:02.689698 16980 solver.cpp:229] Iteration 18280, loss = 0.0807752
I1026 00:40:02.689726 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0422374 (* 1 = 0.0422374 loss)
I1026 00:40:02.689730 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0385379 (* 1 = 0.0385379 loss)
I1026 00:40:02.689734 16980 sgd_solver.cpp:106] Iteration 18280, lr = 0.001
I1026 00:40:06.363152 16980 solver.cpp:229] Iteration 18300, loss = 0.1218
I1026 00:40:06.363183 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0607447 (* 1 = 0.0607447 loss)
I1026 00:40:06.363188 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0610557 (* 1 = 0.0610557 loss)
I1026 00:40:06.363191 16980 sgd_solver.cpp:106] Iteration 18300, lr = 0.001
I1026 00:40:09.988556 16980 solver.cpp:229] Iteration 18320, loss = 0.15376
I1026 00:40:09.988589 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0756617 (* 1 = 0.0756617 loss)
I1026 00:40:09.988593 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0780982 (* 1 = 0.0780982 loss)
I1026 00:40:09.988597 16980 sgd_solver.cpp:106] Iteration 18320, lr = 0.001
I1026 00:40:13.726598 16980 solver.cpp:229] Iteration 18340, loss = 0.175555
I1026 00:40:13.726626 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.101845 (* 1 = 0.101845 loss)
I1026 00:40:13.726630 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0737101 (* 1 = 0.0737101 loss)
I1026 00:40:13.726635 16980 sgd_solver.cpp:106] Iteration 18340, lr = 0.001
I1026 00:40:17.523392 16980 solver.cpp:229] Iteration 18360, loss = 0.102536
I1026 00:40:17.523422 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0600168 (* 1 = 0.0600168 loss)
I1026 00:40:17.523427 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0425192 (* 1 = 0.0425192 loss)
I1026 00:40:17.523433 16980 sgd_solver.cpp:106] Iteration 18360, lr = 0.001
I1026 00:40:21.180012 16980 solver.cpp:229] Iteration 18380, loss = 0.176799
I1026 00:40:21.180040 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0949494 (* 1 = 0.0949494 loss)
I1026 00:40:21.180044 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0818498 (* 1 = 0.0818498 loss)
I1026 00:40:21.180059 16980 sgd_solver.cpp:106] Iteration 18380, lr = 0.001
I1026 00:40:24.838654 16980 solver.cpp:229] Iteration 18400, loss = 0.195352
I1026 00:40:24.838685 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.119221 (* 1 = 0.119221 loss)
I1026 00:40:24.838690 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0761307 (* 1 = 0.0761307 loss)
I1026 00:40:24.838695 16980 sgd_solver.cpp:106] Iteration 18400, lr = 0.001
I1026 00:40:28.450773 16980 solver.cpp:229] Iteration 18420, loss = 0.134719
I1026 00:40:28.450804 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0412983 (* 1 = 0.0412983 loss)
I1026 00:40:28.450809 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0934205 (* 1 = 0.0934205 loss)
I1026 00:40:28.450814 16980 sgd_solver.cpp:106] Iteration 18420, lr = 0.001
I1026 00:40:32.210670 16980 solver.cpp:229] Iteration 18440, loss = 0.123859
I1026 00:40:32.210710 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0635018 (* 1 = 0.0635018 loss)
I1026 00:40:32.210713 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0603569 (* 1 = 0.0603569 loss)
I1026 00:40:32.210718 16980 sgd_solver.cpp:106] Iteration 18440, lr = 0.001
I1026 00:40:36.003423 16980 solver.cpp:229] Iteration 18460, loss = 0.110648
I1026 00:40:36.003465 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0812843 (* 1 = 0.0812843 loss)
I1026 00:40:36.003469 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0293641 (* 1 = 0.0293641 loss)
I1026 00:40:36.003474 16980 sgd_solver.cpp:106] Iteration 18460, lr = 0.001
I1026 00:40:39.700944 16980 solver.cpp:229] Iteration 18480, loss = 0.0818568
I1026 00:40:39.700974 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0567771 (* 1 = 0.0567771 loss)
I1026 00:40:39.700978 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0250797 (* 1 = 0.0250797 loss)
I1026 00:40:39.700983 16980 sgd_solver.cpp:106] Iteration 18480, lr = 0.001
I1026 00:40:43.385610 16980 solver.cpp:229] Iteration 18500, loss = 0.0909539
I1026 00:40:43.385643 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0565848 (* 1 = 0.0565848 loss)
I1026 00:40:43.385646 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0343691 (* 1 = 0.0343691 loss)
I1026 00:40:43.385653 16980 sgd_solver.cpp:106] Iteration 18500, lr = 0.001
I1026 00:40:47.086256 16980 solver.cpp:229] Iteration 18520, loss = 0.134911
I1026 00:40:47.086285 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0734953 (* 1 = 0.0734953 loss)
I1026 00:40:47.086289 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0614158 (* 1 = 0.0614158 loss)
I1026 00:40:47.086293 16980 sgd_solver.cpp:106] Iteration 18520, lr = 0.001
I1026 00:40:50.855651 16980 solver.cpp:229] Iteration 18540, loss = 0.0542636
I1026 00:40:50.855684 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0482275 (* 1 = 0.0482275 loss)
I1026 00:40:50.855690 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.00603607 (* 1 = 0.00603607 loss)
I1026 00:40:50.855705 16980 sgd_solver.cpp:106] Iteration 18540, lr = 0.001
I1026 00:40:54.487726 16980 solver.cpp:229] Iteration 18560, loss = 0.329544
I1026 00:40:54.487759 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.164085 (* 1 = 0.164085 loss)
I1026 00:40:54.487763 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.165459 (* 1 = 0.165459 loss)
I1026 00:40:54.487767 16980 sgd_solver.cpp:106] Iteration 18560, lr = 0.001
I1026 00:40:58.194757 16980 solver.cpp:229] Iteration 18580, loss = 0.162795
I1026 00:40:58.194788 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.129248 (* 1 = 0.129248 loss)
I1026 00:40:58.194793 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0335464 (* 1 = 0.0335464 loss)
I1026 00:40:58.194797 16980 sgd_solver.cpp:106] Iteration 18580, lr = 0.001
I1026 00:41:01.883440 16980 solver.cpp:229] Iteration 18600, loss = 0.0818729
I1026 00:41:01.883466 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0446515 (* 1 = 0.0446515 loss)
I1026 00:41:01.883471 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0372213 (* 1 = 0.0372213 loss)
I1026 00:41:01.883476 16980 sgd_solver.cpp:106] Iteration 18600, lr = 0.001
I1026 00:41:05.571768 16980 solver.cpp:229] Iteration 18620, loss = 0.193274
I1026 00:41:05.571800 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.114306 (* 1 = 0.114306 loss)
I1026 00:41:05.571805 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0789682 (* 1 = 0.0789682 loss)
I1026 00:41:05.571810 16980 sgd_solver.cpp:106] Iteration 18620, lr = 0.001
I1026 00:41:09.322389 16980 solver.cpp:229] Iteration 18640, loss = 0.0969372
I1026 00:41:09.322422 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0627715 (* 1 = 0.0627715 loss)
I1026 00:41:09.322425 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0341657 (* 1 = 0.0341657 loss)
I1026 00:41:09.322430 16980 sgd_solver.cpp:106] Iteration 18640, lr = 0.001
I1026 00:41:12.956413 16980 solver.cpp:229] Iteration 18660, loss = 0.07995
I1026 00:41:12.956442 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0513895 (* 1 = 0.0513895 loss)
I1026 00:41:12.956446 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0285605 (* 1 = 0.0285605 loss)
I1026 00:41:12.956451 16980 sgd_solver.cpp:106] Iteration 18660, lr = 0.001
I1026 00:41:16.669756 16980 solver.cpp:229] Iteration 18680, loss = 0.136934
I1026 00:41:16.669782 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0968521 (* 1 = 0.0968521 loss)
I1026 00:41:16.669787 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0400817 (* 1 = 0.0400817 loss)
I1026 00:41:16.669791 16980 sgd_solver.cpp:106] Iteration 18680, lr = 0.001
I1026 00:41:20.385325 16980 solver.cpp:229] Iteration 18700, loss = 0.220609
I1026 00:41:20.385359 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.127554 (* 1 = 0.127554 loss)
I1026 00:41:20.385362 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0930545 (* 1 = 0.0930545 loss)
I1026 00:41:20.385367 16980 sgd_solver.cpp:106] Iteration 18700, lr = 0.001
I1026 00:41:24.155864 16980 solver.cpp:229] Iteration 18720, loss = 0.0745771
I1026 00:41:24.155903 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0441301 (* 1 = 0.0441301 loss)
I1026 00:41:24.155908 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.030447 (* 1 = 0.030447 loss)
I1026 00:41:24.155912 16980 sgd_solver.cpp:106] Iteration 18720, lr = 0.001
I1026 00:41:27.814333 16980 solver.cpp:229] Iteration 18740, loss = 0.0771402
I1026 00:41:27.814366 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0398414 (* 1 = 0.0398414 loss)
I1026 00:41:27.814370 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0372988 (* 1 = 0.0372988 loss)
I1026 00:41:27.814375 16980 sgd_solver.cpp:106] Iteration 18740, lr = 0.001
I1026 00:41:31.496959 16980 solver.cpp:229] Iteration 18760, loss = 0.0901636
I1026 00:41:31.496992 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0407548 (* 1 = 0.0407548 loss)
I1026 00:41:31.496996 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0494088 (* 1 = 0.0494088 loss)
I1026 00:41:31.497001 16980 sgd_solver.cpp:106] Iteration 18760, lr = 0.001
I1026 00:41:35.192637 16980 solver.cpp:229] Iteration 18780, loss = 0.155541
I1026 00:41:35.192667 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0991517 (* 1 = 0.0991517 loss)
I1026 00:41:35.192672 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0563891 (* 1 = 0.0563891 loss)
I1026 00:41:35.192677 16980 sgd_solver.cpp:106] Iteration 18780, lr = 0.001
I1026 00:41:38.843417 16980 solver.cpp:229] Iteration 18800, loss = 0.0999387
I1026 00:41:38.843451 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0694467 (* 1 = 0.0694467 loss)
I1026 00:41:38.843456 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.030492 (* 1 = 0.030492 loss)
I1026 00:41:38.843459 16980 sgd_solver.cpp:106] Iteration 18800, lr = 0.001
I1026 00:41:42.448384 16980 solver.cpp:229] Iteration 18820, loss = 0.0488647
I1026 00:41:42.448412 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0366048 (* 1 = 0.0366048 loss)
I1026 00:41:42.448416 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0122599 (* 1 = 0.0122599 loss)
I1026 00:41:42.448421 16980 sgd_solver.cpp:106] Iteration 18820, lr = 0.001
I1026 00:41:46.188009 16980 solver.cpp:229] Iteration 18840, loss = 0.137499
I1026 00:41:46.188051 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0639366 (* 1 = 0.0639366 loss)
I1026 00:41:46.188055 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0735625 (* 1 = 0.0735625 loss)
I1026 00:41:46.188069 16980 sgd_solver.cpp:106] Iteration 18840, lr = 0.001
I1026 00:41:49.920325 16980 solver.cpp:229] Iteration 18860, loss = 0.0978617
I1026 00:41:49.920356 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0728038 (* 1 = 0.0728038 loss)
I1026 00:41:49.920359 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0250579 (* 1 = 0.0250579 loss)
I1026 00:41:49.920364 16980 sgd_solver.cpp:106] Iteration 18860, lr = 0.001
I1026 00:41:53.702750 16980 solver.cpp:229] Iteration 18880, loss = 0.170123
I1026 00:41:53.702782 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.087001 (* 1 = 0.087001 loss)
I1026 00:41:53.702786 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0831222 (* 1 = 0.0831222 loss)
I1026 00:41:53.702790 16980 sgd_solver.cpp:106] Iteration 18880, lr = 0.001
I1026 00:41:57.393358 16980 solver.cpp:229] Iteration 18900, loss = 0.107073
I1026 00:41:57.393389 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0616135 (* 1 = 0.0616135 loss)
I1026 00:41:57.393394 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0454594 (* 1 = 0.0454594 loss)
I1026 00:41:57.393399 16980 sgd_solver.cpp:106] Iteration 18900, lr = 0.001
I1026 00:42:01.104298 16980 solver.cpp:229] Iteration 18920, loss = 0.185167
I1026 00:42:01.104331 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.107307 (* 1 = 0.107307 loss)
I1026 00:42:01.104336 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0778599 (* 1 = 0.0778599 loss)
I1026 00:42:01.104342 16980 sgd_solver.cpp:106] Iteration 18920, lr = 0.001
I1026 00:42:04.785856 16980 solver.cpp:229] Iteration 18940, loss = 0.0894761
I1026 00:42:04.785884 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.047281 (* 1 = 0.047281 loss)
I1026 00:42:04.785889 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0421951 (* 1 = 0.0421951 loss)
I1026 00:42:04.785893 16980 sgd_solver.cpp:106] Iteration 18940, lr = 0.001
I1026 00:42:08.534914 16980 solver.cpp:229] Iteration 18960, loss = 0.149272
I1026 00:42:08.534946 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.106506 (* 1 = 0.106506 loss)
I1026 00:42:08.534951 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0427655 (* 1 = 0.0427655 loss)
I1026 00:42:08.534955 16980 sgd_solver.cpp:106] Iteration 18960, lr = 0.001
I1026 00:42:12.257740 16980 solver.cpp:229] Iteration 18980, loss = 0.0836847
I1026 00:42:12.257769 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0568932 (* 1 = 0.0568932 loss)
I1026 00:42:12.257774 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0267915 (* 1 = 0.0267915 loss)
I1026 00:42:12.257778 16980 sgd_solver.cpp:106] Iteration 18980, lr = 0.001
I1026 00:42:15.966800 16980 solver.cpp:229] Iteration 19000, loss = 0.0689754
I1026 00:42:15.966831 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0414819 (* 1 = 0.0414819 loss)
I1026 00:42:15.966835 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0274935 (* 1 = 0.0274935 loss)
I1026 00:42:15.966840 16980 sgd_solver.cpp:106] Iteration 19000, lr = 0.001
I1026 00:42:19.779201 16980 solver.cpp:229] Iteration 19020, loss = 0.173237
I1026 00:42:19.779232 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.103912 (* 1 = 0.103912 loss)
I1026 00:42:19.779237 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0693249 (* 1 = 0.0693249 loss)
I1026 00:42:19.779242 16980 sgd_solver.cpp:106] Iteration 19020, lr = 0.001
I1026 00:42:23.429435 16980 solver.cpp:229] Iteration 19040, loss = 0.17753
I1026 00:42:23.429467 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.107233 (* 1 = 0.107233 loss)
I1026 00:42:23.429472 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0702967 (* 1 = 0.0702967 loss)
I1026 00:42:23.429477 16980 sgd_solver.cpp:106] Iteration 19040, lr = 0.001
I1026 00:42:27.157284 16980 solver.cpp:229] Iteration 19060, loss = 0.145296
I1026 00:42:27.157316 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0914594 (* 1 = 0.0914594 loss)
I1026 00:42:27.157320 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0538368 (* 1 = 0.0538368 loss)
I1026 00:42:27.157325 16980 sgd_solver.cpp:106] Iteration 19060, lr = 0.001
I1026 00:42:30.869905 16980 solver.cpp:229] Iteration 19080, loss = 0.106928
I1026 00:42:30.869938 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0723431 (* 1 = 0.0723431 loss)
I1026 00:42:30.869942 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0345846 (* 1 = 0.0345846 loss)
I1026 00:42:30.869946 16980 sgd_solver.cpp:106] Iteration 19080, lr = 0.001
I1026 00:42:34.517263 16980 solver.cpp:229] Iteration 19100, loss = 0.107089
I1026 00:42:34.517297 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0738482 (* 1 = 0.0738482 loss)
I1026 00:42:34.517302 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.033241 (* 1 = 0.033241 loss)
I1026 00:42:34.517305 16980 sgd_solver.cpp:106] Iteration 19100, lr = 0.001
I1026 00:42:38.156625 16980 solver.cpp:229] Iteration 19120, loss = 0.0601278
I1026 00:42:38.156656 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0541107 (* 1 = 0.0541107 loss)
I1026 00:42:38.156661 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.00601711 (* 1 = 0.00601711 loss)
I1026 00:42:38.156666 16980 sgd_solver.cpp:106] Iteration 19120, lr = 0.001
I1026 00:42:41.791250 16980 solver.cpp:229] Iteration 19140, loss = 0.103053
I1026 00:42:41.791278 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0815346 (* 1 = 0.0815346 loss)
I1026 00:42:41.791283 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0215183 (* 1 = 0.0215183 loss)
I1026 00:42:41.791287 16980 sgd_solver.cpp:106] Iteration 19140, lr = 0.001
I1026 00:42:45.510882 16980 solver.cpp:229] Iteration 19160, loss = 0.172682
I1026 00:42:45.510921 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.107931 (* 1 = 0.107931 loss)
I1026 00:42:45.510926 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0647507 (* 1 = 0.0647507 loss)
I1026 00:42:45.510931 16980 sgd_solver.cpp:106] Iteration 19160, lr = 0.001
I1026 00:42:49.145540 16980 solver.cpp:229] Iteration 19180, loss = 0.0949452
I1026 00:42:49.145573 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0526836 (* 1 = 0.0526836 loss)
I1026 00:42:49.145578 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0422617 (* 1 = 0.0422617 loss)
I1026 00:42:49.145582 16980 sgd_solver.cpp:106] Iteration 19180, lr = 0.001
I1026 00:42:52.794864 16980 solver.cpp:229] Iteration 19200, loss = 0.0790137
I1026 00:42:52.794894 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0495019 (* 1 = 0.0495019 loss)
I1026 00:42:52.794898 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0295118 (* 1 = 0.0295118 loss)
I1026 00:42:52.794903 16980 sgd_solver.cpp:106] Iteration 19200, lr = 0.001
I1026 00:42:56.542029 16980 solver.cpp:229] Iteration 19220, loss = 0.144794
I1026 00:42:56.542060 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0973497 (* 1 = 0.0973497 loss)
I1026 00:42:56.542065 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0474441 (* 1 = 0.0474441 loss)
I1026 00:42:56.542069 16980 sgd_solver.cpp:106] Iteration 19220, lr = 0.001
I1026 00:43:00.232403 16980 solver.cpp:229] Iteration 19240, loss = 0.0892544
I1026 00:43:00.232432 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0387903 (* 1 = 0.0387903 loss)
I1026 00:43:00.232437 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0504641 (* 1 = 0.0504641 loss)
I1026 00:43:00.232441 16980 sgd_solver.cpp:106] Iteration 19240, lr = 0.001
I1026 00:43:03.990304 16980 solver.cpp:229] Iteration 19260, loss = 0.0593969
I1026 00:43:03.990336 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0499076 (* 1 = 0.0499076 loss)
I1026 00:43:03.990341 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.00948925 (* 1 = 0.00948925 loss)
I1026 00:43:03.990345 16980 sgd_solver.cpp:106] Iteration 19260, lr = 0.001
I1026 00:43:07.721024 16980 solver.cpp:229] Iteration 19280, loss = 0.130219
I1026 00:43:07.721067 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0881946 (* 1 = 0.0881946 loss)
I1026 00:43:07.721071 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0420247 (* 1 = 0.0420247 loss)
I1026 00:43:07.721076 16980 sgd_solver.cpp:106] Iteration 19280, lr = 0.001
I1026 00:43:11.450265 16980 solver.cpp:229] Iteration 19300, loss = 0.19763
I1026 00:43:11.450299 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0836525 (* 1 = 0.0836525 loss)
I1026 00:43:11.450304 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.113978 (* 1 = 0.113978 loss)
I1026 00:43:11.450307 16980 sgd_solver.cpp:106] Iteration 19300, lr = 0.001
I1026 00:43:15.123667 16980 solver.cpp:229] Iteration 19320, loss = 0.0823345
I1026 00:43:15.123698 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0427605 (* 1 = 0.0427605 loss)
I1026 00:43:15.123703 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.039574 (* 1 = 0.039574 loss)
I1026 00:43:15.123708 16980 sgd_solver.cpp:106] Iteration 19320, lr = 0.001
I1026 00:43:18.766098 16980 solver.cpp:229] Iteration 19340, loss = 0.0846967
I1026 00:43:18.766125 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0411173 (* 1 = 0.0411173 loss)
I1026 00:43:18.766130 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0435794 (* 1 = 0.0435794 loss)
I1026 00:43:18.766134 16980 sgd_solver.cpp:106] Iteration 19340, lr = 0.001
I1026 00:43:22.441452 16980 solver.cpp:229] Iteration 19360, loss = 0.0874963
I1026 00:43:22.441483 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0607871 (* 1 = 0.0607871 loss)
I1026 00:43:22.441488 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0267092 (* 1 = 0.0267092 loss)
I1026 00:43:22.441493 16980 sgd_solver.cpp:106] Iteration 19360, lr = 0.001
I1026 00:43:26.127140 16980 solver.cpp:229] Iteration 19380, loss = 0.188373
I1026 00:43:26.127171 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0745639 (* 1 = 0.0745639 loss)
I1026 00:43:26.127177 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.11381 (* 1 = 0.11381 loss)
I1026 00:43:26.127180 16980 sgd_solver.cpp:106] Iteration 19380, lr = 0.001
I1026 00:43:29.846822 16980 solver.cpp:229] Iteration 19400, loss = 0.120112
I1026 00:43:29.846849 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0748744 (* 1 = 0.0748744 loss)
I1026 00:43:29.846853 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.045238 (* 1 = 0.045238 loss)
I1026 00:43:29.846858 16980 sgd_solver.cpp:106] Iteration 19400, lr = 0.001
I1026 00:43:33.620831 16980 solver.cpp:229] Iteration 19420, loss = 0.156135
I1026 00:43:33.620874 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0646916 (* 1 = 0.0646916 loss)
I1026 00:43:33.620879 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.091443 (* 1 = 0.091443 loss)
I1026 00:43:33.620893 16980 sgd_solver.cpp:106] Iteration 19420, lr = 0.001
I1026 00:43:37.379106 16980 solver.cpp:229] Iteration 19440, loss = 0.0692376
I1026 00:43:37.379137 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0513665 (* 1 = 0.0513665 loss)
I1026 00:43:37.379142 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0178711 (* 1 = 0.0178711 loss)
I1026 00:43:37.379146 16980 sgd_solver.cpp:106] Iteration 19440, lr = 0.001
I1026 00:43:41.039696 16980 solver.cpp:229] Iteration 19460, loss = 0.104789
I1026 00:43:41.039728 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0548172 (* 1 = 0.0548172 loss)
I1026 00:43:41.039732 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0499719 (* 1 = 0.0499719 loss)
I1026 00:43:41.039736 16980 sgd_solver.cpp:106] Iteration 19460, lr = 0.001
I1026 00:43:44.757443 16980 solver.cpp:229] Iteration 19480, loss = 0.0780925
I1026 00:43:44.757473 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0477265 (* 1 = 0.0477265 loss)
I1026 00:43:44.757477 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.030366 (* 1 = 0.030366 loss)
I1026 00:43:44.757483 16980 sgd_solver.cpp:106] Iteration 19480, lr = 0.001
I1026 00:43:48.342108 16980 solver.cpp:229] Iteration 19500, loss = 0.112354
I1026 00:43:48.342139 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0807444 (* 1 = 0.0807444 loss)
I1026 00:43:48.342144 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0316098 (* 1 = 0.0316098 loss)
I1026 00:43:48.342147 16980 sgd_solver.cpp:106] Iteration 19500, lr = 0.001
I1026 00:43:52.062880 16980 solver.cpp:229] Iteration 19520, loss = 0.0735485
I1026 00:43:52.062911 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0445849 (* 1 = 0.0445849 loss)
I1026 00:43:52.062916 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0289636 (* 1 = 0.0289636 loss)
I1026 00:43:52.062921 16980 sgd_solver.cpp:106] Iteration 19520, lr = 0.001
I1026 00:43:55.696692 16980 solver.cpp:229] Iteration 19540, loss = 0.0879775
I1026 00:43:55.696725 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0543086 (* 1 = 0.0543086 loss)
I1026 00:43:55.696729 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0336689 (* 1 = 0.0336689 loss)
I1026 00:43:55.696734 16980 sgd_solver.cpp:106] Iteration 19540, lr = 0.001
I1026 00:43:59.465772 16980 solver.cpp:229] Iteration 19560, loss = 0.0765982
I1026 00:43:59.465803 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0353575 (* 1 = 0.0353575 loss)
I1026 00:43:59.465807 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0412407 (* 1 = 0.0412407 loss)
I1026 00:43:59.465812 16980 sgd_solver.cpp:106] Iteration 19560, lr = 0.001
I1026 00:44:03.300416 16980 solver.cpp:229] Iteration 19580, loss = 0.0701656
I1026 00:44:03.300448 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0483342 (* 1 = 0.0483342 loss)
I1026 00:44:03.300452 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0218314 (* 1 = 0.0218314 loss)
I1026 00:44:03.300457 16980 sgd_solver.cpp:106] Iteration 19580, lr = 0.001
I1026 00:44:06.930016 16980 solver.cpp:229] Iteration 19600, loss = 0.0565703
I1026 00:44:06.930048 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0394124 (* 1 = 0.0394124 loss)
I1026 00:44:06.930053 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0171578 (* 1 = 0.0171578 loss)
I1026 00:44:06.930059 16980 sgd_solver.cpp:106] Iteration 19600, lr = 0.001
I1026 00:44:10.629299 16980 solver.cpp:229] Iteration 19620, loss = 0.0808292
I1026 00:44:10.629328 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0669722 (* 1 = 0.0669722 loss)
I1026 00:44:10.629333 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.013857 (* 1 = 0.013857 loss)
I1026 00:44:10.629338 16980 sgd_solver.cpp:106] Iteration 19620, lr = 0.001
I1026 00:44:14.269845 16980 solver.cpp:229] Iteration 19640, loss = 0.0814669
I1026 00:44:14.269878 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.03908 (* 1 = 0.03908 loss)
I1026 00:44:14.269882 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0423869 (* 1 = 0.0423869 loss)
I1026 00:44:14.269887 16980 sgd_solver.cpp:106] Iteration 19640, lr = 0.001
I1026 00:44:17.827476 16980 solver.cpp:229] Iteration 19660, loss = 0.10275
I1026 00:44:17.827503 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0834351 (* 1 = 0.0834351 loss)
I1026 00:44:17.827508 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0193153 (* 1 = 0.0193153 loss)
I1026 00:44:17.827512 16980 sgd_solver.cpp:106] Iteration 19660, lr = 0.001
I1026 00:44:21.637158 16980 solver.cpp:229] Iteration 19680, loss = 0.268337
I1026 00:44:21.637192 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.129274 (* 1 = 0.129274 loss)
I1026 00:44:21.637195 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.139063 (* 1 = 0.139063 loss)
I1026 00:44:21.637200 16980 sgd_solver.cpp:106] Iteration 19680, lr = 0.001
I1026 00:44:25.472414 16980 solver.cpp:229] Iteration 19700, loss = 0.122664
I1026 00:44:25.472446 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0583389 (* 1 = 0.0583389 loss)
I1026 00:44:25.472451 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0643248 (* 1 = 0.0643248 loss)
I1026 00:44:25.472455 16980 sgd_solver.cpp:106] Iteration 19700, lr = 0.001
I1026 00:44:29.140635 16980 solver.cpp:229] Iteration 19720, loss = 0.0889204
I1026 00:44:29.140666 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0448086 (* 1 = 0.0448086 loss)
I1026 00:44:29.140671 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0441117 (* 1 = 0.0441117 loss)
I1026 00:44:29.140676 16980 sgd_solver.cpp:106] Iteration 19720, lr = 0.001
I1026 00:44:32.860524 16980 solver.cpp:229] Iteration 19740, loss = 0.0998133
I1026 00:44:32.860555 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.078769 (* 1 = 0.078769 loss)
I1026 00:44:32.860559 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0210443 (* 1 = 0.0210443 loss)
I1026 00:44:32.860564 16980 sgd_solver.cpp:106] Iteration 19740, lr = 0.001
I1026 00:44:36.511541 16980 solver.cpp:229] Iteration 19760, loss = 0.106853
I1026 00:44:36.511574 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0535043 (* 1 = 0.0535043 loss)
I1026 00:44:36.511577 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0533486 (* 1 = 0.0533486 loss)
I1026 00:44:36.511581 16980 sgd_solver.cpp:106] Iteration 19760, lr = 0.001
I1026 00:44:40.242547 16980 solver.cpp:229] Iteration 19780, loss = 0.120883
I1026 00:44:40.242580 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0818394 (* 1 = 0.0818394 loss)
I1026 00:44:40.242584 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0390435 (* 1 = 0.0390435 loss)
I1026 00:44:40.242588 16980 sgd_solver.cpp:106] Iteration 19780, lr = 0.001
I1026 00:44:43.947186 16980 solver.cpp:229] Iteration 19800, loss = 0.0919243
I1026 00:44:43.947217 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.075552 (* 1 = 0.075552 loss)
I1026 00:44:43.947221 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0163723 (* 1 = 0.0163723 loss)
I1026 00:44:43.947226 16980 sgd_solver.cpp:106] Iteration 19800, lr = 0.001
I1026 00:44:47.600750 16980 solver.cpp:229] Iteration 19820, loss = 0.0690256
I1026 00:44:47.600782 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0494355 (* 1 = 0.0494355 loss)
I1026 00:44:47.600786 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.01959 (* 1 = 0.01959 loss)
I1026 00:44:47.600791 16980 sgd_solver.cpp:106] Iteration 19820, lr = 0.001
I1026 00:44:51.276249 16980 solver.cpp:229] Iteration 19840, loss = 0.0620921
I1026 00:44:51.276283 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0505493 (* 1 = 0.0505493 loss)
I1026 00:44:51.276288 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0115428 (* 1 = 0.0115428 loss)
I1026 00:44:51.276293 16980 sgd_solver.cpp:106] Iteration 19840, lr = 0.001
I1026 00:44:54.944345 16980 solver.cpp:229] Iteration 19860, loss = 0.121408
I1026 00:44:54.944381 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0692341 (* 1 = 0.0692341 loss)
I1026 00:44:54.944387 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0521737 (* 1 = 0.0521737 loss)
I1026 00:44:54.944392 16980 sgd_solver.cpp:106] Iteration 19860, lr = 0.001
I1026 00:44:58.644160 16980 solver.cpp:229] Iteration 19880, loss = 0.101359
I1026 00:44:58.644189 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0770415 (* 1 = 0.0770415 loss)
I1026 00:44:58.644194 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0243171 (* 1 = 0.0243171 loss)
I1026 00:44:58.644199 16980 sgd_solver.cpp:106] Iteration 19880, lr = 0.001
I1026 00:45:02.332973 16980 solver.cpp:229] Iteration 19900, loss = 0.091126
I1026 00:45:02.333004 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.066409 (* 1 = 0.066409 loss)
I1026 00:45:02.333009 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.024717 (* 1 = 0.024717 loss)
I1026 00:45:02.333014 16980 sgd_solver.cpp:106] Iteration 19900, lr = 0.001
I1026 00:45:06.049183 16980 solver.cpp:229] Iteration 19920, loss = 0.0558536
I1026 00:45:06.049213 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0371549 (* 1 = 0.0371549 loss)
I1026 00:45:06.049217 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0186987 (* 1 = 0.0186987 loss)
I1026 00:45:06.049221 16980 sgd_solver.cpp:106] Iteration 19920, lr = 0.001
I1026 00:45:09.837446 16980 solver.cpp:229] Iteration 19940, loss = 0.16595
I1026 00:45:09.837479 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0568466 (* 1 = 0.0568466 loss)
I1026 00:45:09.837484 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.109103 (* 1 = 0.109103 loss)
I1026 00:45:09.837489 16980 sgd_solver.cpp:106] Iteration 19940, lr = 0.001
I1026 00:45:13.676262 16980 solver.cpp:229] Iteration 19960, loss = 0.113796
I1026 00:45:13.676297 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0518562 (* 1 = 0.0518562 loss)
I1026 00:45:13.676301 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0619393 (* 1 = 0.0619393 loss)
I1026 00:45:13.676306 16980 sgd_solver.cpp:106] Iteration 19960, lr = 0.001
I1026 00:45:17.461419 16980 solver.cpp:229] Iteration 19980, loss = 0.139593
I1026 00:45:17.461449 16980 solver.cpp:245]     Train net output #0: bbox_loss = 0.0552674 (* 1 = 0.0552674 loss)
I1026 00:45:17.461454 16980 solver.cpp:245]     Train net output #1: cls_loss = 0.0843254 (* 1 = 0.0843254 loss)
I1026 00:45:17.461459 16980 sgd_solver.cpp:106] Iteration 19980, lr = 0.001
s / iter
speed: 0.185s / iter
speed: 0.185s / iter
speed: 0.185s / iter
speed: 0.185s / iter
speed: 0.185s / iter
speed: 0.185s / iter
speed: 0.185s / iter
speed: 0.185s / iter
speed: 0.185s / iter
speed: 0.185s / iter
speed: 0.185s / iter
speed: 0.185s / iter
speed: 0.185s / iter
speed: 0.185s / iter
speed: 0.185s / iter
Wrote snapshot to: /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_fast_rcnn_stage1_iter_20000.caffemodel
done solving
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 2 RPN, init from stage 1 Fast R-CNN model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Init model: /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_fast_rcnn_stage1_iter_20000.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage2',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: gt
Appending horizontally-flipped training examples...

375
[[160 191 227 248]]
500
[[208 164 227 183]]
333
[[ 26 294 176 425]
 [147 378 265 435]
 [252 309 290 352]]
333
[[ 19 224  82 356]
 [250 235 293 291]
 [225 299 264 339]
 [256 246 328 335]]
375
[[ 95  60 259 452]]
500
[[138 225 343 342]
 [131   0 333 193]]
375
[[160 122 261 231]]
375
[[ 36  83 307 453]]
333
[[  0  17 177 476]
 [140 198 189 400]
 [159  49 301 219]]
333
[[ 10  78 312 465]]
376
[[100  88 285 425]]
333
[[ 49   0 285 499]
 [265   0 332 136]]
333
[[ 24  32 317 471]]
376
[[ 57 145 309 417]]
316
[[ 50  28 277 470]]
500
[[254  88 306 248]]
333
[[167 378 268 453]
 [270 211 287 245]
 [104 209 273 293]
 [ 76 361 102 392]
 [ 77 405  92 432]
 [294 199 313 235]]
500
[[142 153 253 276]
 [256 212 265 227]
 [250 202 302 287]
 [283 228 307 278]]
500
[[192 136 366 281]]
500
[[133   0 231  64]
 [278   0 314  76]
 [304  37 370  67]
 [330  64 371  90]
 [250  98 372 270]
 [128  50 233 244]]
500
[[184  98 307 228]]
500
[[203  74 277 148]]
375
[[197 274 324 467]]
500
[[240 147 285 308]]
375
[[  0 234 155 332]
 [141 122 211 210]
 [ 99 332 182 497]
 [175 245 260 303]
 [248 162 320 214]]
500
[[280 210 339 273]]
500
[[ 49  71  79 116]
 [ 67 113  94 153]
 [ 49 206  72 229]
 [ 73 208  91 225]
 [122 294 135 315]
 [101 314 117 332]
 [299  66 320  80]
 [299  94 315 103]
 [377 139 396 150]
 [405 122 424 143]
 [434 206 452 226]
 [463 198 486 218]]
500
[[ 99  28 430 344]]
413
[[ 76 302 250 471]]
500
[[ 53 156 147 264]
 [141 179 194 263]
 [193 180 248 269]
 [254 172 304 250]
 [312 172 371 248]
 [361 167 428 245]
 [409 165 486 247]]
500
[[ 17  25 458 207]]
500
[[ 80  92 371 377]]
375
[[ 97  70 292 498]]
320
[[156 148 202 183]]
500
[[ 99  21 362 345]]
500
[[374 122 431 203]
 [ 50 128 110 200]]
375
[[ 51 164 316 480]
 [273 330 297 371]
 [278 376 304 451]]
480
[[180  78 407 115]
 [176 135 438 319]]
406
[[  0  41 405 499]]
500
[[216  59 403 267]]
337
[[ 88 193 158 353]
 [166 193 234 351]]
500
[[159  46 337 349]]
500
[[155   7 406 331]]
375
[[  0   0 373 499]]
500
[[221 117 350 283]]
500
[[  9 205 117 256]
 [123 180 259 232]
 [264 170 300 208]
 [309 166 365 204]
 [369 160 461 191]]
500
[[250  62 370 193]]
500
[[ 36  61 424 288]
 [123 235 249 299]]
294
[[ 55 186 198 335]]
500
[[ 27 122  69 168]
 [181  24 232 114]
 [230  52 259 114]
 [258  53 298 138]
 [306  34 319 121]
 [327  59 356 121]
 [362  57 392 119]
 [401  45 424 117]
 [198 135 403 227]
 [198 219 408 313]
 [208 308 398 387]]
333
[[  1  40 324 463]]
333
[[142 171 161 187]
 [107 150 164 231]
 [ 93 177 103 191]]
500
[[179 175 321 246]
 [163 215 175 234]
 [151 192 167 213]
 [154 163 169 179]
 [127 174 143 187]
 [122 137 144 160]
 [146 113 159 134]
 [173  74 313 186]
 [157  88 173 104]
 [183  74 204  96]
 [158 115 175 144]]
375
[[ 98  42 246 209]
 [ 59 103  91 265]
 [276  79 330 225]
 [ 17 101  40 141]]
375
[[125 297 225 430]
 [ 86  50 130 118]
 [268  24 307 156]]
375
[[ 72 140 233 280]]
500
[[217 123 278 193]]
375
[[ 45  99 195 398]
 [194 178 335 412]]
500
[[146  34 300 303]]
373
[[ 40  40 318 465]]
353
[[  6   2 335 485]]
333
[[156 379 230 497]
 [113 262 165 328]
 [115 151 188 238]]
271
[[ 13  52 241 493]]
375
[[ 70   0 301 461]
 [ 25 246 101 359]]
500
[[207 106 331 242]]
500
[[ 62  14 333 338]]
333
[[ 83  88 211 401]]
316
[[114 232 201 255]
 [125 199 200 241]]
375
[[ 68  82 277 441]]
333
[[ 33  53  92 394]
 [138  49 275 448]
 [264 451 280 472]
 [282 445 304 471]]
500
[[178 116 299 289]
 [192  79 226 116]
 [229  69 270 111]
 [275  76 311 120]]
500
[[176 189 242 241]]
500
[[204 142 302 237]]
500
[[381 100 448 240]]
334
[[ 28   5 297 484]]
375
[[ 87 220 195 411]]
250
[[ 31  13 222 476]]
333
[[  5 194 194 496]]
305
[[ 16   4 301 489]]
143
[[ 10  10 136 480]]
500
[[208 154 288 211]
 [142 133 200 189]]
217
[[  5   1 215 446]]
333
[[ 22  71 265 376]]
182
[[  4   0 175 486]]
375
[[ 54  24 278 464]]
600
[[187 162 289 249]]
338
[[176   0 300 225]]
333
[[ 49  35 163 174]]
291
[[ 26   0 258 499]]
375
[[136 347 210 423]
 [191 227 226 249]
 [222 218 232 232]]
333
[[ 14 337  96 427]]
297
[[155 215 191 272]]
275
[[ 36  42 227 465]]
360
[[129  76 255 218]]
500
[[148  47 378 254]]
500
[[253 133 375 273]]
375
[[213  64 319 167]]
333
[[ 46 113 319 396]]
352
[[119 138 248 287]]
500
[[170 130 181 145]
 [182 141 194 159]
 [196 136 207 151]
 [202 137 238 166]
 [243 126 299 168]
 [304 123 325 146]
 [323 138 331 152]
 [331 117 342 145]]
500
[[359 112 375 135]
 [376 125 393 145]]
375
[[215 243 233 264]]
332
[[108  20 290 498]]
378
[[126  76 268 183]]
232
[[ 14   3 229 494]]
500
[[197  37 406 224]
 [278 195 291 240]]
334
[[201 252 210 275]]
500
[[313 283 400 343]
 [148 160 373 303]]
500
[[212 166 264 223]]
375
[[125 112 258 356]]
500
[[ 75  89 105 123]
 [109 174 256 218]
 [330 221 430 279]
 [318 164 374 192]
 [379 131 387 147]
 [139 159 151 181]
 [142 213 187 250]]
339
[[  8   9 119 168]
 [ 29  93 189 393]]
339
[[ 30 119  67 249]
 [ 45 104 300 457]]
500
[[107  46 400 355]]
500
[[127  48 414 336]]
500
[[145  10 403 321]]
500
[[175  51 363 259]]
356
[[  5  20 338 479]]
375
[[103 162 256 355]]
500
[[  0  24 169 204]
 [ 56 129 157 250]
 [155 126 337 281]
 [341 122 439 243]
 [464  94 498 168]]
500
[[  0 323 226 374]
 [ 28  50 451 309]]
448
[[113  84 298 225]]
500
[[ 27  74 499 311]
 [ 71   1 343  31]
 [413   0 498  45]]
487
[[ 20   0 484 479]]
375
[[ 38  65 331 416]]
500
[[182  75 329 197]]
500
[[154 117 400 274]]
500
[[ 89 124 414 162]]
500
[[105  20 340 302]
 [187 356 272 372]]
398
[[  5   6 375 493]
 [ 12  98  70 235]]
310
[[ 74 219 200 295]
 [146 111 192 144]
 [135 168 196 205]
 [207  90 251 151]
 [ 10  75  73 131]]
334
[[  0 251  69 388]
 [ 18  91 331 485]]
352
[[ 30 390  49 442]
 [239 287 304 340]
 [ 64 206 272 474]
 [160 302 176 330]]
334
[[ 38 294  95 486]
 [ 61 190 288 461]]
375
[[ 53  64 361 391]]
439
[[  7   0 220 478]
 [214   0 421 473]]
286
[[ 59   3 267 493]]
500
[[ 56 238  85 283]
 [146 175 212 221]
 [220 215 238 264]
 [ 87 215 220 317]]
333
[[ 72 120 249 380]]
500
[[142 125 323 238]]
375
[[ 52  65 334 498]]
500
[[ 24 318 360 374]
 [313 126 498 370]
 [ 45   0 381 239]]
375
[[128  85 374 337]
 [256 304 374 494]]
375
[[ 12   7 297 498]]
333
[[ 74 130 241 359]]
474
[[ 95   8 366 232]]
398
[[ 99 164 280 438]]
345
[[116 244 163 299]
 [199 300 226 333]]
335
[[ 57  51 265 485]]
375
[[103 140 305 436]]
500
[[113 185 135 214]
 [359 194 374 220]
 [137 146 349 236]]
375
[[112  36 301 482]
 [283  66 296  87]]
300
[[ 61  73 262 339]]
500
[[302 123 337 150]]
500
[[ 55 112 287 321]]
500
[[ 85  78 448 223]]
289
[[ 76   4 194 491]]
500
[[ 24  49 466 261]
 [108  77 181 135]
 [158  47 235  94]
 [262  39 303  63]
 [312  41 331  61]
 [120 163 140 197]
 [239 224 285 270]
 [377 259 416 299]
 [272 277 290 294]
 [290 299 309 316]]
375
[[ 80 278 159 387]]
375
[[106 192 129 221]
 [125  89 276 305]
 [175 161 189 198]
 [186 162 207 196]
 [211 139 261 178]
 [308 295 334 356]
 [246 285 275 389]
 [125 262 249 399]]
500
[[121   1 321 362]]
375
[[175  72 207  95]
 [136  93 221 465]]
375
[[141  38 246 377]]
500
[[202  64 238 104]]
500
[[145 211 225 290]]
375
[[225  79 372 269]
 [  0 386 105 432]
 [ 76 394 225 498]]
375
[[176  90 262 266]]
500
[[ 22  29 206 343]
 [ 56 132  75 160]
 [ 78 157 101 185]
 [160 147 184 184]
 [297  47 483 324]
 [319 164 350 191]
 [436 116 454 143]
 [432 162 457 188]
 [348 315 372 348]
 [406 325 424 365]]
500
[[ 33   0 497 331]]
333
[[ 36   0 208 499]]
500
[[284  44 409 250]]
500
[[262  49 354 144]
 [125 211 160 263]
 [159 207 242 283]]
500
[[  0  40  88 235]
 [108  60 176 117]
 [303  73 372 134]
 [176 120 289 259]]
466
[[ 52  28 392 447]]
500
[[253  70 433 283]
 [101 123 340 302]]
500
[[357 219 417 313]
 [268 137 309 165]]
375
[[ 54   0 240 498]]
333
[[ 58  47 266 403]
 [ 45 318  63 399]]
333
[[221 113 281 203]
 [198 253 276 472]
 [ 51 188  91 215]
 [ 62 248  99 279]
 [ 59 288 154 415]]
500
[[120  29 375 312]]
500
[[151  72 370 255]]
334
[[149  88 168 153]
 [218  89 251 121]
 [168 188 204 223]
 [164 183 176 204]
 [240 196 248 214]]
334
[[ 46   1 316 498]]
500
[[119  39 372 321]]
375
[[ 80  10 371 470]]
375
[[102  29 287 467]]
375
[[ 99  93 270 470]]
500
[[261 138 316 163]]
500
[[152 122 399 211]
 [301 196 373 219]]
453
[[  0   0 420 499]]
375
[[ 93  96 268 357]
 [208  75 245 106]
 [ 78   0 235  81]]
500
[[ 16  24 228 139]
 [ 31 162 209 252]
 [274  22 469 162]
 [296 166 473 246]
 [301 196 475 264]]
316
[[ 11  48 288 492]]
333
[[104 184 172 249]]
254
[[ 14  11 240 494]]
500
[[ 27 202  46 223]
 [  2 211  85 324]
 [277 331 349 366]
 [407 177 461 296]
 [316 214 325 229]
 [374 203 380 222]]
500
[[369 240 420 287]
 [389 350 413 372]]
500
[[ 39  68 470 276]]
500
[[ 66  61 475 202]]
500
[[176  34 387 368]]
500
[[ 47 134 466 348]]
500
[[126  26 413 312]
 [201 110 211 125]]
375
[[  0  94 284 389]
 [  7 185  36 219]
 [135  98 167 122]]
500
[[102   6 364 359]]
500
[[ 73 132 326 259]]
375
[[117  19 290 361]]
375
[[ 58   0 269 395]
 [ 33 292  52 363]
 [100 438 141 484]
 [161 437 195 480]
 [209 418 236 455]
 [245 378 271 411]]
480
[[ 68  23 404 296]
 [ 44 298  80 358]
 [ 72 319 121 357]
 [176 341 206 357]
 [215 341 247 357]
 [251 342 294 357]
 [298 329 360 358]
 [362 317 416 359]
 [401 292 451 344]
 [442 182 477 270]]
375
[[ 88   0 296 411]
 [ 82 271  95 302]
 [195 430 220 449]
 [237 331 264 384]
 [283 353 295 375]
 [241 397 262 420]]
480
[[ 84  32 348 302]]
375
[[134  48 264 425]]
500
[[179 165 275 248]
 [269  95 350 161]]
500
[[154  72 364 275]]
500
[[127 133 348 200]
 [133 192 177 235]
 [177 201 224 249]
 [226 216 268 259]]
375
[[178  58 337 450]]
500
[[ 63  79 475 250]]
500
[[356 181 381 208]
 [368 207 386 227]
 [384 225 396 242]]
375
[[ 53  28 338 496]]
256
[[ 25  17 242 485]]
500
[[ 97  59 331 224]
 [194 355 211 374]
 [209 359 222 380]
 [225 344 262 366]
 [264 335 319 362]]
375
[[ 59  60 239 461]
 [214 384 234 422]
 [182 289 231 370]
 [153  74 207 180]
 [205 202 220 233]]
500
[[ 82  38 301 330]]
500
[[170 172 280 269]
 [198 266 233 318]
 [105 231 132 372]]
375
[[130 149 289 429]]
500
[[ 72   0 402 373]]
500
[[189 242 281 296]
 [200 311 345 344]
 [244 131 439 322]]
500
[[ 62  73 358 292]]
500
[[166 115 203 138]]
281
[[ 71  14  92  56]
 [ 73  60 120 121]
 [ 97 126 139 184]
 [118 192 162 247]
 [127 253 177 309]
 [135 312 189 357]
 [148 361 171 380]]
500
[[379 187 396 208]
 [384 201 414 234]
 [ 86 147 116 191]]
375
[[195  20 374 472]
 [  0  23 181 486]]
500
[[ 34   0 499 373]]
269
[[  8 123 259 498]]
375
[[ 38  67 330 431]]
500
[[  0 184  77 363]]
333
[[  8 293 321 428]
 [161 411 171 422]
 [107 418 117 432]
 [213 416 226 428]
 [300 388 309 401]
 [271 326 282 333]
 [ 39 318  51 330]
 [ 22 388  31 400]]
375
[[104 188 301 414]
 [176 430 259 460]
 [257 428 295 452]]
375
[[  5  77 174 376]]
375
[[  0   0 374 498]]
375
[[124  96 278 469]
 [159 206 198 256]]
334
[[117 115 233 350]]
500
[[ 37 190  91 274]
 [321 171 392 255]]
500
[[173 125 259 211]]
500
[[230 127 323 287]]
500
[[274  75 293 115]
 [221 109 428 276]]
498
[[ 23  68 144 323]
 [122 101 348 357]
 [200  38 264  78]
 [338  72 466 334]
 [327 350 338 361]
 [331 364 348 379]
 [343 377 357 389]
 [355 382 363 392]]
332
[[ 56 258 149 321]]
332
[[217 294 275 313]]
500
[[146 192 163 253]
 [169 209 200 312]
 [211 213 257 330]
 [274 210 328 328]
 [248 269 288 306]
 [334 198 369 305]
 [369 190 385 257]]
375
[[ 78 256 247 494]]
300
[[ 40  27 279 471]]
334
[[ 71 133 100 178]
 [219 131 239 161]
 [243 131 314 288]
 [254 308 299 495]]
375
[[137 189 226 349]]
500
[[356 140 372 159]
 [349 155 371 164]]
333
[[ 22   0 258 468]]
442
[[131  42 319 197]]
500
[[  8   5 492 343]]
367
[[199   1 349 359]
 [ 75  81 127 154]]
375
[[ 37   1 373 497]]
303
[[ 77 230 259 487]]
305
[[ 31  30 284 440]
 [115 432 134 455]]
500
[[ 17   6 459 305]]
400
[[173 135 306 266]]
375
[[ 78   2 296 485]]
500
[[ 78  35 422 202]
 [130 232 202 250]
 [318 238 355 250]]
411
[[187 221 242 283]]
333
[[130  44 330 293]]
415
[[ 18   0 219 497]
 [ 56 334  83 404]
 [245   5 409 477]
 [311  18 401 141]]
500
[[365 223 404 356]
 [ 24 244 170 356]
 [164 266 186 289]
 [190 296 213 319]
 [226 318 244 345]
 [139 181 156 194]
 [125 222 153 246]
 [ 60 218  77 235]
 [  0 288  27 315]
 [  3 242  23 264]
 [ 92 193 112 211]
 [174 228 191 249]
 [ 25 206  43 220]]
500
[[ 78  64 121  89]]
375
[[ 57   8 321 495]]
333
[[158 131 259 303]]
334
[[  0 116  17 142]
 [121 104 172 153]
 [147 190 260 323]
 [222 122 246 139]]
375
[[ 37   0 308 494]]
375
[[ 41  51 342 463]
 [245   0 314  73]]
262
[[ 43  12 223 487]]
500
[[141  82 332 255]]
375
[[ 97 148 263 366]]
331
[[100  10 231 491]]
375
[[ 83 205 155 298]
 [179 189 213 243]
 [322 264 367 372]]
375
[[ 86 356 158 407]
 [182 377 236 431]
 [245 364 286 408]]
246
[[ 31   6 218 476]]
375
[[ 66   1 338 430]]
392
[[ 60  84 283 382]
 [194 103 228 142]
 [284 297 302 350]]
500
[[208  76 335 241]]
333
[[  0 287  79 445]
 [ 83 224 233 352]
 [165 127 203 163]
 [208 136 231 162]
 [193 238 206 252]
 [195 276 205 286]
 [204 301 213 310]
 [227 303 254 328]
 [228 321 332 498]]
333
[[141 298 184 342]]
333
[[182 356 241 422]
 [ 33 246 231 353]
 [193 140 236 245]
 [  0 132  95 249]]
333
[[105  71 164 172]
 [119 468 173 498]
 [ 18 327 104 341]
 [ 31 357 112 376]
 [ 40 372 115 394]
 [ 23 343 108 355]
 [228 288 273 301]
 [229 300 274 314]
 [231 310 278 322]
 [236 323 280 338]
 [248 368 291 374]
 [243 355 288 360]
 [ 57 394 121 433]
 [ 63 409 124 449]
 [222 485 263 498]]
333
[[140 234 211 300]
 [253 235 263 264]
 [240 266 265 302]]
333
[[107 189 265 274]
 [288 427 317 459]]
333
[[ 77 237  93 258]
 [ 75 275  85 291]
 [ 98 268 121 292]
 [ 75 303  95 323]
 [ 69 331  78 346]
 [ 87 355 104 367]
 [ 80 411  92 432]
 [ 85 445  95 463]
 [101 455 124 479]
 [ 97 481 107 496]
 [121 225 137 245]
 [159 213 169 233]
 [164 244 179 264]
 [176 242 188 254]
 [193 255 200 269]
 [190 298 197 314]
 [201 372 213 384]
 [178 361 189 376]
 [177 327 189 340]
 [167 302 183 321]
 [130 286 151 301]
 [138 315 161 337]
 [155 339 173 362]
 [126 345 141 361]
 [150 367 167 386]
 [ 47 287  71 334]]
333
[[ 14  51 246 437]]
333
[[ 23 134 166 449]
 [155 327 187 346]
 [124 114 187 178]
 [130 138 138 145]]
500
[[164 214 187 242]
 [175 250 197 286]
 [141 240 156 258]
 [230  77 276 103]
 [243  64 282  85]
 [ 62 266 104 332]
 [107 297 115 325]
 [153 297 161 319]
 [ 20 285  60 332]
 [ 64 263  85 280]]
500
[[ 89 119 333 249]
 [ 70 244  80 278]
 [220 110 235 134]
 [338 102 342 111]]
333
[[ 36 182  95 394]
 [ 89 295 144 414]
 [101 144 301 440]]
500
[[145  19 346 328]]
333
[[  5 168 315 279]]
333
[[ 80 157 231 407]]
500
[[ 75 148 234 208]]
222
[[  5   4 216 492]]
333
[[ 80   0 226 496]]
402
[[129 243 296 315]]
375
[[  0   0  36 203]
 [  2   0 270 495]
 [238 136 362 497]
 [336 406 356 494]]
338
[[123 241 163 303]
 [143 301 175 321]
 [173 274 190 294]
 [167 242 202 281]
 [169 233 180 250]
 [170  75 213 156]
 [214 133 226 146]
 [157 137 170 171]
 [155 178 168 192]
 [146 204 169 228]
 [177 162 229 197]
 [182 207 211 234]
 [180 194 196 211]
 [164 206 173 215]]
336
[[160  99 305 395]]
353
[[ 98   1 277 497]]
253
[[ 40 106 190 271]]
500
[[120  16 365 331]]
345
[[118  33 199 477]
 [113  12 120  19]
 [ 97  32 101  39]
 [ 90  66  97  76]
 [106  56 109  64]
 [111  70 118  80]
 [115  79 120  89]
 [120  30 126  39]
 [131  30 138  39]
 [112  46 118  57]
 [125  38 131  49]
 [136  23 141  27]
 [140  35 146  39]
 [ 94  47  99  53]
 [115  30 120  34]]
500
[[ 69  40  76  66]
 [ 47  59  57  81]
 [ 51 115  62 141]
 [ 71 174  74 183]
 [ 78  94  82 113]
 [ 90 103  94 127]
 [ 92 203  99 225]
 [112 140 130 170]
 [103 112 113 142]
 [120  59 134  76]
 [146  80 170 101]
 [175 126 208 143]
 [168  40 184  58]
 [187 180 219 206]
 [224  84 243 104]
 [242 155 276 183]
 [260  95 280 121]
 [261 132 294 163]
 [254  70 277  79]
 [292  96 319 112]
 [298 122 336 146]
 [302 167 332 192]
 [282 205 307 227]
 [305 230 339 253]
 [312 124 456 342]
 [333 104 368 123]
 [366  88 390 104]
 [396 107 411 131]]
500
[[138  36 373 352]]
500
[[ 92  10 332 279]]
333
[[102  47 258 498]
 [105 296 121 317]]
353
[[ 36  28 150 465]
 [198  28 299 454]]
333
[[ 69 264 108 293]]
500
[[216 198 351 372]]
333
[[ 72 219 166 350]]
375
[[149 136 332 222]]
333
[[ 93 190 215 497]
 [144 481 164 498]
 [200 448 211 474]
 [212 403 223 433]]
500
[[134 205 218 316]]
375
[[122 111 275 417]]
500
[[  0 106  64 177]
 [ 64 128 262 248]
 [272  85 383 371]]
333
[[ 88 128 249 424]]
500
[[148 178 198 230]]
500
[[303 246 338 285]
 [194 251 244 332]]
500
[[ 23   3 193 396]
 [ 46 363  55 396]
 [198   5 363 393]
 [366   1 491 386]]
500
[[ 64 253 370 374]
 [259 324 324 350]
 [293 321 322 334]
 [324 299 364 307]]
439
[[  0 403 295 498]
 [143 207 406 414]
 [156  59 362 244]
 [ 95  55 133  77]]
500
[[232 136 341 230]]
500
[[170 109 301 143]
 [231 136 256 189]
 [262 134 268 139]
 [202 138 213 145]
 [221 102 227 112]]
375
[[ 42  78 345 451]]
400
[[ 83  72 345 438]]
500
[[177 205 408 332]
 [415 128 498 332]
 [436 198 449 207]
 [431 178 443 188]
 [423 168 432 178]]
500
[[138 145 150 178]]
500
[[ 28  76 243 317]
 [358 130 443 301]]
500
[[  0   0  81 188]
 [ 77   0 287 228]
 [298  86 445 373]]
447
[[ 52  51 374 442]]
500
[[ 33 109 178 233]
 [209 155 267 213]
 [312 106 455 233]
 [ 78  66 418 141]]
500
[[ 54  66 259 317]]
333
[[ 57 136 179 264]
 [  0  85  83 263]]
500
[[ 72  76 240 222]
 [237  65 408 208]
 [ 88 234 248 398]
 [259 219 411 387]]
375
[[ 52  52 358 469]]
500
[[170  88 235 188]]
375
[[122 253 308 405]
 [222 301 243 319]]
375
[[ 67  50 310 494]]
500
[[288 216 351 301]
 [239 200 424 225]]
307
[[ 42  23 262 492]]
500
[[306 137 363 200]]
403
[[ 87   0 294 293]]
500
[[ 87 107 303 253]
 [267 110 295 134]
 [298 118 332 144]
 [319 150 339 178]
 [365 142 414 291]
 [454 106 494 243]]
500
[[178 158 305 248]]
500
[[427 266 447 290]
 [  5  67  86 267]]
362
[[101 107 197 204]]
500
[[  1 143 396 294]
 [  0   1 395 144]
 [404   5 498 291]]
375
[[ 30 229 342 367]]
500
[[ 14  50 480 321]]
500
[[ 28  74 308 333]]
500
[[278 160 314 180]]
291
[[ 14  33 263 461]]
500
[[ 66 152 242 429]]
500
[[132 160 336 374]
 [397 254 419 290]]
375
[[123 200 151 256]
 [159 210 176 239]
 [184 212 214 241]
 [136 270 213 320]
 [222 306 232 314]]
500
[[338  91 425 273]]
333
[[ 64 138 174 297]]
500
[[ 41 128 332 332]]
333
[[156 394 173 417]]
333
[[ 30 206 278 324]]
500
[[  0   2 499 483]]
375
[[ 51 287 337 440]
 [  0 284  48 439]]
500
[[ 40 164 374 364]]
500
[[ 28  64 199 310]
 [280  35 454 310]]
500
[[  2  22 136 397]
 [171  12 320 408]
 [342  25 492 409]]
393
[[101 409 144 433]
 [154 414 297 440]
 [ 65  69 318 419]
 [113  71 164 102]]
375
[[115 147 263 320]]
375
[[ 96  71 317 439]]
333
[[173 174 287 329]
 [265 135 332 399]
 [102 146 181 384]
 [ 39 152 144 352]]
500
[[ 50  21 410 294]]
375
[[ 86  58 293 410]]
335
[[ 69   0 221 425]]
394
[[ 79 420 138 484]
 [220 334 272 481]
 [230 491 244 499]
 [291 336 356 488]
 [373 250 391 282]
 [322 103 366 186]
 [240 100 309 146]
 [185 109 228 196]
 [ 84  69  98  87]]
500
[[283  85 456 278]]
500
[[300 310 465 362]]
500
[[  0   0 186 181]
 [186   0 263 107]
 [131 104 257 291]
 [157 293 245 352]]
500
[[145 262 274 307]
 [212 294 292 347]
 [305 431 379 485]]
500
[[ 83 102 199 449]]
389
[[ 68  68 330 469]]
333
[[113 389 234 499]
 [145 165 239 321]
 [ 65 129 131 209]]
375
[[ 30 122 326 466]
 [276 235 294 265]
 [274 167 291 186]]
500
[[ 92  98 402 203]]
500
[[227 215 309 313]
 [314 214 389 290]
 [389 184 461 245]]
500
[[232 119 361 272]]
375
[[ 86 122 287 385]
 [304 170 328 210]
 [297 268 333 307]
 [280 241 332 262]
 [ 62 276 100 320]]
500
[[314 139 421 231]]
500
[[331  29 398 133]
 [ 18  41 156  90]]
375
[[171 218 201 248]
 [175 249 195 282]]
375
[[ 82 123 374 499]]
304
[[ 46 122 303 496]]
300
[[ 39 148 111 205]
 [  0 127  43 171]
 [177  64 204 106]
 [173  35 185  52]
 [242 128 251 138]
 [270  42 299  87]]
500
[[ 27  19 479 394]]
333
[[  7  67 328 413]]
334
[[ 46  25 237 477]
 [ 19 141 120 498]]
333
[[ 99  25 210 475]
 [207   0 249 118]]
400
[[ 89  10 303 486]]
333
[[ 29  23 313 493]]
375
[[ 80   5 272 494]]
334
[[ 82  19 242 476]]
334
[[ 40 407  82 499]
 [ 19  29 310 484]]
500
[[208 195 278 276]]
500
[[ 17  40 219 225]
 [283  53 435 159]]
500
[[202 212 289 304]]
375
[[ 14   0 367 458]
 [166   0 209  61]
 [232   3 273  40]]
500
[[404 313 465 374]
 [439 328 471 353]
 [442 312 461 323]
 [ 51 148 130 204]
 [  0  43 184 156]]
336
[[260 277 308 383]
 [ 90 323 103 332]
 [ 86 305 130 364]
 [ 87 297  99 303]
 [ 23 322  35 335]
 [ 19 297  35 307]
 [ 14 219  42 253]
 [104 294 114 307]
 [ 83 274 103 287]
 [181 223 207 251]
 [186  86 235 146]
 [  0  88  22 106]
 [ 80 129  90 138]
 [115 127 128 137]]
375
[[126 151 214 339]
 [179  98 290 401]
 [  0  96  28 127]
 [ 39 198  66 231]
 [ 73 154 113 226]
 [113 101 163 162]]
472
[[ 54  35 162 105]
 [199 180 229 206]
 [222 133 327 166]
 [233  72 362 118]
 [226  14 376  59]]
500
[[232 140 376 200]
 [174 137 229 210]]
342
[[ 87  84 252 344]]
500
[[ 56  92 377 245]]
377
[[193 188 274 284]
 [205 184 252 247]
 [178 183 209 258]]
333
[[  0 290 332 499]]
333
[[ 31 103 220 215]]
364
[[  9  14 345 466]]
500
[[203 244 323 455]
 [330 186 344 212]
 [244 185 283 251]]
375
[[ 55  16 322 485]]
500
[[163   5 337 374]]
500
[[244 160 348 271]]
500
[[338 221 365 269]]
500
[[224 153 357 333]]
375
[[120  36 326 335]]
375
[[132 134 321 465]]
500
[[138  98 316 278]
 [220  66 235  76]]
450
[[ 22  34 377 272]]
500
[[ 35  76 376 273]]
416
[[ 81  31 326 454]]
282
[[ 59  81 224 401]]
450
[[ 34  11 377 305]]
450
[[ 44  54 377 276]]
500
[[179  29 377 359]]
375
[[ 60 151 149 367]]
375
[[ 44   0 322 480]]
500
[[247 134 274 258]]
333
[[ 31 234  85 327]]
298
[[ 34 159 108 252]
 [127 212 203 287]]
321
[[ 32 284 126 371]]
333
[[ 34 334 135 466]
 [ 56 256  75 325]
 [ 21 257  50 283]
 [ 22 159  70 222]
 [198 229 311 359]
 [179 182 227 228]
 [140 208 184 242]]
294
[[ 25  20 272 498]]
340
[[ 64  13 267 480]]
500
[[163 192 377 238]]
405
[[ 15   1 377 496]]
500
[[ 93 133 365 330]]
500
[[ 31  51 377 332]]
500
[[ 88 132 284 334]
 [189 266 286 373]
 [250  77 283 117]]
299
[[ 27  17 251 496]]
344
[[ 50 204 252 445]]
361
[[ 54  64 225 489]]
500
[[ 92  30 377 313]]
500
[[183 301 250 346]
 [179 214 281 274]
 [104 255 170 301]
 [281 137 354 199]
 [ 21 222 114 272]]
500
[[ 77 148 356 215]]
375
[[148 243 246 348]]
500
[[ 52  33 377 405]]
466
[[ 28  19 145 443]
 [169  17 308 440]
 [326  27 377 437]]
500
[[ 25  74 214 417]
 [259  78 377 396]]
319
[[ 92 428 147 443]
 [175 390 194 416]
 [ 19 174 189 257]]
500
[[282 131 376 266]
 [ 60 136 199 274]]
500
[[ 29  19 377 368]]
333
[[ 22 259 163 497]
 [176 313 261 498]
 [ 85   0 332 244]]
500
[[280 139 379 264]
 [175  72 233 230]
 [109 132 179 262]]
332
[[104 280 201 319]
 [104 185 202 302]]
375
[[ 67   9 321 497]]
432
[[165 193 268 258]]
500
[[187 128 291 209]]
500
[[122   6 332 353]]
500
[[139 149 288 211]
 [ 86 253 134 301]]
500
[[ 96 112 323 372]]
500
[[107 140 241 277]]
375
[[103 180 332 279]]
375
[[ 81 221 331 334]]
257
[[ 29  15 230 459]]
375
[[237 272 292 331]
 [209 312 239 338]
 [138 322 202 375]
 [120 333 156 357]
 [ 76 295 125 333]]
375
[[ 69  31 332 446]]
500
[[ 28   0 332 301]]
333
[[ 42 117 245 388]]
334
[[  0  17 332 497]]
500
[[245 183 315 227]
 [148 230 299 333]]
500
[[ 82   7 332 492]
 [  1 271  78 378]
 [  0 220  66 261]]
333
[[ 82  78 241 435]]
500
[[ 24 264 122 324]]
375
[[ 93  99 267 424]]
333
[[ 78 288 153 367]
 [ 30  67 256 183]]
460
[[247 126 332 208]]
403
[[ 72  39 332 493]]
375
[[135  89 257 207]]
500
[[254 141 332 220]]
334
[[100 398 186 444]]
331
[[  0   1 322  99]
 [  3 112 323 225]
 [ 20 246 326 342]
 [  3 366 329 495]]
333
[[ 48   0 309 496]]
333
[[ 76  45 281 439]]
333
[[ 24 144 275 436]]
333
[[ 19 162 303 454]]
333
[[ 34 170 259 453]]
333
[[ 69  47 249 366]]
333
[[ 67  67 260 435]]
333
[[ 39  50 282 442]]
360
[[ 25  33 331 433]]
341
[[ 38   2 303 438]]
333
[[  2 372  73 464]]
500
[[ 80  92 331 330]]
500
[[161   3 286 330]]
333
[[ 50 345 285 448]]
500
[[ 57 165  73 190]
 [116 233 198 280]
 [247 264 290 281]
 [289 154 332 331]]
243
[[ 11   7 242 496]]
375
[[ 51 169 158 423]]
333
[[122 161 228 255]
 [112 207 329 493]]
375
[[ 60 236 207 412]
 [202 233 229 283]
 [231 203 241 237]
 [201 265 258 365]]
500
[[174  72 332 215]]
313
[[ 79 118 230 353]]
500
[[182  46 314 331]]
333
[[118 158 206 401]]
500
[[111 114 331 286]]
364
[[ 79  44 208 349]]
345
[[  6  39 307 362]
 [174 336 246 469]]
500
[[ 71   6 446 320]]
375
[[ 95  80 228 467]]
500
[[304 142 354 209]
 [368 324 400 373]]
484
[[132 299 262 446]]
500
[[255 321 268 333]
 [276 314 288 336]
 [300 323 311 335]
 [315 312 332 323]
 [284 299 294 312]
 [297 303 309 318]
 [299 284 310 296]
 [319 288 331 298]]
500
[[  0  76 498 373]]
500
[[419 342 484 371]
 [394 234 468 341]]
370
[[278 151 318 195]
 [287 421 322 458]]
500
[[170 122 194 231]
 [191  92 304 270]]
375
[[181 178 283 368]]
375
[[ 55  24 304 308]
 [ 57 291  57 291]
 [343  76 373 187]]
500
[[181 199 224 233]
 [221 288 252 337]
 [170 142 216 181]]
500
[[  5  15 154 359]
 [158  15 324 360]
 [320  15 466 354]
 [465 121 497 254]]
500
[[  8  76  49 171]
 [ 34  16 115  70]
 [ 24 177 118 344]
 [116  88 131 161]
 [121  26 194  66]
 [190  67 283 117]
 [114 166 193 318]
 [298 144 383 236]
 [392 170 470 334]
 [401  80 415 165]
 [408  19 492  79]]
500
[[142  72 337 219]]
351
[[ 27  42 270 427]]
500
[[ 25   0 147  61]
 [ 28   1 492 246]]
500
[[123  96 403 198]]
500
[[ 17 119 491 327]]
500
[[246 146 435 234]
 [ 74  99 240 216]
 [177 373 177 373]
 [179 126 273 224]]
284
[[ 44  17 255 493]]
500
[[ 20  26 201 233]
 [262   2 438 227]]
345
[[113 150 227 467]]
375
[[ 14   2 357 495]]
358
[[ 74  12 269 490]
 [270  11 301 257]
 [259 230 285 306]
 [287 322 301 362]]
500
[[157   1 266 313]]
375
[[ 74  48 296 459]]
500
[[300 154 369 279]]
500
[[102 229 200 312]
 [204 182 255 241]]
500
[[153  61 256 156]
 [274 114 325 156]
 [296  70 377 155]]
500
[[110 165 227 257]
 [250  87 399 175]
 [260  52 326  78]
 [344  68 438 108]
 [349  43 446  83]
 [265  28 329  50]]
500
[[178 124 294 205]]
500
[[127 159 287 244]]
333
[[111  35 235 450]]
192
[[ 74 130 141 188]]
333
[[105 242 231 298]]
173
[[ 63 272 169 441]
 [  7   0 153 217]]
500
[[135 235 177 285]]
500
[[117 236 220 330]
 [289 186 396 299]]
500
[[246  19 376 312]]
333
[[ 37 148 303 462]
 [115 314 171 359]]
335
[[123  56 296 424]]
500
[[ 94 197 403 323]]
500
[[182 128 297 244]]
354
[[ 80  36 332 474]]
333
[[189 298 332 498]]
371
[[ 74  91 274 409]]
333
[[ 62   0 201 419]]
500
[[140 111 354 299]]
333
[[199 287 304 373]
 [101 285 141 327]]
334
[[ 79 160 310 488]]
375
[[ 95  37 213 361]]
308
[[ 48  67 162 175]]
494
[[ 11  36 408 445]]
375
[[ 27 202 208 494]
 [ 68   1 271 174]]
500
[[ 18  25 350 365]
 [380 233 486 282]
 [361  28 479 198]]
375
[[141  39 368 441]]
500
[[ 73 125 342 228]]
500
[[279  23 435 214]
 [152 154 215 276]
 [ 72 257 197 399]
 [191 323 232 362]
 [170 269 219 306]
 [209 199 277 269]
 [459 347 499 398]]
500
[[ 26  57 446 316]]
443
[[ 48   2 383 451]]
500
[[ 69 213 276 319]]
333
[[ 84  38 211 205]
 [102 207 236 381]]
500
[[ 94  56 371 354]]
357
[[ 82 204 286 404]]
375
[[114 150 250 362]
 [113 127 129 157]]
375
[[159   2 284 477]]
375
[[ 65   0 274 498]]
375
[[ 36  42 336 472]]
500
[[233 316 463 450]]
500
[[ 83  79 393 322]]
500
[[144 104 203 160]
 [243  62 445 238]
 [189   0 222  23]]
352
[[ 34 227 251 471]
 [226 283 263 307]]
500
[[ 54 211 193 353]]
332
[[ 47 252 171 483]
 [161 269 242 350]]
500
[[111  12 404 333]]
500
[[ 41  88 473 274]]
333
[[107  78 235 394]]
500
[[172 143 259 330]
 [280 154 362 337]]
500
[[132 178 225 292]]
500
[[ 10  95 478 261]]
500
[[135 268 152 290]
 [156 229 184 269]
 [176 174 240 249]
 [206 250 228 277]
 [248 206 274 231]
 [238 145 278 189]
 [287 181 322 210]
 [334 142 360 169]]
500
[[ 41  33 404 163]]
500
[[114 115 379 221]]
500
[[181 128 328 344]
 [302  95 334 121]]
500
[[ 46  26 466 365]]
500
[[143 122 398 256]]
500
[[ 80  74 121 103]
 [164 190 383 291]
 [137  49 229 209]]
489
[[249  73 415 265]]
405
[[ 79  55 309 379]]
500
[[ 57 158 335 307]]
500
[[  8  49 353 226]
 [121  22 148  44]]
333
[[141 211 219 364]]
332
[[222 148 282 313]
 [ 60 173 102 298]]
500
[[ 73 213 209 276]]
334
[[216 173 333 497]]
269
[[ 74  90 250 484]]
344
[[ 70 126 259 380]]
333
[[  0 105 331 494]
 [ 93  22 264 120]]
408
[[ 31  54  63  76]
 [  0 107  20 138]
 [ 51  76 326 244]
 [242 201 272 216]
 [307 157 330 196]
 [341 120 384 153]]
448
[[  1  18 425 495]]
450
[[157  40 180  61]
 [154  72 273 340]]
375
[[ 38 221 141 477]
 [ 96 394 166 497]
 [ 69 282 187 379]]
450
[[ 49  56 395 209]]
333
[[ 78  67 252 432]]
333
[[ 23  42 288 468]]
500
[[230 146 259 166]]
455
[[148 230 327 428]]
334
[[103  97 229 316]]
150
[[ 42  30 107 142]]
332
[[  0   0 252 466]
 [101  98 147 120]]
343
[[  0   0 338 487]]
500
[[  0   1 492 316]]
357
[[ 31 162 356 499]
 [ 15 321  61 354]
 [  0 409  10 463]]
375
[[ 82 244 162 332]
 [ 81 319 135 497]
 [143 307 306 453]
 [157 419 268 495]
 [296 425 356 462]
 [253 375 351 411]
 [300 373 325 386]
 [182 208 279 299]
 [250 309 283 331]
 [157 196 196 219]]
346
[[ 34 139 305 497]]
333
[[ 72 109 256 338]
 [215 129 258 163]]
342
[[ 80   3 223 495]]
284
[[180 118 189 124]
 [189 133 195 148]
 [198 149 202 159]
 [198 126 203 135]]
333
[[165  15 320 228]]
333
[[ 87 110 229 418]
 [139 444 170 468]]
333
[[  0   0 330 381]
 [ 26 345 279 498]]
500
[[285 142 469 324]]
375
[[ 84 138 287 345]]
333
[[132 271 259 475]]
477
[[ 93  25 354 302]
 [ 28 165 114 298]
 [ 73 322 241 484]
 [324 248 449 345]]
500
[[ 97 127 206 278]
 [ 83  26 161 114]
 [321 116 383 181]]
220
[[ 56  23 163 197]]
500
[[  7  57 425 267]
 [432  89 467 218]]
500
[[ 23  23 459 257]]
500
[[ 64   1 454 385]]
500
[[ 13 111 196 302]
 [306  92 482 299]]
333
[[ 21  47 281 467]]
333
[[  1 173 255 401]
 [  0 397 201 469]
 [ 64 253 135 329]]
333
[[100 232 240 407]
 [141 185 180 232]]
500
[[ 84 131 176 156]]
500
[[ 25 236  66 310]
 [ 62  47 112  86]
 [  0  47  48 168]
 [190  32 281 107]]
500
[[ 80   5 191 211]
 [290  32 403 226]]
433
[[ 86 299 290 486]
 [ 18  12 420 280]]
492
[[158 298 187 335]]
375
[[  4 114 174 483]
 [137 118 223 211]]
335
[[ 72  17 213 310]
 [185 264 260 380]]
335
[[142 141 215 291]
 [207 183 255 361]
 [180 431 218 497]
 [ 11 455  51 496]]
500
[[ 54 217 472 451]]
500
[[319 200 478 235]
 [372 156 426 199]]
410
[[143  15 348 391]
 [275 446 310 475]
 [220 440 262 477]
 [169 423 211 469]
 [102 364 138 410]]
500
[[215 194 323 234]]
375
[[ 93 116 294 371]]
500
[[ 83 117 206 334]
 [166 199 366 347]
 [204 126 246 194]
 [266   5 374 173]]
500
[[119  15 400 334]
 [399 115 485 270]
 [405  35 436 101]]
333
[[ 64 160 149 278]
 [ 95 279 157 344]
 [281 277 307 349]
 [287 178 306 207]]
231
[[ 30  19 177 489]]
500
[[132  55 343 283]]
333
[[115 165 204 373]]
375
[[ 89   1 294 463]]
500
[[161 119 407 288]]
500
[[  1   5 206 351]
 [208   2 498 170]
 [211 179 360 352]
 [361 179 498 350]]
356
[[  0  23 333 491]]
416
[[ 49   0 173 476]
 [252   0 404 498]]
500
[[ 80  52 4WARNING: Logging before InitGoogleLogging() is written to STDERR
I1026 00:45:22.682606 17176 solver.cpp:48] Initializing solver from parameters: 
train_net: "models/pascal_voc/ZF/faster_rcnn_alt_opt/stage2_rpn_train.pt"
base_lr: 0.001
display: 20
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 60000
snapshot: 0
snapshot_prefix: "zf_rpn"
average_loss: 100
I1026 00:45:22.682637 17176 solver.cpp:81] Creating training net from train_net file: models/pascal_voc/ZF/faster_rcnn_alt_opt/stage2_rpn_train.pt
I1026 00:45:22.688401 17176 net.cpp:49] Initializing net from parameters: 
name: "ZF"
state {
  phase: TRAIN
}
layer {
  name: "input-data"
  type: "Python"
  top: "data"
  top: "im_info"
  top: "gt_boxes"
  python_param {
    module: "roi_data_layer.layer"
    layer: "RoIDataLayer"
    param_str: "\'num_classes\': 2"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 3
    kernel_size: 7
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
    engine: CAFFE
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    stride: 2
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
    engine: CAFFE
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "rpn_conv1"
  type: "Convolution"
  bottom: "conv5"
  top: "rpn_conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_relu1"
  type: "ReLU"
  bottom: "rpn_conv1"
  top: "rpn_conv1"
}
layer {
  name: "rpn_cls_score"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_cls_score"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 18
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_bbox_pred"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_bbox_pred"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 36
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_cls_score_reshape"
  type: "Reshape"
  bottom: "rpn_cls_score"
  top: "rpn_cls_score_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 2
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "rpn-data"
  type: "Python"
  bottom: "rpn_cls_score"
  bottom: "gt_boxes"
  bottom: "im_info"
  bottom: "data"
  top: "rpn_labels"
  top: "rpn_bbox_targets"
  top: "rpn_bbox_inside_weights"
  top: "rpn_bbox_outside_weights"
  python_param {
    module: "rpn.anchor_target_layer"
    layer: "AnchorTargetLayer"
    param_str: "\'feat_stride\': 16"
  }
}
layer {
  name: "rpn_loss_cls"
  type: "SoftmaxWithLoss"
  bottom: "rpn_cls_score_reshape"
  bottom: "rpn_labels"
  top: "rpn_cls_loss"
  loss_weight: 1
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: -1
    normalize: true
  }
}
layer {
  name: "rpn_loss_bbox"
  type: "SmoothL1Loss"
  bottom: "rpn_bbox_pred"
  bottom: "rpn_bbox_targets"
  bottom: "rpn_bbox_inside_weights"
  bottom: "rpn_bbox_outside_weights"
  top: "rpn_loss_bbox"
  loss_weight: 1
  smooth_l1_loss_param {
    sigma: 3
  }
}
layer {
  name: "dummy_roi_pool_conv5"
  type: "DummyData"
  top: "dummy_roi_pool_conv5"
  dummy_data_param {
    data_filler {
      type: "gaussian"
      std: 0.01
    }
    shape {
      dim: 1
      dim: 9216
    }
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "dummy_roi_pool_conv5"
  top: "fc6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "silence_fc7"
  type: "Silence"
  bottom: "fc7"
}
I1026 00:45:22.688537 17176 layer_factory.hpp:77] Creating layer input-data
I1026 00:45:22.689038 17176 net.cpp:106] Creating Layer input-data
I1026 00:45:22.689051 17176 net.cpp:411] input-data -> data
I1026 00:45:22.689064 17176 net.cpp:411] input-data -> im_info
I1026 00:45:22.689072 17176 net.cpp:411] input-data -> gt_boxes
I1026 00:45:22.696645 17176 net.cpp:150] Setting up input-data
I1026 00:45:22.696674 17176 net.cpp:157] Top shape: 1 3 600 1000 (1800000)
I1026 00:45:22.696677 17176 net.cpp:157] Top shape: 1 3 (3)
I1026 00:45:22.696679 17176 net.cpp:157] Top shape: 1 4 (4)
I1026 00:45:22.696681 17176 net.cpp:165] Memory required for data: 7200028
I1026 00:45:22.696686 17176 layer_factory.hpp:77] Creating layer data_input-data_0_split
I1026 00:45:22.696696 17176 net.cpp:106] Creating Layer data_input-data_0_split
I1026 00:45:22.696701 17176 net.cpp:454] data_input-data_0_split <- data
I1026 00:45:22.696707 17176 net.cpp:411] data_input-data_0_split -> data_input-data_0_split_0
I1026 00:45:22.696717 17176 net.cpp:411] data_input-data_0_split -> data_input-data_0_split_1
I1026 00:45:22.696754 17176 net.cpp:150] Setting up data_input-data_0_split
I1026 00:45:22.696761 17176 net.cpp:157] Top shape: 1 3 600 1000 (1800000)
I1026 00:45:22.696768 17176 net.cpp:157] Top shape: 1 3 600 1000 (1800000)
I1026 00:45:22.696771 17176 net.cpp:165] Memory required for data: 21600028
I1026 00:45:22.696774 17176 layer_factory.hpp:77] Creating layer conv1
I1026 00:45:22.696790 17176 net.cpp:106] Creating Layer conv1
I1026 00:45:22.696794 17176 net.cpp:454] conv1 <- data_input-data_0_split_0
I1026 00:45:22.696800 17176 net.cpp:411] conv1 -> conv1
I1026 00:45:22.804508 17176 net.cpp:150] Setting up conv1
I1026 00:45:22.804533 17176 net.cpp:157] Top shape: 1 96 300 500 (14400000)
I1026 00:45:22.804534 17176 net.cpp:165] Memory required for data: 79200028
I1026 00:45:22.804548 17176 layer_factory.hpp:77] Creating layer relu1
I1026 00:45:22.804556 17176 net.cpp:106] Creating Layer relu1
I1026 00:45:22.804560 17176 net.cpp:454] relu1 <- conv1
I1026 00:45:22.804564 17176 net.cpp:397] relu1 -> conv1 (in-place)
I1026 00:45:22.804772 17176 net.cpp:150] Setting up relu1
I1026 00:45:22.804780 17176 net.cpp:157] Top shape: 1 96 300 500 (14400000)
I1026 00:45:22.804782 17176 net.cpp:165] Memory required for data: 136800028
I1026 00:45:22.804785 17176 layer_factory.hpp:77] Creating layer norm1
I1026 00:45:22.804792 17176 net.cpp:106] Creating Layer norm1
I1026 00:45:22.804795 17176 net.cpp:454] norm1 <- conv1
I1026 00:45:22.804800 17176 net.cpp:411] norm1 -> norm1
I1026 00:45:22.804874 17176 net.cpp:150] Setting up norm1
I1026 00:45:22.804879 17176 net.cpp:157] Top shape: 1 96 300 500 (14400000)
I1026 00:45:22.804882 17176 net.cpp:165] Memory required for data: 194400028
I1026 00:45:22.804883 17176 layer_factory.hpp:77] Creating layer pool1
I1026 00:45:22.804886 17176 net.cpp:106] Creating Layer pool1
I1026 00:45:22.804888 17176 net.cpp:454] pool1 <- norm1
I1026 00:45:22.804891 17176 net.cpp:411] pool1 -> pool1
I1026 00:45:22.804911 17176 net.cpp:150] Setting up pool1
I1026 00:45:22.804915 17176 net.cpp:157] Top shape: 1 96 151 251 (3638496)
I1026 00:45:22.804916 17176 net.cpp:165] Memory required for data: 208954012
I1026 00:45:22.804919 17176 layer_factory.hpp:77] Creating layer conv2
I1026 00:45:22.804925 17176 net.cpp:106] Creating Layer conv2
I1026 00:45:22.804927 17176 net.cpp:454] conv2 <- pool1
I1026 00:45:22.804930 17176 net.cpp:411] conv2 -> conv2
I1026 00:45:22.806344 17176 net.cpp:150] Setting up conv2
I1026 00:45:22.806352 17176 net.cpp:157] Top shape: 1 256 76 126 (2451456)
I1026 00:45:22.806354 17176 net.cpp:165] Memory required for data: 218759836
I1026 00:45:22.806370 17176 layer_factory.hpp:77] Creating layer relu2
I1026 00:45:22.806375 17176 net.cpp:106] Creating Layer relu2
I1026 00:45:22.806376 17176 net.cpp:454] relu2 <- conv2
I1026 00:45:22.806380 17176 net.cpp:397] relu2 -> conv2 (in-place)
I1026 00:45:22.806597 17176 net.cpp:150] Setting up relu2
I1026 00:45:22.806605 17176 net.cpp:157] Top shape: 1 256 76 126 (2451456)
I1026 00:45:22.806607 17176 net.cpp:165] Memory required for data: 228565660
I1026 00:45:22.806608 17176 layer_factory.hpp:77] Creating layer norm2
I1026 00:45:22.806624 17176 net.cpp:106] Creating Layer norm2
I1026 00:45:22.806627 17176 net.cpp:454] norm2 <- conv2
I1026 00:45:22.806632 17176 net.cpp:411] norm2 -> norm2
I1026 00:45:22.806694 17176 net.cpp:150] Setting up norm2
I1026 00:45:22.806699 17176 net.cpp:157] Top shape: 1 256 76 126 (2451456)
I1026 00:45:22.806699 17176 net.cpp:165] Memory required for data: 238371484
I1026 00:45:22.806701 17176 layer_factory.hpp:77] Creating layer pool2
I1026 00:45:22.806705 17176 net.cpp:106] Creating Layer pool2
I1026 00:45:22.806709 17176 net.cpp:454] pool2 <- norm2
I1026 00:45:22.806710 17176 net.cpp:411] pool2 -> pool2
I1026 00:45:22.806731 17176 net.cpp:150] Setting up pool2
I1026 00:45:22.806735 17176 net.cpp:157] Top shape: 1 256 39 64 (638976)
I1026 00:45:22.806736 17176 net.cpp:165] Memory required for data: 240927388
I1026 00:45:22.806738 17176 layer_factory.hpp:77] Creating layer conv3
I1026 00:45:22.806743 17176 net.cpp:106] Creating Layer conv3
I1026 00:45:22.806746 17176 net.cpp:454] conv3 <- pool2
I1026 00:45:22.806748 17176 net.cpp:411] conv3 -> conv3
I1026 00:45:22.809015 17176 net.cpp:150] Setting up conv3
I1026 00:45:22.809034 17176 net.cpp:157] Top shape: 1 384 39 64 (958464)
I1026 00:45:22.809036 17176 net.cpp:165] Memory required for data: 244761244
I1026 00:45:22.809056 17176 layer_factory.hpp:77] Creating layer relu3
I1026 00:45:22.809062 17176 net.cpp:106] Creating Layer relu3
I1026 00:45:22.809064 17176 net.cpp:454] relu3 <- conv3
I1026 00:45:22.809068 17176 net.cpp:397] relu3 -> conv3 (in-place)
I1026 00:45:22.809200 17176 net.cpp:150] Setting up relu3
I1026 00:45:22.809206 17176 net.cpp:157] Top shape: 1 384 39 64 (958464)
I1026 00:45:22.809218 17176 net.cpp:165] Memory required for data: 248595100
I1026 00:45:22.809221 17176 layer_factory.hpp:77] Creating layer conv4
I1026 00:45:22.809226 17176 net.cpp:106] Creating Layer conv4
I1026 00:45:22.809228 17176 net.cpp:454] conv4 <- conv3
I1026 00:45:22.809232 17176 net.cpp:411] conv4 -> conv4
I1026 00:45:22.811511 17176 net.cpp:150] Setting up conv4
I1026 00:45:22.811524 17176 net.cpp:157] Top shape: 1 384 39 64 (958464)
I1026 00:45:22.811527 17176 net.cpp:165] Memory required for data: 252428956
I1026 00:45:22.811532 17176 layer_factory.hpp:77] Creating layer relu4
I1026 00:45:22.811547 17176 net.cpp:106] Creating Layer relu4
I1026 00:45:22.811549 17176 net.cpp:454] relu4 <- conv4
I1026 00:45:22.811563 17176 net.cpp:397] relu4 -> conv4 (in-place)
I1026 00:45:22.811806 17176 net.cpp:150] Setting up relu4
I1026 00:45:22.811815 17176 net.cpp:157] Top shape: 1 384 39 64 (958464)
I1026 00:45:22.811815 17176 net.cpp:165] Memory required for data: 256262812
I1026 00:45:22.811818 17176 layer_factory.hpp:77] Creating layer conv5
I1026 00:45:22.811825 17176 net.cpp:106] Creating Layer conv5
I1026 00:45:22.811826 17176 net.cpp:454] conv5 <- conv4
I1026 00:45:22.811830 17176 net.cpp:411] conv5 -> conv5
I1026 00:45:22.813552 17176 net.cpp:150] Setting up conv5
I1026 00:45:22.813562 17176 net.cpp:157] Top shape: 1 256 39 64 (638976)
I1026 00:45:22.813565 17176 net.cpp:165] Memory required for data: 258818716
I1026 00:45:22.813581 17176 layer_factory.hpp:77] Creating layer relu5
I1026 00:45:22.813587 17176 net.cpp:106] Creating Layer relu5
I1026 00:45:22.813590 17176 net.cpp:454] relu5 <- conv5
I1026 00:45:22.813593 17176 net.cpp:397] relu5 -> conv5 (in-place)
I1026 00:45:22.813823 17176 net.cpp:150] Setting up relu5
I1026 00:45:22.813832 17176 net.cpp:157] Top shape: 1 256 39 64 (638976)
I1026 00:45:22.813843 17176 net.cpp:165] Memory required for data: 261374620
I1026 00:45:22.813845 17176 layer_factory.hpp:77] Creating layer rpn_conv1
I1026 00:45:22.813856 17176 net.cpp:106] Creating Layer rpn_conv1
I1026 00:45:22.813859 17176 net.cpp:454] rpn_conv1 <- conv5
I1026 00:45:22.813876 17176 net.cpp:411] rpn_conv1 -> rpn_conv1
I1026 00:45:22.819361 17176 net.cpp:150] Setting up rpn_conv1
I1026 00:45:22.819383 17176 net.cpp:157] Top shape: 1 256 39 64 (638976)
I1026 00:45:22.819386 17176 net.cpp:165] Memory required for data: 263930524
I1026 00:45:22.819391 17176 layer_factory.hpp:77] Creating layer rpn_relu1
I1026 00:45:22.819396 17176 net.cpp:106] Creating Layer rpn_relu1
I1026 00:45:22.819397 17176 net.cpp:454] rpn_relu1 <- rpn_conv1
I1026 00:45:22.819411 17176 net.cpp:397] rpn_relu1 -> rpn_conv1 (in-place)
I1026 00:45:22.819560 17176 net.cpp:150] Setting up rpn_relu1
I1026 00:45:22.819566 17176 net.cpp:157] Top shape: 1 256 39 64 (638976)
I1026 00:45:22.819578 17176 net.cpp:165] Memory required for data: 266486428
I1026 00:45:22.819581 17176 layer_factory.hpp:77] Creating layer rpn_conv1_rpn_relu1_0_split
I1026 00:45:22.819584 17176 net.cpp:106] Creating Layer rpn_conv1_rpn_relu1_0_split
I1026 00:45:22.819587 17176 net.cpp:454] rpn_conv1_rpn_relu1_0_split <- rpn_conv1
I1026 00:45:22.819603 17176 net.cpp:411] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_0
I1026 00:45:22.819612 17176 net.cpp:411] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_1
I1026 00:45:22.819654 17176 net.cpp:150] Setting up rpn_conv1_rpn_relu1_0_split
I1026 00:45:22.819659 17176 net.cpp:157] Top shape: 1 256 39 64 (638976)
I1026 00:45:22.819663 17176 net.cpp:157] Top shape: 1 256 39 64 (638976)
I1026 00:45:22.819674 17176 net.cpp:165] Memory required for data: 271598236
I1026 00:45:22.819675 17176 layer_factory.hpp:77] Creating layer rpn_cls_score
I1026 00:45:22.819694 17176 net.cpp:106] Creating Layer rpn_cls_score
I1026 00:45:22.819695 17176 net.cpp:454] rpn_cls_score <- rpn_conv1_rpn_relu1_0_split_0
I1026 00:45:22.819711 17176 net.cpp:411] rpn_cls_score -> rpn_cls_score
I1026 00:45:22.820564 17176 net.cpp:150] Setting up rpn_cls_score
I1026 00:45:22.820572 17176 net.cpp:157] Top shape: 1 18 39 64 (44928)
I1026 00:45:22.820574 17176 net.cpp:165] Memory required for data: 271777948
I1026 00:45:22.820577 17176 layer_factory.hpp:77] Creating layer rpn_cls_score_rpn_cls_score_0_split
I1026 00:45:22.820581 17176 net.cpp:106] Creating Layer rpn_cls_score_rpn_cls_score_0_split
I1026 00:45:22.820583 17176 net.cpp:454] rpn_cls_score_rpn_cls_score_0_split <- rpn_cls_score
I1026 00:45:22.820587 17176 net.cpp:411] rpn_cls_score_rpn_cls_score_0_split -> rpn_cls_score_rpn_cls_score_0_split_0
I1026 00:45:22.820601 17176 net.cpp:411] rpn_cls_score_rpn_cls_score_0_split -> rpn_cls_score_rpn_cls_score_0_split_1
I1026 00:45:22.820650 17176 net.cpp:150] Setting up rpn_cls_score_rpn_cls_score_0_split
I1026 00:45:22.820654 17176 net.cpp:157] Top shape: 1 18 39 64 (44928)
I1026 00:45:22.820657 17176 net.cpp:157] Top shape: 1 18 39 64 (44928)
I1026 00:45:22.820667 17176 net.cpp:165] Memory required for data: 272137372
I1026 00:45:22.820669 17176 layer_factory.hpp:77] Creating layer rpn_bbox_pred
I1026 00:45:22.820686 17176 net.cpp:106] Creating Layer rpn_bbox_pred
I1026 00:45:22.820689 17176 net.cpp:454] rpn_bbox_pred <- rpn_conv1_rpn_relu1_0_split_1
I1026 00:45:22.820693 17176 net.cpp:411] rpn_bbox_pred -> rpn_bbox_pred
I1026 00:45:22.821501 17176 net.cpp:150] Setting up rpn_bbox_pred
I1026 00:45:22.821508 17176 net.cpp:157] Top shape: 1 36 39 64 (89856)
I1026 00:45:22.821511 17176 net.cpp:165] Memory required for data: 272496796
I1026 00:45:22.821514 17176 layer_factory.hpp:77] Creating layer rpn_cls_score_reshape
I1026 00:45:22.821523 17176 net.cpp:106] Creating Layer rpn_cls_score_reshape
I1026 00:45:22.821526 17176 net.cpp:454] rpn_cls_score_reshape <- rpn_cls_score_rpn_cls_score_0_split_0
I1026 00:45:22.821539 17176 net.cpp:411] rpn_cls_score_reshape -> rpn_cls_score_reshape
I1026 00:45:22.821583 17176 net.cpp:150] Setting up rpn_cls_score_reshape
I1026 00:45:22.821588 17176 net.cpp:157] Top shape: 1 2 351 64 (44928)
I1026 00:45:22.821599 17176 net.cpp:165] Memory required for data: 272676508
I1026 00:45:22.821599 17176 layer_factory.hpp:77] Creating layer rpn-data
I1026 00:45:22.821964 17176 net.cpp:106] Creating Layer rpn-data
I1026 00:45:22.821971 17176 net.cpp:454] rpn-data <- rpn_cls_score_rpn_cls_score_0_split_1
I1026 00:45:22.821974 17176 net.cpp:454] rpn-data <- gt_boxes
I1026 00:45:22.821976 17176 net.cpp:454] rpn-data <- im_info
I1026 00:45:22.821979 17176 net.cpp:454] rpn-data <- data_input-data_0_split_1
I1026 00:45:22.821982 17176 net.cpp:411] rpn-data -> rpn_labels
I1026 00:45:22.821997 17176 net.cpp:411] rpn-data -> rpn_bbox_targets
I1026 00:45:22.822003 17176 net.cpp:411] rpn-data -> rpn_bbox_inside_weights
I1026 00:45:22.822017 17176 net.cpp:411] rpn-data -> rpn_bbox_outside_weights
I1026 00:45:22.822976 17176 net.cpp:150] Setting up rpn-data
I1026 00:45:22.822985 17176 net.cpp:157] Top shape: 1 1 351 64 (22464)
I1026 00:45:22.822988 17176 net.cpp:157] Top shape: 1 36 39 64 (89856)
I1026 00:45:22.822989 17176 net.cpp:157] Top shape: 1 36 39 64 (89856)
I1026 00:45:22.822991 17176 net.cpp:157] Top shape: 1 36 39 64 (89856)
I1026 00:45:22.822993 17176 net.cpp:165] Memory required for data: 273844636
I1026 00:45:22.822995 17176 layer_factory.hpp:77] Creating layer rpn_loss_cls
I1026 00:45:22.823000 17176 net.cpp:106] Creating Layer rpn_loss_cls
I1026 00:45:22.823002 17176 net.cpp:454] rpn_loss_cls <- rpn_cls_score_reshape
I1026 00:45:22.823015 17176 net.cpp:454] rpn_loss_cls <- rpn_labels
I1026 00:45:22.823019 17176 net.cpp:411] rpn_loss_cls -> rpn_cls_loss
I1026 00:45:22.823040 17176 layer_factory.hpp:77] Creating layer rpn_loss_cls
I1026 00:45:22.823281 17176 net.cpp:150] Setting up rpn_loss_cls
I1026 00:45:22.823287 17176 net.cpp:157] Top shape: (1)
I1026 00:45:22.823288 17176 net.cpp:160]     with loss weight 1
I1026 00:45:22.823318 17176 net.cpp:165] Memory required for data: 273844640
I1026 00:45:22.823320 17176 layer_factory.hpp:77] Creating layer rpn_loss_bbox
I1026 00:45:22.823328 17176 net.cpp:106] Creating Layer rpn_loss_bbox
I1026 00:45:22.823331 17176 net.cpp:454] rpn_loss_bbox <- rpn_bbox_pred
I1026 00:45:22.823335 17176 net.cpp:454] rpn_loss_bbox <- rpn_bbox_targets
I1026 00:45:22.823349 17176 net.cpp:454] rpn_loss_bbox <- rpn_bbox_inside_weights
I1026 00:45:22.823354 17176 net.cpp:454] rpn_loss_bbox <- rpn_bbox_outside_weights
I1026 00:45:22.823361 17176 net.cpp:411] rpn_loss_bbox -> rpn_loss_bbox
I1026 00:45:22.823855 17176 net.cpp:150] Setting up rpn_loss_bbox
I1026 00:45:22.823863 17176 net.cpp:157] Top shape: (1)
I1026 00:45:22.823865 17176 net.cpp:160]     with loss weight 1
I1026 00:45:22.823869 17176 net.cpp:165] Memory required for data: 273844644
I1026 00:45:22.823882 17176 layer_factory.hpp:77] Creating layer dummy_roi_pool_conv5
I1026 00:45:22.823892 17176 net.cpp:106] Creating Layer dummy_roi_pool_conv5
I1026 00:45:22.823897 17176 net.cpp:411] dummy_roi_pool_conv5 -> dummy_roi_pool_conv5
I1026 00:45:22.823921 17176 net.cpp:150] Setting up dummy_roi_pool_conv5
I1026 00:45:22.823925 17176 net.cpp:157] Top shape: 1 9216 (9216)
I1026 00:45:22.823928 17176 net.cpp:165] Memory required for data: 273881508
I1026 00:45:22.823931 17176 layer_factory.hpp:77] Creating layer fc6
I1026 00:45:22.823938 17176 net.cpp:106] Creating Layer fc6
I1026 00:45:22.823941 17176 net.cpp:454] fc6 <- dummy_roi_pool_conv5
I1026 00:45:22.823946 17176 net.cpp:411] fc6 -> fc6
I1026 00:45:22.869717 17176 net.cpp:150] Setting up fc6
I1026 00:45:22.869741 17176 net.cpp:157] Top shape: 1 4096 (4096)
I1026 00:45:22.869743 17176 net.cpp:165] Memory required for data: 273897892
I1026 00:45:22.869755 17176 layer_factory.hpp:77] Creating layer relu6
I1026 00:45:22.869763 17176 net.cpp:106] Creating Layer relu6
I1026 00:45:22.869767 17176 net.cpp:454] relu6 <- fc6
I1026 00:45:22.869786 17176 net.cpp:397] relu6 -> fc6 (in-place)
I1026 00:45:22.870144 17176 net.cpp:150] Setting up relu6
I1026 00:45:22.870152 17176 net.cpp:157] Top shape: 1 4096 (4096)
I1026 00:45:22.870164 17176 net.cpp:165] Memory required for data: 273914276
I1026 00:45:22.870167 17176 layer_factory.hpp:77] Creating layer fc7
I1026 00:45:22.870170 17176 net.cpp:106] Creating Layer fc7
I1026 00:45:22.870173 17176 net.cpp:454] fc7 <- fc6
I1026 00:45:22.870179 17176 net.cpp:411] fc7 -> fc7
I1026 00:45:22.890442 17176 net.cpp:150] Setting up fc7
I1026 00:45:22.890466 17176 net.cpp:157] Top shape: 1 4096 (4096)
I1026 00:45:22.890468 17176 net.cpp:165] Memory required for data: 273930660
I1026 00:45:22.890476 17176 layer_factory.hpp:77] Creating layer silence_fc7
I1026 00:45:22.890485 17176 net.cpp:106] Creating Layer silence_fc7
I1026 00:45:22.890487 17176 net.cpp:454] silence_fc7 <- fc7
I1026 00:45:22.890494 17176 net.cpp:150] Setting up silence_fc7
I1026 00:45:22.890497 17176 net.cpp:165] Memory required for data: 273930660
I1026 00:45:22.890499 17176 net.cpp:228] silence_fc7 does not need backward computation.
I1026 00:45:22.890513 17176 net.cpp:228] fc7 does not need backward computation.
I1026 00:45:22.890517 17176 net.cpp:228] relu6 does not need backward computation.
I1026 00:45:22.890519 17176 net.cpp:228] fc6 does not need backward computation.
I1026 00:45:22.890523 17176 net.cpp:228] dummy_roi_pool_conv5 does not need backward computation.
I1026 00:45:22.890537 17176 net.cpp:226] rpn_loss_bbox needs backward computation.
I1026 00:45:22.890542 17176 net.cpp:226] rpn_loss_cls needs backward computation.
I1026 00:45:22.890545 17176 net.cpp:226] rpn-data needs backward computation.
I1026 00:45:22.890549 17176 net.cpp:226] rpn_cls_score_reshape needs backward computation.
I1026 00:45:22.890552 17176 net.cpp:226] rpn_bbox_pred needs backward computation.
I1026 00:45:22.890554 17176 net.cpp:226] rpn_cls_score_rpn_cls_score_0_split needs backward computation.
I1026 00:45:22.890557 17176 net.cpp:226] rpn_cls_score needs backward computation.
I1026 00:45:22.890568 17176 net.cpp:226] rpn_conv1_rpn_relu1_0_split needs backward computation.
I1026 00:45:22.890569 17176 net.cpp:226] rpn_relu1 needs backward computation.
I1026 00:45:22.890571 17176 net.cpp:226] rpn_conv1 needs backward computation.
I1026 00:45:22.890573 17176 net.cpp:228] relu5 does not need backward computation.
I1026 00:45:22.890575 17176 net.cpp:228] conv5 does not need backward computation.
I1026 00:45:22.890588 17176 net.cpp:228] relu4 does not need backward computation.
I1026 00:45:22.890589 17176 net.cpp:228] conv4 does not need backward computation.
I1026 00:45:22.890591 17176 net.cpp:228] relu3 does not need backward computation.
I1026 00:45:22.890594 17176 net.cpp:228] conv3 does not need backward computation.
I1026 00:45:22.890599 17176 net.cpp:228] pool2 does not need backward computation.
I1026 00:45:22.890602 17176 net.cpp:228] norm2 does not need backward computation.
I1026 00:45:22.890615 17176 net.cpp:228] relu2 does not need backward computation.
I1026 00:45:22.890619 17176 net.cpp:228] conv2 does not need backward computation.
I1026 00:45:22.890621 17176 net.cpp:228] pool1 does not need backward computation.
I1026 00:45:22.890625 17176 net.cpp:228] norm1 does not need backward computation.
I1026 00:45:22.890630 17176 net.cpp:228] relu1 does not need backward computation.
I1026 00:45:22.890631 17176 net.cpp:228] conv1 does not need backward computation.
I1026 00:45:22.890636 17176 net.cpp:228] data_input-data_0_split does not need backward computation.
I1026 00:45:22.890640 17176 net.cpp:228] input-data does not need backward computation.
I1026 00:45:22.890643 17176 net.cpp:270] This network produces output rpn_cls_loss
I1026 00:45:22.890646 17176 net.cpp:270] This network produces output rpn_loss_bbox
I1026 00:45:22.890666 17176 net.cpp:283] Network initialization done.
I1026 00:45:22.890739 17176 solver.cpp:60] Solver scaffolding done.
I1026 00:45:22.964417 17176 net.cpp:816] Ignoring source layer data
I1026 00:45:22.966686 17176 net.cpp:816] Ignoring source layer conv5_relu5_0_split
I1026 00:45:22.966701 17176 net.cpp:816] Ignoring source layer roi_pool_conv5
I1026 00:45:22.988678 17176 net.cpp:816] Ignoring source layer drop6
I1026 00:45:22.998953 17176 net.cpp:816] Ignoring source layer relu7
I1026 00:45:22.998975 17176 net.cpp:816] Ignoring source layer drop7
I1026 00:45:22.998975 17176 net.cpp:816] Ignoring source layer fc7_drop7_0_split
I1026 00:45:22.998977 17176 net.cpp:816] Ignoring source layer cls_score
I1026 00:45:22.998978 17176 net.cpp:816] Ignoring source layer bbox_pred
I1026 00:45:22.998980 17176 net.cpp:816] Ignoring source layer loss_cls
I1026 00:45:22.998981 17176 net.cpp:816] Ignoring source layer loss_bbox
I1026 00:45:22.999419 17176 net.cpp:816] Ignoring source layer silence_rpn_cls_score
I1026 00:45:22.999423 17176 net.cpp:816] Ignoring source layer silence_rpn_bbox_pred
I1026 00:45:23.080972 17176 solver.cpp:229] Iteration 0, loss = 0.818309
I1026 00:45:23.081014 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.744215 (* 1 = 0.744215 loss)
I1026 00:45:23.081020 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0740938 (* 1 = 0.0740938 loss)
I1026 00:45:23.081025 17176 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I1026 00:45:23.643488 17176 solver.cpp:229] Iteration 20, loss = 0.492659
I1026 00:45:23.643522 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.117289 (* 1 = 0.117289 loss)
I1026 00:45:23.643525 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.37537 (* 1 = 0.37537 loss)
I1026 00:45:23.643529 17176 sgd_solver.cpp:106] Iteration 20, lr = 0.001
I1026 00:45:24.222760 17176 solver.cpp:229] Iteration 40, loss = 0.261651
I1026 00:45:24.222790 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.162215 (* 1 = 0.162215 loss)
I1026 00:45:24.222795 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0994354 (* 1 = 0.0994354 loss)
I1026 00:45:24.222800 17176 sgd_solver.cpp:106] Iteration 40, lr = 0.001
I1026 00:45:24.792270 17176 solver.cpp:229] Iteration 60, loss = 0.249771
I1026 00:45:24.792301 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.192884 (* 1 = 0.192884 loss)
I1026 00:45:24.792306 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0568862 (* 1 = 0.0568862 loss)
I1026 00:45:24.792311 17176 sgd_solver.cpp:106] Iteration 60, lr = 0.001
I1026 00:45:25.350450 17176 solver.cpp:229] Iteration 80, loss = 0.189582
I1026 00:45:25.350483 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.106289 (* 1 = 0.106289 loss)
I1026 00:45:25.350488 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0832936 (* 1 = 0.0832936 loss)
I1026 00:45:25.350492 17176 sgd_solver.cpp:106] Iteration 80, lr = 0.001
I1026 00:45:25.933181 17176 solver.cpp:229] Iteration 100, loss = 0.203186
I1026 00:45:25.933212 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.135971 (* 1 = 0.135971 loss)
I1026 00:45:25.933218 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0672145 (* 1 = 0.0672145 loss)
I1026 00:45:25.933221 17176 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I1026 00:45:26.504793 17176 solver.cpp:229] Iteration 120, loss = 0.129556
I1026 00:45:26.504825 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0872057 (* 1 = 0.0872057 loss)
I1026 00:45:26.504829 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0423503 (* 1 = 0.0423503 loss)
I1026 00:45:26.504833 17176 sgd_solver.cpp:106] Iteration 120, lr = 0.001
I1026 00:45:27.082183 17176 solver.cpp:229] Iteration 140, loss = 0.496144
I1026 00:45:27.082226 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.408759 (* 1 = 0.408759 loss)
I1026 00:45:27.082231 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0873847 (* 1 = 0.0873847 loss)
I1026 00:45:27.082234 17176 sgd_solver.cpp:106] Iteration 140, lr = 0.001
I1026 00:45:27.651187 17176 solver.cpp:229] Iteration 160, loss = 0.176022
I1026 00:45:27.651239 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.134723 (* 1 = 0.134723 loss)
I1026 00:45:27.651244 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0412992 (* 1 = 0.0412992 loss)
I1026 00:45:27.651248 17176 sgd_solver.cpp:106] Iteration 160, lr = 0.001
I1026 00:45:28.216320 17176 solver.cpp:229] Iteration 180, loss = 0.227719
I1026 00:45:28.216351 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.129108 (* 1 = 0.129108 loss)
I1026 00:45:28.216356 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0986108 (* 1 = 0.0986108 loss)
I1026 00:45:28.216359 17176 sgd_solver.cpp:106] Iteration 180, lr = 0.001
I1026 00:45:28.785347 17176 solver.cpp:229] Iteration 200, loss = 0.60342
I1026 00:45:28.785379 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.418232 (* 1 = 0.418232 loss)
I1026 00:45:28.785383 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.185189 (* 1 = 0.185189 loss)
I1026 00:45:28.785388 17176 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I1026 00:45:29.361804 17176 solver.cpp:229] Iteration 220, loss = 0.155566
I1026 00:45:29.361836 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.106486 (* 1 = 0.106486 loss)
I1026 00:45:29.361842 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0490802 (* 1 = 0.0490802 loss)
I1026 00:45:29.361846 17176 sgd_solver.cpp:106] Iteration 220, lr = 0.001
I1026 00:45:29.917930 17176 solver.cpp:229] Iteration 240, loss = 0.268181
I1026 00:45:29.917961 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.190788 (* 1 = 0.190788 loss)
I1026 00:45:29.917966 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0773929 (* 1 = 0.0773929 loss)
I1026 00:45:29.917970 17176 sgd_solver.cpp:106] Iteration 240, lr = 0.001
I1026 00:45:30.499336 17176 solver.cpp:229] Iteration 260, loss = 0.242958
I1026 00:45:30.499367 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.214257 (* 1 = 0.214257 loss)
I1026 00:45:30.499383 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0287007 (* 1 = 0.0287007 loss)
I1026 00:45:30.499387 17176 sgd_solver.cpp:106] Iteration 260, lr = 0.001
I1026 00:45:31.066700 17176 solver.cpp:229] Iteration 280, loss = 0.432724
I1026 00:45:31.066732 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.249507 (* 1 = 0.249507 loss)
I1026 00:45:31.066736 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.183217 (* 1 = 0.183217 loss)
I1026 00:45:31.066740 17176 sgd_solver.cpp:106] Iteration 280, lr = 0.001
I1026 00:45:31.630133 17176 solver.cpp:229] Iteration 300, loss = 0.201923
I1026 00:45:31.630179 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.145111 (* 1 = 0.145111 loss)
I1026 00:45:31.630194 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0568118 (* 1 = 0.0568118 loss)
I1026 00:45:31.630198 17176 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I1026 00:45:32.189041 17176 solver.cpp:229] Iteration 320, loss = 0.0624957
I1026 00:45:32.189074 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0501624 (* 1 = 0.0501624 loss)
I1026 00:45:32.189077 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0123333 (* 1 = 0.0123333 loss)
I1026 00:45:32.189082 17176 sgd_solver.cpp:106] Iteration 320, lr = 0.001
I1026 00:45:32.764608 17176 solver.cpp:229] Iteration 340, loss = 0.403326
I1026 00:45:32.764642 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0465357 (* 1 = 0.0465357 loss)
I1026 00:45:32.764647 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.35679 (* 1 = 0.35679 loss)
I1026 00:45:32.764652 17176 sgd_solver.cpp:106] Iteration 340, lr = 0.001
I1026 00:45:33.337038 17176 solver.cpp:229] Iteration 360, loss = 0.194328
I1026 00:45:33.337069 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.131481 (* 1 = 0.131481 loss)
I1026 00:45:33.337075 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0628468 (* 1 = 0.0628468 loss)
I1026 00:45:33.337080 17176 sgd_solver.cpp:106] Iteration 360, lr = 0.001
I1026 00:45:33.905807 17176 solver.cpp:229] Iteration 380, loss = 0.260454
I1026 00:45:33.905848 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.21711 (* 1 = 0.21711 loss)
I1026 00:45:33.905853 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0433434 (* 1 = 0.0433434 loss)
I1026 00:45:33.905858 17176 sgd_solver.cpp:106] Iteration 380, lr = 0.001
I1026 00:45:34.487510 17176 solver.cpp:229] Iteration 400, loss = 0.226725
I1026 00:45:34.487543 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0906654 (* 1 = 0.0906654 loss)
I1026 00:45:34.487548 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.13606 (* 1 = 0.13606 loss)
I1026 00:45:34.487552 17176 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I1026 00:45:35.071584 17176 solver.cpp:229] Iteration 420, loss = 0.197191
I1026 00:45:35.071616 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.186631 (* 1 = 0.186631 loss)
I1026 00:45:35.071621 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0105597 (* 1 = 0.0105597 loss)
I1026 00:45:35.071625 17176 sgd_solver.cpp:106] Iteration 420, lr = 0.001
I1026 00:45:35.625519 17176 solver.cpp:229] Iteration 440, loss = 0.124026
I1026 00:45:35.625550 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0772049 (* 1 = 0.0772049 loss)
I1026 00:45:35.625555 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0468213 (* 1 = 0.0468213 loss)
I1026 00:45:35.625560 17176 sgd_solver.cpp:106] Iteration 440, lr = 0.001
I1026 00:45:36.182549 17176 solver.cpp:229] Iteration 460, loss = 0.137195
I1026 00:45:36.182592 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.116078 (* 1 = 0.116078 loss)
I1026 00:45:36.182597 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0211177 (* 1 = 0.0211177 loss)
I1026 00:45:36.182610 17176 sgd_solver.cpp:106] Iteration 460, lr = 0.001
I1026 00:45:36.749457 17176 solver.cpp:229] Iteration 480, loss = 0.331535
I1026 00:45:36.749490 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.287209 (* 1 = 0.287209 loss)
I1026 00:45:36.749495 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0443256 (* 1 = 0.0443256 loss)
I1026 00:45:36.749498 17176 sgd_solver.cpp:106] Iteration 480, lr = 0.001
I1026 00:45:37.315008 17176 solver.cpp:229] Iteration 500, loss = 0.097418
I1026 00:45:37.315038 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0731234 (* 1 = 0.0731234 loss)
I1026 00:45:37.315043 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0242946 (* 1 = 0.0242946 loss)
I1026 00:45:37.315047 17176 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I1026 00:45:37.884603 17176 solver.cpp:229] Iteration 520, loss = 0.076075
I1026 00:45:37.884635 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0656986 (* 1 = 0.0656986 loss)
I1026 00:45:37.884640 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0103764 (* 1 = 0.0103764 loss)
I1026 00:45:37.884644 17176 sgd_solver.cpp:106] Iteration 520, lr = 0.001
I1026 00:45:38.434105 17176 solver.cpp:229] Iteration 540, loss = 0.110537
I1026 00:45:38.434137 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0844999 (* 1 = 0.0844999 loss)
I1026 00:45:38.434142 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0260376 (* 1 = 0.0260376 loss)
I1026 00:45:38.434147 17176 sgd_solver.cpp:106] Iteration 540, lr = 0.001
I1026 00:45:39.001893 17176 solver.cpp:229] Iteration 560, loss = 0.232502
I1026 00:45:39.001934 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.193633 (* 1 = 0.193633 loss)
I1026 00:45:39.001940 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0388686 (* 1 = 0.0388686 loss)
I1026 00:45:39.001945 17176 sgd_solver.cpp:106] Iteration 560, lr = 0.001
I1026 00:45:39.566737 17176 solver.cpp:229] Iteration 580, loss = 0.208901
I1026 00:45:39.566768 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.178484 (* 1 = 0.178484 loss)
I1026 00:45:39.566773 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0304174 (* 1 = 0.0304174 loss)
I1026 00:45:39.566777 17176 sgd_solver.cpp:106] Iteration 580, lr = 0.001
I1026 00:45:40.132609 17176 solver.cpp:229] Iteration 600, loss = 0.164174
I1026 00:45:40.132640 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0955526 (* 1 = 0.0955526 loss)
I1026 00:45:40.132645 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0686214 (* 1 = 0.0686214 loss)
I1026 00:45:40.132649 17176 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I1026 00:45:40.703609 17176 solver.cpp:229] Iteration 620, loss = 0.185752
I1026 00:45:40.703652 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0860111 (* 1 = 0.0860111 loss)
I1026 00:45:40.703657 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.099741 (* 1 = 0.099741 loss)
I1026 00:45:40.703661 17176 sgd_solver.cpp:106] Iteration 620, lr = 0.001
I1026 00:45:41.266572 17176 solver.cpp:229] Iteration 640, loss = 0.115274
I1026 00:45:41.266605 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.109911 (* 1 = 0.109911 loss)
I1026 00:45:41.266611 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0053631 (* 1 = 0.0053631 loss)
I1026 00:45:41.266615 17176 sgd_solver.cpp:106] Iteration 640, lr = 0.001
I1026 00:45:41.829488 17176 solver.cpp:229] Iteration 660, loss = 0.146757
I1026 00:45:41.829521 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0932165 (* 1 = 0.0932165 loss)
I1026 00:45:41.829525 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0535402 (* 1 = 0.0535402 loss)
I1026 00:45:41.829530 17176 sgd_solver.cpp:106] Iteration 660, lr = 0.001
I1026 00:45:42.397733 17176 solver.cpp:229] Iteration 680, loss = 0.138968
I1026 00:45:42.397765 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.106227 (* 1 = 0.106227 loss)
I1026 00:45:42.397771 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0327412 (* 1 = 0.0327412 loss)
I1026 00:45:42.397775 17176 sgd_solver.cpp:106] Iteration 680, lr = 0.001
I1026 00:45:42.964571 17176 solver.cpp:229] Iteration 700, loss = 0.162261
I1026 00:45:42.964603 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.148185 (* 1 = 0.148185 loss)
I1026 00:45:42.964608 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0140758 (* 1 = 0.0140758 loss)
I1026 00:45:42.964612 17176 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I1026 00:45:43.537570 17176 solver.cpp:229] Iteration 720, loss = 0.265129
I1026 00:45:43.537603 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.177554 (* 1 = 0.177554 loss)
I1026 00:45:43.537608 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0875756 (* 1 = 0.0875756 loss)
I1026 00:45:43.537612 17176 sgd_solver.cpp:106] Iteration 720, lr = 0.001
I1026 00:45:44.104688 17176 solver.cpp:229] Iteration 740, loss = 0.187756
I1026 00:45:44.104720 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.175039 (* 1 = 0.175039 loss)
I1026 00:45:44.104724 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0127171 (* 1 = 0.0127171 loss)
I1026 00:45:44.104728 17176 sgd_solver.cpp:106] Iteration 740, lr = 0.001
I1026 00:45:44.674052 17176 solver.cpp:229] Iteration 760, loss = 0.840583
I1026 00:45:44.674084 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.436942 (* 1 = 0.436942 loss)
I1026 00:45:44.674089 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.403641 (* 1 = 0.403641 loss)
I1026 00:45:44.674093 17176 sgd_solver.cpp:106] Iteration 760, lr = 0.001
I1026 00:45:45.245142 17176 solver.cpp:229] Iteration 780, loss = 0.320208
I1026 00:45:45.245172 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.28336 (* 1 = 0.28336 loss)
I1026 00:45:45.245177 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0368484 (* 1 = 0.0368484 loss)
I1026 00:45:45.245180 17176 sgd_solver.cpp:106] Iteration 780, lr = 0.001
I1026 00:45:45.806079 17176 solver.cpp:229] Iteration 800, loss = 0.806042
I1026 00:45:45.806113 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.380394 (* 1 = 0.380394 loss)
I1026 00:45:45.806118 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.425648 (* 1 = 0.425648 loss)
I1026 00:45:45.806121 17176 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I1026 00:45:46.384217 17176 solver.cpp:229] Iteration 820, loss = 0.304524
I1026 00:45:46.384248 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.22415 (* 1 = 0.22415 loss)
I1026 00:45:46.384253 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0803742 (* 1 = 0.0803742 loss)
I1026 00:45:46.384258 17176 sgd_solver.cpp:106] Iteration 820, lr = 0.001
I1026 00:45:46.957007 17176 solver.cpp:229] Iteration 840, loss = 0.0819363
I1026 00:45:46.957039 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0728067 (* 1 = 0.0728067 loss)
I1026 00:45:46.957061 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00912956 (* 1 = 0.00912956 loss)
I1026 00:45:46.957064 17176 sgd_solver.cpp:106] Iteration 840, lr = 0.001
I1026 00:45:47.523476 17176 solver.cpp:229] Iteration 860, loss = 0.259213
I1026 00:45:47.523510 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0815873 (* 1 = 0.0815873 loss)
I1026 00:45:47.523514 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.177626 (* 1 = 0.177626 loss)
I1026 00:45:47.523519 17176 sgd_solver.cpp:106] Iteration 860, lr = 0.001
I1026 00:45:48.089623 17176 solver.cpp:229] Iteration 880, loss = 0.101748
I1026 00:45:48.089656 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0756967 (* 1 = 0.0756967 loss)
I1026 00:45:48.089660 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0260509 (* 1 = 0.0260509 loss)
I1026 00:45:48.089664 17176 sgd_solver.cpp:106] Iteration 880, lr = 0.001
I1026 00:45:48.666456 17176 solver.cpp:229] Iteration 900, loss = 0.212977
I1026 00:45:48.666491 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.135629 (* 1 = 0.135629 loss)
I1026 00:45:48.666496 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0773481 (* 1 = 0.0773481 loss)
I1026 00:45:48.666501 17176 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I1026 00:45:49.217473 17176 solver.cpp:229] Iteration 920, loss = 0.0617297
I1026 00:45:49.217505 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0554135 (* 1 = 0.0554135 loss)
I1026 00:45:49.217510 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00631624 (* 1 = 0.00631624 loss)
I1026 00:45:49.217514 17176 sgd_solver.cpp:106] Iteration 920, lr = 0.001
I1026 00:45:49.798468 17176 solver.cpp:229] Iteration 940, loss = 0.174349
I1026 00:45:49.798501 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.162673 (* 1 = 0.162673 loss)
I1026 00:45:49.798506 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0116756 (* 1 = 0.0116756 loss)
I1026 00:45:49.798511 17176 sgd_solver.cpp:106] Iteration 940, lr = 0.001
I1026 00:45:50.351311 17176 solver.cpp:229] Iteration 960, loss = 0.210946
I1026 00:45:50.351344 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0982125 (* 1 = 0.0982125 loss)
I1026 00:45:50.351348 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.112733 (* 1 = 0.112733 loss)
I1026 00:45:50.351353 17176 sgd_solver.cpp:106] Iteration 960, lr = 0.001
I1026 00:45:50.912591 17176 solver.cpp:229] Iteration 980, loss = 0.0748269
I1026 00:45:50.912634 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0570734 (* 1 = 0.0570734 loss)
I1026 00:45:50.912639 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0177535 (* 1 = 0.0177535 loss)
I1026 00:45:50.912643 17176 sgd_solver.cpp:106] Iteration 980, lr = 0.001
I1026 00:45:51.486438 17176 solver.cpp:229] Iteration 1000, loss = 0.0639078
I1026 00:45:51.486471 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0598935 (* 1 = 0.0598935 loss)
I1026 00:45:51.486479 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00401431 (* 1 = 0.00401431 loss)
I1026 00:45:51.486484 17176 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I1026 00:45:52.042573 17176 solver.cpp:229] Iteration 1020, loss = 0.0776573
I1026 00:45:52.042608 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0714629 (* 1 = 0.0714629 loss)
I1026 00:45:52.042614 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00619434 (* 1 = 0.00619434 loss)
I1026 00:45:52.042620 17176 sgd_solver.cpp:106] Iteration 1020, lr = 0.001
I1026 00:45:52.611376 17176 solver.cpp:229] Iteration 1040, loss = 0.152215
I1026 00:45:52.611408 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0948189 (* 1 = 0.0948189 loss)
I1026 00:45:52.611415 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0573965 (* 1 = 0.0573965 loss)
I1026 00:45:52.611421 17176 sgd_solver.cpp:106] Iteration 1040, lr = 0.001
I1026 00:45:53.164052 17176 solver.cpp:229] Iteration 1060, loss = 0.310007
I1026 00:45:53.164084 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.222919 (* 1 = 0.222919 loss)
I1026 00:45:53.164089 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0870877 (* 1 = 0.0870877 loss)
I1026 00:45:53.164093 17176 sgd_solver.cpp:106] Iteration 1060, lr = 0.001
I1026 00:45:53.730612 17176 solver.cpp:229] Iteration 1080, loss = 0.0608259
I1026 00:45:53.730643 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0449685 (* 1 = 0.0449685 loss)
I1026 00:45:53.730648 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0158573 (* 1 = 0.0158573 loss)
I1026 00:45:53.730651 17176 sgd_solver.cpp:106] Iteration 1080, lr = 0.001
I1026 00:45:54.302243 17176 solver.cpp:229] Iteration 1100, loss = 0.204587
I1026 00:45:54.302284 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.190074 (* 1 = 0.190074 loss)
I1026 00:45:54.302290 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0145127 (* 1 = 0.0145127 loss)
I1026 00:45:54.302294 17176 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I1026 00:45:54.852356 17176 solver.cpp:229] Iteration 1120, loss = 0.0815925
I1026 00:45:54.852388 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0768771 (* 1 = 0.0768771 loss)
I1026 00:45:54.852393 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00471537 (* 1 = 0.00471537 loss)
I1026 00:45:54.852396 17176 sgd_solver.cpp:106] Iteration 1120, lr = 0.001
I1026 00:45:55.425282 17176 solver.cpp:229] Iteration 1140, loss = 0.0819108
I1026 00:45:55.425314 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0643612 (* 1 = 0.0643612 loss)
I1026 00:45:55.425318 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0175497 (* 1 = 0.0175497 loss)
I1026 00:45:55.425323 17176 sgd_solver.cpp:106] Iteration 1140, lr = 0.001
I1026 00:45:55.985450 17176 solver.cpp:229] Iteration 1160, loss = 0.152542
I1026 00:45:55.985481 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.120841 (* 1 = 0.120841 loss)
I1026 00:45:55.985486 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0317011 (* 1 = 0.0317011 loss)
I1026 00:45:55.985491 17176 sgd_solver.cpp:106] Iteration 1160, lr = 0.001
I1026 00:45:56.549688 17176 solver.cpp:229] Iteration 1180, loss = 0.105785
I1026 00:45:56.549721 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0674972 (* 1 = 0.0674972 loss)
I1026 00:45:56.549724 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0382879 (* 1 = 0.0382879 loss)
I1026 00:45:56.549728 17176 sgd_solver.cpp:106] Iteration 1180, lr = 0.001
I1026 00:45:57.123302 17176 solver.cpp:229] Iteration 1200, loss = 0.188662
I1026 00:45:57.123334 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.137528 (* 1 = 0.137528 loss)
I1026 00:45:57.123339 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0511341 (* 1 = 0.0511341 loss)
I1026 00:45:57.123343 17176 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I1026 00:45:57.694437 17176 solver.cpp:229] Iteration 1220, loss = 0.180176
I1026 00:45:57.694469 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.103637 (* 1 = 0.103637 loss)
I1026 00:45:57.694474 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0765385 (* 1 = 0.0765385 loss)
I1026 00:45:57.694478 17176 sgd_solver.cpp:106] Iteration 1220, lr = 0.001
I1026 00:45:58.280009 17176 solver.cpp:229] Iteration 1240, loss = 0.16704
I1026 00:45:58.280042 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.150073 (* 1 = 0.150073 loss)
I1026 00:45:58.280048 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0169666 (* 1 = 0.0169666 loss)
I1026 00:45:58.280052 17176 sgd_solver.cpp:106] Iteration 1240, lr = 0.001
I1026 00:45:58.868504 17176 solver.cpp:229] Iteration 1260, loss = 0.130159
I1026 00:45:58.868535 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.106748 (* 1 = 0.106748 loss)
I1026 00:45:58.868541 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0234105 (* 1 = 0.0234105 loss)
I1026 00:45:58.868544 17176 sgd_solver.cpp:106] Iteration 1260, lr = 0.001
I1026 00:45:59.436264 17176 solver.cpp:229] Iteration 1280, loss = 0.119035
I1026 00:45:59.436296 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0984022 (* 1 = 0.0984022 loss)
I1026 00:45:59.436301 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0206332 (* 1 = 0.0206332 loss)
I1026 00:45:59.436305 17176 sgd_solver.cpp:106] Iteration 1280, lr = 0.001
I1026 00:46:00.000833 17176 solver.cpp:229] Iteration 1300, loss = 0.0826318
I1026 00:46:00.000864 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0465963 (* 1 = 0.0465963 loss)
I1026 00:46:00.000872 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0360355 (* 1 = 0.0360355 loss)
I1026 00:46:00.000879 17176 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I1026 00:46:00.581416 17176 solver.cpp:229] Iteration 1320, loss = 0.215744
I1026 00:46:00.581449 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.158197 (* 1 = 0.158197 loss)
I1026 00:46:00.581454 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0575467 (* 1 = 0.0575467 loss)
I1026 00:46:00.581459 17176 sgd_solver.cpp:106] Iteration 1320, lr = 0.001
I1026 00:46:01.142467 17176 solver.cpp:229] Iteration 1340, loss = 0.237565
I1026 00:46:01.142499 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.209483 (* 1 = 0.209483 loss)
I1026 00:46:01.142504 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0280815 (* 1 = 0.0280815 loss)
I1026 00:46:01.142508 17176 sgd_solver.cpp:106] Iteration 1340, lr = 0.001
I1026 00:46:01.717104 17176 solver.cpp:229] Iteration 1360, loss = 0.439164
I1026 00:46:01.717136 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0697385 (* 1 = 0.0697385 loss)
I1026 00:46:01.717141 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.369425 (* 1 = 0.369425 loss)
I1026 00:46:01.717145 17176 sgd_solver.cpp:106] Iteration 1360, lr = 0.001
I1026 00:46:02.291525 17176 solver.cpp:229] Iteration 1380, loss = 0.165811
I1026 00:46:02.291558 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0799786 (* 1 = 0.0799786 loss)
I1026 00:46:02.291563 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0858324 (* 1 = 0.0858324 loss)
I1026 00:46:02.291568 17176 sgd_solver.cpp:106] Iteration 1380, lr = 0.001
I1026 00:46:02.878412 17176 solver.cpp:229] Iteration 1400, loss = 0.0808391
I1026 00:46:02.878444 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0667601 (* 1 = 0.0667601 loss)
I1026 00:46:02.878449 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.014079 (* 1 = 0.014079 loss)
I1026 00:46:02.878453 17176 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I1026 00:46:03.464325 17176 solver.cpp:229] Iteration 1420, loss = 0.113059
I1026 00:46:03.464359 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0703691 (* 1 = 0.0703691 loss)
I1026 00:46:03.464364 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0426904 (* 1 = 0.0426904 loss)
I1026 00:46:03.464367 17176 sgd_solver.cpp:106] Iteration 1420, lr = 0.001
I1026 00:46:04.028859 17176 solver.cpp:229] Iteration 1440, loss = 0.627766
I1026 00:46:04.028892 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.332142 (* 1 = 0.332142 loss)
I1026 00:46:04.028897 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.295624 (* 1 = 0.295624 loss)
I1026 00:46:04.028900 17176 sgd_solver.cpp:106] Iteration 1440, lr = 0.001
I1026 00:46:04.581719 17176 solver.cpp:229] Iteration 1460, loss = 0.13872
I1026 00:46:04.581753 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.102907 (* 1 = 0.102907 loss)
I1026 00:46:04.581758 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0358131 (* 1 = 0.0358131 loss)
I1026 00:46:04.581761 17176 sgd_solver.cpp:106] Iteration 1460, lr = 0.001
I1026 00:46:05.152086 17176 solver.cpp:229] Iteration 1480, loss = 0.224593
I1026 00:46:05.152118 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0663927 (* 1 = 0.0663927 loss)
I1026 00:46:05.152122 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.1582 (* 1 = 0.1582 loss)
I1026 00:46:05.152127 17176 sgd_solver.cpp:106] Iteration 1480, lr = 0.001
I1026 00:46:05.736073 17176 solver.cpp:229] Iteration 1500, loss = 0.115197
I1026 00:46:05.736106 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.105587 (* 1 = 0.105587 loss)
I1026 00:46:05.736111 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00960957 (* 1 = 0.00960957 loss)
I1026 00:46:05.736115 17176 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I1026 00:46:06.296871 17176 solver.cpp:229] Iteration 1520, loss = 0.189169
I1026 00:46:06.296905 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.101766 (* 1 = 0.101766 loss)
I1026 00:46:06.296910 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0874028 (* 1 = 0.0874028 loss)
I1026 00:46:06.296914 17176 sgd_solver.cpp:106] Iteration 1520, lr = 0.001
I1026 00:46:06.862749 17176 solver.cpp:229] Iteration 1540, loss = 0.335955
I1026 00:46:06.862782 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.206622 (* 1 = 0.206622 loss)
I1026 00:46:06.862787 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.129333 (* 1 = 0.129333 loss)
I1026 00:46:06.862792 17176 sgd_solver.cpp:106] Iteration 1540, lr = 0.001
I1026 00:46:07.438441 17176 solver.cpp:229] Iteration 1560, loss = 0.0599101
I1026 00:46:07.438472 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0578042 (* 1 = 0.0578042 loss)
I1026 00:46:07.438477 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00210595 (* 1 = 0.00210595 loss)
I1026 00:46:07.438482 17176 sgd_solver.cpp:106] Iteration 1560, lr = 0.001
I1026 00:46:08.017248 17176 solver.cpp:229] Iteration 1580, loss = 0.0284266
I1026 00:46:08.017282 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0256884 (* 1 = 0.0256884 loss)
I1026 00:46:08.017287 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00273819 (* 1 = 0.00273819 loss)
I1026 00:46:08.017289 17176 sgd_solver.cpp:106] Iteration 1580, lr = 0.001
I1026 00:46:08.587586 17176 solver.cpp:229] Iteration 1600, loss = 0.109448
I1026 00:46:08.587620 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.102971 (* 1 = 0.102971 loss)
I1026 00:46:08.587625 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00647723 (* 1 = 0.00647723 loss)
I1026 00:46:08.587627 17176 sgd_solver.cpp:106] Iteration 1600, lr = 0.001
I1026 00:46:09.152366 17176 solver.cpp:229] Iteration 1620, loss = 0.398138
I1026 00:46:09.152398 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0597774 (* 1 = 0.0597774 loss)
I1026 00:46:09.152403 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.338361 (* 1 = 0.338361 loss)
I1026 00:46:09.152407 17176 sgd_solver.cpp:106] Iteration 1620, lr = 0.001
I1026 00:46:09.733937 17176 solver.cpp:229] Iteration 1640, loss = 0.0735744
I1026 00:46:09.733968 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0506047 (* 1 = 0.0506047 loss)
I1026 00:46:09.733973 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0229697 (* 1 = 0.0229697 loss)
I1026 00:46:09.733978 17176 sgd_solver.cpp:106] Iteration 1640, lr = 0.001
I1026 00:46:10.298897 17176 solver.cpp:229] Iteration 1660, loss = 0.29544
I1026 00:46:10.298928 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.156094 (* 1 = 0.156094 loss)
I1026 00:46:10.298933 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.139346 (* 1 = 0.139346 loss)
I1026 00:46:10.298938 17176 sgd_solver.cpp:106] Iteration 1660, lr = 0.001
I1026 00:46:10.868142 17176 solver.cpp:229] Iteration 1680, loss = 0.119441
I1026 00:46:10.868175 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0646693 (* 1 = 0.0646693 loss)
I1026 00:46:10.868178 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0547721 (* 1 = 0.0547721 loss)
I1026 00:46:10.868183 17176 sgd_solver.cpp:106] Iteration 1680, lr = 0.001
I1026 00:46:11.438382 17176 solver.cpp:229] Iteration 1700, loss = 0.242819
I1026 00:46:11.438416 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.143034 (* 1 = 0.143034 loss)
I1026 00:46:11.438421 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0997857 (* 1 = 0.0997857 loss)
I1026 00:46:11.438424 17176 sgd_solver.cpp:106] Iteration 1700, lr = 0.001
I1026 00:46:12.027479 17176 solver.cpp:229] Iteration 1720, loss = 0.231691
I1026 00:46:12.027511 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.139262 (* 1 = 0.139262 loss)
I1026 00:46:12.027516 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0924289 (* 1 = 0.0924289 loss)
I1026 00:46:12.027521 17176 sgd_solver.cpp:106] Iteration 1720, lr = 0.001
I1026 00:46:12.588258 17176 solver.cpp:229] Iteration 1740, loss = 1.00765
I1026 00:46:12.588290 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.499586 (* 1 = 0.499586 loss)
I1026 00:46:12.588295 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.50806 (* 1 = 0.50806 loss)
I1026 00:46:12.588299 17176 sgd_solver.cpp:106] Iteration 1740, lr = 0.001
I1026 00:46:13.165372 17176 solver.cpp:229] Iteration 1760, loss = 0.272643
I1026 00:46:13.165403 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.193544 (* 1 = 0.193544 loss)
I1026 00:46:13.165408 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0790991 (* 1 = 0.0790991 loss)
I1026 00:46:13.165412 17176 sgd_solver.cpp:106] Iteration 1760, lr = 0.001
I1026 00:46:13.740490 17176 solver.cpp:229] Iteration 1780, loss = 0.09553
I1026 00:46:13.740523 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0739051 (* 1 = 0.0739051 loss)
I1026 00:46:13.740543 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0216248 (* 1 = 0.0216248 loss)
I1026 00:46:13.740548 17176 sgd_solver.cpp:106] Iteration 1780, lr = 0.001
I1026 00:46:14.314056 17176 solver.cpp:229] Iteration 1800, loss = 0.416907
I1026 00:46:14.314088 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.271311 (* 1 = 0.271311 loss)
I1026 00:46:14.314093 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.145597 (* 1 = 0.145597 loss)
I1026 00:46:14.314096 17176 sgd_solver.cpp:106] Iteration 1800, lr = 0.001
I1026 00:46:14.892907 17176 solver.cpp:229] Iteration 1820, loss = 0.327239
I1026 00:46:14.892940 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.219233 (* 1 = 0.219233 loss)
I1026 00:46:14.892946 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.108006 (* 1 = 0.108006 loss)
I1026 00:46:14.892949 17176 sgd_solver.cpp:106] Iteration 1820, lr = 0.001
I1026 00:46:15.463845 17176 solver.cpp:229] Iteration 1840, loss = 0.196879
I1026 00:46:15.463876 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.177865 (* 1 = 0.177865 loss)
I1026 00:46:15.463881 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0190136 (* 1 = 0.0190136 loss)
I1026 00:46:15.463886 17176 sgd_solver.cpp:106] Iteration 1840, lr = 0.001
I1026 00:46:16.019304 17176 solver.cpp:229] Iteration 1860, loss = 0.0335138
I1026 00:46:16.019346 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0282062 (* 1 = 0.0282062 loss)
I1026 00:46:16.019351 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00530759 (* 1 = 0.00530759 loss)
I1026 00:46:16.019356 17176 sgd_solver.cpp:106] Iteration 1860, lr = 0.001
I1026 00:46:16.590653 17176 solver.cpp:229] Iteration 1880, loss = 0.0964727
I1026 00:46:16.590684 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0845403 (* 1 = 0.0845403 loss)
I1026 00:46:16.590689 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0119324 (* 1 = 0.0119324 loss)
I1026 00:46:16.590693 17176 sgd_solver.cpp:106] Iteration 1880, lr = 0.001
I1026 00:46:17.167628 17176 solver.cpp:229] Iteration 1900, loss = 0.162252
I1026 00:46:17.167659 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.152935 (* 1 = 0.152935 loss)
I1026 00:46:17.167665 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00931707 (* 1 = 0.00931707 loss)
I1026 00:46:17.167668 17176 sgd_solver.cpp:106] Iteration 1900, lr = 0.001
I1026 00:46:17.743739 17176 solver.cpp:229] Iteration 1920, loss = 0.0676603
I1026 00:46:17.743772 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.063534 (* 1 = 0.063534 loss)
I1026 00:46:17.743777 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00412639 (* 1 = 0.00412639 loss)
I1026 00:46:17.743780 17176 sgd_solver.cpp:106] Iteration 1920, lr = 0.001
I1026 00:46:18.314554 17176 solver.cpp:229] Iteration 1940, loss = 0.38774
I1026 00:46:18.314586 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.227755 (* 1 = 0.227755 loss)
I1026 00:46:18.314591 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.159984 (* 1 = 0.159984 loss)
I1026 00:46:18.314595 17176 sgd_solver.cpp:106] Iteration 1940, lr = 0.001
I1026 00:46:18.866899 17176 solver.cpp:229] Iteration 1960, loss = 1.10959
I1026 00:46:18.866931 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.742204 (* 1 = 0.742204 loss)
I1026 00:46:18.866936 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.367383 (* 1 = 0.367383 loss)
I1026 00:46:18.866940 17176 sgd_solver.cpp:106] Iteration 1960, lr = 0.001
I1026 00:46:19.432447 17176 solver.cpp:229] Iteration 1980, loss = 0.0675064
I1026 00:46:19.432478 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0353873 (* 1 = 0.0353873 loss)
I1026 00:46:19.432483 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0321191 (* 1 = 0.0321191 loss)
I1026 00:46:19.432487 17176 sgd_solver.cpp:106] Iteration 1980, lr = 0.001
I1026 00:46:20.001224 17176 solver.cpp:229] Iteration 2000, loss = 0.0887783
I1026 00:46:20.001255 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.086289 (* 1 = 0.086289 loss)
I1026 00:46:20.001260 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00248929 (* 1 = 0.00248929 loss)
I1026 00:46:20.001265 17176 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I1026 00:46:20.553647 17176 solver.cpp:229] Iteration 2020, loss = 0.129197
I1026 00:46:20.553689 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.122351 (* 1 = 0.122351 loss)
I1026 00:46:20.553694 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00684617 (* 1 = 0.00684617 loss)
I1026 00:46:20.553697 17176 sgd_solver.cpp:106] Iteration 2020, lr = 0.001
I1026 00:46:21.121995 17176 solver.cpp:229] Iteration 2040, loss = 0.113347
I1026 00:46:21.122028 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.102647 (* 1 = 0.102647 loss)
I1026 00:46:21.122033 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0107005 (* 1 = 0.0107005 loss)
I1026 00:46:21.122036 17176 sgd_solver.cpp:106] Iteration 2040, lr = 0.001
I1026 00:46:21.698806 17176 solver.cpp:229] Iteration 2060, loss = 0.103604
I1026 00:46:21.698837 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0659444 (* 1 = 0.0659444 loss)
I1026 00:46:21.698843 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0376594 (* 1 = 0.0376594 loss)
I1026 00:46:21.698846 17176 sgd_solver.cpp:106] Iteration 2060, lr = 0.001
I1026 00:46:22.279580 17176 solver.cpp:229] Iteration 2080, loss = 0.107319
I1026 00:46:22.279613 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0586649 (* 1 = 0.0586649 loss)
I1026 00:46:22.279618 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0486537 (* 1 = 0.0486537 loss)
I1026 00:46:22.279631 17176 sgd_solver.cpp:106] Iteration 2080, lr = 0.001
I1026 00:46:22.847998 17176 solver.cpp:229] Iteration 2100, loss = 0.127144
I1026 00:46:22.848031 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0545023 (* 1 = 0.0545023 loss)
I1026 00:46:22.848036 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0726421 (* 1 = 0.0726421 loss)
I1026 00:46:22.848039 17176 sgd_solver.cpp:106] Iteration 2100, lr = 0.001
I1026 00:46:23.406337 17176 solver.cpp:229] Iteration 2120, loss = 1.42742
I1026 00:46:23.406369 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.532251 (* 1 = 0.532251 loss)
I1026 00:46:23.406373 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.895166 (* 1 = 0.895166 loss)
I1026 00:46:23.406378 17176 sgd_solver.cpp:106] Iteration 2120, lr = 0.001
I1026 00:46:23.964792 17176 solver.cpp:229] Iteration 2140, loss = 0.296314
I1026 00:46:23.964823 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.277538 (* 1 = 0.277538 loss)
I1026 00:46:23.964828 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0187764 (* 1 = 0.0187764 loss)
I1026 00:46:23.964833 17176 sgd_solver.cpp:106] Iteration 2140, lr = 0.001
I1026 00:46:24.543787 17176 solver.cpp:229] Iteration 2160, loss = 0.209304
I1026 00:46:24.543819 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.172943 (* 1 = 0.172943 loss)
I1026 00:46:24.543824 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0363613 (* 1 = 0.0363613 loss)
I1026 00:46:24.543829 17176 sgd_solver.cpp:106] Iteration 2160, lr = 0.001
I1026 00:46:25.108918 17176 solver.cpp:229] Iteration 2180, loss = 0.0676848
I1026 00:46:25.108950 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0624695 (* 1 = 0.0624695 loss)
I1026 00:46:25.108955 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0052153 (* 1 = 0.0052153 loss)
I1026 00:46:25.108959 17176 sgd_solver.cpp:106] Iteration 2180, lr = 0.001
I1026 00:46:25.678654 17176 solver.cpp:229] Iteration 2200, loss = 0.200846
I1026 00:46:25.678689 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.096933 (* 1 = 0.096933 loss)
I1026 00:46:25.678694 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.103913 (* 1 = 0.103913 loss)
I1026 00:46:25.678697 17176 sgd_solver.cpp:106] Iteration 2200, lr = 0.001
I1026 00:46:26.247376 17176 solver.cpp:229] Iteration 2220, loss = 0.131724
I1026 00:46:26.247408 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0809012 (* 1 = 0.0809012 loss)
I1026 00:46:26.247413 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0508228 (* 1 = 0.0508228 loss)
I1026 00:46:26.247417 17176 sgd_solver.cpp:106] Iteration 2220, lr = 0.001
I1026 00:46:26.826797 17176 solver.cpp:229] Iteration 2240, loss = 0.156056
I1026 00:46:26.826829 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0528159 (* 1 = 0.0528159 loss)
I1026 00:46:26.826834 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.10324 (* 1 = 0.10324 loss)
I1026 00:46:26.826838 17176 sgd_solver.cpp:106] Iteration 2240, lr = 0.001
I1026 00:46:27.397702 17176 solver.cpp:229] Iteration 2260, loss = 0.140808
I1026 00:46:27.397734 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.105497 (* 1 = 0.105497 loss)
I1026 00:46:27.397739 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0353111 (* 1 = 0.0353111 loss)
I1026 00:46:27.397742 17176 sgd_solver.cpp:106] Iteration 2260, lr = 0.001
I1026 00:46:27.977717 17176 solver.cpp:229] Iteration 2280, loss = 0.41136
I1026 00:46:27.977759 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.362118 (* 1 = 0.362118 loss)
I1026 00:46:27.977764 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0492424 (* 1 = 0.0492424 loss)
I1026 00:46:27.977768 17176 sgd_solver.cpp:106] Iteration 2280, lr = 0.001
I1026 00:46:28.544206 17176 solver.cpp:229] Iteration 2300, loss = 0.0480338
I1026 00:46:28.544237 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.047029 (* 1 = 0.047029 loss)
I1026 00:46:28.544242 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00100483 (* 1 = 0.00100483 loss)
I1026 00:46:28.544247 17176 sgd_solver.cpp:106] Iteration 2300, lr = 0.001
I1026 00:46:29.117938 17176 solver.cpp:229] Iteration 2320, loss = 0.119978
I1026 00:46:29.117972 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0852136 (* 1 = 0.0852136 loss)
I1026 00:46:29.117979 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0347639 (* 1 = 0.0347639 loss)
I1026 00:46:29.117982 17176 sgd_solver.cpp:106] Iteration 2320, lr = 0.001
I1026 00:46:29.682879 17176 solver.cpp:229] Iteration 2340, loss = 0.112065
I1026 00:46:29.682911 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0801153 (* 1 = 0.0801153 loss)
I1026 00:46:29.682916 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0319493 (* 1 = 0.0319493 loss)
I1026 00:46:29.682920 17176 sgd_solver.cpp:106] Iteration 2340, lr = 0.001
I1026 00:46:30.245913 17176 solver.cpp:229] Iteration 2360, loss = 0.131656
I1026 00:46:30.245959 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0747734 (* 1 = 0.0747734 loss)
I1026 00:46:30.245965 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0568829 (* 1 = 0.0568829 loss)
I1026 00:46:30.245968 17176 sgd_solver.cpp:106] Iteration 2360, lr = 0.001
I1026 00:46:30.818704 17176 solver.cpp:229] Iteration 2380, loss = 0.622073
I1026 00:46:30.818747 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.338881 (* 1 = 0.338881 loss)
I1026 00:46:30.818750 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.283192 (* 1 = 0.283192 loss)
I1026 00:46:30.818754 17176 sgd_solver.cpp:106] Iteration 2380, lr = 0.001
I1026 00:46:31.389070 17176 solver.cpp:229] Iteration 2400, loss = 0.609251
I1026 00:46:31.389102 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.322947 (* 1 = 0.322947 loss)
I1026 00:46:31.389106 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.286305 (* 1 = 0.286305 loss)
I1026 00:46:31.389111 17176 sgd_solver.cpp:106] Iteration 2400, lr = 0.001
I1026 00:46:31.962098 17176 solver.cpp:229] Iteration 2420, loss = 0.273303
I1026 00:46:31.962141 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.128647 (* 1 = 0.128647 loss)
I1026 00:46:31.962144 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.144656 (* 1 = 0.144656 loss)
I1026 00:46:31.962149 17176 sgd_solver.cpp:106] Iteration 2420, lr = 0.001
I1026 00:46:32.523424 17176 solver.cpp:229] Iteration 2440, loss = 0.0402311
I1026 00:46:32.523463 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0344645 (* 1 = 0.0344645 loss)
I1026 00:46:32.523468 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00576664 (* 1 = 0.00576664 loss)
I1026 00:46:32.523470 17176 sgd_solver.cpp:106] Iteration 2440, lr = 0.001
I1026 00:46:33.100579 17176 solver.cpp:229] Iteration 2460, loss = 0.449756
I1026 00:46:33.100611 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.329402 (* 1 = 0.329402 loss)
I1026 00:46:33.100615 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.120354 (* 1 = 0.120354 loss)
I1026 00:46:33.100620 17176 sgd_solver.cpp:106] Iteration 2460, lr = 0.001
I1026 00:46:33.676275 17176 solver.cpp:229] Iteration 2480, loss = 0.0423357
I1026 00:46:33.676306 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0396604 (* 1 = 0.0396604 loss)
I1026 00:46:33.676311 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00267524 (* 1 = 0.00267524 loss)
I1026 00:46:33.676316 17176 sgd_solver.cpp:106] Iteration 2480, lr = 0.001
I1026 00:46:34.241266 17176 solver.cpp:229] Iteration 2500, loss = 0.156859
I1026 00:46:34.241295 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.126218 (* 1 = 0.126218 loss)
I1026 00:46:34.241300 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0306409 (* 1 = 0.0306409 loss)
I1026 00:46:34.241304 17176 sgd_solver.cpp:106] Iteration 2500, lr = 0.001
I1026 00:46:34.812315 17176 solver.cpp:229] Iteration 2520, loss = 0.206792
I1026 00:46:34.812346 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.126335 (* 1 = 0.126335 loss)
I1026 00:46:34.812351 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0804568 (* 1 = 0.0804568 loss)
I1026 00:46:34.812356 17176 sgd_solver.cpp:106] Iteration 2520, lr = 0.001
I1026 00:46:35.379516 17176 solver.cpp:229] Iteration 2540, loss = 0.149695
I1026 00:46:35.379559 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0691279 (* 1 = 0.0691279 loss)
I1026 00:46:35.379565 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0805667 (* 1 = 0.0805667 loss)
I1026 00:46:35.379568 17176 sgd_solver.cpp:106] Iteration 2540, lr = 0.001
I1026 00:46:35.944901 17176 solver.cpp:229] Iteration 2560, loss = 0.589566
I1026 00:46:35.944933 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.201931 (* 1 = 0.201931 loss)
I1026 00:46:35.944938 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.387635 (* 1 = 0.387635 loss)
I1026 00:46:35.944942 17176 sgd_solver.cpp:106] Iteration 2560, lr = 0.001
I1026 00:46:36.488288 17176 solver.cpp:229] Iteration 2580, loss = 0.207478
I1026 00:46:36.488319 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.120083 (* 1 = 0.120083 loss)
I1026 00:46:36.488324 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0873944 (* 1 = 0.0873944 loss)
I1026 00:46:36.488327 17176 sgd_solver.cpp:106] Iteration 2580, lr = 0.001
I1026 00:46:37.074192 17176 solver.cpp:229] Iteration 2600, loss = 0.278289
I1026 00:46:37.074223 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0894662 (* 1 = 0.0894662 loss)
I1026 00:46:37.074228 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.188823 (* 1 = 0.188823 loss)
I1026 00:46:37.074232 17176 sgd_solver.cpp:106] Iteration 2600, lr = 0.001
I1026 00:46:37.632282 17176 solver.cpp:229] Iteration 2620, loss = 0.0772807
I1026 00:46:37.632314 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0704819 (* 1 = 0.0704819 loss)
I1026 00:46:37.632319 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0067988 (* 1 = 0.0067988 loss)
I1026 00:46:37.632324 17176 sgd_solver.cpp:106] Iteration 2620, lr = 0.001
I1026 00:46:38.202522 17176 solver.cpp:229] Iteration 2640, loss = 0.127856
I1026 00:46:38.202555 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.123677 (* 1 = 0.123677 loss)
I1026 00:46:38.202561 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00417975 (* 1 = 0.00417975 loss)
I1026 00:46:38.202565 17176 sgd_solver.cpp:106] Iteration 2640, lr = 0.001
I1026 00:46:38.776677 17176 solver.cpp:229] Iteration 2660, loss = 0.0945974
I1026 00:46:38.776721 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0786098 (* 1 = 0.0786098 loss)
I1026 00:46:38.776726 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0159876 (* 1 = 0.0159876 loss)
I1026 00:46:38.776731 17176 sgd_solver.cpp:106] Iteration 2660, lr = 0.001
I1026 00:46:39.351022 17176 solver.cpp:229] Iteration 2680, loss = 0.329772
I1026 00:46:39.351055 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.179318 (* 1 = 0.179318 loss)
I1026 00:46:39.351059 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.150454 (* 1 = 0.150454 loss)
I1026 00:46:39.351063 17176 sgd_solver.cpp:106] Iteration 2680, lr = 0.001
I1026 00:46:39.926225 17176 solver.cpp:229] Iteration 2700, loss = 0.211354
I1026 00:46:39.926257 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.188398 (* 1 = 0.188398 loss)
I1026 00:46:39.926262 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0229564 (* 1 = 0.0229564 loss)
I1026 00:46:39.926267 17176 sgd_solver.cpp:106] Iteration 2700, lr = 0.001
I1026 00:46:40.489564 17176 solver.cpp:229] Iteration 2720, loss = 0.0887434
I1026 00:46:40.489596 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0353582 (* 1 = 0.0353582 loss)
I1026 00:46:40.489601 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0533852 (* 1 = 0.0533852 loss)
I1026 00:46:40.489605 17176 sgd_solver.cpp:106] Iteration 2720, lr = 0.001
I1026 00:46:41.050359 17176 solver.cpp:229] Iteration 2740, loss = 0.471098
I1026 00:46:41.050391 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.41479 (* 1 = 0.41479 loss)
I1026 00:46:41.050397 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0563082 (* 1 = 0.0563082 loss)
I1026 00:46:41.050401 17176 sgd_solver.cpp:106] Iteration 2740, lr = 0.001
I1026 00:46:41.617426 17176 solver.cpp:229] Iteration 2760, loss = 0.853858
I1026 00:46:41.617457 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.483577 (* 1 = 0.483577 loss)
I1026 00:46:41.617480 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.370281 (* 1 = 0.370281 loss)
I1026 00:46:41.617483 17176 sgd_solver.cpp:106] Iteration 2760, lr = 0.001
I1026 00:46:42.187666 17176 solver.cpp:229] Iteration 2780, loss = 0.0941586
I1026 00:46:42.187697 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0457827 (* 1 = 0.0457827 loss)
I1026 00:46:42.187702 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0483759 (* 1 = 0.0483759 loss)
I1026 00:46:42.187706 17176 sgd_solver.cpp:106] Iteration 2780, lr = 0.001
I1026 00:46:42.761737 17176 solver.cpp:229] Iteration 2800, loss = 0.169282
I1026 00:46:42.761770 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.11573 (* 1 = 0.11573 loss)
I1026 00:46:42.761773 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0535527 (* 1 = 0.0535527 loss)
I1026 00:46:42.761778 17176 sgd_solver.cpp:106] Iteration 2800, lr = 0.001
I1026 00:46:43.325481 17176 solver.cpp:229] Iteration 2820, loss = 0.335851
I1026 00:46:43.325515 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.230745 (* 1 = 0.230745 loss)
I1026 00:46:43.325518 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.105107 (* 1 = 0.105107 loss)
I1026 00:46:43.325522 17176 sgd_solver.cpp:106] Iteration 2820, lr = 0.001
I1026 00:46:43.885598 17176 solver.cpp:229] Iteration 2840, loss = 0.180629
I1026 00:46:43.885630 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.133111 (* 1 = 0.133111 loss)
I1026 00:46:43.885635 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0475178 (* 1 = 0.0475178 loss)
I1026 00:46:43.885638 17176 sgd_solver.cpp:106] Iteration 2840, lr = 0.001
I1026 00:46:44.464198 17176 solver.cpp:229] Iteration 2860, loss = 0.0680306
I1026 00:46:44.464231 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0467934 (* 1 = 0.0467934 loss)
I1026 00:46:44.464236 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0212372 (* 1 = 0.0212372 loss)
I1026 00:46:44.464241 17176 sgd_solver.cpp:106] Iteration 2860, lr = 0.001
I1026 00:46:45.019665 17176 solver.cpp:229] Iteration 2880, loss = 0.101126
I1026 00:46:45.019696 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.056643 (* 1 = 0.056643 loss)
I1026 00:46:45.019701 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0444834 (* 1 = 0.0444834 loss)
I1026 00:46:45.019704 17176 sgd_solver.cpp:106] Iteration 2880, lr = 0.001
I1026 00:46:45.598057 17176 solver.cpp:229] Iteration 2900, loss = 0.0787888
I1026 00:46:45.598088 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0586532 (* 1 = 0.0586532 loss)
I1026 00:46:45.598093 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0201356 (* 1 = 0.0201356 loss)
I1026 00:46:45.598098 17176 sgd_solver.cpp:106] Iteration 2900, lr = 0.001
I1026 00:46:46.174901 17176 solver.cpp:229] Iteration 2920, loss = 0.173668
I1026 00:46:46.174932 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.152367 (* 1 = 0.152367 loss)
I1026 00:46:46.174937 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0213007 (* 1 = 0.0213007 loss)
I1026 00:46:46.174942 17176 sgd_solver.cpp:106] Iteration 2920, lr = 0.001
I1026 00:46:46.749032 17176 solver.cpp:229] Iteration 2940, loss = 0.096835
I1026 00:46:46.749063 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0820632 (* 1 = 0.0820632 loss)
I1026 00:46:46.749068 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0147718 (* 1 = 0.0147718 loss)
I1026 00:46:46.749073 17176 sgd_solver.cpp:106] Iteration 2940, lr = 0.001
I1026 00:46:47.307791 17176 solver.cpp:229] Iteration 2960, loss = 0.351802
I1026 00:46:47.307824 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.204171 (* 1 = 0.204171 loss)
I1026 00:46:47.307829 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.147632 (* 1 = 0.147632 loss)
I1026 00:46:47.307833 17176 sgd_solver.cpp:106] Iteration 2960, lr = 0.001
I1026 00:46:47.874181 17176 solver.cpp:229] Iteration 2980, loss = 0.0777789
I1026 00:46:47.874213 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0604316 (* 1 = 0.0604316 loss)
I1026 00:46:47.874218 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0173473 (* 1 = 0.0173473 loss)
I1026 00:46:47.874223 17176 sgd_solver.cpp:106] Iteration 2980, lr = 0.001
I1026 00:46:48.434242 17176 solver.cpp:229] Iteration 3000, loss = 0.135142
I1026 00:46:48.434274 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.123604 (* 1 = 0.123604 loss)
I1026 00:46:48.434279 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0115375 (* 1 = 0.0115375 loss)
I1026 00:46:48.434283 17176 sgd_solver.cpp:106] Iteration 3000, lr = 0.001
I1026 00:46:48.993259 17176 solver.cpp:229] Iteration 3020, loss = 0.0826453
I1026 00:46:48.993291 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0440641 (* 1 = 0.0440641 loss)
I1026 00:46:48.993297 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0385813 (* 1 = 0.0385813 loss)
I1026 00:46:48.993301 17176 sgd_solver.cpp:106] Iteration 3020, lr = 0.001
I1026 00:46:49.556310 17176 solver.cpp:229] Iteration 3040, loss = 0.135774
I1026 00:46:49.556342 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0506294 (* 1 = 0.0506294 loss)
I1026 00:46:49.556347 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0851441 (* 1 = 0.0851441 loss)
I1026 00:46:49.556351 17176 sgd_solver.cpp:106] Iteration 3040, lr = 0.001
I1026 00:46:50.112587 17176 solver.cpp:229] Iteration 3060, loss = 0.233645
I1026 00:46:50.112620 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.206344 (* 1 = 0.206344 loss)
I1026 00:46:50.112625 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0273007 (* 1 = 0.0273007 loss)
I1026 00:46:50.112629 17176 sgd_solver.cpp:106] Iteration 3060, lr = 0.001
I1026 00:46:50.676390 17176 solver.cpp:229] Iteration 3080, loss = 0.197038
I1026 00:46:50.676424 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.108112 (* 1 = 0.108112 loss)
I1026 00:46:50.676429 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0889259 (* 1 = 0.0889259 loss)
I1026 00:46:50.676435 17176 sgd_solver.cpp:106] Iteration 3080, lr = 0.001
I1026 00:46:51.233487 17176 solver.cpp:229] Iteration 3100, loss = 0.0289285
I1026 00:46:51.233520 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0267818 (* 1 = 0.0267818 loss)
I1026 00:46:51.233525 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00214666 (* 1 = 0.00214666 loss)
I1026 00:46:51.233528 17176 sgd_solver.cpp:106] Iteration 3100, lr = 0.001
I1026 00:46:51.816248 17176 solver.cpp:229] Iteration 3120, loss = 0.0997939
I1026 00:46:51.816282 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0558164 (* 1 = 0.0558164 loss)
I1026 00:46:51.816285 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0439775 (* 1 = 0.0439775 loss)
I1026 00:46:51.816289 17176 sgd_solver.cpp:106] Iteration 3120, lr = 0.001
I1026 00:46:52.390154 17176 solver.cpp:229] Iteration 3140, loss = 0.0701236
I1026 00:46:52.390187 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0682181 (* 1 = 0.0682181 loss)
I1026 00:46:52.390192 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00190551 (* 1 = 0.00190551 loss)
I1026 00:46:52.390195 17176 sgd_solver.cpp:106] Iteration 3140, lr = 0.001
I1026 00:46:52.945190 17176 solver.cpp:229] Iteration 3160, loss = 0.0916467
I1026 00:46:52.945224 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.087257 (* 1 = 0.087257 loss)
I1026 00:46:52.945230 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00438966 (* 1 = 0.00438966 loss)
I1026 00:46:52.945233 17176 sgd_solver.cpp:106] Iteration 3160, lr = 0.001
I1026 00:46:53.504216 17176 solver.cpp:229] Iteration 3180, loss = 0.106177
I1026 00:46:53.504248 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0987647 (* 1 = 0.0987647 loss)
I1026 00:46:53.504253 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00741186 (* 1 = 0.00741186 loss)
I1026 00:46:53.504257 17176 sgd_solver.cpp:106] Iteration 3180, lr = 0.001
I1026 00:46:54.059087 17176 solver.cpp:229] Iteration 3200, loss = 0.145225
I1026 00:46:54.059118 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0509861 (* 1 = 0.0509861 loss)
I1026 00:46:54.059123 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0942391 (* 1 = 0.0942391 loss)
I1026 00:46:54.059128 17176 sgd_solver.cpp:106] Iteration 3200, lr = 0.001
I1026 00:46:54.626875 17176 solver.cpp:229] Iteration 3220, loss = 0.0510346
I1026 00:46:54.626907 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0401924 (* 1 = 0.0401924 loss)
I1026 00:46:54.626912 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0108422 (* 1 = 0.0108422 loss)
I1026 00:46:54.626917 17176 sgd_solver.cpp:106] Iteration 3220, lr = 0.001
I1026 00:46:55.194506 17176 solver.cpp:229] Iteration 3240, loss = 0.153281
I1026 00:46:55.194548 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.118866 (* 1 = 0.118866 loss)
I1026 00:46:55.194553 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0344151 (* 1 = 0.0344151 loss)
I1026 00:46:55.194557 17176 sgd_solver.cpp:106] Iteration 3240, lr = 0.001
I1026 00:46:55.763072 17176 solver.cpp:229] Iteration 3260, loss = 0.162128
I1026 00:46:55.763105 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0331746 (* 1 = 0.0331746 loss)
I1026 00:46:55.763110 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.128953 (* 1 = 0.128953 loss)
I1026 00:46:55.763114 17176 sgd_solver.cpp:106] Iteration 3260, lr = 0.001
I1026 00:46:56.318429 17176 solver.cpp:229] Iteration 3280, loss = 0.730671
I1026 00:46:56.318459 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.300248 (* 1 = 0.300248 loss)
I1026 00:46:56.318464 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.430423 (* 1 = 0.430423 loss)
I1026 00:46:56.318469 17176 sgd_solver.cpp:106] Iteration 3280, lr = 0.001
I1026 00:46:56.882278 17176 solver.cpp:229] Iteration 3300, loss = 0.120803
I1026 00:46:56.882309 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0596943 (* 1 = 0.0596943 loss)
I1026 00:46:56.882314 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0611084 (* 1 = 0.0611084 loss)
I1026 00:46:56.882318 17176 sgd_solver.cpp:106] Iteration 3300, lr = 0.001
I1026 00:46:57.457026 17176 solver.cpp:229] Iteration 3320, loss = 0.149466
I1026 00:46:57.457068 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.106882 (* 1 = 0.106882 loss)
I1026 00:46:57.457072 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0425837 (* 1 = 0.0425837 loss)
I1026 00:46:57.457077 17176 sgd_solver.cpp:106] Iteration 3320, lr = 0.001
I1026 00:46:58.050174 17176 solver.cpp:229] Iteration 3340, loss = 0.237595
I1026 00:46:58.050205 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0411134 (* 1 = 0.0411134 loss)
I1026 00:46:58.050210 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.196481 (* 1 = 0.196481 loss)
I1026 00:46:58.050215 17176 sgd_solver.cpp:106] Iteration 3340, lr = 0.001
I1026 00:46:58.617643 17176 solver.cpp:229] Iteration 3360, loss = 0.148619
I1026 00:46:58.617676 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.083664 (* 1 = 0.083664 loss)
I1026 00:46:58.617681 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0649546 (* 1 = 0.0649546 loss)
I1026 00:46:58.617684 17176 sgd_solver.cpp:106] Iteration 3360, lr = 0.001
I1026 00:46:59.175416 17176 solver.cpp:229] Iteration 3380, loss = 0.0390133
I1026 00:46:59.175451 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0350348 (* 1 = 0.0350348 loss)
I1026 00:46:59.175457 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00397854 (* 1 = 0.00397854 loss)
I1026 00:46:59.175462 17176 sgd_solver.cpp:106] Iteration 3380, lr = 0.001
I1026 00:46:59.770118 17176 solver.cpp:229] Iteration 3400, loss = 0.15864
I1026 00:46:59.770151 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0989949 (* 1 = 0.0989949 loss)
I1026 00:46:59.770156 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0596452 (* 1 = 0.0596452 loss)
I1026 00:46:59.770170 17176 sgd_solver.cpp:106] Iteration 3400, lr = 0.001
I1026 00:47:00.336872 17176 solver.cpp:229] Iteration 3420, loss = 0.332782
I1026 00:47:00.336905 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.174415 (* 1 = 0.174415 loss)
I1026 00:47:00.336910 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.158368 (* 1 = 0.158368 loss)
I1026 00:47:00.336913 17176 sgd_solver.cpp:106] Iteration 3420, lr = 0.001
I1026 00:47:00.905256 17176 solver.cpp:229] Iteration 3440, loss = 0.0776678
I1026 00:47:00.905298 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0626606 (* 1 = 0.0626606 loss)
I1026 00:47:00.905304 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0150072 (* 1 = 0.0150072 loss)
I1026 00:47:00.905308 17176 sgd_solver.cpp:106] Iteration 3440, lr = 0.001
I1026 00:47:01.474764 17176 solver.cpp:229] Iteration 3460, loss = 0.406756
I1026 00:47:01.474797 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.324506 (* 1 = 0.324506 loss)
I1026 00:47:01.474802 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0822496 (* 1 = 0.0822496 loss)
I1026 00:47:01.474805 17176 sgd_solver.cpp:106] Iteration 3460, lr = 0.001
I1026 00:47:02.044533 17176 solver.cpp:229] Iteration 3480, loss = 0.206518
I1026 00:47:02.044566 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.128183 (* 1 = 0.128183 loss)
I1026 00:47:02.044571 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.078334 (* 1 = 0.078334 loss)
I1026 00:47:02.044575 17176 sgd_solver.cpp:106] Iteration 3480, lr = 0.001
I1026 00:47:02.617962 17176 solver.cpp:229] Iteration 3500, loss = 0.145778
I1026 00:47:02.618005 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.060321 (* 1 = 0.060321 loss)
I1026 00:47:02.618010 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0854567 (* 1 = 0.0854567 loss)
I1026 00:47:02.618013 17176 sgd_solver.cpp:106] Iteration 3500, lr = 0.001
I1026 00:47:03.180589 17176 solver.cpp:229] Iteration 3520, loss = 0.164956
I1026 00:47:03.180621 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.131106 (* 1 = 0.131106 loss)
I1026 00:47:03.180627 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0338509 (* 1 = 0.0338509 loss)
I1026 00:47:03.180631 17176 sgd_solver.cpp:106] Iteration 3520, lr = 0.001
I1026 00:47:03.757920 17176 solver.cpp:229] Iteration 3540, loss = 0.129936
I1026 00:47:03.757953 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.107604 (* 1 = 0.107604 loss)
I1026 00:47:03.757958 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0223322 (* 1 = 0.0223322 loss)
I1026 00:47:03.757961 17176 sgd_solver.cpp:106] Iteration 3540, lr = 0.001
I1026 00:47:04.324414 17176 solver.cpp:229] Iteration 3560, loss = 0.170152
I1026 00:47:04.324446 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0773515 (* 1 = 0.0773515 loss)
I1026 00:47:04.324451 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0928005 (* 1 = 0.0928005 loss)
I1026 00:47:04.324455 17176 sgd_solver.cpp:106] Iteration 3560, lr = 0.001
I1026 00:47:04.907527 17176 solver.cpp:229] Iteration 3580, loss = 0.152539
I1026 00:47:04.907560 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.133321 (* 1 = 0.133321 loss)
I1026 00:47:04.907565 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0192176 (* 1 = 0.0192176 loss)
I1026 00:47:04.907570 17176 sgd_solver.cpp:106] Iteration 3580, lr = 0.001
I1026 00:47:05.477584 17176 solver.cpp:229] Iteration 3600, loss = 0.131779
I1026 00:47:05.477617 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.112855 (* 1 = 0.112855 loss)
I1026 00:47:05.477622 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0189237 (* 1 = 0.0189237 loss)
I1026 00:47:05.477627 17176 sgd_solver.cpp:106] Iteration 3600, lr = 0.001
I1026 00:47:06.033289 17176 solver.cpp:229] Iteration 3620, loss = 0.26517
I1026 00:47:06.033321 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.227533 (* 1 = 0.227533 loss)
I1026 00:47:06.033326 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0376372 (* 1 = 0.0376372 loss)
I1026 00:47:06.033330 17176 sgd_solver.cpp:106] Iteration 3620, lr = 0.001
I1026 00:47:06.596740 17176 solver.cpp:229] Iteration 3640, loss = 0.158572
I1026 00:47:06.596772 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.135581 (* 1 = 0.135581 loss)
I1026 00:47:06.596777 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0229915 (* 1 = 0.0229915 loss)
I1026 00:47:06.596782 17176 sgd_solver.cpp:106] Iteration 3640, lr = 0.001
I1026 00:47:07.154417 17176 solver.cpp:229] Iteration 3660, loss = 0.115983
I1026 00:47:07.154448 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0472779 (* 1 = 0.0472779 loss)
I1026 00:47:07.154454 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0687046 (* 1 = 0.0687046 loss)
I1026 00:47:07.154458 17176 sgd_solver.cpp:106] Iteration 3660, lr = 0.001
I1026 00:47:07.733214 17176 solver.cpp:229] Iteration 3680, loss = 0.130158
I1026 00:47:07.733245 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.121885 (* 1 = 0.121885 loss)
I1026 00:47:07.733252 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00827299 (* 1 = 0.00827299 loss)
I1026 00:47:07.733255 17176 sgd_solver.cpp:106] Iteration 3680, lr = 0.001
I1026 00:47:08.310109 17176 solver.cpp:229] Iteration 3700, loss = 0.297285
I1026 00:47:08.310140 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.177843 (* 1 = 0.177843 loss)
I1026 00:47:08.310145 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.119442 (* 1 = 0.119442 loss)
I1026 00:47:08.310148 17176 sgd_solver.cpp:106] Iteration 3700, lr = 0.001
I1026 00:47:08.902237 17176 solver.cpp:229] Iteration 3720, loss = 0.121459
I1026 00:47:08.902269 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0868423 (* 1 = 0.0868423 loss)
I1026 00:47:08.902274 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0346167 (* 1 = 0.0346167 loss)
I1026 00:47:08.902277 17176 sgd_solver.cpp:106] Iteration 3720, lr = 0.001
I1026 00:47:09.474679 17176 solver.cpp:229] Iteration 3740, loss = 0.364705
I1026 00:47:09.474712 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.188164 (* 1 = 0.188164 loss)
I1026 00:47:09.474719 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.176541 (* 1 = 0.176541 loss)
I1026 00:47:09.474735 17176 sgd_solver.cpp:106] Iteration 3740, lr = 0.001
I1026 00:47:10.054252 17176 solver.cpp:229] Iteration 3760, loss = 0.0967575
I1026 00:47:10.054285 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0901447 (* 1 = 0.0901447 loss)
I1026 00:47:10.054289 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00661274 (* 1 = 0.00661274 loss)
I1026 00:47:10.054292 17176 sgd_solver.cpp:106] Iteration 3760, lr = 0.001
I1026 00:47:10.627460 17176 solver.cpp:229] Iteration 3780, loss = 0.0534633
I1026 00:47:10.627490 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0499027 (* 1 = 0.0499027 loss)
I1026 00:47:10.627495 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0035606 (* 1 = 0.0035606 loss)
I1026 00:47:10.627499 17176 sgd_solver.cpp:106] Iteration 3780, lr = 0.001
I1026 00:47:11.213018 17176 solver.cpp:229] Iteration 3800, loss = 0.0977306
I1026 00:47:11.213050 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0576235 (* 1 = 0.0576235 loss)
I1026 00:47:11.213073 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0401071 (* 1 = 0.0401071 loss)
I1026 00:47:11.213076 17176 sgd_solver.cpp:106] Iteration 3800, lr = 0.001
I1026 00:47:11.783987 17176 solver.cpp:229] Iteration 3820, loss = 0.105372
I1026 00:47:11.784019 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.101044 (* 1 = 0.101044 loss)
I1026 00:47:11.784025 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00432757 (* 1 = 0.00432757 loss)
I1026 00:47:11.784029 17176 sgd_solver.cpp:106] Iteration 3820, lr = 0.001
I1026 00:47:12.347959 17176 solver.cpp:229] Iteration 3840, loss = 0.130563
I1026 00:47:12.348001 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0669591 (* 1 = 0.0669591 loss)
I1026 00:47:12.348007 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0636044 (* 1 = 0.0636044 loss)
I1026 00:47:12.348012 17176 sgd_solver.cpp:106] Iteration 3840, lr = 0.001
I1026 00:47:12.930721 17176 solver.cpp:229] Iteration 3860, loss = 0.297425
I1026 00:47:12.930763 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.180651 (* 1 = 0.180651 loss)
I1026 00:47:12.930768 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.116773 (* 1 = 0.116773 loss)
I1026 00:47:12.930773 17176 sgd_solver.cpp:106] Iteration 3860, lr = 0.001
I1026 00:47:13.503412 17176 solver.cpp:229] Iteration 3880, loss = 0.141773
I1026 00:47:13.503461 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.105107 (* 1 = 0.105107 loss)
I1026 00:47:13.503468 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0366661 (* 1 = 0.0366661 loss)
I1026 00:47:13.503473 17176 sgd_solver.cpp:106] Iteration 3880, lr = 0.001
I1026 00:47:14.080585 17176 solver.cpp:229] Iteration 3900, loss = 0.296654
I1026 00:47:14.080617 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.228532 (* 1 = 0.228532 loss)
I1026 00:47:14.080623 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.068122 (* 1 = 0.068122 loss)
I1026 00:47:14.080627 17176 sgd_solver.cpp:106] Iteration 3900, lr = 0.001
I1026 00:47:14.636018 17176 solver.cpp:229] Iteration 3920, loss = 0.158869
I1026 00:47:14.636049 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.149332 (* 1 = 0.149332 loss)
I1026 00:47:14.636054 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0095371 (* 1 = 0.0095371 loss)
I1026 00:47:14.636059 17176 sgd_solver.cpp:106] Iteration 3920, lr = 0.001
I1026 00:47:15.197268 17176 solver.cpp:229] Iteration 3940, loss = 0.331973
I1026 00:47:15.197300 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.197444 (* 1 = 0.197444 loss)
I1026 00:47:15.197305 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.13453 (* 1 = 0.13453 loss)
I1026 00:47:15.197309 17176 sgd_solver.cpp:106] Iteration 3940, lr = 0.001
I1026 00:47:15.784936 17176 solver.cpp:229] Iteration 3960, loss = 0.10228
I1026 00:47:15.784970 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0889658 (* 1 = 0.0889658 loss)
I1026 00:47:15.784973 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0133138 (* 1 = 0.0133138 loss)
I1026 00:47:15.784988 17176 sgd_solver.cpp:106] Iteration 3960, lr = 0.001
I1026 00:47:16.364260 17176 solver.cpp:229] Iteration 3980, loss = 0.221726
I1026 00:47:16.364294 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0795318 (* 1 = 0.0795318 loss)
I1026 00:47:16.364298 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.142194 (* 1 = 0.142194 loss)
I1026 00:47:16.364303 17176 sgd_solver.cpp:106] Iteration 3980, lr = 0.001
I1026 00:47:16.945907 17176 solver.cpp:229] Iteration 4000, loss = 0.102734
I1026 00:47:16.945941 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0735207 (* 1 = 0.0735207 loss)
I1026 00:47:16.945946 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0292128 (* 1 = 0.0292128 loss)
I1026 00:47:16.945950 17176 sgd_solver.cpp:106] Iteration 4000, lr = 0.001
I1026 00:47:17.524616 17176 solver.cpp:229] Iteration 4020, loss = 0.171216
I1026 00:47:17.524647 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.150486 (* 1 = 0.150486 loss)
I1026 00:47:17.524653 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0207299 (* 1 = 0.0207299 loss)
I1026 00:47:17.524657 17176 sgd_solver.cpp:106] Iteration 4020, lr = 0.001
I1026 00:47:18.101147 17176 solver.cpp:229] Iteration 4040, loss = 0.0576853
I1026 00:47:18.101178 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0505649 (* 1 = 0.0505649 loss)
I1026 00:47:18.101183 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00712043 (* 1 = 0.00712043 loss)
I1026 00:47:18.101186 17176 sgd_solver.cpp:106] Iteration 4040, lr = 0.001
I1026 00:47:18.657918 17176 solver.cpp:229] Iteration 4060, loss = 0.267679
I1026 00:47:18.657950 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.184973 (* 1 = 0.184973 loss)
I1026 00:47:18.657955 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0827053 (* 1 = 0.0827053 loss)
I1026 00:47:18.657959 17176 sgd_solver.cpp:106] Iteration 4060, lr = 0.001
I1026 00:47:19.212482 17176 solver.cpp:229] Iteration 4080, loss = 0.935108
I1026 00:47:19.212513 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.455481 (* 1 = 0.455481 loss)
I1026 00:47:19.212518 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.479627 (* 1 = 0.479627 loss)
I1026 00:47:19.212523 17176 sgd_solver.cpp:106] Iteration 4080, lr = 0.001
I1026 00:47:19.780499 17176 solver.cpp:229] Iteration 4100, loss = 0.153744
I1026 00:47:19.780544 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.112847 (* 1 = 0.112847 loss)
I1026 00:47:19.780549 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0408971 (* 1 = 0.0408971 loss)
I1026 00:47:19.780552 17176 sgd_solver.cpp:106] Iteration 4100, lr = 0.001
I1026 00:47:20.352437 17176 solver.cpp:229] Iteration 4120, loss = 0.109825
I1026 00:47:20.352480 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0890628 (* 1 = 0.0890628 loss)
I1026 00:47:20.352485 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0207623 (* 1 = 0.0207623 loss)
I1026 00:47:20.352489 17176 sgd_solver.cpp:106] Iteration 4120, lr = 0.001
I1026 00:47:20.918195 17176 solver.cpp:229] Iteration 4140, loss = 0.214366
I1026 00:47:20.918226 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.133844 (* 1 = 0.133844 loss)
I1026 00:47:20.918231 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0805217 (* 1 = 0.0805217 loss)
I1026 00:47:20.918236 17176 sgd_solver.cpp:106] Iteration 4140, lr = 0.001
I1026 00:47:21.501734 17176 solver.cpp:229] Iteration 4160, loss = 0.146867
I1026 00:47:21.501767 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0954504 (* 1 = 0.0954504 loss)
I1026 00:47:21.501771 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0514163 (* 1 = 0.0514163 loss)
I1026 00:47:21.501775 17176 sgd_solver.cpp:106] Iteration 4160, lr = 0.001
I1026 00:47:22.062214 17176 solver.cpp:229] Iteration 4180, loss = 0.361479
I1026 00:47:22.062247 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.156235 (* 1 = 0.156235 loss)
I1026 00:47:22.062252 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.205245 (* 1 = 0.205245 loss)
I1026 00:47:22.062255 17176 sgd_solver.cpp:106] Iteration 4180, lr = 0.001
I1026 00:47:22.642156 17176 solver.cpp:229] Iteration 4200, loss = 0.133702
I1026 00:47:22.642197 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.125786 (* 1 = 0.125786 loss)
I1026 00:47:22.642204 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00791522 (* 1 = 0.00791522 loss)
I1026 00:47:22.642207 17176 sgd_solver.cpp:106] Iteration 4200, lr = 0.001
I1026 00:47:23.210638 17176 solver.cpp:229] Iteration 4220, loss = 0.603709
I1026 00:47:23.210670 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.348052 (* 1 = 0.348052 loss)
I1026 00:47:23.210675 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.255657 (* 1 = 0.255657 loss)
I1026 00:47:23.210680 17176 sgd_solver.cpp:106] Iteration 4220, lr = 0.001
I1026 00:47:23.757256 17176 solver.cpp:229] Iteration 4240, loss = 0.0869392
I1026 00:47:23.757287 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0784776 (* 1 = 0.0784776 loss)
I1026 00:47:23.757292 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0084616 (* 1 = 0.0084616 loss)
I1026 00:47:23.757297 17176 sgd_solver.cpp:106] Iteration 4240, lr = 0.001
I1026 00:47:24.337860 17176 solver.cpp:229] Iteration 4260, loss = 0.174279
I1026 00:47:24.337893 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.142193 (* 1 = 0.142193 loss)
I1026 00:47:24.337898 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0320865 (* 1 = 0.0320865 loss)
I1026 00:47:24.337901 17176 sgd_solver.cpp:106] Iteration 4260, lr = 0.001
I1026 00:47:24.912758 17176 solver.cpp:229] Iteration 4280, loss = 0.0831088
I1026 00:47:24.912791 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.023676 (* 1 = 0.023676 loss)
I1026 00:47:24.912796 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0594328 (* 1 = 0.0594328 loss)
I1026 00:47:24.912798 17176 sgd_solver.cpp:106] Iteration 4280, lr = 0.001
I1026 00:47:25.499328 17176 solver.cpp:229] Iteration 4300, loss = 0.687475
I1026 00:47:25.499361 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.29506 (* 1 = 0.29506 loss)
I1026 00:47:25.499366 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.392415 (* 1 = 0.392415 loss)
I1026 00:47:25.499369 17176 sgd_solver.cpp:106] Iteration 4300, lr = 0.001
I1026 00:47:26.073161 17176 solver.cpp:229] Iteration 4320, loss = 0.127069
I1026 00:47:26.073194 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0825559 (* 1 = 0.0825559 loss)
I1026 00:47:26.073199 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0445135 (* 1 = 0.0445135 loss)
I1026 00:47:26.073202 17176 sgd_solver.cpp:106] Iteration 4320, lr = 0.001
I1026 00:47:26.647927 17176 solver.cpp:229] Iteration 4340, loss = 0.106312
I1026 00:47:26.647961 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0337835 (* 1 = 0.0337835 loss)
I1026 00:47:26.647966 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.072528 (* 1 = 0.072528 loss)
I1026 00:47:26.647970 17176 sgd_solver.cpp:106] Iteration 4340, lr = 0.001
I1026 00:47:27.221292 17176 solver.cpp:229] Iteration 4360, loss = 0.635976
I1026 00:47:27.221323 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.276186 (* 1 = 0.276186 loss)
I1026 00:47:27.221328 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.359789 (* 1 = 0.359789 loss)
I1026 00:47:27.221333 17176 sgd_solver.cpp:106] Iteration 4360, lr = 0.001
I1026 00:47:27.793381 17176 solver.cpp:229] Iteration 4380, loss = 0.0542623
I1026 00:47:27.793413 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0429904 (* 1 = 0.0429904 loss)
I1026 00:47:27.793418 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0112719 (* 1 = 0.0112719 loss)
I1026 00:47:27.793422 17176 sgd_solver.cpp:106] Iteration 4380, lr = 0.001
I1026 00:47:28.355991 17176 solver.cpp:229] Iteration 4400, loss = 0.208929
I1026 00:47:28.356022 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.145245 (* 1 = 0.145245 loss)
I1026 00:47:28.356027 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0636843 (* 1 = 0.0636843 loss)
I1026 00:47:28.356031 17176 sgd_solver.cpp:106] Iteration 4400, lr = 0.001
I1026 00:47:28.913341 17176 solver.cpp:229] Iteration 4420, loss = 0.112949
I1026 00:47:28.913372 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0374789 (* 1 = 0.0374789 loss)
I1026 00:47:28.913377 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0754704 (* 1 = 0.0754704 loss)
I1026 00:47:28.913380 17176 sgd_solver.cpp:106] Iteration 4420, lr = 0.001
I1026 00:47:29.490381 17176 solver.cpp:229] Iteration 4440, loss = 0.117403
I1026 00:47:29.490413 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0567878 (* 1 = 0.0567878 loss)
I1026 00:47:29.490417 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0606152 (* 1 = 0.0606152 loss)
I1026 00:47:29.490422 17176 sgd_solver.cpp:106] Iteration 4440, lr = 0.001
I1026 00:47:30.050637 17176 solver.cpp:229] Iteration 4460, loss = 0.233285
I1026 00:47:30.050668 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.211546 (* 1 = 0.211546 loss)
I1026 00:47:30.050671 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0217397 (* 1 = 0.0217397 loss)
I1026 00:47:30.050675 17176 sgd_solver.cpp:106] Iteration 4460, lr = 0.001
I1026 00:47:30.624629 17176 solver.cpp:229] Iteration 4480, loss = 0.0997849
I1026 00:47:30.624660 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0844845 (* 1 = 0.0844845 loss)
I1026 00:47:30.624665 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0153004 (* 1 = 0.0153004 loss)
I1026 00:47:30.624670 17176 sgd_solver.cpp:106] Iteration 4480, lr = 0.001
I1026 00:47:31.193682 17176 solver.cpp:229] Iteration 4500, loss = 0.103663
I1026 00:47:31.193714 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0864336 (* 1 = 0.0864336 loss)
I1026 00:47:31.193719 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0172294 (* 1 = 0.0172294 loss)
I1026 00:47:31.193723 17176 sgd_solver.cpp:106] Iteration 4500, lr = 0.001
I1026 00:47:31.779618 17176 solver.cpp:229] Iteration 4520, loss = 0.0714896
I1026 00:47:31.779650 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0648063 (* 1 = 0.0648063 loss)
I1026 00:47:31.779655 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00668336 (* 1 = 0.00668336 loss)
I1026 00:47:31.779659 17176 sgd_solver.cpp:106] Iteration 4520, lr = 0.001
I1026 00:47:32.349614 17176 solver.cpp:229] Iteration 4540, loss = 0.0523358
I1026 00:47:32.349658 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0282272 (* 1 = 0.0282272 loss)
I1026 00:47:32.349663 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0241086 (* 1 = 0.0241086 loss)
I1026 00:47:32.349666 17176 sgd_solver.cpp:106] Iteration 4540, lr = 0.001
I1026 00:47:32.904680 17176 solver.cpp:229] Iteration 4560, loss = 2.01851
I1026 00:47:32.904723 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.551772 (* 1 = 0.551772 loss)
I1026 00:47:32.904727 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 1.46674 (* 1 = 1.46674 loss)
I1026 00:47:32.904731 17176 sgd_solver.cpp:106] Iteration 4560, lr = 0.001
I1026 00:47:33.481523 17176 solver.cpp:229] Iteration 4580, loss = 0.237243
I1026 00:47:33.481566 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.22191 (* 1 = 0.22191 loss)
I1026 00:47:33.481571 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0153336 (* 1 = 0.0153336 loss)
I1026 00:47:33.481576 17176 sgd_solver.cpp:106] Iteration 4580, lr = 0.001
I1026 00:47:34.044204 17176 solver.cpp:229] Iteration 4600, loss = 0.113474
I1026 00:47:34.044237 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0379202 (* 1 = 0.0379202 loss)
I1026 00:47:34.044242 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.075554 (* 1 = 0.075554 loss)
I1026 00:47:34.044245 17176 sgd_solver.cpp:106] Iteration 4600, lr = 0.001
I1026 00:47:34.618757 17176 solver.cpp:229] Iteration 4620, loss = 2.91699
I1026 00:47:34.618788 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.689082 (* 1 = 0.689082 loss)
I1026 00:47:34.618793 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 2.2279 (* 1 = 2.2279 loss)
I1026 00:47:34.618798 17176 sgd_solver.cpp:106] Iteration 4620, lr = 0.001
I1026 00:47:35.196419 17176 solver.cpp:229] Iteration 4640, loss = 0.313695
I1026 00:47:35.196451 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.106703 (* 1 = 0.106703 loss)
I1026 00:47:35.196455 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.206991 (* 1 = 0.206991 loss)
I1026 00:47:35.196460 17176 sgd_solver.cpp:106] Iteration 4640, lr = 0.001
I1026 00:47:35.766312 17176 solver.cpp:229] Iteration 4660, loss = 0.0692616
I1026 00:47:35.766353 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0442925 (* 1 = 0.0442925 loss)
I1026 00:47:35.766358 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.024969 (* 1 = 0.024969 loss)
I1026 00:47:35.766362 17176 sgd_solver.cpp:106] Iteration 4660, lr = 0.001
I1026 00:47:36.350073 17176 solver.cpp:229] Iteration 4680, loss = 0.0449664
I1026 00:47:36.350116 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0402443 (* 1 = 0.0402443 loss)
I1026 00:47:36.350121 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0047221 (* 1 = 0.0047221 loss)
I1026 00:47:36.350126 17176 sgd_solver.cpp:106] Iteration 4680, lr = 0.001
I1026 00:47:36.911607 17176 solver.cpp:229] Iteration 4700, loss = 0.0846114
I1026 00:47:36.911650 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0638859 (* 1 = 0.0638859 loss)
I1026 00:47:36.911656 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0207256 (* 1 = 0.0207256 loss)
I1026 00:47:36.911659 17176 sgd_solver.cpp:106] Iteration 4700, lr = 0.001
I1026 00:47:37.494264 17176 solver.cpp:229] Iteration 4720, loss = 0.0859113
I1026 00:47:37.494295 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0791044 (* 1 = 0.0791044 loss)
I1026 00:47:37.494300 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00680689 (* 1 = 0.00680689 loss)
I1026 00:47:37.494304 17176 sgd_solver.cpp:106] Iteration 4720, lr = 0.001
I1026 00:47:38.062934 17176 solver.cpp:229] Iteration 4740, loss = 0.0837602
I1026 00:47:38.062966 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0630578 (* 1 = 0.0630578 loss)
I1026 00:47:38.062971 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0207024 (* 1 = 0.0207024 loss)
I1026 00:47:38.062974 17176 sgd_solver.cpp:106] Iteration 4740, lr = 0.001
I1026 00:47:38.635504 17176 solver.cpp:229] Iteration 4760, loss = 0.139512
I1026 00:47:38.635535 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0996418 (* 1 = 0.0996418 loss)
I1026 00:47:38.635540 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0398699 (* 1 = 0.0398699 loss)
I1026 00:47:38.635545 17176 sgd_solver.cpp:106] Iteration 4760, lr = 0.001
I1026 00:47:39.209431 17176 solver.cpp:229] Iteration 4780, loss = 0.146559
I1026 00:47:39.209462 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.128408 (* 1 = 0.128408 loss)
I1026 00:47:39.209468 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0181514 (* 1 = 0.0181514 loss)
I1026 00:47:39.209472 17176 sgd_solver.cpp:106] Iteration 4780, lr = 0.001
I1026 00:47:39.781029 17176 solver.cpp:229] Iteration 4800, loss = 0.0316348
I1026 00:47:39.781072 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0266831 (* 1 = 0.0266831 loss)
I1026 00:47:39.781077 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00495169 (* 1 = 0.00495169 loss)
I1026 00:47:39.781081 17176 sgd_solver.cpp:106] Iteration 4800, lr = 0.001
I1026 00:47:40.349087 17176 solver.cpp:229] Iteration 4820, loss = 0.0692088
I1026 00:47:40.349130 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0442003 (* 1 = 0.0442003 loss)
I1026 00:47:40.349135 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0250085 (* 1 = 0.0250085 loss)
I1026 00:47:40.349138 17176 sgd_solver.cpp:106] Iteration 4820, lr = 0.001
I1026 00:47:40.908764 17176 solver.cpp:229] Iteration 4840, loss = 1.51581
I1026 00:47:40.908807 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.736014 (* 1 = 0.736014 loss)
I1026 00:47:40.908813 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.779791 (* 1 = 0.779791 loss)
I1026 00:47:40.908819 17176 sgd_solver.cpp:106] Iteration 4840, lr = 0.001
I1026 00:47:41.465752 17176 solver.cpp:229] Iteration 4860, loss = 0.248055
I1026 00:47:41.465785 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.174031 (* 1 = 0.174031 loss)
I1026 00:47:41.465790 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0740238 (* 1 = 0.0740238 loss)
I1026 00:47:41.465793 17176 sgd_solver.cpp:106] Iteration 4860, lr = 0.001
I1026 00:47:42.029830 17176 solver.cpp:229] Iteration 4880, loss = 0.124161
I1026 00:47:42.029872 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0760665 (* 1 = 0.0760665 loss)
I1026 00:47:42.029877 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0480944 (* 1 = 0.0480944 loss)
I1026 00:47:42.029881 17176 sgd_solver.cpp:106] Iteration 4880, lr = 0.001
I1026 00:47:42.608901 17176 solver.cpp:229] Iteration 4900, loss = 0.0696377
I1026 00:47:42.608933 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.056927 (* 1 = 0.056927 loss)
I1026 00:47:42.608938 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0127107 (* 1 = 0.0127107 loss)
I1026 00:47:42.608942 17176 sgd_solver.cpp:106] Iteration 4900, lr = 0.001
I1026 00:47:43.181702 17176 solver.cpp:229] Iteration 4920, loss = 0.105365
I1026 00:47:43.181743 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0788097 (* 1 = 0.0788097 loss)
I1026 00:47:43.181748 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0265549 (* 1 = 0.0265549 loss)
I1026 00:47:43.181752 17176 sgd_solver.cpp:106] Iteration 4920, lr = 0.001
I1026 00:47:43.754073 17176 solver.cpp:229] Iteration 4940, loss = 0.625256
I1026 00:47:43.754106 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.422952 (* 1 = 0.422952 loss)
I1026 00:47:43.754111 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.202304 (* 1 = 0.202304 loss)
I1026 00:47:43.754114 17176 sgd_solver.cpp:106] Iteration 4940, lr = 0.001
I1026 00:47:44.329069 17176 solver.cpp:229] Iteration 4960, loss = 0.116444
I1026 00:47:44.329100 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.106012 (* 1 = 0.106012 loss)
I1026 00:47:44.329105 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0104314 (* 1 = 0.0104314 loss)
I1026 00:47:44.329109 17176 sgd_solver.cpp:106] Iteration 4960, lr = 0.001
I1026 00:47:44.891400 17176 solver.cpp:229] Iteration 4980, loss = 0.545183
I1026 00:47:44.891435 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.298585 (* 1 = 0.298585 loss)
I1026 00:47:44.891441 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.246599 (* 1 = 0.246599 loss)
I1026 00:47:44.891445 17176 sgd_solver.cpp:106] Iteration 4980, lr = 0.001
I1026 00:47:45.452024 17176 solver.cpp:229] Iteration 5000, loss = 0.0905166
I1026 00:47:45.452067 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0808658 (* 1 = 0.0808658 loss)
I1026 00:47:45.452071 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00965085 (* 1 = 0.00965085 loss)
I1026 00:47:45.452075 17176 sgd_solver.cpp:106] Iteration 5000, lr = 0.001
I1026 00:47:46.019013 17176 solver.cpp:229] Iteration 5020, loss = 0.046629
I1026 00:47:46.019043 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0424164 (* 1 = 0.0424164 loss)
I1026 00:47:46.019048 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00421264 (* 1 = 0.00421264 loss)
I1026 00:47:46.019053 17176 sgd_solver.cpp:106] Iteration 5020, lr = 0.001
I1026 00:47:46.572942 17176 solver.cpp:229] Iteration 5040, loss = 0.209108
I1026 00:47:46.572983 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.153268 (* 1 = 0.153268 loss)
I1026 00:47:46.572988 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0558405 (* 1 = 0.0558405 loss)
I1026 00:47:46.572991 17176 sgd_solver.cpp:106] Iteration 5040, lr = 0.001
I1026 00:47:47.136535 17176 solver.cpp:229] Iteration 5060, loss = 0.403444
I1026 00:47:47.136565 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.307088 (* 1 = 0.307088 loss)
I1026 00:47:47.136570 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0963564 (* 1 = 0.0963564 loss)
I1026 00:47:47.136575 17176 sgd_solver.cpp:106] Iteration 5060, lr = 0.001
I1026 00:47:47.708313 17176 solver.cpp:229] Iteration 5080, loss = 0.141124
I1026 00:47:47.708356 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.134529 (* 1 = 0.134529 loss)
I1026 00:47:47.708361 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00659449 (* 1 = 0.00659449 loss)
I1026 00:47:47.708365 17176 sgd_solver.cpp:106] Iteration 5080, lr = 0.001
I1026 00:47:48.279464 17176 solver.cpp:229] Iteration 5100, loss = 0.104483
I1026 00:47:48.279503 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0459094 (* 1 = 0.0459094 loss)
I1026 00:47:48.279507 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0585738 (* 1 = 0.0585738 loss)
I1026 00:47:48.279511 17176 sgd_solver.cpp:106] Iteration 5100, lr = 0.001
I1026 00:47:48.846779 17176 solver.cpp:229] Iteration 5120, loss = 0.118467
I1026 00:47:48.846812 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0490334 (* 1 = 0.0490334 loss)
I1026 00:47:48.846817 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0694332 (* 1 = 0.0694332 loss)
I1026 00:47:48.846822 17176 sgd_solver.cpp:106] Iteration 5120, lr = 0.001
I1026 00:47:49.421955 17176 solver.cpp:229] Iteration 5140, loss = 0.383539
I1026 00:47:49.421996 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.280784 (* 1 = 0.280784 loss)
I1026 00:47:49.422001 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.102755 (* 1 = 0.102755 loss)
I1026 00:47:49.422014 17176 sgd_solver.cpp:106] Iteration 5140, lr = 0.001
I1026 00:47:49.979012 17176 solver.cpp:229] Iteration 5160, loss = 0.219593
I1026 00:47:49.979054 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.195072 (* 1 = 0.195072 loss)
I1026 00:47:49.979059 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0245216 (* 1 = 0.0245216 loss)
I1026 00:47:49.979063 17176 sgd_solver.cpp:106] Iteration 5160, lr = 0.001
I1026 00:47:50.546602 17176 solver.cpp:229] Iteration 5180, loss = 0.0576577
I1026 00:47:50.546634 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0461255 (* 1 = 0.0461255 loss)
I1026 00:47:50.546639 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0115322 (* 1 = 0.0115322 loss)
I1026 00:47:50.546644 17176 sgd_solver.cpp:106] Iteration 5180, lr = 0.001
I1026 00:47:51.123019 17176 solver.cpp:229] Iteration 5200, loss = 0.268216
I1026 00:47:51.123052 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.142261 (* 1 = 0.142261 loss)
I1026 00:47:51.123059 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.125955 (* 1 = 0.125955 loss)
I1026 00:47:51.123062 17176 sgd_solver.cpp:106] Iteration 5200, lr = 0.001
I1026 00:47:51.680995 17176 solver.cpp:229] Iteration 5220, loss = 0.48281
I1026 00:47:51.681028 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.182966 (* 1 = 0.182966 loss)
I1026 00:47:51.681033 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.299844 (* 1 = 0.299844 loss)
I1026 00:47:51.681037 17176 sgd_solver.cpp:106] Iteration 5220, lr = 0.001
I1026 00:47:52.237666 17176 solver.cpp:229] Iteration 5240, loss = 0.127077
I1026 00:47:52.237699 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0713836 (* 1 = 0.0713836 loss)
I1026 00:47:52.237705 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0556929 (* 1 = 0.0556929 loss)
I1026 00:47:52.237710 17176 sgd_solver.cpp:106] Iteration 5240, lr = 0.001
I1026 00:47:52.822898 17176 solver.cpp:229] Iteration 5260, loss = 0.153393
I1026 00:47:52.822932 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.123731 (* 1 = 0.123731 loss)
I1026 00:47:52.822937 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0296615 (* 1 = 0.0296615 loss)
I1026 00:47:52.822942 17176 sgd_solver.cpp:106] Iteration 5260, lr = 0.001
I1026 00:47:53.394911 17176 solver.cpp:229] Iteration 5280, loss = 0.0759496
I1026 00:47:53.394943 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0720339 (* 1 = 0.0720339 loss)
I1026 00:47:53.394948 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00391565 (* 1 = 0.00391565 loss)
I1026 00:47:53.394960 17176 sgd_solver.cpp:106] Iteration 5280, lr = 0.001
I1026 00:47:53.966456 17176 solver.cpp:229] Iteration 5300, loss = 0.0816468
I1026 00:47:53.966485 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.075206 (* 1 = 0.075206 loss)
I1026 00:47:53.966490 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00644085 (* 1 = 0.00644085 loss)
I1026 00:47:53.966495 17176 sgd_solver.cpp:106] Iteration 5300, lr = 0.001
I1026 00:47:54.526342 17176 solver.cpp:229] Iteration 5320, loss = 0.140793
I1026 00:47:54.526373 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.131675 (* 1 = 0.131675 loss)
I1026 00:47:54.526379 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00911799 (* 1 = 0.00911799 loss)
I1026 00:47:54.526383 17176 sgd_solver.cpp:106] Iteration 5320, lr = 0.001
I1026 00:47:55.089212 17176 solver.cpp:229] Iteration 5340, loss = 0.343826
I1026 00:47:55.089242 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.255495 (* 1 = 0.255495 loss)
I1026 00:47:55.089247 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0883308 (* 1 = 0.0883308 loss)
I1026 00:47:55.089251 17176 sgd_solver.cpp:106] Iteration 5340, lr = 0.001
I1026 00:47:55.659992 17176 solver.cpp:229] Iteration 5360, loss = 0.190784
I1026 00:47:55.660024 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.134613 (* 1 = 0.134613 loss)
I1026 00:47:55.660029 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0561702 (* 1 = 0.0561702 loss)
I1026 00:47:55.660033 17176 sgd_solver.cpp:106] Iteration 5360, lr = 0.001
I1026 00:47:56.226831 17176 solver.cpp:229] Iteration 5380, loss = 0.201437
I1026 00:47:56.226863 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.173357 (* 1 = 0.173357 loss)
I1026 00:47:56.226868 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0280793 (* 1 = 0.0280793 loss)
I1026 00:47:56.226872 17176 sgd_solver.cpp:106] Iteration 5380, lr = 0.001
I1026 00:47:56.793469 17176 solver.cpp:229] Iteration 5400, loss = 0.221281
I1026 00:47:56.793501 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.1993 (* 1 = 0.1993 loss)
I1026 00:47:56.793506 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.021981 (* 1 = 0.021981 loss)
I1026 00:47:56.793510 17176 sgd_solver.cpp:106] Iteration 5400, lr = 0.001
I1026 00:47:57.344795 17176 solver.cpp:229] Iteration 5420, loss = 0.127261
I1026 00:47:57.344830 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0882954 (* 1 = 0.0882954 loss)
I1026 00:47:57.344833 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0389656 (* 1 = 0.0389656 loss)
I1026 00:47:57.344837 17176 sgd_solver.cpp:106] Iteration 5420, lr = 0.001
I1026 00:47:57.920475 17176 solver.cpp:229] Iteration 5440, loss = 0.0473129
I1026 00:47:57.920508 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0357384 (* 1 = 0.0357384 loss)
I1026 00:47:57.920513 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0115745 (* 1 = 0.0115745 loss)
I1026 00:47:57.920517 17176 sgd_solver.cpp:106] Iteration 5440, lr = 0.001
I1026 00:47:58.492780 17176 solver.cpp:229] Iteration 5460, loss = 1.13737
I1026 00:47:58.492810 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.646014 (* 1 = 0.646014 loss)
I1026 00:47:58.492813 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.491351 (* 1 = 0.491351 loss)
I1026 00:47:58.492817 17176 sgd_solver.cpp:106] Iteration 5460, lr = 0.001
I1026 00:47:59.072504 17176 solver.cpp:229] Iteration 5480, loss = 0.121414
I1026 00:47:59.072537 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0945918 (* 1 = 0.0945918 loss)
I1026 00:47:59.072542 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0268221 (* 1 = 0.0268221 loss)
I1026 00:47:59.072546 17176 sgd_solver.cpp:106] Iteration 5480, lr = 0.001
I1026 00:47:59.646953 17176 solver.cpp:229] Iteration 5500, loss = 0.192528
I1026 00:47:59.646994 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.133795 (* 1 = 0.133795 loss)
I1026 00:47:59.647011 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0587324 (* 1 = 0.0587324 loss)
I1026 00:47:59.647014 17176 sgd_solver.cpp:106] Iteration 5500, lr = 0.001
I1026 00:48:00.203466 17176 solver.cpp:229] Iteration 5520, loss = 0.160245
I1026 00:48:00.203500 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.136786 (* 1 = 0.136786 loss)
I1026 00:48:00.203505 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0234591 (* 1 = 0.0234591 loss)
I1026 00:48:00.203510 17176 sgd_solver.cpp:106] Iteration 5520, lr = 0.001
I1026 00:48:00.769695 17176 solver.cpp:229] Iteration 5540, loss = 0.104119
I1026 00:48:00.769728 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0375001 (* 1 = 0.0375001 loss)
I1026 00:48:00.769733 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0666187 (* 1 = 0.0666187 loss)
I1026 00:48:00.769737 17176 sgd_solver.cpp:106] Iteration 5540, lr = 0.001
I1026 00:48:01.348724 17176 solver.cpp:229] Iteration 5560, loss = 0.188118
I1026 00:48:01.348757 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.176597 (* 1 = 0.176597 loss)
I1026 00:48:01.348762 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0115217 (* 1 = 0.0115217 loss)
I1026 00:48:01.348767 17176 sgd_solver.cpp:106] Iteration 5560, lr = 0.001
I1026 00:48:01.924034 17176 solver.cpp:229] Iteration 5580, loss = 0.141567
I1026 00:48:01.924067 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0699084 (* 1 = 0.0699084 loss)
I1026 00:48:01.924072 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0716587 (* 1 = 0.0716587 loss)
I1026 00:48:01.924075 17176 sgd_solver.cpp:106] Iteration 5580, lr = 0.001
I1026 00:48:02.490845 17176 solver.cpp:229] Iteration 5600, loss = 0.147721
I1026 00:48:02.490877 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.117785 (* 1 = 0.117785 loss)
I1026 00:48:02.490882 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0299361 (* 1 = 0.0299361 loss)
I1026 00:48:02.490886 17176 sgd_solver.cpp:106] Iteration 5600, lr = 0.001
I1026 00:48:03.061964 17176 solver.cpp:229] Iteration 5620, loss = 0.0685582
I1026 00:48:03.061996 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0410669 (* 1 = 0.0410669 loss)
I1026 00:48:03.062000 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0274913 (* 1 = 0.0274913 loss)
I1026 00:48:03.062005 17176 sgd_solver.cpp:106] Iteration 5620, lr = 0.001
I1026 00:48:03.648792 17176 solver.cpp:229] Iteration 5640, loss = 0.163141
I1026 00:48:03.648823 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.142245 (* 1 = 0.142245 loss)
I1026 00:48:03.648828 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0208965 (* 1 = 0.0208965 loss)
I1026 00:48:03.648831 17176 sgd_solver.cpp:106] Iteration 5640, lr = 0.001
I1026 00:48:04.227532 17176 solver.cpp:229] Iteration 5660, loss = 0.430627
I1026 00:48:04.227565 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.359321 (* 1 = 0.359321 loss)
I1026 00:48:04.227569 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0713056 (* 1 = 0.0713056 loss)
I1026 00:48:04.227573 17176 sgd_solver.cpp:106] Iteration 5660, lr = 0.001
I1026 00:48:04.794945 17176 solver.cpp:229] Iteration 5680, loss = 0.0942292
I1026 00:48:04.794978 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0869142 (* 1 = 0.0869142 loss)
I1026 00:48:04.794983 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00731499 (* 1 = 0.00731499 loss)
I1026 00:48:04.794987 17176 sgd_solver.cpp:106] Iteration 5680, lr = 0.001
I1026 00:48:05.357516 17176 solver.cpp:229] Iteration 5700, loss = 0.0537734
I1026 00:48:05.357548 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0336052 (* 1 = 0.0336052 loss)
I1026 00:48:05.357553 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0201682 (* 1 = 0.0201682 loss)
I1026 00:48:05.357556 17176 sgd_solver.cpp:106] Iteration 5700, lr = 0.001
I1026 00:48:05.929384 17176 solver.cpp:229] Iteration 5720, loss = 0.0847371
I1026 00:48:05.929416 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0483015 (* 1 = 0.0483015 loss)
I1026 00:48:05.929421 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0364356 (* 1 = 0.0364356 loss)
I1026 00:48:05.929425 17176 sgd_solver.cpp:106] Iteration 5720, lr = 0.001
I1026 00:48:06.492432 17176 solver.cpp:229] Iteration 5740, loss = 0.19556
I1026 00:48:06.492475 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.11148 (* 1 = 0.11148 loss)
I1026 00:48:06.492480 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0840802 (* 1 = 0.0840802 loss)
I1026 00:48:06.492492 17176 sgd_solver.cpp:106] Iteration 5740, lr = 0.001
I1026 00:48:07.063904 17176 solver.cpp:229] Iteration 5760, loss = 0.0436839
I1026 00:48:07.063935 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0421644 (* 1 = 0.0421644 loss)
I1026 00:48:07.063941 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00151945 (* 1 = 0.00151945 loss)
I1026 00:48:07.063944 17176 sgd_solver.cpp:106] Iteration 5760, lr = 0.001
I1026 00:48:07.627339 17176 solver.cpp:229] Iteration 5780, loss = 0.0894019
I1026 00:48:07.627372 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0787665 (* 1 = 0.0787665 loss)
I1026 00:48:07.627377 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0106353 (* 1 = 0.0106353 loss)
I1026 00:48:07.627380 17176 sgd_solver.cpp:106] Iteration 5780, lr = 0.001
I1026 00:48:08.214128 17176 solver.cpp:229] Iteration 5800, loss = 0.0476868
I1026 00:48:08.214159 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0261512 (* 1 = 0.0261512 loss)
I1026 00:48:08.214164 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0215355 (* 1 = 0.0215355 loss)
I1026 00:48:08.214169 17176 sgd_solver.cpp:106] Iteration 5800, lr = 0.001
I1026 00:48:08.783126 17176 solver.cpp:229] Iteration 5820, loss = 0.121679
I1026 00:48:08.783159 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0707439 (* 1 = 0.0707439 loss)
I1026 00:48:08.783164 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0509346 (* 1 = 0.0509346 loss)
I1026 00:48:08.783169 17176 sgd_solver.cpp:106] Iteration 5820, lr = 0.001
I1026 00:48:09.337041 17176 solver.cpp:229] Iteration 5840, loss = 0.0596971
I1026 00:48:09.337074 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0490123 (* 1 = 0.0490123 loss)
I1026 00:48:09.337079 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0106848 (* 1 = 0.0106848 loss)
I1026 00:48:09.337082 17176 sgd_solver.cpp:106] Iteration 5840, lr = 0.001
I1026 00:48:09.916012 17176 solver.cpp:229] Iteration 5860, loss = 0.152644
I1026 00:48:09.916043 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.147168 (* 1 = 0.147168 loss)
I1026 00:48:09.916049 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00547642 (* 1 = 0.00547642 loss)
I1026 00:48:09.916052 17176 sgd_solver.cpp:106] Iteration 5860, lr = 0.001
I1026 00:48:10.486676 17176 solver.cpp:229] Iteration 5880, loss = 0.0965119
I1026 00:48:10.486708 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0613885 (* 1 = 0.0613885 loss)
I1026 00:48:10.486713 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0351235 (* 1 = 0.0351235 loss)
I1026 00:48:10.486717 17176 sgd_solver.cpp:106] Iteration 5880, lr = 0.001
I1026 00:48:11.073115 17176 solver.cpp:229] Iteration 5900, loss = 0.077171
I1026 00:48:11.073149 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.029398 (* 1 = 0.029398 loss)
I1026 00:48:11.073153 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.047773 (* 1 = 0.047773 loss)
I1026 00:48:11.073158 17176 sgd_solver.cpp:106] Iteration 5900, lr = 0.001
I1026 00:48:11.645128 17176 solver.cpp:229] Iteration 5920, loss = 0.0598022
I1026 00:48:11.645170 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0413731 (* 1 = 0.0413731 loss)
I1026 00:48:11.645174 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0184291 (* 1 = 0.0184291 loss)
I1026 00:48:11.645187 17176 sgd_solver.cpp:106] Iteration 5920, lr = 0.001
I1026 00:48:12.205755 17176 solver.cpp:229] Iteration 5940, loss = 0.0731231
I1026 00:48:12.205787 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0523147 (* 1 = 0.0523147 loss)
I1026 00:48:12.205791 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0208084 (* 1 = 0.0208084 loss)
I1026 00:48:12.205796 17176 sgd_solver.cpp:106] Iteration 5940, lr = 0.001
I1026 00:48:12.770812 17176 solver.cpp:229] Iteration 5960, loss = 0.118244
I1026 00:48:12.770845 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.100206 (* 1 = 0.100206 loss)
I1026 00:48:12.770850 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0180376 (* 1 = 0.0180376 loss)
I1026 00:48:12.770853 17176 sgd_solver.cpp:106] Iteration 5960, lr = 0.001
I1026 00:48:13.348201 17176 solver.cpp:229] Iteration 5980, loss = 0.0952366
I1026 00:48:13.348233 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0544771 (* 1 = 0.0544771 loss)
I1026 00:48:13.348238 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0407595 (* 1 = 0.0407595 loss)
I1026 00:48:13.348242 17176 sgd_solver.cpp:106] Iteration 5980, lr = 0.001
I1026 00:48:13.917820 17176 solver.cpp:229] Iteration 6000, loss = 0.0702581
I1026 00:48:13.917852 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.059445 (* 1 = 0.059445 loss)
I1026 00:48:13.917872 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0108131 (* 1 = 0.0108131 loss)
I1026 00:48:13.917876 17176 sgd_solver.cpp:106] Iteration 6000, lr = 0.001
I1026 00:48:14.476852 17176 solver.cpp:229] Iteration 6020, loss = 0.0582777
I1026 00:48:14.476884 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0559873 (* 1 = 0.0559873 loss)
I1026 00:48:14.476889 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00229043 (* 1 = 0.00229043 loss)
I1026 00:48:14.476893 17176 sgd_solver.cpp:106] Iteration 6020, lr = 0.001
I1026 00:48:15.044492 17176 solver.cpp:229] Iteration 6040, loss = 0.064683
I1026 00:48:15.044524 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0521955 (* 1 = 0.0521955 loss)
I1026 00:48:15.044528 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0124875 (* 1 = 0.0124875 loss)
I1026 00:48:15.044533 17176 sgd_solver.cpp:106] Iteration 6040, lr = 0.001
I1026 00:48:15.614605 17176 solver.cpp:229] Iteration 6060, loss = 0.10079
I1026 00:48:15.614639 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0559455 (* 1 = 0.0559455 loss)
I1026 00:48:15.614644 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0448444 (* 1 = 0.0448444 loss)
I1026 00:48:15.614660 17176 sgd_solver.cpp:106] Iteration 6060, lr = 0.001
I1026 00:48:16.181766 17176 solver.cpp:229] Iteration 6080, loss = 0.19702
I1026 00:48:16.181797 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0448723 (* 1 = 0.0448723 loss)
I1026 00:48:16.181802 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.152147 (* 1 = 0.152147 loss)
I1026 00:48:16.181807 17176 sgd_solver.cpp:106] Iteration 6080, lr = 0.001
I1026 00:48:16.749233 17176 solver.cpp:229] Iteration 6100, loss = 0.0808115
I1026 00:48:16.749267 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0636903 (* 1 = 0.0636903 loss)
I1026 00:48:16.749271 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0171212 (* 1 = 0.0171212 loss)
I1026 00:48:16.749275 17176 sgd_solver.cpp:106] Iteration 6100, lr = 0.001
I1026 00:48:17.320098 17176 solver.cpp:229] Iteration 6120, loss = 0.409683
I1026 00:48:17.320132 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.238776 (* 1 = 0.238776 loss)
I1026 00:48:17.320137 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.170907 (* 1 = 0.170907 loss)
I1026 00:48:17.320140 17176 sgd_solver.cpp:106] Iteration 6120, lr = 0.001
I1026 00:48:17.888171 17176 solver.cpp:229] Iteration 6140, loss = 0.0989005
I1026 00:48:17.888205 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0702488 (* 1 = 0.0702488 loss)
I1026 00:48:17.888209 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0286517 (* 1 = 0.0286517 loss)
I1026 00:48:17.888213 17176 sgd_solver.cpp:106] Iteration 6140, lr = 0.001
I1026 00:48:18.464745 17176 solver.cpp:229] Iteration 6160, loss = 0.213933
I1026 00:48:18.464781 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.12597 (* 1 = 0.12597 loss)
I1026 00:48:18.464787 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0879626 (* 1 = 0.0879626 loss)
I1026 00:48:18.464792 17176 sgd_solver.cpp:106] Iteration 6160, lr = 0.001
I1026 00:48:19.036293 17176 solver.cpp:229] Iteration 6180, loss = 0.131209
I1026 00:48:19.036325 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.126381 (* 1 = 0.126381 loss)
I1026 00:48:19.036332 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00482803 (* 1 = 0.00482803 loss)
I1026 00:48:19.036347 17176 sgd_solver.cpp:106] Iteration 6180, lr = 0.001
I1026 00:48:19.604653 17176 solver.cpp:229] Iteration 6200, loss = 0.240272
I1026 00:48:19.604686 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.132919 (* 1 = 0.132919 loss)
I1026 00:48:19.604692 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.107352 (* 1 = 0.107352 loss)
I1026 00:48:19.604697 17176 sgd_solver.cpp:106] Iteration 6200, lr = 0.001
I1026 00:48:20.162740 17176 solver.cpp:229] Iteration 6220, loss = 0.15628
I1026 00:48:20.162775 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.114164 (* 1 = 0.114164 loss)
I1026 00:48:20.162781 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0421166 (* 1 = 0.0421166 loss)
I1026 00:48:20.162786 17176 sgd_solver.cpp:106] Iteration 6220, lr = 0.001
I1026 00:48:20.723150 17176 solver.cpp:229] Iteration 6240, loss = 0.0875583
I1026 00:48:20.723182 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0317634 (* 1 = 0.0317634 loss)
I1026 00:48:20.723187 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0557949 (* 1 = 0.0557949 loss)
I1026 00:48:20.723192 17176 sgd_solver.cpp:106] Iteration 6240, lr = 0.001
I1026 00:48:21.295636 17176 solver.cpp:229] Iteration 6260, loss = 0.138724
I1026 00:48:21.295670 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0466239 (* 1 = 0.0466239 loss)
I1026 00:48:21.295675 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0921003 (* 1 = 0.0921003 loss)
I1026 00:48:21.295678 17176 sgd_solver.cpp:106] Iteration 6260, lr = 0.001
I1026 00:48:21.857801 17176 solver.cpp:229] Iteration 6280, loss = 0.235841
I1026 00:48:21.857833 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.182489 (* 1 = 0.182489 loss)
I1026 00:48:21.857837 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0533523 (* 1 = 0.0533523 loss)
I1026 00:48:21.857841 17176 sgd_solver.cpp:106] Iteration 6280, lr = 0.001
I1026 00:48:22.424863 17176 solver.cpp:229] Iteration 6300, loss = 0.236812
I1026 00:48:22.424896 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.171671 (* 1 = 0.171671 loss)
I1026 00:48:22.424901 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0651413 (* 1 = 0.0651413 loss)
I1026 00:48:22.424906 17176 sgd_solver.cpp:106] Iteration 6300, lr = 0.001
I1026 00:48:22.996920 17176 solver.cpp:229] Iteration 6320, loss = 0.462137
I1026 00:48:22.996953 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.276094 (* 1 = 0.276094 loss)
I1026 00:48:22.996958 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.186042 (* 1 = 0.186042 loss)
I1026 00:48:22.996961 17176 sgd_solver.cpp:106] Iteration 6320, lr = 0.001
I1026 00:48:23.565846 17176 solver.cpp:229] Iteration 6340, loss = 0.38085
I1026 00:48:23.565878 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.331643 (* 1 = 0.331643 loss)
I1026 00:48:23.565882 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0492074 (* 1 = 0.0492074 loss)
I1026 00:48:23.565887 17176 sgd_solver.cpp:106] Iteration 6340, lr = 0.001
I1026 00:48:24.150440 17176 solver.cpp:229] Iteration 6360, loss = 0.164031
I1026 00:48:24.150475 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0758552 (* 1 = 0.0758552 loss)
I1026 00:48:24.150478 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0881757 (* 1 = 0.0881757 loss)
I1026 00:48:24.150482 17176 sgd_solver.cpp:106] Iteration 6360, lr = 0.001
I1026 00:48:24.717159 17176 solver.cpp:229] Iteration 6380, loss = 0.162902
I1026 00:48:24.717190 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.151225 (* 1 = 0.151225 loss)
I1026 00:48:24.717195 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0116774 (* 1 = 0.0116774 loss)
I1026 00:48:24.717200 17176 sgd_solver.cpp:106] Iteration 6380, lr = 0.001
I1026 00:48:25.291468 17176 solver.cpp:229] Iteration 6400, loss = 0.34535
I1026 00:48:25.291501 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.26481 (* 1 = 0.26481 loss)
I1026 00:48:25.291506 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0805396 (* 1 = 0.0805396 loss)
I1026 00:48:25.291519 17176 sgd_solver.cpp:106] Iteration 6400, lr = 0.001
I1026 00:48:25.862913 17176 solver.cpp:229] Iteration 6420, loss = 0.0455178
I1026 00:48:25.862944 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0417695 (* 1 = 0.0417695 loss)
I1026 00:48:25.862949 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00374833 (* 1 = 0.00374833 loss)
I1026 00:48:25.862953 17176 sgd_solver.cpp:106] Iteration 6420, lr = 0.001
I1026 00:48:26.439637 17176 solver.cpp:229] Iteration 6440, loss = 0.08807
I1026 00:48:26.439671 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0734645 (* 1 = 0.0734645 loss)
I1026 00:48:26.439674 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0146055 (* 1 = 0.0146055 loss)
I1026 00:48:26.439678 17176 sgd_solver.cpp:106] Iteration 6440, lr = 0.001
I1026 00:48:27.012012 17176 solver.cpp:229] Iteration 6460, loss = 0.0624009
I1026 00:48:27.012045 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0394223 (* 1 = 0.0394223 loss)
I1026 00:48:27.012049 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0229786 (* 1 = 0.0229786 loss)
I1026 00:48:27.012053 17176 sgd_solver.cpp:106] Iteration 6460, lr = 0.001
I1026 00:48:27.580484 17176 solver.cpp:229] Iteration 6480, loss = 0.249585
I1026 00:48:27.580516 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.136419 (* 1 = 0.136419 loss)
I1026 00:48:27.580521 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.113166 (* 1 = 0.113166 loss)
I1026 00:48:27.580524 17176 sgd_solver.cpp:106] Iteration 6480, lr = 0.001
I1026 00:48:28.136427 17176 solver.cpp:229] Iteration 6500, loss = 0.261564
I1026 00:48:28.136458 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.177948 (* 1 = 0.177948 loss)
I1026 00:48:28.136463 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0836162 (* 1 = 0.0836162 loss)
I1026 00:48:28.136466 17176 sgd_solver.cpp:106] Iteration 6500, lr = 0.001
I1026 00:48:28.706985 17176 solver.cpp:229] Iteration 6520, loss = 0.651974
I1026 00:48:28.707016 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.227888 (* 1 = 0.227888 loss)
I1026 00:48:28.707022 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.424085 (* 1 = 0.424085 loss)
I1026 00:48:28.707036 17176 sgd_solver.cpp:106] Iteration 6520, lr = 0.001
I1026 00:48:29.259902 17176 solver.cpp:229] Iteration 6540, loss = 0.0876363
I1026 00:48:29.259933 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0374778 (* 1 = 0.0374778 loss)
I1026 00:48:29.259938 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0501585 (* 1 = 0.0501585 loss)
I1026 00:48:29.259941 17176 sgd_solver.cpp:106] Iteration 6540, lr = 0.001
I1026 00:48:29.828924 17176 solver.cpp:229] Iteration 6560, loss = 0.034836
I1026 00:48:29.828956 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0326924 (* 1 = 0.0326924 loss)
I1026 00:48:29.828961 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00214363 (* 1 = 0.00214363 loss)
I1026 00:48:29.828965 17176 sgd_solver.cpp:106] Iteration 6560, lr = 0.001
I1026 00:48:30.407763 17176 solver.cpp:229] Iteration 6580, loss = 0.119003
I1026 00:48:30.407794 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0803308 (* 1 = 0.0803308 loss)
I1026 00:48:30.407799 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0386724 (* 1 = 0.0386724 loss)
I1026 00:48:30.407802 17176 sgd_solver.cpp:106] Iteration 6580, lr = 0.001
I1026 00:48:30.986554 17176 solver.cpp:229] Iteration 6600, loss = 0.179697
I1026 00:48:30.986588 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0465867 (* 1 = 0.0465867 loss)
I1026 00:48:30.986595 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.133111 (* 1 = 0.133111 loss)
I1026 00:48:30.986610 17176 sgd_solver.cpp:106] Iteration 6600, lr = 0.001
I1026 00:48:31.571801 17176 solver.cpp:229] Iteration 6620, loss = 0.10241
I1026 00:48:31.571835 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0889563 (* 1 = 0.0889563 loss)
I1026 00:48:31.571842 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0134537 (* 1 = 0.0134537 loss)
I1026 00:48:31.571847 17176 sgd_solver.cpp:106] Iteration 6620, lr = 0.001
I1026 00:48:32.135077 17176 solver.cpp:229] Iteration 6640, loss = 0.0329429
I1026 00:48:32.135109 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0262803 (* 1 = 0.0262803 loss)
I1026 00:48:32.135114 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00666252 (* 1 = 0.00666252 loss)
I1026 00:48:32.135118 17176 sgd_solver.cpp:106] Iteration 6640, lr = 0.001
I1026 00:48:32.701161 17176 solver.cpp:229] Iteration 6660, loss = 0.0325024
I1026 00:48:32.701194 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0289135 (* 1 = 0.0289135 loss)
I1026 00:48:32.701197 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0035889 (* 1 = 0.0035889 loss)
I1026 00:48:32.701202 17176 sgd_solver.cpp:106] Iteration 6660, lr = 0.001
I1026 00:48:33.293126 17176 solver.cpp:229] Iteration 6680, loss = 0.077446
I1026 00:48:33.293159 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0696729 (* 1 = 0.0696729 loss)
I1026 00:48:33.293162 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00777318 (* 1 = 0.00777318 loss)
I1026 00:48:33.293167 17176 sgd_solver.cpp:106] Iteration 6680, lr = 0.001
I1026 00:48:33.866824 17176 solver.cpp:229] Iteration 6700, loss = 0.0937039
I1026 00:48:33.866858 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.024722 (* 1 = 0.024722 loss)
I1026 00:48:33.866863 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0689818 (* 1 = 0.0689818 loss)
I1026 00:48:33.866865 17176 sgd_solver.cpp:106] Iteration 6700, lr = 0.001
I1026 00:48:34.430344 17176 solver.cpp:229] Iteration 6720, loss = 0.117312
I1026 00:48:34.430375 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0788203 (* 1 = 0.0788203 loss)
I1026 00:48:34.430380 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0384919 (* 1 = 0.0384919 loss)
I1026 00:48:34.430384 17176 sgd_solver.cpp:106] Iteration 6720, lr = 0.001
I1026 00:48:35.004487 17176 solver.cpp:229] Iteration 6740, loss = 0.220645
I1026 00:48:35.004519 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0719989 (* 1 = 0.0719989 loss)
I1026 00:48:35.004523 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.148646 (* 1 = 0.148646 loss)
I1026 00:48:35.004528 17176 sgd_solver.cpp:106] Iteration 6740, lr = 0.001
I1026 00:48:35.584098 17176 solver.cpp:229] Iteration 6760, loss = 0.0446149
I1026 00:48:35.584131 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0186917 (* 1 = 0.0186917 loss)
I1026 00:48:35.584136 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0259232 (* 1 = 0.0259232 loss)
I1026 00:48:35.584139 17176 sgd_solver.cpp:106] Iteration 6760, lr = 0.001
I1026 00:48:36.162009 17176 solver.cpp:229] Iteration 6780, loss = 0.259435
I1026 00:48:36.162040 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.220382 (* 1 = 0.220382 loss)
I1026 00:48:36.162045 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0390528 (* 1 = 0.0390528 loss)
I1026 00:48:36.162050 17176 sgd_solver.cpp:106] Iteration 6780, lr = 0.001
I1026 00:48:36.710582 17176 solver.cpp:229] Iteration 6800, loss = 0.204044
I1026 00:48:36.710614 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.183128 (* 1 = 0.183128 loss)
I1026 00:48:36.710619 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0209161 (* 1 = 0.0209161 loss)
I1026 00:48:36.710624 17176 sgd_solver.cpp:106] Iteration 6800, lr = 0.001
I1026 00:48:37.281926 17176 solver.cpp:229] Iteration 6820, loss = 0.126412
I1026 00:48:37.281971 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0984125 (* 1 = 0.0984125 loss)
I1026 00:48:37.281976 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.028 (* 1 = 0.028 loss)
I1026 00:48:37.281980 17176 sgd_solver.cpp:106] Iteration 6820, lr = 0.001
I1026 00:48:37.860040 17176 solver.cpp:229] Iteration 6840, loss = 0.0604581
I1026 00:48:37.860074 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0502706 (* 1 = 0.0502706 loss)
I1026 00:48:37.860079 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0101875 (* 1 = 0.0101875 loss)
I1026 00:48:37.860082 17176 sgd_solver.cpp:106] Iteration 6840, lr = 0.001
I1026 00:48:38.441006 17176 solver.cpp:229] Iteration 6860, loss = 0.150632
I1026 00:48:38.441038 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.12208 (* 1 = 0.12208 loss)
I1026 00:48:38.441043 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0285514 (* 1 = 0.0285514 loss)
I1026 00:48:38.441048 17176 sgd_solver.cpp:106] Iteration 6860, lr = 0.001
I1026 00:48:38.989775 17176 solver.cpp:229] Iteration 6880, loss = 0.0767297
I1026 00:48:38.989807 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0724297 (* 1 = 0.0724297 loss)
I1026 00:48:38.989812 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00430003 (* 1 = 0.00430003 loss)
I1026 00:48:38.989816 17176 sgd_solver.cpp:106] Iteration 6880, lr = 0.001
I1026 00:48:39.557761 17176 solver.cpp:229] Iteration 6900, loss = 0.175991
I1026 00:48:39.557795 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.14866 (* 1 = 0.14866 loss)
I1026 00:48:39.557798 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.027331 (* 1 = 0.027331 loss)
I1026 00:48:39.557802 17176 sgd_solver.cpp:106] Iteration 6900, lr = 0.001
I1026 00:48:40.133024 17176 solver.cpp:229] Iteration 6920, loss = 0.11619
I1026 00:48:40.133055 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0907086 (* 1 = 0.0907086 loss)
I1026 00:48:40.133060 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0254815 (* 1 = 0.0254815 loss)
I1026 00:48:40.133064 17176 sgd_solver.cpp:106] Iteration 6920, lr = 0.001
I1026 00:48:40.696528 17176 solver.cpp:229] Iteration 6940, loss = 0.195233
I1026 00:48:40.696562 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.141448 (* 1 = 0.141448 loss)
I1026 00:48:40.696566 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0537845 (* 1 = 0.0537845 loss)
I1026 00:48:40.696570 17176 sgd_solver.cpp:106] Iteration 6940, lr = 0.001
I1026 00:48:41.254281 17176 solver.cpp:229] Iteration 6960, loss = 0.115205
I1026 00:48:41.254313 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.109285 (* 1 = 0.109285 loss)
I1026 00:48:41.254318 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00592017 (* 1 = 0.00592017 loss)
I1026 00:48:41.254323 17176 sgd_solver.cpp:106] Iteration 6960, lr = 0.001
I1026 00:48:41.819033 17176 solver.cpp:229] Iteration 6980, loss = 0.0602895
I1026 00:48:41.819077 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0324501 (* 1 = 0.0324501 loss)
I1026 00:48:41.819082 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0278395 (* 1 = 0.0278395 loss)
I1026 00:48:41.819087 17176 sgd_solver.cpp:106] Iteration 6980, lr = 0.001
I1026 00:48:42.387146 17176 solver.cpp:229] Iteration 7000, loss = 0.0225775
I1026 00:48:42.387178 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0213452 (* 1 = 0.0213452 loss)
I1026 00:48:42.387183 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0012323 (* 1 = 0.0012323 loss)
I1026 00:48:42.387187 17176 sgd_solver.cpp:106] Iteration 7000, lr = 0.001
I1026 00:48:42.973525 17176 solver.cpp:229] Iteration 7020, loss = 0.178682
I1026 00:48:42.973557 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0414171 (* 1 = 0.0414171 loss)
I1026 00:48:42.973562 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.137265 (* 1 = 0.137265 loss)
I1026 00:48:42.973567 17176 sgd_solver.cpp:106] Iteration 7020, lr = 0.001
I1026 00:48:43.531533 17176 solver.cpp:229] Iteration 7040, loss = 0.462072
I1026 00:48:43.531564 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.195039 (* 1 = 0.195039 loss)
I1026 00:48:43.531569 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.267033 (* 1 = 0.267033 loss)
I1026 00:48:43.531574 17176 sgd_solver.cpp:106] Iteration 7040, lr = 0.001
I1026 00:48:44.110224 17176 solver.cpp:229] Iteration 7060, loss = 0.137868
I1026 00:48:44.110265 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0691812 (* 1 = 0.0691812 loss)
I1026 00:48:44.110280 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0686865 (* 1 = 0.0686865 loss)
I1026 00:48:44.110283 17176 sgd_solver.cpp:106] Iteration 7060, lr = 0.001
I1026 00:48:44.669173 17176 solver.cpp:229] Iteration 7080, loss = 0.0510951
I1026 00:48:44.669206 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0392258 (* 1 = 0.0392258 loss)
I1026 00:48:44.669210 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0118694 (* 1 = 0.0118694 loss)
I1026 00:48:44.669215 17176 sgd_solver.cpp:106] Iteration 7080, lr = 0.001
I1026 00:48:45.229996 17176 solver.cpp:229] Iteration 7100, loss = 0.091565
I1026 00:48:45.230028 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0717527 (* 1 = 0.0717527 loss)
I1026 00:48:45.230033 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0198123 (* 1 = 0.0198123 loss)
I1026 00:48:45.230038 17176 sgd_solver.cpp:106] Iteration 7100, lr = 0.001
I1026 00:48:45.798703 17176 solver.cpp:229] Iteration 7120, loss = 0.0686617
I1026 00:48:45.798735 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0184806 (* 1 = 0.0184806 loss)
I1026 00:48:45.798740 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.050181 (* 1 = 0.050181 loss)
I1026 00:48:45.798744 17176 sgd_solver.cpp:106] Iteration 7120, lr = 0.001
I1026 00:48:46.365747 17176 solver.cpp:229] Iteration 7140, loss = 0.158315
I1026 00:48:46.365778 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.142552 (* 1 = 0.142552 loss)
I1026 00:48:46.365783 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0157631 (* 1 = 0.0157631 loss)
I1026 00:48:46.365788 17176 sgd_solver.cpp:106] Iteration 7140, lr = 0.001
I1026 00:48:46.932785 17176 solver.cpp:229] Iteration 7160, loss = 0.483416
I1026 00:48:46.932816 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.257559 (* 1 = 0.257559 loss)
I1026 00:48:46.932821 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.225858 (* 1 = 0.225858 loss)
I1026 00:48:46.932824 17176 sgd_solver.cpp:106] Iteration 7160, lr = 0.001
I1026 00:48:47.487701 17176 solver.cpp:229] Iteration 7180, loss = 0.0507417
I1026 00:48:47.487733 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0330484 (* 1 = 0.0330484 loss)
I1026 00:48:47.487737 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0176932 (* 1 = 0.0176932 loss)
I1026 00:48:47.487741 17176 sgd_solver.cpp:106] Iteration 7180, lr = 0.001
I1026 00:48:48.058293 17176 solver.cpp:229] Iteration 7200, loss = 0.0391259
I1026 00:48:48.058325 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0338083 (* 1 = 0.0338083 loss)
I1026 00:48:48.058328 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00531757 (* 1 = 0.00531757 loss)
I1026 00:48:48.058332 17176 sgd_solver.cpp:106] Iteration 7200, lr = 0.001
I1026 00:48:48.627912 17176 solver.cpp:229] Iteration 7220, loss = 0.08196
I1026 00:48:48.627948 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0706494 (* 1 = 0.0706494 loss)
I1026 00:48:48.627954 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0113106 (* 1 = 0.0113106 loss)
I1026 00:48:48.627959 17176 sgd_solver.cpp:106] Iteration 7220, lr = 0.001
I1026 00:48:49.203534 17176 solver.cpp:229] Iteration 7240, loss = 0.068784
I1026 00:48:49.203577 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0528449 (* 1 = 0.0528449 loss)
I1026 00:48:49.203582 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0159391 (* 1 = 0.0159391 loss)
I1026 00:48:49.203585 17176 sgd_solver.cpp:106] Iteration 7240, lr = 0.001
I1026 00:48:49.769207 17176 solver.cpp:229] Iteration 7260, loss = 0.0812394
I1026 00:48:49.769239 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0434763 (* 1 = 0.0434763 loss)
I1026 00:48:49.769244 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0377631 (* 1 = 0.0377631 loss)
I1026 00:48:49.769249 17176 sgd_solver.cpp:106] Iteration 7260, lr = 0.001
I1026 00:48:50.333283 17176 solver.cpp:229] Iteration 7280, loss = 0.11309
I1026 00:48:50.333326 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0935145 (* 1 = 0.0935145 loss)
I1026 00:48:50.333330 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0195756 (* 1 = 0.0195756 loss)
I1026 00:48:50.333334 17176 sgd_solver.cpp:106] Iteration 7280, lr = 0.001
I1026 00:48:50.909420 17176 solver.cpp:229] Iteration 7300, loss = 0.0413387
I1026 00:48:50.909453 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0405386 (* 1 = 0.0405386 loss)
I1026 00:48:50.909457 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.000800142 (* 1 = 0.000800142 loss)
I1026 00:48:50.909462 17176 sgd_solver.cpp:106] Iteration 7300, lr = 0.001
I1026 00:48:51.492002 17176 solver.cpp:229] Iteration 7320, loss = 0.0626558
I1026 00:48:51.492034 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0493288 (* 1 = 0.0493288 loss)
I1026 00:48:51.492038 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0133271 (* 1 = 0.0133271 loss)
I1026 00:48:51.492043 17176 sgd_solver.cpp:106] Iteration 7320, lr = 0.001
I1026 00:48:52.069089 17176 solver.cpp:229] Iteration 7340, loss = 0.0648691
I1026 00:48:52.069131 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0283074 (* 1 = 0.0283074 loss)
I1026 00:48:52.069135 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0365617 (* 1 = 0.0365617 loss)
I1026 00:48:52.069139 17176 sgd_solver.cpp:106] Iteration 7340, lr = 0.001
I1026 00:48:52.620724 17176 solver.cpp:229] Iteration 7360, loss = 0.136229
I1026 00:48:52.620766 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.12393 (* 1 = 0.12393 loss)
I1026 00:48:52.620771 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0122989 (* 1 = 0.0122989 loss)
I1026 00:48:52.620775 17176 sgd_solver.cpp:106] Iteration 7360, lr = 0.001
I1026 00:48:53.194025 17176 solver.cpp:229] Iteration 7380, loss = 0.0942378
I1026 00:48:53.194056 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0705874 (* 1 = 0.0705874 loss)
I1026 00:48:53.194061 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0236504 (* 1 = 0.0236504 loss)
I1026 00:48:53.194064 17176 sgd_solver.cpp:106] Iteration 7380, lr = 0.001
I1026 00:48:53.745712 17176 solver.cpp:229] Iteration 7400, loss = 0.0495011
I1026 00:48:53.745744 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0407714 (* 1 = 0.0407714 loss)
I1026 00:48:53.745748 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00872967 (* 1 = 0.00872967 loss)
I1026 00:48:53.745753 17176 sgd_solver.cpp:106] Iteration 7400, lr = 0.001
I1026 00:48:54.309797 17176 solver.cpp:229] Iteration 7420, loss = 0.143399
I1026 00:48:54.309828 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.116621 (* 1 = 0.116621 loss)
I1026 00:48:54.309833 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0267786 (* 1 = 0.0267786 loss)
I1026 00:48:54.309836 17176 sgd_solver.cpp:106] Iteration 7420, lr = 0.001
I1026 00:48:54.871971 17176 solver.cpp:229] Iteration 7440, loss = 0.156058
I1026 00:48:54.872014 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.145971 (* 1 = 0.145971 loss)
I1026 00:48:54.872018 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0100871 (* 1 = 0.0100871 loss)
I1026 00:48:54.872022 17176 sgd_solver.cpp:106] Iteration 7440, lr = 0.001
I1026 00:48:55.439023 17176 solver.cpp:229] Iteration 7460, loss = 0.0661706
I1026 00:48:55.439056 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0593888 (* 1 = 0.0593888 loss)
I1026 00:48:55.439060 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00678173 (* 1 = 0.00678173 loss)
I1026 00:48:55.439065 17176 sgd_solver.cpp:106] Iteration 7460, lr = 0.001
I1026 00:48:56.023093 17176 solver.cpp:229] Iteration 7480, loss = 0.0387135
I1026 00:48:56.023125 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0171241 (* 1 = 0.0171241 loss)
I1026 00:48:56.023130 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0215894 (* 1 = 0.0215894 loss)
I1026 00:48:56.023134 17176 sgd_solver.cpp:106] Iteration 7480, lr = 0.001
I1026 00:48:56.590651 17176 solver.cpp:229] Iteration 7500, loss = 0.0782608
I1026 00:48:56.590682 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0523006 (* 1 = 0.0523006 loss)
I1026 00:48:56.590687 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0259602 (* 1 = 0.0259602 loss)
I1026 00:48:56.590692 17176 sgd_solver.cpp:106] Iteration 7500, lr = 0.001
I1026 00:48:57.146749 17176 solver.cpp:229] Iteration 7520, loss = 0.0790352
I1026 00:48:57.146780 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0225037 (* 1 = 0.0225037 loss)
I1026 00:48:57.146785 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0565315 (* 1 = 0.0565315 loss)
I1026 00:48:57.146788 17176 sgd_solver.cpp:106] Iteration 7520, lr = 0.001
I1026 00:48:57.705438 17176 solver.cpp:229] Iteration 7540, loss = 0.162484
I1026 00:48:57.705471 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.128599 (* 1 = 0.128599 loss)
I1026 00:48:57.705476 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0338854 (* 1 = 0.0338854 loss)
I1026 00:48:57.705482 17176 sgd_solver.cpp:106] Iteration 7540, lr = 0.001
I1026 00:48:58.280998 17176 solver.cpp:229] Iteration 7560, loss = 0.0715087
I1026 00:48:58.281033 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0264894 (* 1 = 0.0264894 loss)
I1026 00:48:58.281038 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0450193 (* 1 = 0.0450193 loss)
I1026 00:48:58.281044 17176 sgd_solver.cpp:106] Iteration 7560, lr = 0.001
I1026 00:48:58.844753 17176 solver.cpp:229] Iteration 7580, loss = 0.474978
I1026 00:48:58.844785 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.258244 (* 1 = 0.258244 loss)
I1026 00:48:58.844792 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.216734 (* 1 = 0.216734 loss)
I1026 00:48:58.844797 17176 sgd_solver.cpp:106] Iteration 7580, lr = 0.001
I1026 00:48:59.415127 17176 solver.cpp:229] Iteration 7600, loss = 0.298585
I1026 00:48:59.415161 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.18026 (* 1 = 0.18026 loss)
I1026 00:48:59.415169 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.118325 (* 1 = 0.118325 loss)
I1026 00:48:59.415174 17176 sgd_solver.cpp:106] Iteration 7600, lr = 0.001
I1026 00:48:59.992169 17176 solver.cpp:229] Iteration 7620, loss = 0.0948626
I1026 00:48:59.992202 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0408596 (* 1 = 0.0408596 loss)
I1026 00:48:59.992209 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0540031 (* 1 = 0.0540031 loss)
I1026 00:48:59.992214 17176 sgd_solver.cpp:106] Iteration 7620, lr = 0.001
I1026 00:49:00.557354 17176 solver.cpp:229] Iteration 7640, loss = 0.0276574
I1026 00:49:00.557387 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0221252 (* 1 = 0.0221252 loss)
I1026 00:49:00.557394 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0055323 (* 1 = 0.0055323 loss)
I1026 00:49:00.557399 17176 sgd_solver.cpp:106] Iteration 7640, lr = 0.001
I1026 00:49:01.138034 17176 solver.cpp:229] Iteration 7660, loss = 0.0885001
I1026 00:49:01.138070 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0641601 (* 1 = 0.0641601 loss)
I1026 00:49:01.138077 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0243399 (* 1 = 0.0243399 loss)
I1026 00:49:01.138082 17176 sgd_solver.cpp:106] Iteration 7660, lr = 0.001
I1026 00:49:01.710933 17176 solver.cpp:229] Iteration 7680, loss = 0.0940681
I1026 00:49:01.710968 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0804388 (* 1 = 0.0804388 loss)
I1026 00:49:01.710973 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0136293 (* 1 = 0.0136293 loss)
I1026 00:49:01.710978 17176 sgd_solver.cpp:106] Iteration 7680, lr = 0.001
I1026 00:49:02.273396 17176 solver.cpp:229] Iteration 7700, loss = 0.169671
I1026 00:49:02.273429 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0959152 (* 1 = 0.0959152 loss)
I1026 00:49:02.273437 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0737559 (* 1 = 0.0737559 loss)
I1026 00:49:02.273442 17176 sgd_solver.cpp:106] Iteration 7700, lr = 0.001
I1026 00:49:02.846488 17176 solver.cpp:229] Iteration 7720, loss = 0.0839564
I1026 00:49:02.846524 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0273553 (* 1 = 0.0273553 loss)
I1026 00:49:02.846530 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0566011 (* 1 = 0.0566011 loss)
I1026 00:49:02.846545 17176 sgd_solver.cpp:106] Iteration 7720, lr = 0.001
I1026 00:49:03.416560 17176 solver.cpp:229] Iteration 7740, loss = 0.163448
I1026 00:49:03.416594 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.150378 (* 1 = 0.150378 loss)
I1026 00:49:03.416601 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0130696 (* 1 = 0.0130696 loss)
I1026 00:49:03.416606 17176 sgd_solver.cpp:106] Iteration 7740, lr = 0.001
I1026 00:49:03.985512 17176 solver.cpp:229] Iteration 7760, loss = 0.0882236
I1026 00:49:03.985545 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0740298 (* 1 = 0.0740298 loss)
I1026 00:49:03.985553 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0141938 (* 1 = 0.0141938 loss)
I1026 00:49:03.985558 17176 sgd_solver.cpp:106] Iteration 7760, lr = 0.001
I1026 00:49:04.554461 17176 solver.cpp:229] Iteration 7780, loss = 0.11781
I1026 00:49:04.554494 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.090292 (* 1 = 0.090292 loss)
I1026 00:49:04.554502 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0275179 (* 1 = 0.0275179 loss)
I1026 00:49:04.554507 17176 sgd_solver.cpp:106] Iteration 7780, lr = 0.001
I1026 00:49:05.130612 17176 solver.cpp:229] Iteration 7800, loss = 0.108672
I1026 00:49:05.130645 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.104109 (* 1 = 0.104109 loss)
I1026 00:49:05.130651 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00456264 (* 1 = 0.00456264 loss)
I1026 00:49:05.130657 17176 sgd_solver.cpp:106] Iteration 7800, lr = 0.001
I1026 00:49:05.688272 17176 solver.cpp:229] Iteration 7820, loss = 0.116136
I1026 00:49:05.688305 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0706603 (* 1 = 0.0706603 loss)
I1026 00:49:05.688313 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0454757 (* 1 = 0.0454757 loss)
I1026 00:49:05.688328 17176 sgd_solver.cpp:106] Iteration 7820, lr = 0.001
I1026 00:49:06.253870 17176 solver.cpp:229] Iteration 7840, loss = 0.0796084
I1026 00:49:06.253904 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0684724 (* 1 = 0.0684724 loss)
I1026 00:49:06.253911 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0111361 (* 1 = 0.0111361 loss)
I1026 00:49:06.253916 17176 sgd_solver.cpp:106] Iteration 7840, lr = 0.001
I1026 00:49:06.823299 17176 solver.cpp:229] Iteration 7860, loss = 0.133112
I1026 00:49:06.823333 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.112372 (* 1 = 0.112372 loss)
I1026 00:49:06.823339 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0207398 (* 1 = 0.0207398 loss)
I1026 00:49:06.823344 17176 sgd_solver.cpp:106] Iteration 7860, lr = 0.001
I1026 00:49:07.366261 17176 solver.cpp:229] Iteration 7880, loss = 0.101235
I1026 00:49:07.366294 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0414654 (* 1 = 0.0414654 loss)
I1026 00:49:07.366302 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0597697 (* 1 = 0.0597697 loss)
I1026 00:49:07.366307 17176 sgd_solver.cpp:106] Iteration 7880, lr = 0.001
I1026 00:49:07.936633 17176 solver.cpp:229] Iteration 7900, loss = 0.141859
I1026 00:49:07.936666 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.116701 (* 1 = 0.116701 loss)
I1026 00:49:07.936673 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0251584 (* 1 = 0.0251584 loss)
I1026 00:49:07.936679 17176 sgd_solver.cpp:106] Iteration 7900, lr = 0.001
I1026 00:49:08.507691 17176 solver.cpp:229] Iteration 7920, loss = 0.0947239
I1026 00:49:08.507725 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0891493 (* 1 = 0.0891493 loss)
I1026 00:49:08.507731 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00557462 (* 1 = 0.00557462 loss)
I1026 00:49:08.507737 17176 sgd_solver.cpp:106] Iteration 7920, lr = 0.001
I1026 00:49:09.090834 17176 solver.cpp:229] Iteration 7940, loss = 0.0938267
I1026 00:49:09.090869 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0795946 (* 1 = 0.0795946 loss)
I1026 00:49:09.090876 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0142321 (* 1 = 0.0142321 loss)
I1026 00:49:09.090891 17176 sgd_solver.cpp:106] Iteration 7940, lr = 0.001
I1026 00:49:09.669162 17176 solver.cpp:229] Iteration 7960, loss = 0.0304673
I1026 00:49:09.669203 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0205827 (* 1 = 0.0205827 loss)
I1026 00:49:09.669209 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00988461 (* 1 = 0.00988461 loss)
I1026 00:49:09.669212 17176 sgd_solver.cpp:106] Iteration 7960, lr = 0.001
I1026 00:49:10.235678 17176 solver.cpp:229] Iteration 7980, loss = 0.0809573
I1026 00:49:10.235710 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0720059 (* 1 = 0.0720059 loss)
I1026 00:49:10.235715 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00895137 (* 1 = 0.00895137 loss)
I1026 00:49:10.235718 17176 sgd_solver.cpp:106] Iteration 7980, lr = 0.001
I1026 00:49:10.790122 17176 solver.cpp:229] Iteration 8000, loss = 0.0847774
I1026 00:49:10.790154 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0829047 (* 1 = 0.0829047 loss)
I1026 00:49:10.790159 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00187275 (* 1 = 0.00187275 loss)
I1026 00:49:10.790176 17176 sgd_solver.cpp:106] Iteration 8000, lr = 0.001
I1026 00:49:11.364290 17176 solver.cpp:229] Iteration 8020, loss = 0.133499
I1026 00:49:11.364320 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0735338 (* 1 = 0.0735338 loss)
I1026 00:49:11.364326 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0599652 (* 1 = 0.0599652 loss)
I1026 00:49:11.364329 17176 sgd_solver.cpp:106] Iteration 8020, lr = 0.001
I1026 00:49:11.935987 17176 solver.cpp:229] Iteration 8040, loss = 0.826715
I1026 00:49:11.936020 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.552778 (* 1 = 0.552778 loss)
I1026 00:49:11.936025 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.273937 (* 1 = 0.273937 loss)
I1026 00:49:11.936030 17176 sgd_solver.cpp:106] Iteration 8040, lr = 0.001
I1026 00:49:12.502656 17176 solver.cpp:229] Iteration 8060, loss = 0.12969
I1026 00:49:12.502691 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0879554 (* 1 = 0.0879554 loss)
I1026 00:49:12.502696 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0417351 (* 1 = 0.0417351 loss)
I1026 00:49:12.502699 17176 sgd_solver.cpp:106] Iteration 8060, lr = 0.001
I1026 00:49:13.073706 17176 solver.cpp:229] Iteration 8080, loss = 0.0548296
I1026 00:49:13.073737 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0501696 (* 1 = 0.0501696 loss)
I1026 00:49:13.073742 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00466002 (* 1 = 0.00466002 loss)
I1026 00:49:13.073747 17176 sgd_solver.cpp:106] Iteration 8080, lr = 0.001
I1026 00:49:13.648969 17176 solver.cpp:229] Iteration 8100, loss = 0.174109
I1026 00:49:13.649001 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.11024 (* 1 = 0.11024 loss)
I1026 00:49:13.649006 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.063869 (* 1 = 0.063869 loss)
I1026 00:49:13.649010 17176 sgd_solver.cpp:106] Iteration 8100, lr = 0.001
I1026 00:49:14.209761 17176 solver.cpp:229] Iteration 8120, loss = 0.56341
I1026 00:49:14.209794 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.312103 (* 1 = 0.312103 loss)
I1026 00:49:14.209799 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.251307 (* 1 = 0.251307 loss)
I1026 00:49:14.209803 17176 sgd_solver.cpp:106] Iteration 8120, lr = 0.001
I1026 00:49:14.772343 17176 solver.cpp:229] Iteration 8140, loss = 0.0832885
I1026 00:49:14.772377 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0773857 (* 1 = 0.0773857 loss)
I1026 00:49:14.772382 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00590278 (* 1 = 0.00590278 loss)
I1026 00:49:14.772385 17176 sgd_solver.cpp:106] Iteration 8140, lr = 0.001
I1026 00:49:15.350441 17176 solver.cpp:229] Iteration 8160, loss = 0.177105
I1026 00:49:15.350474 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.161969 (* 1 = 0.161969 loss)
I1026 00:49:15.350479 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0151357 (* 1 = 0.0151357 loss)
I1026 00:49:15.350483 17176 sgd_solver.cpp:106] Iteration 8160, lr = 0.001
I1026 00:49:15.928586 17176 solver.cpp:229] Iteration 8180, loss = 0.0849785
I1026 00:49:15.928618 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0656128 (* 1 = 0.0656128 loss)
I1026 00:49:15.928623 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0193657 (* 1 = 0.0193657 loss)
I1026 00:49:15.928628 17176 sgd_solver.cpp:106] Iteration 8180, lr = 0.001
I1026 00:49:16.498661 17176 solver.cpp:229] Iteration 8200, loss = 0.115658
I1026 00:49:16.498694 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0896785 (* 1 = 0.0896785 loss)
I1026 00:49:16.498699 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0259798 (* 1 = 0.0259798 loss)
I1026 00:49:16.498704 17176 sgd_solver.cpp:106] Iteration 8200, lr = 0.001
I1026 00:49:17.071072 17176 solver.cpp:229] Iteration 8220, loss = 0.101649
I1026 00:49:17.071116 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0992392 (* 1 = 0.0992392 loss)
I1026 00:49:17.071121 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00240953 (* 1 = 0.00240953 loss)
I1026 00:49:17.071135 17176 sgd_solver.cpp:106] Iteration 8220, lr = 0.001
I1026 00:49:17.633050 17176 solver.cpp:229] Iteration 8240, loss = 0.0334813
I1026 00:49:17.633082 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0107022 (* 1 = 0.0107022 loss)
I1026 00:49:17.633087 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0227791 (* 1 = 0.0227791 loss)
I1026 00:49:17.633091 17176 sgd_solver.cpp:106] Iteration 8240, lr = 0.001
I1026 00:49:18.205054 17176 solver.cpp:229] Iteration 8260, loss = 0.088951
I1026 00:49:18.205085 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0792809 (* 1 = 0.0792809 loss)
I1026 00:49:18.205091 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00967013 (* 1 = 0.00967013 loss)
I1026 00:49:18.205094 17176 sgd_solver.cpp:106] Iteration 8260, lr = 0.001
I1026 00:49:18.779573 17176 solver.cpp:229] Iteration 8280, loss = 0.714615
I1026 00:49:18.779604 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.230797 (* 1 = 0.230797 loss)
I1026 00:49:18.779609 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.483818 (* 1 = 0.483818 loss)
I1026 00:49:18.779613 17176 sgd_solver.cpp:106] Iteration 8280, lr = 0.001
I1026 00:49:19.345854 17176 solver.cpp:229] Iteration 8300, loss = 0.0956066
I1026 00:49:19.345885 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0883806 (* 1 = 0.0883806 loss)
I1026 00:49:19.345890 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00722599 (* 1 = 0.00722599 loss)
I1026 00:49:19.345895 17176 sgd_solver.cpp:106] Iteration 8300, lr = 0.001
I1026 00:49:19.918478 17176 solver.cpp:229] Iteration 8320, loss = 0.39972
I1026 00:49:19.918511 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.308104 (* 1 = 0.308104 loss)
I1026 00:49:19.918516 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0916157 (* 1 = 0.0916157 loss)
I1026 00:49:19.918520 17176 sgd_solver.cpp:106] Iteration 8320, lr = 0.001
I1026 00:49:20.471906 17176 solver.cpp:229] Iteration 8340, loss = 0.126168
I1026 00:49:20.471937 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0902832 (* 1 = 0.0902832 loss)
I1026 00:49:20.471941 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0358848 (* 1 = 0.0358848 loss)
I1026 00:49:20.471946 17176 sgd_solver.cpp:106] Iteration 8340, lr = 0.001
I1026 00:49:21.043067 17176 solver.cpp:229] Iteration 8360, loss = 0.0874985
I1026 00:49:21.043100 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0429924 (* 1 = 0.0429924 loss)
I1026 00:49:21.043105 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.044506 (* 1 = 0.044506 loss)
I1026 00:49:21.043109 17176 sgd_solver.cpp:106] Iteration 8360, lr = 0.001
I1026 00:49:21.611050 17176 solver.cpp:229] Iteration 8380, loss = 0.0941203
I1026 00:49:21.611083 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0751906 (* 1 = 0.0751906 loss)
I1026 00:49:21.611088 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0189297 (* 1 = 0.0189297 loss)
I1026 00:49:21.611093 17176 sgd_solver.cpp:106] Iteration 8380, lr = 0.001
I1026 00:49:22.178870 17176 solver.cpp:229] Iteration 8400, loss = 0.39373
I1026 00:49:22.178902 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.247265 (* 1 = 0.247265 loss)
I1026 00:49:22.178906 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.146465 (* 1 = 0.146465 loss)
I1026 00:49:22.178910 17176 sgd_solver.cpp:106] Iteration 8400, lr = 0.001
I1026 00:49:22.769657 17176 solver.cpp:229] Iteration 8420, loss = 0.60933
I1026 00:49:22.769690 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.301631 (* 1 = 0.301631 loss)
I1026 00:49:22.769695 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.307699 (* 1 = 0.307699 loss)
I1026 00:49:22.769700 17176 sgd_solver.cpp:106] Iteration 8420, lr = 0.001
I1026 00:49:23.332548 17176 solver.cpp:229] Iteration 8440, loss = 0.121041
I1026 00:49:23.332581 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0907772 (* 1 = 0.0907772 loss)
I1026 00:49:23.332586 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0302635 (* 1 = 0.0302635 loss)
I1026 00:49:23.332590 17176 sgd_solver.cpp:106] Iteration 8440, lr = 0.001
I1026 00:49:23.899339 17176 solver.cpp:229] Iteration 8460, loss = 0.142281
I1026 00:49:23.899370 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.127304 (* 1 = 0.127304 loss)
I1026 00:49:23.899375 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0149773 (* 1 = 0.0149773 loss)
I1026 00:49:23.899379 17176 sgd_solver.cpp:106] Iteration 8460, lr = 0.001
I1026 00:49:24.470902 17176 solver.cpp:229] Iteration 8480, loss = 0.0882997
I1026 00:49:24.470933 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0371878 (* 1 = 0.0371878 loss)
I1026 00:49:24.470938 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0511119 (* 1 = 0.0511119 loss)
I1026 00:49:24.470942 17176 sgd_solver.cpp:106] Iteration 8480, lr = 0.001
I1026 00:49:25.049126 17176 solver.cpp:229] Iteration 8500, loss = 0.251892
I1026 00:49:25.049159 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.230607 (* 1 = 0.230607 loss)
I1026 00:49:25.049163 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0212854 (* 1 = 0.0212854 loss)
I1026 00:49:25.049167 17176 sgd_solver.cpp:106] Iteration 8500, lr = 0.001
I1026 00:49:25.619424 17176 solver.cpp:229] Iteration 8520, loss = 0.126372
I1026 00:49:25.619462 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0756151 (* 1 = 0.0756151 loss)
I1026 00:49:25.619467 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0507574 (* 1 = 0.0507574 loss)
I1026 00:49:25.619472 17176 sgd_solver.cpp:106] Iteration 8520, lr = 0.001
I1026 00:49:26.193527 17176 solver.cpp:229] Iteration 8540, loss = 0.0378629
I1026 00:49:26.193557 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0324821 (* 1 = 0.0324821 loss)
I1026 00:49:26.193562 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00538079 (* 1 = 0.00538079 loss)
I1026 00:49:26.193565 17176 sgd_solver.cpp:106] Iteration 8540, lr = 0.001
I1026 00:49:26.769084 17176 solver.cpp:229] Iteration 8560, loss = 0.355803
I1026 00:49:26.769116 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.246609 (* 1 = 0.246609 loss)
I1026 00:49:26.769121 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.109194 (* 1 = 0.109194 loss)
I1026 00:49:26.769126 17176 sgd_solver.cpp:106] Iteration 8560, lr = 0.001
I1026 00:49:27.323290 17176 solver.cpp:229] Iteration 8580, loss = 0.357691
I1026 00:49:27.323333 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.197945 (* 1 = 0.197945 loss)
I1026 00:49:27.323338 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.159746 (* 1 = 0.159746 loss)
I1026 00:49:27.323341 17176 sgd_solver.cpp:106] Iteration 8580, lr = 0.001
I1026 00:49:27.881244 17176 solver.cpp:229] Iteration 8600, loss = 0.0850765
I1026 00:49:27.881275 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0392015 (* 1 = 0.0392015 loss)
I1026 00:49:27.881280 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.045875 (* 1 = 0.045875 loss)
I1026 00:49:27.881284 17176 sgd_solver.cpp:106] Iteration 8600, lr = 0.001
I1026 00:49:28.434902 17176 solver.cpp:229] Iteration 8620, loss = 0.0489596
I1026 00:49:28.434936 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.043348 (* 1 = 0.043348 loss)
I1026 00:49:28.434940 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00561164 (* 1 = 0.00561164 loss)
I1026 00:49:28.434945 17176 sgd_solver.cpp:106] Iteration 8620, lr = 0.001
I1026 00:49:28.998409 17176 solver.cpp:229] Iteration 8640, loss = 0.976117
I1026 00:49:28.998442 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.484861 (* 1 = 0.484861 loss)
I1026 00:49:28.998447 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.491256 (* 1 = 0.491256 loss)
I1026 00:49:28.998451 17176 sgd_solver.cpp:106] Iteration 8640, lr = 0.001
I1026 00:49:29.567181 17176 solver.cpp:229] Iteration 8660, loss = 0.182263
I1026 00:49:29.567214 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.149436 (* 1 = 0.149436 loss)
I1026 00:49:29.567219 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0328271 (* 1 = 0.0328271 loss)
I1026 00:49:29.567224 17176 sgd_solver.cpp:106] Iteration 8660, lr = 0.001
I1026 00:49:30.142527 17176 solver.cpp:229] Iteration 8680, loss = 0.106242
I1026 00:49:30.142559 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0571796 (* 1 = 0.0571796 loss)
I1026 00:49:30.142563 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0490622 (* 1 = 0.0490622 loss)
I1026 00:49:30.142568 17176 sgd_solver.cpp:106] Iteration 8680, lr = 0.001
I1026 00:49:30.722172 17176 solver.cpp:229] Iteration 8700, loss = 0.0982338
I1026 00:49:30.722205 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0912986 (* 1 = 0.0912986 loss)
I1026 00:49:30.722210 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00693524 (* 1 = 0.00693524 loss)
I1026 00:49:30.722215 17176 sgd_solver.cpp:106] Iteration 8700, lr = 0.001
I1026 00:49:31.282529 17176 solver.cpp:229] Iteration 8720, loss = 0.0614884
I1026 00:49:31.282562 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0386004 (* 1 = 0.0386004 loss)
I1026 00:49:31.282567 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.022888 (* 1 = 0.022888 loss)
I1026 00:49:31.282572 17176 sgd_solver.cpp:106] Iteration 8720, lr = 0.001
I1026 00:49:31.850123 17176 solver.cpp:229] Iteration 8740, loss = 0.068277
I1026 00:49:31.850157 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0581038 (* 1 = 0.0581038 loss)
I1026 00:49:31.850160 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0101732 (* 1 = 0.0101732 loss)
I1026 00:49:31.850164 17176 sgd_solver.cpp:106] Iteration 8740, lr = 0.001
I1026 00:49:32.436287 17176 solver.cpp:229] Iteration 8760, loss = 0.0389026
I1026 00:49:32.436342 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0349114 (* 1 = 0.0349114 loss)
I1026 00:49:32.436347 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00399124 (* 1 = 0.00399124 loss)
I1026 00:49:32.436350 17176 sgd_solver.cpp:106] Iteration 8760, lr = 0.001
I1026 00:49:32.989738 17176 solver.cpp:229] Iteration 8780, loss = 0.127337
I1026 00:49:32.989781 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.120121 (* 1 = 0.120121 loss)
I1026 00:49:32.989795 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00721596 (* 1 = 0.00721596 loss)
I1026 00:49:32.989799 17176 sgd_solver.cpp:106] Iteration 8780, lr = 0.001
I1026 00:49:33.560688 17176 solver.cpp:229] Iteration 8800, loss = 0.106063
I1026 00:49:33.560747 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.063921 (* 1 = 0.063921 loss)
I1026 00:49:33.560761 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0421416 (* 1 = 0.0421416 loss)
I1026 00:49:33.560765 17176 sgd_solver.cpp:106] Iteration 8800, lr = 0.001
I1026 00:49:34.135080 17176 solver.cpp:229] Iteration 8820, loss = 0.0807314
I1026 00:49:34.135113 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0365002 (* 1 = 0.0365002 loss)
I1026 00:49:34.135118 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0442312 (* 1 = 0.0442312 loss)
I1026 00:49:34.135123 17176 sgd_solver.cpp:106] Iteration 8820, lr = 0.001
I1026 00:49:34.703066 17176 solver.cpp:229] Iteration 8840, loss = 0.0993107
I1026 00:49:34.703099 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0576976 (* 1 = 0.0576976 loss)
I1026 00:49:34.703104 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0416132 (* 1 = 0.0416132 loss)
I1026 00:49:34.703107 17176 sgd_solver.cpp:106] Iteration 8840, lr = 0.001
I1026 00:49:35.263782 17176 solver.cpp:229] Iteration 8860, loss = 0.0545736
I1026 00:49:35.263814 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0493983 (* 1 = 0.0493983 loss)
I1026 00:49:35.263819 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00517525 (* 1 = 0.00517525 loss)
I1026 00:49:35.263824 17176 sgd_solver.cpp:106] Iteration 8860, lr = 0.001
I1026 00:49:35.841239 17176 solver.cpp:229] Iteration 8880, loss = 0.143751
I1026 00:49:35.841271 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.122607 (* 1 = 0.122607 loss)
I1026 00:49:35.841277 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0211438 (* 1 = 0.0211438 loss)
I1026 00:49:35.841281 17176 sgd_solver.cpp:106] Iteration 8880, lr = 0.001
I1026 00:49:36.425676 17176 solver.cpp:229] Iteration 8900, loss = 0.491783
I1026 00:49:36.425707 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.246377 (* 1 = 0.246377 loss)
I1026 00:49:36.425711 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.245407 (* 1 = 0.245407 loss)
I1026 00:49:36.425715 17176 sgd_solver.cpp:106] Iteration 8900, lr = 0.001
I1026 00:49:36.980020 17176 solver.cpp:229] Iteration 8920, loss = 0.0992827
I1026 00:49:36.980052 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0784736 (* 1 = 0.0784736 loss)
I1026 00:49:36.980057 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0208091 (* 1 = 0.0208091 loss)
I1026 00:49:36.980062 17176 sgd_solver.cpp:106] Iteration 8920, lr = 0.001
I1026 00:49:37.548817 17176 solver.cpp:229] Iteration 8940, loss = 0.165393
I1026 00:49:37.548848 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.117953 (* 1 = 0.117953 loss)
I1026 00:49:37.548853 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0474391 (* 1 = 0.0474391 loss)
I1026 00:49:37.548858 17176 sgd_solver.cpp:106] Iteration 8940, lr = 0.001
I1026 00:49:38.110750 17176 solver.cpp:229] Iteration 8960, loss = 0.106959
I1026 00:49:38.110783 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0368597 (* 1 = 0.0368597 loss)
I1026 00:49:38.110788 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0700995 (* 1 = 0.0700995 loss)
I1026 00:49:38.110791 17176 sgd_solver.cpp:106] Iteration 8960, lr = 0.001
I1026 00:49:38.675498 17176 solver.cpp:229] Iteration 8980, loss = 0.0875553
I1026 00:49:38.675529 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0645781 (* 1 = 0.0645781 loss)
I1026 00:49:38.675534 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0229772 (* 1 = 0.0229772 loss)
I1026 00:49:38.675539 17176 sgd_solver.cpp:106] Iteration 8980, lr = 0.001
I1026 00:49:39.240711 17176 solver.cpp:229] Iteration 9000, loss = 0.142327
I1026 00:49:39.240744 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.128418 (* 1 = 0.128418 loss)
I1026 00:49:39.240749 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0139093 (* 1 = 0.0139093 loss)
I1026 00:49:39.240753 17176 sgd_solver.cpp:106] Iteration 9000, lr = 0.001
I1026 00:49:39.814231 17176 solver.cpp:229] Iteration 9020, loss = 0.142297
I1026 00:49:39.814265 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0727297 (* 1 = 0.0727297 loss)
I1026 00:49:39.814270 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0695674 (* 1 = 0.0695674 loss)
I1026 00:49:39.814272 17176 sgd_solver.cpp:106] Iteration 9020, lr = 0.001
I1026 00:49:40.391983 17176 solver.cpp:229] Iteration 9040, loss = 0.14592
I1026 00:49:40.392014 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.125556 (* 1 = 0.125556 loss)
I1026 00:49:40.392019 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0203645 (* 1 = 0.0203645 loss)
I1026 00:49:40.392022 17176 sgd_solver.cpp:106] Iteration 9040, lr = 0.001
I1026 00:49:40.965175 17176 solver.cpp:229] Iteration 9060, loss = 0.102863
I1026 00:49:40.965207 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0863283 (* 1 = 0.0863283 loss)
I1026 00:49:40.965212 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.016535 (* 1 = 0.016535 loss)
I1026 00:49:40.965216 17176 sgd_solver.cpp:106] Iteration 9060, lr = 0.001
I1026 00:49:41.538395 17176 solver.cpp:229] Iteration 9080, loss = 0.109903
I1026 00:49:41.538427 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0836578 (* 1 = 0.0836578 loss)
I1026 00:49:41.538432 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0262456 (* 1 = 0.0262456 loss)
I1026 00:49:41.538436 17176 sgd_solver.cpp:106] Iteration 9080, lr = 0.001
I1026 00:49:42.105968 17176 solver.cpp:229] Iteration 9100, loss = 0.224961
I1026 00:49:42.106000 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.180827 (* 1 = 0.180827 loss)
I1026 00:49:42.106005 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0441348 (* 1 = 0.0441348 loss)
I1026 00:49:42.106010 17176 sgd_solver.cpp:106] Iteration 9100, lr = 0.001
I1026 00:49:42.673632 17176 solver.cpp:229] Iteration 9120, loss = 0.0992518
I1026 00:49:42.673665 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0719636 (* 1 = 0.0719636 loss)
I1026 00:49:42.673669 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0272882 (* 1 = 0.0272882 loss)
I1026 00:49:42.673673 17176 sgd_solver.cpp:106] Iteration 9120, lr = 0.001
I1026 00:49:43.252578 17176 solver.cpp:229] Iteration 9140, loss = 0.225893
I1026 00:49:43.252609 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.13161 (* 1 = 0.13161 loss)
I1026 00:49:43.252614 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0942833 (* 1 = 0.0942833 loss)
I1026 00:49:43.252617 17176 sgd_solver.cpp:106] Iteration 9140, lr = 0.001
I1026 00:49:43.822644 17176 solver.cpp:229] Iteration 9160, loss = 0.0777547
I1026 00:49:43.822675 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0281415 (* 1 = 0.0281415 loss)
I1026 00:49:43.822680 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0496132 (* 1 = 0.0496132 loss)
I1026 00:49:43.822685 17176 sgd_solver.cpp:106] Iteration 9160, lr = 0.001
I1026 00:49:44.387064 17176 solver.cpp:229] Iteration 9180, loss = 0.15606
I1026 00:49:44.387096 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.122252 (* 1 = 0.122252 loss)
I1026 00:49:44.387101 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0338081 (* 1 = 0.0338081 loss)
I1026 00:49:44.387105 17176 sgd_solver.cpp:106] Iteration 9180, lr = 0.001
I1026 00:49:44.972213 17176 solver.cpp:229] Iteration 9200, loss = 0.125216
I1026 00:49:44.972246 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0860818 (* 1 = 0.0860818 loss)
I1026 00:49:44.972251 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0391341 (* 1 = 0.0391341 loss)
I1026 00:49:44.972254 17176 sgd_solver.cpp:106] Iteration 9200, lr = 0.001
I1026 00:49:45.534991 17176 solver.cpp:229] Iteration 9220, loss = 0.129014
I1026 00:49:45.535023 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0927799 (* 1 = 0.0927799 loss)
I1026 00:49:45.535028 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0362346 (* 1 = 0.0362346 loss)
I1026 00:49:45.535032 17176 sgd_solver.cpp:106] Iteration 9220, lr = 0.001
I1026 00:49:46.114799 17176 solver.cpp:229] Iteration 9240, loss = 0.0743941
I1026 00:49:46.114830 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0595792 (* 1 = 0.0595792 loss)
I1026 00:49:46.114835 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0148149 (* 1 = 0.0148149 loss)
I1026 00:49:46.114840 17176 sgd_solver.cpp:106] Iteration 9240, lr = 0.001
I1026 00:49:46.670436 17176 solver.cpp:229] Iteration 9260, loss = 0.103245
I1026 00:49:46.670469 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0739328 (* 1 = 0.0739328 loss)
I1026 00:49:46.670472 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0293126 (* 1 = 0.0293126 loss)
I1026 00:49:46.670476 17176 sgd_solver.cpp:106] Iteration 9260, lr = 0.001
I1026 00:49:47.251781 17176 solver.cpp:229] Iteration 9280, loss = 0.1132
I1026 00:49:47.251814 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0792353 (* 1 = 0.0792353 loss)
I1026 00:49:47.251818 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0339647 (* 1 = 0.0339647 loss)
I1026 00:49:47.251823 17176 sgd_solver.cpp:106] Iteration 9280, lr = 0.001
I1026 00:49:47.839380 17176 solver.cpp:229] Iteration 9300, loss = 0.469351
I1026 00:49:47.839412 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.226458 (* 1 = 0.226458 loss)
I1026 00:49:47.839417 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.242894 (* 1 = 0.242894 loss)
I1026 00:49:47.839421 17176 sgd_solver.cpp:106] Iteration 9300, lr = 0.001
I1026 00:49:48.416124 17176 solver.cpp:229] Iteration 9320, loss = 0.248454
I1026 00:49:48.416157 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.112644 (* 1 = 0.112644 loss)
I1026 00:49:48.416162 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.13581 (* 1 = 0.13581 loss)
I1026 00:49:48.416165 17176 sgd_solver.cpp:106] Iteration 9320, lr = 0.001
I1026 00:49:48.967180 17176 solver.cpp:229] Iteration 9340, loss = 0.0893799
I1026 00:49:48.967211 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0761489 (* 1 = 0.0761489 loss)
I1026 00:49:48.967216 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.013231 (* 1 = 0.013231 loss)
I1026 00:49:48.967221 17176 sgd_solver.cpp:106] Iteration 9340, lr = 0.001
I1026 00:49:49.532769 17176 solver.cpp:229] Iteration 9360, loss = 0.156719
I1026 00:49:49.532800 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.142961 (* 1 = 0.142961 loss)
I1026 00:49:49.532805 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0137572 (* 1 = 0.0137572 loss)
I1026 00:49:49.532810 17176 sgd_solver.cpp:106] Iteration 9360, lr = 0.001
I1026 00:49:50.103790 17176 solver.cpp:229] Iteration 9380, loss = 0.0704617
I1026 00:49:50.103822 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0644322 (* 1 = 0.0644322 loss)
I1026 00:49:50.103827 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00602956 (* 1 = 0.00602956 loss)
I1026 00:49:50.103832 17176 sgd_solver.cpp:106] Iteration 9380, lr = 0.001
I1026 00:49:50.668683 17176 solver.cpp:229] Iteration 9400, loss = 0.0435002
I1026 00:49:50.668715 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0270362 (* 1 = 0.0270362 loss)
I1026 00:49:50.668720 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.016464 (* 1 = 0.016464 loss)
I1026 00:49:50.668722 17176 sgd_solver.cpp:106] Iteration 9400, lr = 0.001
I1026 00:49:51.226797 17176 solver.cpp:229] Iteration 9420, loss = 0.217832
I1026 00:49:51.226829 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0974514 (* 1 = 0.0974514 loss)
I1026 00:49:51.226833 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.12038 (* 1 = 0.12038 loss)
I1026 00:49:51.226837 17176 sgd_solver.cpp:106] Iteration 9420, lr = 0.001
I1026 00:49:51.805691 17176 solver.cpp:229] Iteration 9440, loss = 0.14841
I1026 00:49:51.805722 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0944964 (* 1 = 0.0944964 loss)
I1026 00:49:51.805727 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0539136 (* 1 = 0.0539136 loss)
I1026 00:49:51.805732 17176 sgd_solver.cpp:106] Iteration 9440, lr = 0.001
I1026 00:49:52.382086 17176 solver.cpp:229] Iteration 9460, loss = 0.0507918
I1026 00:49:52.382128 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0450803 (* 1 = 0.0450803 loss)
I1026 00:49:52.382133 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00571148 (* 1 = 0.00571148 loss)
I1026 00:49:52.382136 17176 sgd_solver.cpp:106] Iteration 9460, lr = 0.001
I1026 00:49:52.945968 17176 solver.cpp:229] Iteration 9480, loss = 0.129178
I1026 00:49:52.945999 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0368855 (* 1 = 0.0368855 loss)
I1026 00:49:52.946004 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0922922 (* 1 = 0.0922922 loss)
I1026 00:49:52.946007 17176 sgd_solver.cpp:106] Iteration 9480, lr = 0.001
I1026 00:49:53.519518 17176 solver.cpp:229] Iteration 9500, loss = 0.115794
I1026 00:49:53.519561 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.100551 (* 1 = 0.100551 loss)
I1026 00:49:53.519567 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0152434 (* 1 = 0.0152434 loss)
I1026 00:49:53.519570 17176 sgd_solver.cpp:106] Iteration 9500, lr = 0.001
I1026 00:49:54.084607 17176 solver.cpp:229] Iteration 9520, loss = 1.12183
I1026 00:49:54.084638 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.430622 (* 1 = 0.430622 loss)
I1026 00:49:54.084642 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.691213 (* 1 = 0.691213 loss)
I1026 00:49:54.084646 17176 sgd_solver.cpp:106] Iteration 9520, lr = 0.001
I1026 00:49:54.664285 17176 solver.cpp:229] Iteration 9540, loss = 0.0895913
I1026 00:49:54.664327 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.077589 (* 1 = 0.077589 loss)
I1026 00:49:54.664332 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0120023 (* 1 = 0.0120023 loss)
I1026 00:49:54.664336 17176 sgd_solver.cpp:106] Iteration 9540, lr = 0.001
I1026 00:49:55.248497 17176 solver.cpp:229] Iteration 9560, loss = 0.109808
I1026 00:49:55.248528 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0552138 (* 1 = 0.0552138 loss)
I1026 00:49:55.248531 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0545943 (* 1 = 0.0545943 loss)
I1026 00:49:55.248536 17176 sgd_solver.cpp:106] Iteration 9560, lr = 0.001
I1026 00:49:55.814386 17176 solver.cpp:229] Iteration 9580, loss = 0.0820642
I1026 00:49:55.814419 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0719247 (* 1 = 0.0719247 loss)
I1026 00:49:55.814424 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0101396 (* 1 = 0.0101396 loss)
I1026 00:49:55.814429 17176 sgd_solver.cpp:106] Iteration 9580, lr = 0.001
I1026 00:49:56.390789 17176 solver.cpp:229] Iteration 9600, loss = 0.0777979
I1026 00:49:56.390822 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.07267 (* 1 = 0.07267 loss)
I1026 00:49:56.390827 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00512791 (* 1 = 0.00512791 loss)
I1026 00:49:56.390831 17176 sgd_solver.cpp:106] Iteration 9600, lr = 0.001
I1026 00:49:56.942637 17176 solver.cpp:229] Iteration 9620, loss = 0.112775
I1026 00:49:56.942670 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.104434 (* 1 = 0.104434 loss)
I1026 00:49:56.942675 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00834107 (* 1 = 0.00834107 loss)
I1026 00:49:56.942679 17176 sgd_solver.cpp:106] Iteration 9620, lr = 0.001
I1026 00:49:57.522281 17176 solver.cpp:229] Iteration 9640, loss = 0.0765809
I1026 00:49:57.522313 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0342906 (* 1 = 0.0342906 loss)
I1026 00:49:57.522318 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0422902 (* 1 = 0.0422902 loss)
I1026 00:49:57.522323 17176 sgd_solver.cpp:106] Iteration 9640, lr = 0.001
I1026 00:49:58.097097 17176 solver.cpp:229] Iteration 9660, loss = 0.393579
I1026 00:49:58.097129 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.340065 (* 1 = 0.340065 loss)
I1026 00:49:58.097133 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0535147 (* 1 = 0.0535147 loss)
I1026 00:49:58.097137 17176 sgd_solver.cpp:106] Iteration 9660, lr = 0.001
I1026 00:49:58.677435 17176 solver.cpp:229] Iteration 9680, loss = 0.110277
I1026 00:49:58.677469 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0760788 (* 1 = 0.0760788 loss)
I1026 00:49:58.677474 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0341984 (* 1 = 0.0341984 loss)
I1026 00:49:58.677476 17176 sgd_solver.cpp:106] Iteration 9680, lr = 0.001
I1026 00:49:59.235404 17176 solver.cpp:229] Iteration 9700, loss = 0.162011
I1026 00:49:59.235458 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0681061 (* 1 = 0.0681061 loss)
I1026 00:49:59.235463 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0939045 (* 1 = 0.0939045 loss)
I1026 00:49:59.235468 17176 sgd_solver.cpp:106] Iteration 9700, lr = 0.001
I1026 00:49:59.799891 17176 solver.cpp:229] Iteration 9720, loss = 0.0856877
I1026 00:49:59.799923 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0463423 (* 1 = 0.0463423 loss)
I1026 00:49:59.799928 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0393454 (* 1 = 0.0393454 loss)
I1026 00:49:59.799933 17176 sgd_solver.cpp:106] Iteration 9720, lr = 0.001
I1026 00:50:00.348742 17176 solver.cpp:229] Iteration 9740, loss = 0.193404
I1026 00:50:00.348783 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0586406 (* 1 = 0.0586406 loss)
I1026 00:50:00.348788 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.134764 (* 1 = 0.134764 loss)
I1026 00:50:00.348793 17176 sgd_solver.cpp:106] Iteration 9740, lr = 0.001
I1026 00:50:00.908970 17176 solver.cpp:229] Iteration 9760, loss = 0.0711145
I1026 00:50:00.909013 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0456772 (* 1 = 0.0456772 loss)
I1026 00:50:00.909018 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0254373 (* 1 = 0.0254373 loss)
I1026 00:50:00.909023 17176 sgd_solver.cpp:106] Iteration 9760, lr = 0.001
I1026 00:50:01.482452 17176 solver.cpp:229] Iteration 9780, loss = 0.0632614
I1026 00:50:01.482486 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0550316 (* 1 = 0.0550316 loss)
I1026 00:50:01.482491 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00822978 (* 1 = 0.00822978 loss)
I1026 00:50:01.482494 17176 sgd_solver.cpp:106] Iteration 9780, lr = 0.001
I1026 00:50:02.052767 17176 solver.cpp:229] Iteration 9800, loss = 0.0939112
I1026 00:50:02.052799 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0690925 (* 1 = 0.0690925 loss)
I1026 00:50:02.052803 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0248186 (* 1 = 0.0248186 loss)
I1026 00:50:02.052809 17176 sgd_solver.cpp:106] Iteration 9800, lr = 0.001
I1026 00:50:02.635953 17176 solver.cpp:229] Iteration 9820, loss = 0.074719
I1026 00:50:02.635994 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0397726 (* 1 = 0.0397726 loss)
I1026 00:50:02.635999 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0349465 (* 1 = 0.0349465 loss)
I1026 00:50:02.636003 17176 sgd_solver.cpp:106] Iteration 9820, lr = 0.001
I1026 00:50:03.206885 17176 solver.cpp:229] Iteration 9840, loss = 0.0785244
I1026 00:50:03.206918 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0604876 (* 1 = 0.0604876 loss)
I1026 00:50:03.206923 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0180368 (* 1 = 0.0180368 loss)
I1026 00:50:03.206925 17176 sgd_solver.cpp:106] Iteration 9840, lr = 0.001
I1026 00:50:03.770398 17176 solver.cpp:229] Iteration 9860, loss = 0.453504
I1026 00:50:03.770439 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.240418 (* 1 = 0.240418 loss)
I1026 00:50:03.770444 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.213085 (* 1 = 0.213085 loss)
I1026 00:50:03.770448 17176 sgd_solver.cpp:106] Iteration 9860, lr = 0.001
I1026 00:50:04.336419 17176 solver.cpp:229] Iteration 9880, loss = 0.223544
I1026 00:50:04.336452 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.16156 (* 1 = 0.16156 loss)
I1026 00:50:04.336457 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0619837 (* 1 = 0.0619837 loss)
I1026 00:50:04.336462 17176 sgd_solver.cpp:106] Iteration 9880, lr = 0.001
I1026 00:50:04.910189 17176 solver.cpp:229] Iteration 9900, loss = 0.0265324
I1026 00:50:04.910221 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0213504 (* 1 = 0.0213504 loss)
I1026 00:50:04.910226 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00518205 (* 1 = 0.00518205 loss)
I1026 00:50:04.910230 17176 sgd_solver.cpp:106] Iteration 9900, lr = 0.001
I1026 00:50:05.466342 17176 solver.cpp:229] Iteration 9920, loss = 0.0983118
I1026 00:50:05.466374 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0854355 (* 1 = 0.0854355 loss)
I1026 00:50:05.466379 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0128762 (* 1 = 0.0128762 loss)
I1026 00:50:05.466383 17176 sgd_solver.cpp:106] Iteration 9920, lr = 0.001
I1026 00:50:06.033830 17176 solver.cpp:229] Iteration 9940, loss = 0.499601
I1026 00:50:06.033862 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.255615 (* 1 = 0.255615 loss)
I1026 00:50:06.033867 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.243986 (* 1 = 0.243986 loss)
I1026 00:50:06.033871 17176 sgd_solver.cpp:106] Iteration 9940, lr = 0.001
I1026 00:50:06.609174 17176 solver.cpp:229] Iteration 9960, loss = 0.0984242
I1026 00:50:06.609206 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0695266 (* 1 = 0.0695266 loss)
I1026 00:50:06.609211 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0288977 (* 1 = 0.0288977 loss)
I1026 00:50:06.609216 17176 sgd_solver.cpp:106] Iteration 9960, lr = 0.001
I1026 00:50:07.162160 17176 solver.cpp:229] Iteration 9980, loss = 0.0604825
I1026 00:50:07.162192 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0534899 (* 1 = 0.0534899 loss)
I1026 00:50:07.162197 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00699267 (* 1 = 0.00699267 loss)
I1026 00:50:07.162202 17176 sgd_solver.cpp:106] Iteration 9980, lr = 0.001
I1026 00:50:08.164985 17176 solver.cpp:229] Iteration 10000, loss = 0.124222
I1026 00:50:08.165019 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0458931 (* 1 = 0.0458931 loss)
I1026 00:50:08.165022 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0783289 (* 1 = 0.0783289 loss)
I1026 00:50:08.165027 17176 sgd_solver.cpp:106] Iteration 10000, lr = 0.001
I1026 00:50:08.742776 17176 solver.cpp:229] Iteration 10020, loss = 0.131171
I1026 00:50:08.742810 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0925976 (* 1 = 0.0925976 loss)
I1026 00:50:08.742815 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0385731 (* 1 = 0.0385731 loss)
I1026 00:50:08.742818 17176 sgd_solver.cpp:106] Iteration 10020, lr = 0.001
I1026 00:50:09.299438 17176 solver.cpp:229] Iteration 10040, loss = 0.0490843
I1026 00:50:09.299470 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0208452 (* 1 = 0.0208452 loss)
I1026 00:50:09.299475 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0282391 (* 1 = 0.0282391 loss)
I1026 00:50:09.299480 17176 sgd_solver.cpp:106] Iteration 10040, lr = 0.001
I1026 00:50:09.873332 17176 solver.cpp:229] Iteration 10060, loss = 0.119127
I1026 00:50:09.873365 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.032067 (* 1 = 0.032067 loss)
I1026 00:50:09.873371 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0870603 (* 1 = 0.0870603 loss)
I1026 00:50:09.873375 17176 sgd_solver.cpp:106] Iteration 10060, lr = 0.001
I1026 00:50:10.446548 17176 solver.cpp:229] Iteration 10080, loss = 0.118385
I1026 00:50:10.446591 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0865876 (* 1 = 0.0865876 loss)
I1026 00:50:10.446597 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0317978 (* 1 = 0.0317978 loss)
I1026 00:50:10.446601 17176 sgd_solver.cpp:106] Iteration 10080, lr = 0.001
I1026 00:50:11.002079 17176 solver.cpp:229] Iteration 10100, loss = 0.109969
I1026 00:50:11.002113 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.104672 (* 1 = 0.104672 loss)
I1026 00:50:11.002120 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00529737 (* 1 = 0.00529737 loss)
I1026 00:50:11.002123 17176 sgd_solver.cpp:106] Iteration 10100, lr = 0.001
I1026 00:50:11.576012 17176 solver.cpp:229] Iteration 10120, loss = 0.120015
I1026 00:50:11.576045 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0917731 (* 1 = 0.0917731 loss)
I1026 00:50:11.576050 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0282416 (* 1 = 0.0282416 loss)
I1026 00:50:11.576056 17176 sgd_solver.cpp:106] Iteration 10120, lr = 0.001
I1026 00:50:12.137163 17176 solver.cpp:229] Iteration 10140, loss = 0.0641574
I1026 00:50:12.137197 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0531963 (* 1 = 0.0531963 loss)
I1026 00:50:12.137202 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0109611 (* 1 = 0.0109611 loss)
I1026 00:50:12.137207 17176 sgd_solver.cpp:106] Iteration 10140, lr = 0.001
I1026 00:50:12.694434 17176 solver.cpp:229] Iteration 10160, loss = 0.179298
I1026 00:50:12.694468 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0679467 (* 1 = 0.0679467 loss)
I1026 00:50:12.694473 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.111351 (* 1 = 0.111351 loss)
I1026 00:50:12.694478 17176 sgd_solver.cpp:106] Iteration 10160, lr = 0.001
I1026 00:50:13.258133 17176 solver.cpp:229] Iteration 10180, loss = 0.157443
I1026 00:50:13.258167 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.108248 (* 1 = 0.108248 loss)
I1026 00:50:13.258172 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0491952 (* 1 = 0.0491952 loss)
I1026 00:50:13.258177 17176 sgd_solver.cpp:106] Iteration 10180, lr = 0.001
I1026 00:50:13.824281 17176 solver.cpp:229] Iteration 10200, loss = 0.161811
I1026 00:50:13.824323 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.145117 (* 1 = 0.145117 loss)
I1026 00:50:13.824331 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0166942 (* 1 = 0.0166942 loss)
I1026 00:50:13.824334 17176 sgd_solver.cpp:106] Iteration 10200, lr = 0.001
I1026 00:50:14.373394 17176 solver.cpp:229] Iteration 10220, loss = 0.110651
I1026 00:50:14.373425 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0763541 (* 1 = 0.0763541 loss)
I1026 00:50:14.373430 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0342974 (* 1 = 0.0342974 loss)
I1026 00:50:14.373433 17176 sgd_solver.cpp:106] Iteration 10220, lr = 0.001
I1026 00:50:14.942278 17176 solver.cpp:229] Iteration 10240, loss = 0.043078
I1026 00:50:14.942312 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0123358 (* 1 = 0.0123358 loss)
I1026 00:50:14.942315 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0307421 (* 1 = 0.0307421 loss)
I1026 00:50:14.942320 17176 sgd_solver.cpp:106] Iteration 10240, lr = 0.001
I1026 00:50:15.503024 17176 solver.cpp:229] Iteration 10260, loss = 0.0869612
I1026 00:50:15.503057 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0807704 (* 1 = 0.0807704 loss)
I1026 00:50:15.503062 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0061908 (* 1 = 0.0061908 loss)
I1026 00:50:15.503067 17176 sgd_solver.cpp:106] Iteration 10260, lr = 0.001
I1026 00:50:16.066270 17176 solver.cpp:229] Iteration 10280, loss = 0.311672
I1026 00:50:16.066313 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.281064 (* 1 = 0.281064 loss)
I1026 00:50:16.066318 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0306082 (* 1 = 0.0306082 loss)
I1026 00:50:16.066323 17176 sgd_solver.cpp:106] Iteration 10280, lr = 0.001
I1026 00:50:16.625589 17176 solver.cpp:229] Iteration 10300, loss = 0.488774
I1026 00:50:16.625633 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.150995 (* 1 = 0.150995 loss)
I1026 00:50:16.625636 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.337779 (* 1 = 0.337779 loss)
I1026 00:50:16.625640 17176 sgd_solver.cpp:106] Iteration 10300, lr = 0.001
I1026 00:50:17.173602 17176 solver.cpp:229] Iteration 10320, loss = 0.34235
I1026 00:50:17.173635 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.253559 (* 1 = 0.253559 loss)
I1026 00:50:17.173640 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0887915 (* 1 = 0.0887915 loss)
I1026 00:50:17.173645 17176 sgd_solver.cpp:106] Iteration 10320, lr = 0.001
I1026 00:50:17.733536 17176 solver.cpp:229] Iteration 10340, loss = 0.159686
I1026 00:50:17.733567 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.039039 (* 1 = 0.039039 loss)
I1026 00:50:17.733572 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.120647 (* 1 = 0.120647 loss)
I1026 00:50:17.733577 17176 sgd_solver.cpp:106] Iteration 10340, lr = 0.001
I1026 00:50:18.281386 17176 solver.cpp:229] Iteration 10360, loss = 0.0754389
I1026 00:50:18.281417 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.07258 (* 1 = 0.07258 loss)
I1026 00:50:18.281422 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00285893 (* 1 = 0.00285893 loss)
I1026 00:50:18.281427 17176 sgd_solver.cpp:106] Iteration 10360, lr = 0.001
I1026 00:50:18.836053 17176 solver.cpp:229] Iteration 10380, loss = 0.0557648
I1026 00:50:18.836086 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.053714 (* 1 = 0.053714 loss)
I1026 00:50:18.836091 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00205082 (* 1 = 0.00205082 loss)
I1026 00:50:18.836094 17176 sgd_solver.cpp:106] Iteration 10380, lr = 0.001
I1026 00:50:19.388576 17176 solver.cpp:229] Iteration 10400, loss = 0.135975
I1026 00:50:19.388608 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.051325 (* 1 = 0.051325 loss)
I1026 00:50:19.388613 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0846499 (* 1 = 0.0846499 loss)
I1026 00:50:19.388617 17176 sgd_solver.cpp:106] Iteration 10400, lr = 0.001
I1026 00:50:19.951665 17176 solver.cpp:229] Iteration 10420, loss = 0.472622
I1026 00:50:19.951699 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.251099 (* 1 = 0.251099 loss)
I1026 00:50:19.951704 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.221524 (* 1 = 0.221524 loss)
I1026 00:50:19.951717 17176 sgd_solver.cpp:106] Iteration 10420, lr = 0.001
I1026 00:50:20.508994 17176 solver.cpp:229] Iteration 10440, loss = 0.18314
I1026 00:50:20.509027 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.069673 (* 1 = 0.069673 loss)
I1026 00:50:20.509032 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.113467 (* 1 = 0.113467 loss)
I1026 00:50:20.509037 17176 sgd_solver.cpp:106] Iteration 10440, lr = 0.001
I1026 00:50:21.069103 17176 solver.cpp:229] Iteration 10460, loss = 0.0783627
I1026 00:50:21.069145 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0736548 (* 1 = 0.0736548 loss)
I1026 00:50:21.069150 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00470788 (* 1 = 0.00470788 loss)
I1026 00:50:21.069154 17176 sgd_solver.cpp:106] Iteration 10460, lr = 0.001
I1026 00:50:21.620817 17176 solver.cpp:229] Iteration 10480, loss = 0.223036
I1026 00:50:21.620851 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.170327 (* 1 = 0.170327 loss)
I1026 00:50:21.620856 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.052709 (* 1 = 0.052709 loss)
I1026 00:50:21.620859 17176 sgd_solver.cpp:106] Iteration 10480, lr = 0.001
I1026 00:50:22.184782 17176 solver.cpp:229] Iteration 10500, loss = 0.0979219
I1026 00:50:22.184834 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0795369 (* 1 = 0.0795369 loss)
I1026 00:50:22.184839 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.018385 (* 1 = 0.018385 loss)
I1026 00:50:22.184842 17176 sgd_solver.cpp:106] Iteration 10500, lr = 0.001
I1026 00:50:22.754194 17176 solver.cpp:229] Iteration 10520, loss = 0.12626
I1026 00:50:22.754226 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0596858 (* 1 = 0.0596858 loss)
I1026 00:50:22.754230 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0665746 (* 1 = 0.0665746 loss)
I1026 00:50:22.754235 17176 sgd_solver.cpp:106] Iteration 10520, lr = 0.001
I1026 00:50:23.304107 17176 solver.cpp:229] Iteration 10540, loss = 0.133583
I1026 00:50:23.304139 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.129825 (* 1 = 0.129825 loss)
I1026 00:50:23.304162 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00375778 (* 1 = 0.00375778 loss)
I1026 00:50:23.304165 17176 sgd_solver.cpp:106] Iteration 10540, lr = 0.001
I1026 00:50:23.866363 17176 solver.cpp:229] Iteration 10560, loss = 0.0455048
I1026 00:50:23.866395 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0417454 (* 1 = 0.0417454 loss)
I1026 00:50:23.866400 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00375931 (* 1 = 0.00375931 loss)
I1026 00:50:23.866405 17176 sgd_solver.cpp:106] Iteration 10560, lr = 0.001
I1026 00:50:24.432723 17176 solver.cpp:229] Iteration 10580, loss = 0.214358
I1026 00:50:24.432763 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.152966 (* 1 = 0.152966 loss)
I1026 00:50:24.432770 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0613923 (* 1 = 0.0613923 loss)
I1026 00:50:24.432773 17176 sgd_solver.cpp:106] Iteration 10580, lr = 0.001
I1026 00:50:24.980868 17176 solver.cpp:229] Iteration 10600, loss = 0.083366
I1026 00:50:24.980911 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0581305 (* 1 = 0.0581305 loss)
I1026 00:50:24.980916 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0252355 (* 1 = 0.0252355 loss)
I1026 00:50:24.980921 17176 sgd_solver.cpp:106] Iteration 10600, lr = 0.001
I1026 00:50:25.545361 17176 solver.cpp:229] Iteration 10620, loss = 0.0939812
I1026 00:50:25.545393 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0735039 (* 1 = 0.0735039 loss)
I1026 00:50:25.545397 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0204774 (* 1 = 0.0204774 loss)
I1026 00:50:25.545402 17176 sgd_solver.cpp:106] Iteration 10620, lr = 0.001
I1026 00:50:26.113507 17176 solver.cpp:229] Iteration 10640, loss = 0.240089
I1026 00:50:26.113541 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.18688 (* 1 = 0.18688 loss)
I1026 00:50:26.113546 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0532093 (* 1 = 0.0532093 loss)
I1026 00:50:26.113550 17176 sgd_solver.cpp:106] Iteration 10640, lr = 0.001
I1026 00:50:26.668931 17176 solver.cpp:229] Iteration 10660, loss = 0.0831981
I1026 00:50:26.668972 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0711375 (* 1 = 0.0711375 loss)
I1026 00:50:26.668978 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0120606 (* 1 = 0.0120606 loss)
I1026 00:50:26.668992 17176 sgd_solver.cpp:106] Iteration 10660, lr = 0.001
I1026 00:50:27.225445 17176 solver.cpp:229] Iteration 10680, loss = 0.119919
I1026 00:50:27.225477 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.10235 (* 1 = 0.10235 loss)
I1026 00:50:27.225482 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0175682 (* 1 = 0.0175682 loss)
I1026 00:50:27.225486 17176 sgd_solver.cpp:106] Iteration 10680, lr = 0.001
I1026 00:50:27.786649 17176 solver.cpp:229] Iteration 10700, loss = 0.0386549
I1026 00:50:27.786681 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0360018 (* 1 = 0.0360018 loss)
I1026 00:50:27.786686 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00265302 (* 1 = 0.00265302 loss)
I1026 00:50:27.786690 17176 sgd_solver.cpp:106] Iteration 10700, lr = 0.001
I1026 00:50:28.349422 17176 solver.cpp:229] Iteration 10720, loss = 0.0610481
I1026 00:50:28.349454 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.035995 (* 1 = 0.035995 loss)
I1026 00:50:28.349459 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0250532 (* 1 = 0.0250532 loss)
I1026 00:50:28.349463 17176 sgd_solver.cpp:106] Iteration 10720, lr = 0.001
I1026 00:50:28.911420 17176 solver.cpp:229] Iteration 10740, loss = 0.0288542
I1026 00:50:28.911458 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0256186 (* 1 = 0.0256186 loss)
I1026 00:50:28.911463 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00323561 (* 1 = 0.00323561 loss)
I1026 00:50:28.911466 17176 sgd_solver.cpp:106] Iteration 10740, lr = 0.001
I1026 00:50:29.475381 17176 solver.cpp:229] Iteration 10760, loss = 0.240153
I1026 00:50:29.475424 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.127841 (* 1 = 0.127841 loss)
I1026 00:50:29.475428 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.112312 (* 1 = 0.112312 loss)
I1026 00:50:29.475435 17176 sgd_solver.cpp:106] Iteration 10760, lr = 0.001
I1026 00:50:30.045589 17176 solver.cpp:229] Iteration 10780, loss = 0.307224
I1026 00:50:30.045622 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.267209 (* 1 = 0.267209 loss)
I1026 00:50:30.045629 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0400145 (* 1 = 0.0400145 loss)
I1026 00:50:30.045644 17176 sgd_solver.cpp:106] Iteration 10780, lr = 0.001
I1026 00:50:30.594576 17176 solver.cpp:229] Iteration 10800, loss = 0.113778
I1026 00:50:30.594619 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0868974 (* 1 = 0.0868974 loss)
I1026 00:50:30.594622 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0268809 (* 1 = 0.0268809 loss)
I1026 00:50:30.594626 17176 sgd_solver.cpp:106] Iteration 10800, lr = 0.001
I1026 00:50:31.151463 17176 solver.cpp:229] Iteration 10820, loss = 0.110196
I1026 00:50:31.151495 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0701328 (* 1 = 0.0701328 loss)
I1026 00:50:31.151500 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0400633 (* 1 = 0.0400633 loss)
I1026 00:50:31.151504 17176 sgd_solver.cpp:106] Iteration 10820, lr = 0.001
I1026 00:50:31.719620 17176 solver.cpp:229] Iteration 10840, loss = 0.0640163
I1026 00:50:31.719652 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.045165 (* 1 = 0.045165 loss)
I1026 00:50:31.719657 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0188513 (* 1 = 0.0188513 loss)
I1026 00:50:31.719661 17176 sgd_solver.cpp:106] Iteration 10840, lr = 0.001
I1026 00:50:32.291009 17176 solver.cpp:229] Iteration 10860, loss = 0.0700737
I1026 00:50:32.291041 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.051344 (* 1 = 0.051344 loss)
I1026 00:50:32.291046 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0187297 (* 1 = 0.0187297 loss)
I1026 00:50:32.291051 17176 sgd_solver.cpp:106] Iteration 10860, lr = 0.001
I1026 00:50:32.844537 17176 solver.cpp:229] Iteration 10880, loss = 0.362086
I1026 00:50:32.844569 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.254552 (* 1 = 0.254552 loss)
I1026 00:50:32.844574 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.107535 (* 1 = 0.107535 loss)
I1026 00:50:32.844579 17176 sgd_solver.cpp:106] Iteration 10880, lr = 0.001
I1026 00:50:33.402144 17176 solver.cpp:229] Iteration 10900, loss = 0.260493
I1026 00:50:33.402176 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.179872 (* 1 = 0.179872 loss)
I1026 00:50:33.402181 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0806205 (* 1 = 0.0806205 loss)
I1026 00:50:33.402186 17176 sgd_solver.cpp:106] Iteration 10900, lr = 0.001
I1026 00:50:33.959540 17176 solver.cpp:229] Iteration 10920, loss = 0.329651
I1026 00:50:33.959573 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.135663 (* 1 = 0.135663 loss)
I1026 00:50:33.959578 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.193988 (* 1 = 0.193988 loss)
I1026 00:50:33.959581 17176 sgd_solver.cpp:106] Iteration 10920, lr = 0.001
I1026 00:50:34.527389 17176 solver.cpp:229] Iteration 10940, loss = 0.141794
I1026 00:50:34.527422 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.114268 (* 1 = 0.114268 loss)
I1026 00:50:34.527427 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.027526 (* 1 = 0.027526 loss)
I1026 00:50:34.527436 17176 sgd_solver.cpp:106] Iteration 10940, lr = 0.001
I1026 00:50:35.092056 17176 solver.cpp:229] Iteration 10960, loss = 0.0756031
I1026 00:50:35.092087 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0610538 (* 1 = 0.0610538 loss)
I1026 00:50:35.092092 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0145493 (* 1 = 0.0145493 loss)
I1026 00:50:35.092097 17176 sgd_solver.cpp:106] Iteration 10960, lr = 0.001
I1026 00:50:35.639725 17176 solver.cpp:229] Iteration 10980, loss = 0.135511
I1026 00:50:35.639758 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0558636 (* 1 = 0.0558636 loss)
I1026 00:50:35.639763 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0796471 (* 1 = 0.0796471 loss)
I1026 00:50:35.639767 17176 sgd_solver.cpp:106] Iteration 10980, lr = 0.001
I1026 00:50:36.213718 17176 solver.cpp:229] Iteration 11000, loss = 0.25209
I1026 00:50:36.213752 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.235754 (* 1 = 0.235754 loss)
I1026 00:50:36.213757 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0163362 (* 1 = 0.0163362 loss)
I1026 00:50:36.213762 17176 sgd_solver.cpp:106] Iteration 11000, lr = 0.001
I1026 00:50:36.768473 17176 solver.cpp:229] Iteration 11020, loss = 0.173559
I1026 00:50:36.768504 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.141521 (* 1 = 0.141521 loss)
I1026 00:50:36.768508 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0320379 (* 1 = 0.0320379 loss)
I1026 00:50:36.768513 17176 sgd_solver.cpp:106] Iteration 11020, lr = 0.001
I1026 00:50:37.335561 17176 solver.cpp:229] Iteration 11040, loss = 0.0860479
I1026 00:50:37.335592 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0454983 (* 1 = 0.0454983 loss)
I1026 00:50:37.335608 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0405495 (* 1 = 0.0405495 loss)
I1026 00:50:37.335613 17176 sgd_solver.cpp:106] Iteration 11040, lr = 0.001
I1026 00:50:37.898748 17176 solver.cpp:229] Iteration 11060, loss = 0.0242267
I1026 00:50:37.898782 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0200887 (* 1 = 0.0200887 loss)
I1026 00:50:37.898787 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00413803 (* 1 = 0.00413803 loss)
I1026 00:50:37.898790 17176 sgd_solver.cpp:106] Iteration 11060, lr = 0.001
I1026 00:50:38.457064 17176 solver.cpp:229] Iteration 11080, loss = 0.105526
I1026 00:50:38.457096 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0824149 (* 1 = 0.0824149 loss)
I1026 00:50:38.457101 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0231108 (* 1 = 0.0231108 loss)
I1026 00:50:38.457105 17176 sgd_solver.cpp:106] Iteration 11080, lr = 0.001
I1026 00:50:39.027290 17176 solver.cpp:229] Iteration 11100, loss = 0.129414
I1026 00:50:39.027323 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.105879 (* 1 = 0.105879 loss)
I1026 00:50:39.027328 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0235347 (* 1 = 0.0235347 loss)
I1026 00:50:39.027333 17176 sgd_solver.cpp:106] Iteration 11100, lr = 0.001
I1026 00:50:39.584173 17176 solver.cpp:229] Iteration 11120, loss = 0.158586
I1026 00:50:39.584216 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0513282 (* 1 = 0.0513282 loss)
I1026 00:50:39.584220 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.107258 (* 1 = 0.107258 loss)
I1026 00:50:39.584224 17176 sgd_solver.cpp:106] Iteration 11120, lr = 0.001
I1026 00:50:40.147208 17176 solver.cpp:229] Iteration 11140, loss = 0.217569
I1026 00:50:40.147250 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0960685 (* 1 = 0.0960685 loss)
I1026 00:50:40.147255 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.1215 (* 1 = 0.1215 loss)
I1026 00:50:40.147259 17176 sgd_solver.cpp:106] Iteration 11140, lr = 0.001
I1026 00:50:40.697473 17176 solver.cpp:229] Iteration 11160, loss = 0.0672408
I1026 00:50:40.697504 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0448202 (* 1 = 0.0448202 loss)
I1026 00:50:40.697510 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0224206 (* 1 = 0.0224206 loss)
I1026 00:50:40.697513 17176 sgd_solver.cpp:106] Iteration 11160, lr = 0.001
I1026 00:50:41.262981 17176 solver.cpp:229] Iteration 11180, loss = 0.0693696
I1026 00:50:41.263015 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0435208 (* 1 = 0.0435208 loss)
I1026 00:50:41.263020 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0258487 (* 1 = 0.0258487 loss)
I1026 00:50:41.263025 17176 sgd_solver.cpp:106] Iteration 11180, lr = 0.001
I1026 00:50:41.828743 17176 solver.cpp:229] Iteration 11200, loss = 0.112483
I1026 00:50:41.828776 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0987598 (* 1 = 0.0987598 loss)
I1026 00:50:41.828781 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0137232 (* 1 = 0.0137232 loss)
I1026 00:50:41.828786 17176 sgd_solver.cpp:106] Iteration 11200, lr = 0.001
I1026 00:50:42.389771 17176 solver.cpp:229] Iteration 11220, loss = 0.113766
I1026 00:50:42.389803 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0601715 (* 1 = 0.0601715 loss)
I1026 00:50:42.389808 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0535942 (* 1 = 0.0535942 loss)
I1026 00:50:42.389812 17176 sgd_solver.cpp:106] Iteration 11220, lr = 0.001
I1026 00:50:42.956959 17176 solver.cpp:229] Iteration 11240, loss = 0.0632416
I1026 00:50:42.957003 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0514997 (* 1 = 0.0514997 loss)
I1026 00:50:42.957008 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0117419 (* 1 = 0.0117419 loss)
I1026 00:50:42.957013 17176 sgd_solver.cpp:106] Iteration 11240, lr = 0.001
I1026 00:50:43.501679 17176 solver.cpp:229] Iteration 11260, loss = 0.223843
I1026 00:50:43.501713 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.209338 (* 1 = 0.209338 loss)
I1026 00:50:43.501718 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0145049 (* 1 = 0.0145049 loss)
I1026 00:50:43.501721 17176 sgd_solver.cpp:106] Iteration 11260, lr = 0.001
I1026 00:50:44.051331 17176 solver.cpp:229] Iteration 11280, loss = 0.0964685
I1026 00:50:44.051363 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0888041 (* 1 = 0.0888041 loss)
I1026 00:50:44.051368 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00766443 (* 1 = 0.00766443 loss)
I1026 00:50:44.051373 17176 sgd_solver.cpp:106] Iteration 11280, lr = 0.001
I1026 00:50:44.621827 17176 solver.cpp:229] Iteration 11300, loss = 0.0733432
I1026 00:50:44.621871 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0438469 (* 1 = 0.0438469 loss)
I1026 00:50:44.621876 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0294963 (* 1 = 0.0294963 loss)
I1026 00:50:44.621881 17176 sgd_solver.cpp:106] Iteration 11300, lr = 0.001
I1026 00:50:45.188879 17176 solver.cpp:229] Iteration 11320, loss = 2.5041
I1026 00:50:45.188911 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.649242 (* 1 = 0.649242 loss)
I1026 00:50:45.188917 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 1.85486 (* 1 = 1.85486 loss)
I1026 00:50:45.188921 17176 sgd_solver.cpp:106] Iteration 11320, lr = 0.001
I1026 00:50:45.747057 17176 solver.cpp:229] Iteration 11340, loss = 0.165526
I1026 00:50:45.747090 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0545625 (* 1 = 0.0545625 loss)
I1026 00:50:45.747095 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.110964 (* 1 = 0.110964 loss)
I1026 00:50:45.747099 17176 sgd_solver.cpp:106] Iteration 11340, lr = 0.001
I1026 00:50:46.310751 17176 solver.cpp:229] Iteration 11360, loss = 0.180618
I1026 00:50:46.310783 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.153577 (* 1 = 0.153577 loss)
I1026 00:50:46.310788 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0270412 (* 1 = 0.0270412 loss)
I1026 00:50:46.310792 17176 sgd_solver.cpp:106] Iteration 11360, lr = 0.001
I1026 00:50:46.872149 17176 solver.cpp:229] Iteration 11380, loss = 0.603938
I1026 00:50:46.872181 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.372503 (* 1 = 0.372503 loss)
I1026 00:50:46.872186 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.231436 (* 1 = 0.231436 loss)
I1026 00:50:46.872191 17176 sgd_solver.cpp:106] Iteration 11380, lr = 0.001
I1026 00:50:47.442885 17176 solver.cpp:229] Iteration 11400, loss = 0.242582
I1026 00:50:47.442927 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0904476 (* 1 = 0.0904476 loss)
I1026 00:50:47.442932 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.152134 (* 1 = 0.152134 loss)
I1026 00:50:47.442936 17176 sgd_solver.cpp:106] Iteration 11400, lr = 0.001
I1026 00:50:48.007989 17176 solver.cpp:229] Iteration 11420, loss = 0.152644
I1026 00:50:48.008023 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.133196 (* 1 = 0.133196 loss)
I1026 00:50:48.008028 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0194483 (* 1 = 0.0194483 loss)
I1026 00:50:48.008033 17176 sgd_solver.cpp:106] Iteration 11420, lr = 0.001
I1026 00:50:48.569561 17176 solver.cpp:229] Iteration 11440, loss = 0.648814
I1026 00:50:48.569594 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.158551 (* 1 = 0.158551 loss)
I1026 00:50:48.569600 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.490263 (* 1 = 0.490263 loss)
I1026 00:50:48.569605 17176 sgd_solver.cpp:106] Iteration 11440, lr = 0.001
I1026 00:50:49.141762 17176 solver.cpp:229] Iteration 11460, loss = 0.0189166
I1026 00:50:49.141795 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0150969 (* 1 = 0.0150969 loss)
I1026 00:50:49.141801 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00381971 (* 1 = 0.00381971 loss)
I1026 00:50:49.141805 17176 sgd_solver.cpp:106] Iteration 11460, lr = 0.001
I1026 00:50:49.702204 17176 solver.cpp:229] Iteration 11480, loss = 0.0223017
I1026 00:50:49.702236 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0197618 (* 1 = 0.0197618 loss)
I1026 00:50:49.702241 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00253989 (* 1 = 0.00253989 loss)
I1026 00:50:49.702245 17176 sgd_solver.cpp:106] Iteration 11480, lr = 0.001
I1026 00:50:50.258121 17176 solver.cpp:229] Iteration 11500, loss = 0.543799
I1026 00:50:50.258157 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.206797 (* 1 = 0.206797 loss)
I1026 00:50:50.258162 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.337002 (* 1 = 0.337002 loss)
I1026 00:50:50.258167 17176 sgd_solver.cpp:106] Iteration 11500, lr = 0.001
I1026 00:50:50.818912 17176 solver.cpp:229] Iteration 11520, loss = 0.0707938
I1026 00:50:50.818959 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0527366 (* 1 = 0.0527366 loss)
I1026 00:50:50.818964 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0180572 (* 1 = 0.0180572 loss)
I1026 00:50:50.818967 17176 sgd_solver.cpp:106] Iteration 11520, lr = 0.001
I1026 00:50:51.374454 17176 solver.cpp:229] Iteration 11540, loss = 0.176555
I1026 00:50:51.374495 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.132242 (* 1 = 0.132242 loss)
I1026 00:50:51.374500 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0443122 (* 1 = 0.0443122 loss)
I1026 00:50:51.374503 17176 sgd_solver.cpp:106] Iteration 11540, lr = 0.001
I1026 00:50:51.930502 17176 solver.cpp:229] Iteration 11560, loss = 0.107961
I1026 00:50:51.930536 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0986785 (* 1 = 0.0986785 loss)
I1026 00:50:51.930541 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00928293 (* 1 = 0.00928293 loss)
I1026 00:50:51.930546 17176 sgd_solver.cpp:106] Iteration 11560, lr = 0.001
I1026 00:50:52.502714 17176 solver.cpp:229] Iteration 11580, loss = 0.935346
I1026 00:50:52.502748 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.308239 (* 1 = 0.308239 loss)
I1026 00:50:52.502754 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.627107 (* 1 = 0.627107 loss)
I1026 00:50:52.502759 17176 sgd_solver.cpp:106] Iteration 11580, lr = 0.001
I1026 00:50:53.062986 17176 solver.cpp:229] Iteration 11600, loss = 0.091533
I1026 00:50:53.063019 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0344458 (* 1 = 0.0344458 loss)
I1026 00:50:53.063024 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0570872 (* 1 = 0.0570872 loss)
I1026 00:50:53.063029 17176 sgd_solver.cpp:106] Iteration 11600, lr = 0.001
I1026 00:50:53.632624 17176 solver.cpp:229] Iteration 11620, loss = 0.0848747
I1026 00:50:53.632658 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0755867 (* 1 = 0.0755867 loss)
I1026 00:50:53.632663 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00928798 (* 1 = 0.00928798 loss)
I1026 00:50:53.632668 17176 sgd_solver.cpp:106] Iteration 11620, lr = 0.001
I1026 00:50:54.187198 17176 solver.cpp:229] Iteration 11640, loss = 0.275829
I1026 00:50:54.187232 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.154983 (* 1 = 0.154983 loss)
I1026 00:50:54.187238 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.120846 (* 1 = 0.120846 loss)
I1026 00:50:54.187245 17176 sgd_solver.cpp:106] Iteration 11640, lr = 0.001
I1026 00:50:54.761786 17176 solver.cpp:229] Iteration 11660, loss = 0.0417157
I1026 00:50:54.761819 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0336087 (* 1 = 0.0336087 loss)
I1026 00:50:54.761826 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00810697 (* 1 = 0.00810697 loss)
I1026 00:50:54.761829 17176 sgd_solver.cpp:106] Iteration 11660, lr = 0.001
I1026 00:50:55.323695 17176 solver.cpp:229] Iteration 11680, loss = 0.0468254
I1026 00:50:55.323729 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0389903 (* 1 = 0.0389903 loss)
I1026 00:50:55.323734 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00783505 (* 1 = 0.00783505 loss)
I1026 00:50:55.323739 17176 sgd_solver.cpp:106] Iteration 11680, lr = 0.001
I1026 00:50:55.897500 17176 solver.cpp:229] Iteration 11700, loss = 0.0417934
I1026 00:50:55.897533 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0342052 (* 1 = 0.0342052 loss)
I1026 00:50:55.897538 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0075882 (* 1 = 0.0075882 loss)
I1026 00:50:55.897542 17176 sgd_solver.cpp:106] Iteration 11700, lr = 0.001
I1026 00:50:56.446776 17176 solver.cpp:229] Iteration 11720, loss = 0.102885
I1026 00:50:56.446810 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0584818 (* 1 = 0.0584818 loss)
I1026 00:50:56.446815 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0444029 (* 1 = 0.0444029 loss)
I1026 00:50:56.446820 17176 sgd_solver.cpp:106] Iteration 11720, lr = 0.001
I1026 00:50:57.009094 17176 solver.cpp:229] Iteration 11740, loss = 0.176143
I1026 00:50:57.009127 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.167509 (* 1 = 0.167509 loss)
I1026 00:50:57.009133 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00863374 (* 1 = 0.00863374 loss)
I1026 00:50:57.009140 17176 sgd_solver.cpp:106] Iteration 11740, lr = 0.001
I1026 00:50:57.576529 17176 solver.cpp:229] Iteration 11760, loss = 0.18872
I1026 00:50:57.576565 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.120365 (* 1 = 0.120365 loss)
I1026 00:50:57.576570 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0683554 (* 1 = 0.0683554 loss)
I1026 00:50:57.576575 17176 sgd_solver.cpp:106] Iteration 11760, lr = 0.001
I1026 00:50:58.155421 17176 solver.cpp:229] Iteration 11780, loss = 0.0499984
I1026 00:50:58.155458 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0179979 (* 1 = 0.0179979 loss)
I1026 00:50:58.155463 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0320005 (* 1 = 0.0320005 loss)
I1026 00:50:58.155467 17176 sgd_solver.cpp:106] Iteration 11780, lr = 0.001
I1026 00:50:58.723829 17176 solver.cpp:229] Iteration 11800, loss = 0.0849496
I1026 00:50:58.723862 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0790679 (* 1 = 0.0790679 loss)
I1026 00:50:58.723870 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00588163 (* 1 = 0.00588163 loss)
I1026 00:50:58.723875 17176 sgd_solver.cpp:106] Iteration 11800, lr = 0.001
I1026 00:50:59.291901 17176 solver.cpp:229] Iteration 11820, loss = 0.15389
I1026 00:50:59.291934 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0910826 (* 1 = 0.0910826 loss)
I1026 00:50:59.291939 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0628075 (* 1 = 0.0628075 loss)
I1026 00:50:59.291944 17176 sgd_solver.cpp:106] Iteration 11820, lr = 0.001
I1026 00:50:59.848795 17176 solver.cpp:229] Iteration 11840, loss = 0.213763
I1026 00:50:59.848829 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0883259 (* 1 = 0.0883259 loss)
I1026 00:50:59.848834 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.125437 (* 1 = 0.125437 loss)
I1026 00:50:59.848839 17176 sgd_solver.cpp:106] Iteration 11840, lr = 0.001
I1026 00:51:00.417246 17176 solver.cpp:229] Iteration 11860, loss = 0.281707
I1026 00:51:00.417279 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.240409 (* 1 = 0.240409 loss)
I1026 00:51:00.417284 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0412984 (* 1 = 0.0412984 loss)
I1026 00:51:00.417290 17176 sgd_solver.cpp:106] Iteration 11860, lr = 0.001
I1026 00:51:00.973311 17176 solver.cpp:229] Iteration 11880, loss = 0.132207
I1026 00:51:00.973345 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.117437 (* 1 = 0.117437 loss)
I1026 00:51:00.973351 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0147703 (* 1 = 0.0147703 loss)
I1026 00:51:00.973356 17176 sgd_solver.cpp:106] Iteration 11880, lr = 0.001
I1026 00:51:01.541436 17176 solver.cpp:229] Iteration 11900, loss = 0.123036
I1026 00:51:01.541471 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0632951 (* 1 = 0.0632951 loss)
I1026 00:51:01.541476 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0597413 (* 1 = 0.0597413 loss)
I1026 00:51:01.541491 17176 sgd_solver.cpp:106] Iteration 11900, lr = 0.001
I1026 00:51:02.101891 17176 solver.cpp:229] Iteration 11920, loss = 0.0500112
I1026 00:51:02.101922 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0321248 (* 1 = 0.0321248 loss)
I1026 00:51:02.101929 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0178864 (* 1 = 0.0178864 loss)
I1026 00:51:02.101936 17176 sgd_solver.cpp:106] Iteration 11920, lr = 0.001
I1026 00:51:02.674085 17176 solver.cpp:229] Iteration 11940, loss = 0.0402755
I1026 00:51:02.674120 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0360177 (* 1 = 0.0360177 loss)
I1026 00:51:02.674125 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00425786 (* 1 = 0.00425786 loss)
I1026 00:51:02.674132 17176 sgd_solver.cpp:106] Iteration 11940, lr = 0.001
I1026 00:51:03.231943 17176 solver.cpp:229] Iteration 11960, loss = 0.249934
I1026 00:51:03.231987 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.115928 (* 1 = 0.115928 loss)
I1026 00:51:03.231993 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.134005 (* 1 = 0.134005 loss)
I1026 00:51:03.232000 17176 sgd_solver.cpp:106] Iteration 11960, lr = 0.001
I1026 00:51:03.800161 17176 solver.cpp:229] Iteration 11980, loss = 0.102432
I1026 00:51:03.800194 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0861144 (* 1 = 0.0861144 loss)
I1026 00:51:03.800199 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.016318 (* 1 = 0.016318 loss)
I1026 00:51:03.800205 17176 sgd_solver.cpp:106] Iteration 11980, lr = 0.001
I1026 00:51:04.358484 17176 solver.cpp:229] Iteration 12000, loss = 0.0264181
I1026 00:51:04.358517 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0252785 (* 1 = 0.0252785 loss)
I1026 00:51:04.358523 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00113964 (* 1 = 0.00113964 loss)
I1026 00:51:04.358528 17176 sgd_solver.cpp:106] Iteration 12000, lr = 0.001
I1026 00:51:04.918586 17176 solver.cpp:229] Iteration 12020, loss = 0.0721117
I1026 00:51:04.918620 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0533897 (* 1 = 0.0533897 loss)
I1026 00:51:04.918627 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.018722 (* 1 = 0.018722 loss)
I1026 00:51:04.918633 17176 sgd_solver.cpp:106] Iteration 12020, lr = 0.001
I1026 00:51:05.459645 17176 solver.cpp:229] Iteration 12040, loss = 0.235828
I1026 00:51:05.459678 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.135704 (* 1 = 0.135704 loss)
I1026 00:51:05.459684 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.100124 (* 1 = 0.100124 loss)
I1026 00:51:05.459691 17176 sgd_solver.cpp:106] Iteration 12040, lr = 0.001
I1026 00:51:06.030155 17176 solver.cpp:229] Iteration 12060, loss = 0.13103
I1026 00:51:06.030189 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.118846 (* 1 = 0.118846 loss)
I1026 00:51:06.030195 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.012184 (* 1 = 0.012184 loss)
I1026 00:51:06.030200 17176 sgd_solver.cpp:106] Iteration 12060, lr = 0.001
I1026 00:51:06.590597 17176 solver.cpp:229] Iteration 12080, loss = 0.0842099
I1026 00:51:06.590631 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0570163 (* 1 = 0.0570163 loss)
I1026 00:51:06.590637 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0271935 (* 1 = 0.0271935 loss)
I1026 00:51:06.590644 17176 sgd_solver.cpp:106] Iteration 12080, lr = 0.001
I1026 00:51:07.159062 17176 solver.cpp:229] Iteration 12100, loss = 0.188403
I1026 00:51:07.159096 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.119508 (* 1 = 0.119508 loss)
I1026 00:51:07.159101 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0688954 (* 1 = 0.0688954 loss)
I1026 00:51:07.159106 17176 sgd_solver.cpp:106] Iteration 12100, lr = 0.001
I1026 00:51:07.718924 17176 solver.cpp:229] Iteration 12120, loss = 0.132899
I1026 00:51:07.718956 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.115846 (* 1 = 0.115846 loss)
I1026 00:51:07.718963 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.017053 (* 1 = 0.017053 loss)
I1026 00:51:07.718971 17176 sgd_solver.cpp:106] Iteration 12120, lr = 0.001
I1026 00:51:08.282066 17176 solver.cpp:229] Iteration 12140, loss = 0.441816
I1026 00:51:08.282100 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.206546 (* 1 = 0.206546 loss)
I1026 00:51:08.282122 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.235269 (* 1 = 0.235269 loss)
I1026 00:51:08.282127 17176 sgd_solver.cpp:106] Iteration 12140, lr = 0.001
I1026 00:51:08.856322 17176 solver.cpp:229] Iteration 12160, loss = 0.0836677
I1026 00:51:08.856355 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0401182 (* 1 = 0.0401182 loss)
I1026 00:51:08.856361 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0435494 (* 1 = 0.0435494 loss)
I1026 00:51:08.856367 17176 sgd_solver.cpp:106] Iteration 12160, lr = 0.001
I1026 00:51:09.432387 17176 solver.cpp:229] Iteration 12180, loss = 0.215665
I1026 00:51:09.432420 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.151106 (* 1 = 0.151106 loss)
I1026 00:51:09.432425 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0645594 (* 1 = 0.0645594 loss)
I1026 00:51:09.432432 17176 sgd_solver.cpp:106] Iteration 12180, lr = 0.001
I1026 00:51:09.996983 17176 solver.cpp:229] Iteration 12200, loss = 0.111695
I1026 00:51:09.997017 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.047559 (* 1 = 0.047559 loss)
I1026 00:51:09.997023 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0641356 (* 1 = 0.0641356 loss)
I1026 00:51:09.997030 17176 sgd_solver.cpp:106] Iteration 12200, lr = 0.001
I1026 00:51:10.556246 17176 solver.cpp:229] Iteration 12220, loss = 0.0702437
I1026 00:51:10.556275 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.044224 (* 1 = 0.044224 loss)
I1026 00:51:10.556280 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0260197 (* 1 = 0.0260197 loss)
I1026 00:51:10.556284 17176 sgd_solver.cpp:106] Iteration 12220, lr = 0.001
I1026 00:51:11.111646 17176 solver.cpp:229] Iteration 12240, loss = 0.573467
I1026 00:51:11.111678 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.307364 (* 1 = 0.307364 loss)
I1026 00:51:11.111685 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.266103 (* 1 = 0.266103 loss)
I1026 00:51:11.111691 17176 sgd_solver.cpp:106] Iteration 12240, lr = 0.001
I1026 00:51:11.681607 17176 solver.cpp:229] Iteration 12260, loss = 0.0420337
I1026 00:51:11.681640 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0275568 (* 1 = 0.0275568 loss)
I1026 00:51:11.681645 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0144769 (* 1 = 0.0144769 loss)
I1026 00:51:11.681650 17176 sgd_solver.cpp:106] Iteration 12260, lr = 0.001
I1026 00:51:12.249301 17176 solver.cpp:229] Iteration 12280, loss = 0.111314
I1026 00:51:12.249335 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0655609 (* 1 = 0.0655609 loss)
I1026 00:51:12.249341 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0457531 (* 1 = 0.0457531 loss)
I1026 00:51:12.249347 17176 sgd_solver.cpp:106] Iteration 12280, lr = 0.001
I1026 00:51:12.804381 17176 solver.cpp:229] Iteration 12300, loss = 0.0540511
I1026 00:51:12.804414 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0445879 (* 1 = 0.0445879 loss)
I1026 00:51:12.804419 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00946317 (* 1 = 0.00946317 loss)
I1026 00:51:12.804424 17176 sgd_solver.cpp:106] Iteration 12300, lr = 0.001
I1026 00:51:13.366463 17176 solver.cpp:229] Iteration 12320, loss = 0.090943
I1026 00:51:13.366497 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0537361 (* 1 = 0.0537361 loss)
I1026 00:51:13.366503 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0372069 (* 1 = 0.0372069 loss)
I1026 00:51:13.366509 17176 sgd_solver.cpp:106] Iteration 12320, lr = 0.001
I1026 00:51:13.917264 17176 solver.cpp:229] Iteration 12340, loss = 0.124071
I1026 00:51:13.917299 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.118654 (* 1 = 0.118654 loss)
I1026 00:51:13.917305 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00541634 (* 1 = 0.00541634 loss)
I1026 00:51:13.917311 17176 sgd_solver.cpp:106] Iteration 12340, lr = 0.001
I1026 00:51:14.480850 17176 solver.cpp:229] Iteration 12360, loss = 0.935328
I1026 00:51:14.480882 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.358921 (* 1 = 0.358921 loss)
I1026 00:51:14.480888 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.576407 (* 1 = 0.576407 loss)
I1026 00:51:14.480893 17176 sgd_solver.cpp:106] Iteration 12360, lr = 0.001
I1026 00:51:15.056465 17176 solver.cpp:229] Iteration 12380, loss = 0.096304
I1026 00:51:15.056499 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0824475 (* 1 = 0.0824475 loss)
I1026 00:51:15.056505 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0138565 (* 1 = 0.0138565 loss)
I1026 00:51:15.056510 17176 sgd_solver.cpp:106] Iteration 12380, lr = 0.001
I1026 00:51:15.625756 17176 solver.cpp:229] Iteration 12400, loss = 0.125138
I1026 00:51:15.625789 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.089772 (* 1 = 0.089772 loss)
I1026 00:51:15.625795 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0353656 (* 1 = 0.0353656 loss)
I1026 00:51:15.625800 17176 sgd_solver.cpp:106] Iteration 12400, lr = 0.001
I1026 00:51:16.182131 17176 solver.cpp:229] Iteration 12420, loss = 1.75783
I1026 00:51:16.182163 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.543233 (* 1 = 0.543233 loss)
I1026 00:51:16.182169 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 1.21459 (* 1 = 1.21459 loss)
I1026 00:51:16.182174 17176 sgd_solver.cpp:106] Iteration 12420, lr = 0.001
I1026 00:51:16.754537 17176 solver.cpp:229] Iteration 12440, loss = 0.150975
I1026 00:51:16.754571 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.123043 (* 1 = 0.123043 loss)
I1026 00:51:16.754576 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0279314 (* 1 = 0.0279314 loss)
I1026 00:51:16.754581 17176 sgd_solver.cpp:106] Iteration 12440, lr = 0.001
I1026 00:51:17.320411 17176 solver.cpp:229] Iteration 12460, loss = 0.112071
I1026 00:51:17.320453 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0957477 (* 1 = 0.0957477 loss)
I1026 00:51:17.320459 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0163228 (* 1 = 0.0163228 loss)
I1026 00:51:17.320463 17176 sgd_solver.cpp:106] Iteration 12460, lr = 0.001
I1026 00:51:17.887898 17176 solver.cpp:229] Iteration 12480, loss = 0.0427477
I1026 00:51:17.887943 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0266004 (* 1 = 0.0266004 loss)
I1026 00:51:17.887948 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0161472 (* 1 = 0.0161472 loss)
I1026 00:51:17.887953 17176 sgd_solver.cpp:106] Iteration 12480, lr = 0.001
I1026 00:51:18.425995 17176 solver.cpp:229] Iteration 12500, loss = 0.0696439
I1026 00:51:18.426028 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0383375 (* 1 = 0.0383375 loss)
I1026 00:51:18.426033 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0313064 (* 1 = 0.0313064 loss)
I1026 00:51:18.426038 17176 sgd_solver.cpp:106] Iteration 12500, lr = 0.001
I1026 00:51:18.991834 17176 solver.cpp:229] Iteration 12520, loss = 0.0697801
I1026 00:51:18.991885 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0635124 (* 1 = 0.0635124 loss)
I1026 00:51:18.991891 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00626773 (* 1 = 0.00626773 loss)
I1026 00:51:18.991895 17176 sgd_solver.cpp:106] Iteration 12520, lr = 0.001
I1026 00:51:19.543318 17176 solver.cpp:229] Iteration 12540, loss = 0.189186
I1026 00:51:19.543350 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0148194 (* 1 = 0.0148194 loss)
I1026 00:51:19.543354 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.174366 (* 1 = 0.174366 loss)
I1026 00:51:19.543359 17176 sgd_solver.cpp:106] Iteration 12540, lr = 0.001
I1026 00:51:20.112596 17176 solver.cpp:229] Iteration 12560, loss = 0.118225
I1026 00:51:20.112628 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0643105 (* 1 = 0.0643105 loss)
I1026 00:51:20.112632 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0539142 (* 1 = 0.0539142 loss)
I1026 00:51:20.112637 17176 sgd_solver.cpp:106] Iteration 12560, lr = 0.001
I1026 00:51:20.671612 17176 solver.cpp:229] Iteration 12580, loss = 0.0784099
I1026 00:51:20.671659 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0267958 (* 1 = 0.0267958 loss)
I1026 00:51:20.671664 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.051614 (* 1 = 0.051614 loss)
I1026 00:51:20.671684 17176 sgd_solver.cpp:106] Iteration 12580, lr = 0.001
I1026 00:51:21.223042 17176 solver.cpp:229] Iteration 12600, loss = 0.0723599
I1026 00:51:21.223074 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0630242 (* 1 = 0.0630242 loss)
I1026 00:51:21.223079 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00933573 (* 1 = 0.00933573 loss)
I1026 00:51:21.223083 17176 sgd_solver.cpp:106] Iteration 12600, lr = 0.001
I1026 00:51:21.770717 17176 solver.cpp:229] Iteration 12620, loss = 1.34581
I1026 00:51:21.770750 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.44111 (* 1 = 0.44111 loss)
I1026 00:51:21.770754 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.904695 (* 1 = 0.904695 loss)
I1026 00:51:21.770757 17176 sgd_solver.cpp:106] Iteration 12620, lr = 0.001
I1026 00:51:22.340698 17176 solver.cpp:229] Iteration 12640, loss = 0.413273
I1026 00:51:22.340726 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.233933 (* 1 = 0.233933 loss)
I1026 00:51:22.340731 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.179339 (* 1 = 0.179339 loss)
I1026 00:51:22.340735 17176 sgd_solver.cpp:106] Iteration 12640, lr = 0.001
I1026 00:51:22.901182 17176 solver.cpp:229] Iteration 12660, loss = 0.111711
I1026 00:51:22.901216 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0304951 (* 1 = 0.0304951 loss)
I1026 00:51:22.901221 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0812159 (* 1 = 0.0812159 loss)
I1026 00:51:22.901226 17176 sgd_solver.cpp:106] Iteration 12660, lr = 0.001
I1026 00:51:23.464608 17176 solver.cpp:229] Iteration 12680, loss = 0.201775
I1026 00:51:23.464642 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0999429 (* 1 = 0.0999429 loss)
I1026 00:51:23.464646 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.101833 (* 1 = 0.101833 loss)
I1026 00:51:23.464651 17176 sgd_solver.cpp:106] Iteration 12680, lr = 0.001
I1026 00:51:24.021387 17176 solver.cpp:229] Iteration 12700, loss = 0.148385
I1026 00:51:24.021422 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.133666 (* 1 = 0.133666 loss)
I1026 00:51:24.021431 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0147189 (* 1 = 0.0147189 loss)
I1026 00:51:24.021446 17176 sgd_solver.cpp:106] Iteration 12700, lr = 0.001
I1026 00:51:24.603435 17176 solver.cpp:229] Iteration 12720, loss = 0.166857
I1026 00:51:24.603471 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.10694 (* 1 = 0.10694 loss)
I1026 00:51:24.603479 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0599171 (* 1 = 0.0599171 loss)
I1026 00:51:24.603485 17176 sgd_solver.cpp:106] Iteration 12720, lr = 0.001
I1026 00:51:25.176470 17176 solver.cpp:229] Iteration 12740, loss = 0.0565438
I1026 00:51:25.176506 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0488151 (* 1 = 0.0488151 loss)
I1026 00:51:25.176514 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00772877 (* 1 = 0.00772877 loss)
I1026 00:51:25.176530 17176 sgd_solver.cpp:106] Iteration 12740, lr = 0.001
I1026 00:51:25.748482 17176 solver.cpp:229] Iteration 12760, loss = 0.177679
I1026 00:51:25.748517 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.131143 (* 1 = 0.131143 loss)
I1026 00:51:25.748522 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0465356 (* 1 = 0.0465356 loss)
I1026 00:51:25.748527 17176 sgd_solver.cpp:106] Iteration 12760, lr = 0.001
I1026 00:51:26.309541 17176 solver.cpp:229] Iteration 12780, loss = 0.0350383
I1026 00:51:26.309574 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0270132 (* 1 = 0.0270132 loss)
I1026 00:51:26.309579 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0080251 (* 1 = 0.0080251 loss)
I1026 00:51:26.309582 17176 sgd_solver.cpp:106] Iteration 12780, lr = 0.001
I1026 00:51:26.885591 17176 solver.cpp:229] Iteration 12800, loss = 0.0803663
I1026 00:51:26.885622 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0429119 (* 1 = 0.0429119 loss)
I1026 00:51:26.885627 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0374544 (* 1 = 0.0374544 loss)
I1026 00:51:26.885630 17176 sgd_solver.cpp:106] Iteration 12800, lr = 0.001
I1026 00:51:27.443083 17176 solver.cpp:229] Iteration 12820, loss = 0.134158
I1026 00:51:27.443115 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.121239 (* 1 = 0.121239 loss)
I1026 00:51:27.443120 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0129195 (* 1 = 0.0129195 loss)
I1026 00:51:27.443125 17176 sgd_solver.cpp:106] Iteration 12820, lr = 0.001
I1026 00:51:28.008694 17176 solver.cpp:229] Iteration 12840, loss = 0.152167
I1026 00:51:28.008738 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.142699 (* 1 = 0.142699 loss)
I1026 00:51:28.008743 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00946818 (* 1 = 0.00946818 loss)
I1026 00:51:28.008746 17176 sgd_solver.cpp:106] Iteration 12840, lr = 0.001
I1026 00:51:28.570096 17176 solver.cpp:229] Iteration 12860, loss = 0.0636726
I1026 00:51:28.570128 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0329837 (* 1 = 0.0329837 loss)
I1026 00:51:28.570133 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0306889 (* 1 = 0.0306889 loss)
I1026 00:51:28.570137 17176 sgd_solver.cpp:106] Iteration 12860, lr = 0.001
I1026 00:51:29.133693 17176 solver.cpp:229] Iteration 12880, loss = 0.0473881
I1026 00:51:29.133735 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0399286 (* 1 = 0.0399286 loss)
I1026 00:51:29.133740 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00745951 (* 1 = 0.00745951 loss)
I1026 00:51:29.133754 17176 sgd_solver.cpp:106] Iteration 12880, lr = 0.001
I1026 00:51:29.685451 17176 solver.cpp:229] Iteration 12900, loss = 0.0962917
I1026 00:51:29.685497 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0913346 (* 1 = 0.0913346 loss)
I1026 00:51:29.685502 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00495704 (* 1 = 0.00495704 loss)
I1026 00:51:29.685505 17176 sgd_solver.cpp:106] Iteration 12900, lr = 0.001
I1026 00:51:30.230687 17176 solver.cpp:229] Iteration 12920, loss = 0.244185
I1026 00:51:30.230731 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.178936 (* 1 = 0.178936 loss)
I1026 00:51:30.230736 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0652487 (* 1 = 0.0652487 loss)
I1026 00:51:30.230748 17176 sgd_solver.cpp:106] Iteration 12920, lr = 0.001
I1026 00:51:30.790148 17176 solver.cpp:229] Iteration 12940, loss = 0.178166
I1026 00:51:30.790180 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0450538 (* 1 = 0.0450538 loss)
I1026 00:51:30.790185 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.133112 (* 1 = 0.133112 loss)
I1026 00:51:30.790189 17176 sgd_solver.cpp:106] Iteration 12940, lr = 0.001
I1026 00:51:31.347239 17176 solver.cpp:229] Iteration 12960, loss = 0.323429
I1026 00:51:31.347270 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.106357 (* 1 = 0.106357 loss)
I1026 00:51:31.347275 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.217072 (* 1 = 0.217072 loss)
I1026 00:51:31.347278 17176 sgd_solver.cpp:106] Iteration 12960, lr = 0.001
I1026 00:51:31.907321 17176 solver.cpp:229] Iteration 12980, loss = 0.0918183
I1026 00:51:31.907354 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0878081 (* 1 = 0.0878081 loss)
I1026 00:51:31.907359 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0040102 (* 1 = 0.0040102 loss)
I1026 00:51:31.907364 17176 sgd_solver.cpp:106] Iteration 12980, lr = 0.001
I1026 00:51:32.468785 17176 solver.cpp:229] Iteration 13000, loss = 0.184149
I1026 00:51:32.468817 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.114764 (* 1 = 0.114764 loss)
I1026 00:51:32.468822 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0693848 (* 1 = 0.0693848 loss)
I1026 00:51:32.468825 17176 sgd_solver.cpp:106] Iteration 13000, lr = 0.001
I1026 00:51:33.017310 17176 solver.cpp:229] Iteration 13020, loss = 0.21358
I1026 00:51:33.017343 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.166017 (* 1 = 0.166017 loss)
I1026 00:51:33.017348 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0475635 (* 1 = 0.0475635 loss)
I1026 00:51:33.017351 17176 sgd_solver.cpp:106] Iteration 13020, lr = 0.001
I1026 00:51:33.568431 17176 solver.cpp:229] Iteration 13040, loss = 0.0551213
I1026 00:51:33.568464 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0370865 (* 1 = 0.0370865 loss)
I1026 00:51:33.568469 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0180348 (* 1 = 0.0180348 loss)
I1026 00:51:33.568471 17176 sgd_solver.cpp:106] Iteration 13040, lr = 0.001
I1026 00:51:34.123970 17176 solver.cpp:229] Iteration 13060, loss = 0.130086
I1026 00:51:34.123999 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0782515 (* 1 = 0.0782515 loss)
I1026 00:51:34.124004 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0518341 (* 1 = 0.0518341 loss)
I1026 00:51:34.124008 17176 sgd_solver.cpp:106] Iteration 13060, lr = 0.001
I1026 00:51:34.680214 17176 solver.cpp:229] Iteration 13080, loss = 0.0749217
I1026 00:51:34.680248 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0466789 (* 1 = 0.0466789 loss)
I1026 00:51:34.680253 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0282428 (* 1 = 0.0282428 loss)
I1026 00:51:34.680256 17176 sgd_solver.cpp:106] Iteration 13080, lr = 0.001
I1026 00:51:35.231478 17176 solver.cpp:229] Iteration 13100, loss = 0.0950964
I1026 00:51:35.231511 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0572346 (* 1 = 0.0572346 loss)
I1026 00:51:35.231516 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0378618 (* 1 = 0.0378618 loss)
I1026 00:51:35.231520 17176 sgd_solver.cpp:106] Iteration 13100, lr = 0.001
I1026 00:51:35.787312 17176 solver.cpp:229] Iteration 13120, loss = 0.0355279
I1026 00:51:35.787346 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0285623 (* 1 = 0.0285623 loss)
I1026 00:51:35.787353 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00696553 (* 1 = 0.00696553 loss)
I1026 00:51:35.787358 17176 sgd_solver.cpp:106] Iteration 13120, lr = 0.001
I1026 00:51:36.353328 17176 solver.cpp:229] Iteration 13140, loss = 0.156609
I1026 00:51:36.353360 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0579124 (* 1 = 0.0579124 loss)
I1026 00:51:36.353365 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0986964 (* 1 = 0.0986964 loss)
I1026 00:51:36.353369 17176 sgd_solver.cpp:106] Iteration 13140, lr = 0.001
I1026 00:51:36.908443 17176 solver.cpp:229] Iteration 13160, loss = 0.143337
I1026 00:51:36.908475 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.066162 (* 1 = 0.066162 loss)
I1026 00:51:36.908479 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0771751 (* 1 = 0.0771751 loss)
I1026 00:51:36.908483 17176 sgd_solver.cpp:106] Iteration 13160, lr = 0.001
I1026 00:51:37.466589 17176 solver.cpp:229] Iteration 13180, loss = 0.108967
I1026 00:51:37.466621 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.093874 (* 1 = 0.093874 loss)
I1026 00:51:37.466625 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0150926 (* 1 = 0.0150926 loss)
I1026 00:51:37.466629 17176 sgd_solver.cpp:106] Iteration 13180, lr = 0.001
I1026 00:51:38.019330 17176 solver.cpp:229] Iteration 13200, loss = 0.021696
I1026 00:51:38.019361 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0197577 (* 1 = 0.0197577 loss)
I1026 00:51:38.019366 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00193834 (* 1 = 0.00193834 loss)
I1026 00:51:38.019371 17176 sgd_solver.cpp:106] Iteration 13200, lr = 0.001
I1026 00:51:38.571657 17176 solver.cpp:229] Iteration 13220, loss = 0.0630516
I1026 00:51:38.571702 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0376924 (* 1 = 0.0376924 loss)
I1026 00:51:38.571705 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0253592 (* 1 = 0.0253592 loss)
I1026 00:51:38.571709 17176 sgd_solver.cpp:106] Iteration 13220, lr = 0.001
I1026 00:51:39.136847 17176 solver.cpp:229] Iteration 13240, loss = 0.0603929
I1026 00:51:39.136889 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0266362 (* 1 = 0.0266362 loss)
I1026 00:51:39.136895 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0337567 (* 1 = 0.0337567 loss)
I1026 00:51:39.136899 17176 sgd_solver.cpp:106] Iteration 13240, lr = 0.001
I1026 00:51:39.701695 17176 solver.cpp:229] Iteration 13260, loss = 0.0615111
I1026 00:51:39.701730 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0192261 (* 1 = 0.0192261 loss)
I1026 00:51:39.701735 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0422851 (* 1 = 0.0422851 loss)
I1026 00:51:39.701740 17176 sgd_solver.cpp:106] Iteration 13260, lr = 0.001
I1026 00:51:40.257241 17176 solver.cpp:229] Iteration 13280, loss = 0.102803
I1026 00:51:40.257274 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0893728 (* 1 = 0.0893728 loss)
I1026 00:51:40.257279 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.01343 (* 1 = 0.01343 loss)
I1026 00:51:40.257283 17176 sgd_solver.cpp:106] Iteration 13280, lr = 0.001
I1026 00:51:40.824396 17176 solver.cpp:229] Iteration 13300, loss = 0.0321128
I1026 00:51:40.824429 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0261944 (* 1 = 0.0261944 loss)
I1026 00:51:40.824434 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00591847 (* 1 = 0.00591847 loss)
I1026 00:51:40.824440 17176 sgd_solver.cpp:106] Iteration 13300, lr = 0.001
I1026 00:51:41.390141 17176 solver.cpp:229] Iteration 13320, loss = 0.164911
I1026 00:51:41.390184 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.157154 (* 1 = 0.157154 loss)
I1026 00:51:41.390190 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00775726 (* 1 = 0.00775726 loss)
I1026 00:51:41.390194 17176 sgd_solver.cpp:106] Iteration 13320, lr = 0.001
I1026 00:51:41.951956 17176 solver.cpp:229] Iteration 13340, loss = 0.0687337
I1026 00:51:41.952000 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0332013 (* 1 = 0.0332013 loss)
I1026 00:51:41.952005 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0355324 (* 1 = 0.0355324 loss)
I1026 00:51:41.952010 17176 sgd_solver.cpp:106] Iteration 13340, lr = 0.001
I1026 00:51:42.513098 17176 solver.cpp:229] Iteration 13360, loss = 0.0282405
I1026 00:51:42.513130 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0250005 (* 1 = 0.0250005 loss)
I1026 00:51:42.513135 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00324008 (* 1 = 0.00324008 loss)
I1026 00:51:42.513140 17176 sgd_solver.cpp:106] Iteration 13360, lr = 0.001
I1026 00:51:43.080117 17176 solver.cpp:229] Iteration 13380, loss = 0.0690306
I1026 00:51:43.080150 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0357878 (* 1 = 0.0357878 loss)
I1026 00:51:43.080155 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0332428 (* 1 = 0.0332428 loss)
I1026 00:51:43.080160 17176 sgd_solver.cpp:106] Iteration 13380, lr = 0.001
I1026 00:51:43.642874 17176 solver.cpp:229] Iteration 13400, loss = 0.14262
I1026 00:51:43.642907 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.117308 (* 1 = 0.117308 loss)
I1026 00:51:43.642913 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0253115 (* 1 = 0.0253115 loss)
I1026 00:51:43.642917 17176 sgd_solver.cpp:106] Iteration 13400, lr = 0.001
I1026 00:51:44.202654 17176 solver.cpp:229] Iteration 13420, loss = 0.0790932
I1026 00:51:44.202689 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0711422 (* 1 = 0.0711422 loss)
I1026 00:51:44.202695 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00795102 (* 1 = 0.00795102 loss)
I1026 00:51:44.202699 17176 sgd_solver.cpp:106] Iteration 13420, lr = 0.001
I1026 00:51:44.768020 17176 solver.cpp:229] Iteration 13440, loss = 0.0269256
I1026 00:51:44.768055 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0209539 (* 1 = 0.0209539 loss)
I1026 00:51:44.768060 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0059717 (* 1 = 0.0059717 loss)
I1026 00:51:44.768065 17176 sgd_solver.cpp:106] Iteration 13440, lr = 0.001
I1026 00:51:45.331759 17176 solver.cpp:229] Iteration 13460, loss = 0.0789329
I1026 00:51:45.331791 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0431509 (* 1 = 0.0431509 loss)
I1026 00:51:45.331797 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.035782 (* 1 = 0.035782 loss)
I1026 00:51:45.331802 17176 sgd_solver.cpp:106] Iteration 13460, lr = 0.001
I1026 00:51:45.898033 17176 solver.cpp:229] Iteration 13480, loss = 0.101652
I1026 00:51:45.898068 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0454824 (* 1 = 0.0454824 loss)
I1026 00:51:45.898073 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0561698 (* 1 = 0.0561698 loss)
I1026 00:51:45.898077 17176 sgd_solver.cpp:106] Iteration 13480, lr = 0.001
I1026 00:51:46.460631 17176 solver.cpp:229] Iteration 13500, loss = 0.124273
I1026 00:51:46.460664 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.118062 (* 1 = 0.118062 loss)
I1026 00:51:46.460670 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00621054 (* 1 = 0.00621054 loss)
I1026 00:51:46.460675 17176 sgd_solver.cpp:106] Iteration 13500, lr = 0.001
I1026 00:51:47.018810 17176 solver.cpp:229] Iteration 13520, loss = 0.0671521
I1026 00:51:47.018842 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0488398 (* 1 = 0.0488398 loss)
I1026 00:51:47.018846 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0183123 (* 1 = 0.0183123 loss)
I1026 00:51:47.018851 17176 sgd_solver.cpp:106] Iteration 13520, lr = 0.001
I1026 00:51:47.572443 17176 solver.cpp:229] Iteration 13540, loss = 0.26676
I1026 00:51:47.572476 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.071433 (* 1 = 0.071433 loss)
I1026 00:51:47.572480 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.195327 (* 1 = 0.195327 loss)
I1026 00:51:47.572494 17176 sgd_solver.cpp:106] Iteration 13540, lr = 0.001
I1026 00:51:48.131973 17176 solver.cpp:229] Iteration 13560, loss = 0.0310522
I1026 00:51:48.132004 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0191178 (* 1 = 0.0191178 loss)
I1026 00:51:48.132009 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0119344 (* 1 = 0.0119344 loss)
I1026 00:51:48.132012 17176 sgd_solver.cpp:106] Iteration 13560, lr = 0.001
I1026 00:51:48.704308 17176 solver.cpp:229] Iteration 13580, loss = 0.0533883
I1026 00:51:48.704341 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0453978 (* 1 = 0.0453978 loss)
I1026 00:51:48.704346 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00799051 (* 1 = 0.00799051 loss)
I1026 00:51:48.704349 17176 sgd_solver.cpp:106] Iteration 13580, lr = 0.001
I1026 00:51:49.261171 17176 solver.cpp:229] Iteration 13600, loss = 0.177025
I1026 00:51:49.261204 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.114544 (* 1 = 0.114544 loss)
I1026 00:51:49.261209 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0624808 (* 1 = 0.0624808 loss)
I1026 00:51:49.261212 17176 sgd_solver.cpp:106] Iteration 13600, lr = 0.001
I1026 00:51:49.835130 17176 solver.cpp:229] Iteration 13620, loss = 0.0615952
I1026 00:51:49.835175 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0446812 (* 1 = 0.0446812 loss)
I1026 00:51:49.835178 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.016914 (* 1 = 0.016914 loss)
I1026 00:51:49.835182 17176 sgd_solver.cpp:106] Iteration 13620, lr = 0.001
I1026 00:51:50.398804 17176 solver.cpp:229] Iteration 13640, loss = 0.0693165
I1026 00:51:50.398847 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0569646 (* 1 = 0.0569646 loss)
I1026 00:51:50.398852 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.012352 (* 1 = 0.012352 loss)
I1026 00:51:50.398856 17176 sgd_solver.cpp:106] Iteration 13640, lr = 0.001
I1026 00:51:50.962427 17176 solver.cpp:229] Iteration 13660, loss = 0.121648
I1026 00:51:50.962460 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0998384 (* 1 = 0.0998384 loss)
I1026 00:51:50.962465 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0218092 (* 1 = 0.0218092 loss)
I1026 00:51:50.962469 17176 sgd_solver.cpp:106] Iteration 13660, lr = 0.001
I1026 00:51:51.520733 17176 solver.cpp:229] Iteration 13680, loss = 0.193219
I1026 00:51:51.520766 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.151794 (* 1 = 0.151794 loss)
I1026 00:51:51.520771 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0414251 (* 1 = 0.0414251 loss)
I1026 00:51:51.520776 17176 sgd_solver.cpp:106] Iteration 13680, lr = 0.001
I1026 00:51:52.078315 17176 solver.cpp:229] Iteration 13700, loss = 0.120727
I1026 00:51:52.078347 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0856589 (* 1 = 0.0856589 loss)
I1026 00:51:52.078352 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0350681 (* 1 = 0.0350681 loss)
I1026 00:51:52.078356 17176 sgd_solver.cpp:106] Iteration 13700, lr = 0.001
I1026 00:51:52.631716 17176 solver.cpp:229] Iteration 13720, loss = 0.0988405
I1026 00:51:52.631747 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0622574 (* 1 = 0.0622574 loss)
I1026 00:51:52.631752 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0365832 (* 1 = 0.0365832 loss)
I1026 00:51:52.631755 17176 sgd_solver.cpp:106] Iteration 13720, lr = 0.001
I1026 00:51:53.190563 17176 solver.cpp:229] Iteration 13740, loss = 0.125548
I1026 00:51:53.190596 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.115403 (* 1 = 0.115403 loss)
I1026 00:51:53.190600 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0101453 (* 1 = 0.0101453 loss)
I1026 00:51:53.190604 17176 sgd_solver.cpp:106] Iteration 13740, lr = 0.001
I1026 00:51:53.755249 17176 solver.cpp:229] Iteration 13760, loss = 0.0679045
I1026 00:51:53.755281 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0576517 (* 1 = 0.0576517 loss)
I1026 00:51:53.755286 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0102528 (* 1 = 0.0102528 loss)
I1026 00:51:53.755291 17176 sgd_solver.cpp:106] Iteration 13760, lr = 0.001
I1026 00:51:54.304829 17176 solver.cpp:229] Iteration 13780, loss = 0.0783335
I1026 00:51:54.304862 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0593617 (* 1 = 0.0593617 loss)
I1026 00:51:54.304867 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0189718 (* 1 = 0.0189718 loss)
I1026 00:51:54.304870 17176 sgd_solver.cpp:106] Iteration 13780, lr = 0.001
I1026 00:51:54.862965 17176 solver.cpp:229] Iteration 13800, loss = 0.261129
I1026 00:51:54.863008 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.15653 (* 1 = 0.15653 loss)
I1026 00:51:54.863013 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.104599 (* 1 = 0.104599 loss)
I1026 00:51:54.863016 17176 sgd_solver.cpp:106] Iteration 13800, lr = 0.001
I1026 00:51:55.436121 17176 solver.cpp:229] Iteration 13820, loss = 0.179962
I1026 00:51:55.436157 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0696621 (* 1 = 0.0696621 loss)
I1026 00:51:55.436162 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.1103 (* 1 = 0.1103 loss)
I1026 00:51:55.436167 17176 sgd_solver.cpp:106] Iteration 13820, lr = 0.001
I1026 00:51:55.990852 17176 solver.cpp:229] Iteration 13840, loss = 0.186924
I1026 00:51:55.990895 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.170589 (* 1 = 0.170589 loss)
I1026 00:51:55.990900 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0163344 (* 1 = 0.0163344 loss)
I1026 00:51:55.990905 17176 sgd_solver.cpp:106] Iteration 13840, lr = 0.001
I1026 00:51:56.567922 17176 solver.cpp:229] Iteration 13860, loss = 0.48941
I1026 00:51:56.567956 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.435341 (* 1 = 0.435341 loss)
I1026 00:51:56.567961 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0540682 (* 1 = 0.0540682 loss)
I1026 00:51:56.567965 17176 sgd_solver.cpp:106] Iteration 13860, lr = 0.001
I1026 00:51:57.133996 17176 solver.cpp:229] Iteration 13880, loss = 0.0667454
I1026 00:51:57.134030 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0624827 (* 1 = 0.0624827 loss)
I1026 00:51:57.134034 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00426275 (* 1 = 0.00426275 loss)
I1026 00:51:57.134038 17176 sgd_solver.cpp:106] Iteration 13880, lr = 0.001
I1026 00:51:57.704830 17176 solver.cpp:229] Iteration 13900, loss = 0.123894
I1026 00:51:57.704874 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0799717 (* 1 = 0.0799717 loss)
I1026 00:51:57.704879 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0439228 (* 1 = 0.0439228 loss)
I1026 00:51:57.704885 17176 sgd_solver.cpp:106] Iteration 13900, lr = 0.001
I1026 00:51:58.261720 17176 solver.cpp:229] Iteration 13920, loss = 0.0641474
I1026 00:51:58.261755 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.045902 (* 1 = 0.045902 loss)
I1026 00:51:58.261760 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0182454 (* 1 = 0.0182454 loss)
I1026 00:51:58.261765 17176 sgd_solver.cpp:106] Iteration 13920, lr = 0.001
I1026 00:51:58.824579 17176 solver.cpp:229] Iteration 13940, loss = 0.0917549
I1026 00:51:58.824623 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0505492 (* 1 = 0.0505492 loss)
I1026 00:51:58.824628 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0412057 (* 1 = 0.0412057 loss)
I1026 00:51:58.824633 17176 sgd_solver.cpp:106] Iteration 13940, lr = 0.001
I1026 00:51:59.385979 17176 solver.cpp:229] Iteration 13960, loss = 0.292473
I1026 00:51:59.386014 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.220374 (* 1 = 0.220374 loss)
I1026 00:51:59.386019 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0720996 (* 1 = 0.0720996 loss)
I1026 00:51:59.386024 17176 sgd_solver.cpp:106] Iteration 13960, lr = 0.001
I1026 00:51:59.958287 17176 solver.cpp:229] Iteration 13980, loss = 0.0955738
I1026 00:51:59.958336 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0483765 (* 1 = 0.0483765 loss)
I1026 00:51:59.958343 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0471973 (* 1 = 0.0471973 loss)
I1026 00:51:59.958346 17176 sgd_solver.cpp:106] Iteration 13980, lr = 0.001
I1026 00:52:00.510682 17176 solver.cpp:229] Iteration 14000, loss = 0.0522304
I1026 00:52:00.510717 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0209955 (* 1 = 0.0209955 loss)
I1026 00:52:00.510723 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0312349 (* 1 = 0.0312349 loss)
I1026 00:52:00.510728 17176 sgd_solver.cpp:106] Iteration 14000, lr = 0.001
I1026 00:52:01.075603 17176 solver.cpp:229] Iteration 14020, loss = 0.120787
I1026 00:52:01.075636 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.104308 (* 1 = 0.104308 loss)
I1026 00:52:01.075642 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0164786 (* 1 = 0.0164786 loss)
I1026 00:52:01.075647 17176 sgd_solver.cpp:106] Iteration 14020, lr = 0.001
I1026 00:52:01.628499 17176 solver.cpp:229] Iteration 14040, loss = 0.0764201
I1026 00:52:01.628531 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0427629 (* 1 = 0.0427629 loss)
I1026 00:52:01.628537 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0336572 (* 1 = 0.0336572 loss)
I1026 00:52:01.628542 17176 sgd_solver.cpp:106] Iteration 14040, lr = 0.001
I1026 00:52:02.199018 17176 solver.cpp:229] Iteration 14060, loss = 0.0501568
I1026 00:52:02.199053 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0195562 (* 1 = 0.0195562 loss)
I1026 00:52:02.199059 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0306006 (* 1 = 0.0306006 loss)
I1026 00:52:02.199064 17176 sgd_solver.cpp:106] Iteration 14060, lr = 0.001
I1026 00:52:02.762341 17176 solver.cpp:229] Iteration 14080, loss = 0.176971
I1026 00:52:02.762375 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0612837 (* 1 = 0.0612837 loss)
I1026 00:52:02.762383 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.115688 (* 1 = 0.115688 loss)
I1026 00:52:02.762389 17176 sgd_solver.cpp:106] Iteration 14080, lr = 0.001
I1026 00:52:03.333199 17176 solver.cpp:229] Iteration 14100, loss = 0.0402314
I1026 00:52:03.333233 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0378469 (* 1 = 0.0378469 loss)
I1026 00:52:03.333238 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00238448 (* 1 = 0.00238448 loss)
I1026 00:52:03.333241 17176 sgd_solver.cpp:106] Iteration 14100, lr = 0.001
I1026 00:52:03.894206 17176 solver.cpp:229] Iteration 14120, loss = 0.0950927
I1026 00:52:03.894237 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0481132 (* 1 = 0.0481132 loss)
I1026 00:52:03.894243 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0469795 (* 1 = 0.0469795 loss)
I1026 00:52:03.894246 17176 sgd_solver.cpp:106] Iteration 14120, lr = 0.001
I1026 00:52:04.473117 17176 solver.cpp:229] Iteration 14140, loss = 0.0805382
I1026 00:52:04.473162 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0619315 (* 1 = 0.0619315 loss)
I1026 00:52:04.473168 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0186067 (* 1 = 0.0186067 loss)
I1026 00:52:04.473172 17176 sgd_solver.cpp:106] Iteration 14140, lr = 0.001
I1026 00:52:05.041218 17176 solver.cpp:229] Iteration 14160, loss = 0.10637
I1026 00:52:05.041250 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0776396 (* 1 = 0.0776396 loss)
I1026 00:52:05.041256 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.02873 (* 1 = 0.02873 loss)
I1026 00:52:05.041260 17176 sgd_solver.cpp:106] Iteration 14160, lr = 0.001
I1026 00:52:05.619828 17176 solver.cpp:229] Iteration 14180, loss = 0.409201
I1026 00:52:05.619861 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.228998 (* 1 = 0.228998 loss)
I1026 00:52:05.619866 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.180202 (* 1 = 0.180202 loss)
I1026 00:52:05.619871 17176 sgd_solver.cpp:106] Iteration 14180, lr = 0.001
I1026 00:52:06.176988 17176 solver.cpp:229] Iteration 14200, loss = 0.195265
I1026 00:52:06.177021 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0938809 (* 1 = 0.0938809 loss)
I1026 00:52:06.177026 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.101384 (* 1 = 0.101384 loss)
I1026 00:52:06.177031 17176 sgd_solver.cpp:106] Iteration 14200, lr = 0.001
I1026 00:52:06.749246 17176 solver.cpp:229] Iteration 14220, loss = 0.113847
I1026 00:52:06.749280 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.103728 (* 1 = 0.103728 loss)
I1026 00:52:06.749286 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0101191 (* 1 = 0.0101191 loss)
I1026 00:52:06.749290 17176 sgd_solver.cpp:106] Iteration 14220, lr = 0.001
I1026 00:52:07.306957 17176 solver.cpp:229] Iteration 14240, loss = 0.0403982
I1026 00:52:07.306990 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0323821 (* 1 = 0.0323821 loss)
I1026 00:52:07.306995 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00801612 (* 1 = 0.00801612 loss)
I1026 00:52:07.307001 17176 sgd_solver.cpp:106] Iteration 14240, lr = 0.001
I1026 00:52:07.873874 17176 solver.cpp:229] Iteration 14260, loss = 0.0708099
I1026 00:52:07.873906 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0203706 (* 1 = 0.0203706 loss)
I1026 00:52:07.873911 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0504393 (* 1 = 0.0504393 loss)
I1026 00:52:07.873916 17176 sgd_solver.cpp:106] Iteration 14260, lr = 0.001
I1026 00:52:08.439278 17176 solver.cpp:229] Iteration 14280, loss = 0.103618
I1026 00:52:08.439313 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0873376 (* 1 = 0.0873376 loss)
I1026 00:52:08.439318 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0162806 (* 1 = 0.0162806 loss)
I1026 00:52:08.439323 17176 sgd_solver.cpp:106] Iteration 14280, lr = 0.001
I1026 00:52:09.001343 17176 solver.cpp:229] Iteration 14300, loss = 0.100827
I1026 00:52:09.001377 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0814365 (* 1 = 0.0814365 loss)
I1026 00:52:09.001384 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0193905 (* 1 = 0.0193905 loss)
I1026 00:52:09.001389 17176 sgd_solver.cpp:106] Iteration 14300, lr = 0.001
I1026 00:52:09.559695 17176 solver.cpp:229] Iteration 14320, loss = 0.0542303
I1026 00:52:09.559727 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0477917 (* 1 = 0.0477917 loss)
I1026 00:52:09.559732 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0064386 (* 1 = 0.0064386 loss)
I1026 00:52:09.559736 17176 sgd_solver.cpp:106] Iteration 14320, lr = 0.001
I1026 00:52:10.133558 17176 solver.cpp:229] Iteration 14340, loss = 0.0550396
I1026 00:52:10.133604 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0466579 (* 1 = 0.0466579 loss)
I1026 00:52:10.133610 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00838174 (* 1 = 0.00838174 loss)
I1026 00:52:10.133622 17176 sgd_solver.cpp:106] Iteration 14340, lr = 0.001
I1026 00:52:10.699129 17176 solver.cpp:229] Iteration 14360, loss = 0.0446027
I1026 00:52:10.699162 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0247061 (* 1 = 0.0247061 loss)
I1026 00:52:10.699167 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0198967 (* 1 = 0.0198967 loss)
I1026 00:52:10.699172 17176 sgd_solver.cpp:106] Iteration 14360, lr = 0.001
I1026 00:52:11.260185 17176 solver.cpp:229] Iteration 14380, loss = 0.057936
I1026 00:52:11.260218 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0493791 (* 1 = 0.0493791 loss)
I1026 00:52:11.260241 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00855696 (* 1 = 0.00855696 loss)
I1026 00:52:11.260246 17176 sgd_solver.cpp:106] Iteration 14380, lr = 0.001
I1026 00:52:11.832878 17176 solver.cpp:229] Iteration 14400, loss = 0.0763729
I1026 00:52:11.832911 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0559919 (* 1 = 0.0559919 loss)
I1026 00:52:11.832916 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.020381 (* 1 = 0.020381 loss)
I1026 00:52:11.832921 17176 sgd_solver.cpp:106] Iteration 14400, lr = 0.001
I1026 00:52:12.404927 17176 solver.cpp:229] Iteration 14420, loss = 0.0498136
I1026 00:52:12.404960 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0408429 (* 1 = 0.0408429 loss)
I1026 00:52:12.404965 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00897066 (* 1 = 0.00897066 loss)
I1026 00:52:12.404969 17176 sgd_solver.cpp:106] Iteration 14420, lr = 0.001
I1026 00:52:12.978415 17176 solver.cpp:229] Iteration 14440, loss = 0.550837
I1026 00:52:12.978443 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.257752 (* 1 = 0.257752 loss)
I1026 00:52:12.978448 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.293085 (* 1 = 0.293085 loss)
I1026 00:52:12.978452 17176 sgd_solver.cpp:106] Iteration 14440, lr = 0.001
I1026 00:52:13.544636 17176 solver.cpp:229] Iteration 14460, loss = 0.0477089
I1026 00:52:13.544669 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0357479 (* 1 = 0.0357479 loss)
I1026 00:52:13.544674 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.011961 (* 1 = 0.011961 loss)
I1026 00:52:13.544679 17176 sgd_solver.cpp:106] Iteration 14460, lr = 0.001
I1026 00:52:14.109568 17176 solver.cpp:229] Iteration 14480, loss = 0.109629
I1026 00:52:14.109601 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.064373 (* 1 = 0.064373 loss)
I1026 00:52:14.109606 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0452558 (* 1 = 0.0452558 loss)
I1026 00:52:14.109611 17176 sgd_solver.cpp:106] Iteration 14480, lr = 0.001
I1026 00:52:14.663427 17176 solver.cpp:229] Iteration 14500, loss = 0.15821
I1026 00:52:14.663463 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.129089 (* 1 = 0.129089 loss)
I1026 00:52:14.663468 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.029121 (* 1 = 0.029121 loss)
I1026 00:52:14.663472 17176 sgd_solver.cpp:106] Iteration 14500, lr = 0.001
I1026 00:52:15.228524 17176 solver.cpp:229] Iteration 14520, loss = 0.144496
I1026 00:52:15.228556 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0749439 (* 1 = 0.0749439 loss)
I1026 00:52:15.228560 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0695523 (* 1 = 0.0695523 loss)
I1026 00:52:15.228564 17176 sgd_solver.cpp:106] Iteration 14520, lr = 0.001
I1026 00:52:15.778470 17176 solver.cpp:229] Iteration 14540, loss = 0.20567
I1026 00:52:15.778503 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.142103 (* 1 = 0.142103 loss)
I1026 00:52:15.778508 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0635675 (* 1 = 0.0635675 loss)
I1026 00:52:15.778512 17176 sgd_solver.cpp:106] Iteration 14540, lr = 0.001
I1026 00:52:16.343719 17176 solver.cpp:229] Iteration 14560, loss = 0.0275467
I1026 00:52:16.343751 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0253041 (* 1 = 0.0253041 loss)
I1026 00:52:16.343756 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00224265 (* 1 = 0.00224265 loss)
I1026 00:52:16.343760 17176 sgd_solver.cpp:106] Iteration 14560, lr = 0.001
I1026 00:52:16.894388 17176 solver.cpp:229] Iteration 14580, loss = 0.0181433
I1026 00:52:16.894418 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0139754 (* 1 = 0.0139754 loss)
I1026 00:52:16.894423 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0041679 (* 1 = 0.0041679 loss)
I1026 00:52:16.894428 17176 sgd_solver.cpp:106] Iteration 14580, lr = 0.001
I1026 00:52:17.445209 17176 solver.cpp:229] Iteration 14600, loss = 0.0632747
I1026 00:52:17.445242 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0517182 (* 1 = 0.0517182 loss)
I1026 00:52:17.445246 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0115565 (* 1 = 0.0115565 loss)
I1026 00:52:17.445251 17176 sgd_solver.cpp:106] Iteration 14600, lr = 0.001
I1026 00:52:18.012431 17176 solver.cpp:229] Iteration 14620, loss = 0.0826285
I1026 00:52:18.012465 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0599862 (* 1 = 0.0599862 loss)
I1026 00:52:18.012485 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0226423 (* 1 = 0.0226423 loss)
I1026 00:52:18.012490 17176 sgd_solver.cpp:106] Iteration 14620, lr = 0.001
I1026 00:52:18.578944 17176 solver.cpp:229] Iteration 14640, loss = 0.172631
I1026 00:52:18.578976 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0285914 (* 1 = 0.0285914 loss)
I1026 00:52:18.578981 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.144039 (* 1 = 0.144039 loss)
I1026 00:52:18.578985 17176 sgd_solver.cpp:106] Iteration 14640, lr = 0.001
I1026 00:52:19.132164 17176 solver.cpp:229] Iteration 14660, loss = 0.232245
I1026 00:52:19.132207 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.11119 (* 1 = 0.11119 loss)
I1026 00:52:19.132212 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.121056 (* 1 = 0.121056 loss)
I1026 00:52:19.132216 17176 sgd_solver.cpp:106] Iteration 14660, lr = 0.001
I1026 00:52:19.693387 17176 solver.cpp:229] Iteration 14680, loss = 0.0736157
I1026 00:52:19.693418 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.041545 (* 1 = 0.041545 loss)
I1026 00:52:19.693423 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0320707 (* 1 = 0.0320707 loss)
I1026 00:52:19.693426 17176 sgd_solver.cpp:106] Iteration 14680, lr = 0.001
I1026 00:52:20.264679 17176 solver.cpp:229] Iteration 14700, loss = 0.10432
I1026 00:52:20.264713 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0572355 (* 1 = 0.0572355 loss)
I1026 00:52:20.264717 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0470847 (* 1 = 0.0470847 loss)
I1026 00:52:20.264721 17176 sgd_solver.cpp:106] Iteration 14700, lr = 0.001
I1026 00:52:20.825637 17176 solver.cpp:229] Iteration 14720, loss = 0.0870606
I1026 00:52:20.825670 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0793502 (* 1 = 0.0793502 loss)
I1026 00:52:20.825673 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00771044 (* 1 = 0.00771044 loss)
I1026 00:52:20.825678 17176 sgd_solver.cpp:106] Iteration 14720, lr = 0.001
I1026 00:52:21.391402 17176 solver.cpp:229] Iteration 14740, loss = 0.0472545
I1026 00:52:21.391436 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0410805 (* 1 = 0.0410805 loss)
I1026 00:52:21.391441 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00617396 (* 1 = 0.00617396 loss)
I1026 00:52:21.391445 17176 sgd_solver.cpp:106] Iteration 14740, lr = 0.001
I1026 00:52:21.949784 17176 solver.cpp:229] Iteration 14760, loss = 0.0682592
I1026 00:52:21.949815 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.038647 (* 1 = 0.038647 loss)
I1026 00:52:21.949836 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0296122 (* 1 = 0.0296122 loss)
I1026 00:52:21.949841 17176 sgd_solver.cpp:106] Iteration 14760, lr = 0.001
I1026 00:52:22.510357 17176 solver.cpp:229] Iteration 14780, loss = 0.0718394
I1026 00:52:22.510391 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0558405 (* 1 = 0.0558405 loss)
I1026 00:52:22.510396 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.015999 (* 1 = 0.015999 loss)
I1026 00:52:22.510399 17176 sgd_solver.cpp:106] Iteration 14780, lr = 0.001
I1026 00:52:23.069687 17176 solver.cpp:229] Iteration 14800, loss = 0.0605503
I1026 00:52:23.069720 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.053423 (* 1 = 0.053423 loss)
I1026 00:52:23.069725 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00712737 (* 1 = 0.00712737 loss)
I1026 00:52:23.069727 17176 sgd_solver.cpp:106] Iteration 14800, lr = 0.001
I1026 00:52:23.612968 17176 solver.cpp:229] Iteration 14820, loss = 0.387354
I1026 00:52:23.613000 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.291413 (* 1 = 0.291413 loss)
I1026 00:52:23.613005 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0959407 (* 1 = 0.0959407 loss)
I1026 00:52:23.613010 17176 sgd_solver.cpp:106] Iteration 14820, lr = 0.001
I1026 00:52:24.169723 17176 solver.cpp:229] Iteration 14840, loss = 0.131859
I1026 00:52:24.169754 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0258892 (* 1 = 0.0258892 loss)
I1026 00:52:24.169759 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.10597 (* 1 = 0.10597 loss)
I1026 00:52:24.169764 17176 sgd_solver.cpp:106] Iteration 14840, lr = 0.001
I1026 00:52:24.725971 17176 solver.cpp:229] Iteration 14860, loss = 0.141576
I1026 00:52:24.726004 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.125261 (* 1 = 0.125261 loss)
I1026 00:52:24.726008 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0163155 (* 1 = 0.0163155 loss)
I1026 00:52:24.726012 17176 sgd_solver.cpp:106] Iteration 14860, lr = 0.001
I1026 00:52:25.286911 17176 solver.cpp:229] Iteration 14880, loss = 0.255842
I1026 00:52:25.286944 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0718676 (* 1 = 0.0718676 loss)
I1026 00:52:25.286949 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.183975 (* 1 = 0.183975 loss)
I1026 00:52:25.286953 17176 sgd_solver.cpp:106] Iteration 14880, lr = 0.001
I1026 00:52:25.842207 17176 solver.cpp:229] Iteration 14900, loss = 0.130258
I1026 00:52:25.842241 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0232376 (* 1 = 0.0232376 loss)
I1026 00:52:25.842244 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.10702 (* 1 = 0.10702 loss)
I1026 00:52:25.842248 17176 sgd_solver.cpp:106] Iteration 14900, lr = 0.001
I1026 00:52:26.398695 17176 solver.cpp:229] Iteration 14920, loss = 0.516618
I1026 00:52:26.398727 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.112342 (* 1 = 0.112342 loss)
I1026 00:52:26.398731 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.404276 (* 1 = 0.404276 loss)
I1026 00:52:26.398736 17176 sgd_solver.cpp:106] Iteration 14920, lr = 0.001
I1026 00:52:26.963605 17176 solver.cpp:229] Iteration 14940, loss = 0.157343
I1026 00:52:26.963637 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0499129 (* 1 = 0.0499129 loss)
I1026 00:52:26.963642 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.10743 (* 1 = 0.10743 loss)
I1026 00:52:26.963647 17176 sgd_solver.cpp:106] Iteration 14940, lr = 0.001
I1026 00:52:27.534658 17176 solver.cpp:229] Iteration 14960, loss = 0.197773
I1026 00:52:27.534692 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.181544 (* 1 = 0.181544 loss)
I1026 00:52:27.534696 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0162287 (* 1 = 0.0162287 loss)
I1026 00:52:27.534700 17176 sgd_solver.cpp:106] Iteration 14960, lr = 0.001
I1026 00:52:28.099202 17176 solver.cpp:229] Iteration 14980, loss = 0.0434073
I1026 00:52:28.099244 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0354926 (* 1 = 0.0354926 loss)
I1026 00:52:28.099249 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00791465 (* 1 = 0.00791465 loss)
I1026 00:52:28.099253 17176 sgd_solver.cpp:106] Iteration 14980, lr = 0.001
I1026 00:52:28.664065 17176 solver.cpp:229] Iteration 15000, loss = 0.0457605
I1026 00:52:28.664098 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0378098 (* 1 = 0.0378098 loss)
I1026 00:52:28.664103 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00795074 (* 1 = 0.00795074 loss)
I1026 00:52:28.664106 17176 sgd_solver.cpp:106] Iteration 15000, lr = 0.001
I1026 00:52:29.226727 17176 solver.cpp:229] Iteration 15020, loss = 0.0867449
I1026 00:52:29.226760 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0488329 (* 1 = 0.0488329 loss)
I1026 00:52:29.226765 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.037912 (* 1 = 0.037912 loss)
I1026 00:52:29.226768 17176 sgd_solver.cpp:106] Iteration 15020, lr = 0.001
I1026 00:52:29.782603 17176 solver.cpp:229] Iteration 15040, loss = 0.429086
I1026 00:52:29.782635 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.166468 (* 1 = 0.166468 loss)
I1026 00:52:29.782639 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.262618 (* 1 = 0.262618 loss)
I1026 00:52:29.782644 17176 sgd_solver.cpp:106] Iteration 15040, lr = 0.001
I1026 00:52:30.337903 17176 solver.cpp:229] Iteration 15060, loss = 0.0781807
I1026 00:52:30.337935 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0533734 (* 1 = 0.0533734 loss)
I1026 00:52:30.337939 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0248073 (* 1 = 0.0248073 loss)
I1026 00:52:30.337944 17176 sgd_solver.cpp:106] Iteration 15060, lr = 0.001
I1026 00:52:30.901005 17176 solver.cpp:229] Iteration 15080, loss = 0.232219
I1026 00:52:30.901037 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.185193 (* 1 = 0.185193 loss)
I1026 00:52:30.901042 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0470263 (* 1 = 0.0470263 loss)
I1026 00:52:30.901046 17176 sgd_solver.cpp:106] Iteration 15080, lr = 0.001
I1026 00:52:31.458593 17176 solver.cpp:229] Iteration 15100, loss = 0.0378401
I1026 00:52:31.458626 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0315607 (* 1 = 0.0315607 loss)
I1026 00:52:31.458631 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00627939 (* 1 = 0.00627939 loss)
I1026 00:52:31.458634 17176 sgd_solver.cpp:106] Iteration 15100, lr = 0.001
I1026 00:52:32.019191 17176 solver.cpp:229] Iteration 15120, loss = 0.128682
I1026 00:52:32.019222 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.121291 (* 1 = 0.121291 loss)
I1026 00:52:32.019227 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00739032 (* 1 = 0.00739032 loss)
I1026 00:52:32.019232 17176 sgd_solver.cpp:106] Iteration 15120, lr = 0.001
I1026 00:52:32.563313 17176 solver.cpp:229] Iteration 15140, loss = 0.0855565
I1026 00:52:32.563344 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0805199 (* 1 = 0.0805199 loss)
I1026 00:52:32.563349 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00503653 (* 1 = 0.00503653 loss)
I1026 00:52:32.563354 17176 sgd_solver.cpp:106] Iteration 15140, lr = 0.001
I1026 00:52:33.113565 17176 solver.cpp:229] Iteration 15160, loss = 0.0660276
I1026 00:52:33.113598 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0299085 (* 1 = 0.0299085 loss)
I1026 00:52:33.113603 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.036119 (* 1 = 0.036119 loss)
I1026 00:52:33.113607 17176 sgd_solver.cpp:106] Iteration 15160, lr = 0.001
I1026 00:52:33.679576 17176 solver.cpp:229] Iteration 15180, loss = 0.118789
I1026 00:52:33.679610 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0628778 (* 1 = 0.0628778 loss)
I1026 00:52:33.679615 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.055911 (* 1 = 0.055911 loss)
I1026 00:52:33.679618 17176 sgd_solver.cpp:106] Iteration 15180, lr = 0.001
I1026 00:52:34.246222 17176 solver.cpp:229] Iteration 15200, loss = 0.400457
I1026 00:52:34.246254 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.236679 (* 1 = 0.236679 loss)
I1026 00:52:34.246259 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.163778 (* 1 = 0.163778 loss)
I1026 00:52:34.246263 17176 sgd_solver.cpp:106] Iteration 15200, lr = 0.001
I1026 00:52:34.823902 17176 solver.cpp:229] Iteration 15220, loss = 0.0844228
I1026 00:52:34.823943 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0547754 (* 1 = 0.0547754 loss)
I1026 00:52:34.823947 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0296474 (* 1 = 0.0296474 loss)
I1026 00:52:34.823951 17176 sgd_solver.cpp:106] Iteration 15220, lr = 0.001
I1026 00:52:35.405673 17176 solver.cpp:229] Iteration 15240, loss = 0.138666
I1026 00:52:35.405715 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.127583 (* 1 = 0.127583 loss)
I1026 00:52:35.405720 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.011083 (* 1 = 0.011083 loss)
I1026 00:52:35.405725 17176 sgd_solver.cpp:106] Iteration 15240, lr = 0.001
I1026 00:52:35.970827 17176 solver.cpp:229] Iteration 15260, loss = 0.122439
I1026 00:52:35.970861 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0415088 (* 1 = 0.0415088 loss)
I1026 00:52:35.970866 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0809305 (* 1 = 0.0809305 loss)
I1026 00:52:35.970871 17176 sgd_solver.cpp:106] Iteration 15260, lr = 0.001
I1026 00:52:36.532795 17176 solver.cpp:229] Iteration 15280, loss = 0.0065074
I1026 00:52:36.532827 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00420444 (* 1 = 0.00420444 loss)
I1026 00:52:36.532832 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00230296 (* 1 = 0.00230296 loss)
I1026 00:52:36.532835 17176 sgd_solver.cpp:106] Iteration 15280, lr = 0.001
I1026 00:52:37.087890 17176 solver.cpp:229] Iteration 15300, loss = 0.113189
I1026 00:52:37.087923 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0800804 (* 1 = 0.0800804 loss)
I1026 00:52:37.087927 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0331087 (* 1 = 0.0331087 loss)
I1026 00:52:37.087931 17176 sgd_solver.cpp:106] Iteration 15300, lr = 0.001
I1026 00:52:37.643496 17176 solver.cpp:229] Iteration 15320, loss = 0.0105289
I1026 00:52:37.643528 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00890562 (* 1 = 0.00890562 loss)
I1026 00:52:37.643533 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0016233 (* 1 = 0.0016233 loss)
I1026 00:52:37.643537 17176 sgd_solver.cpp:106] Iteration 15320, lr = 0.001
I1026 00:52:38.189364 17176 solver.cpp:229] Iteration 15340, loss = 0.0954151
I1026 00:52:38.189406 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0562543 (* 1 = 0.0562543 loss)
I1026 00:52:38.189411 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0391607 (* 1 = 0.0391607 loss)
I1026 00:52:38.189415 17176 sgd_solver.cpp:106] Iteration 15340, lr = 0.001
I1026 00:52:38.756593 17176 solver.cpp:229] Iteration 15360, loss = 0.152587
I1026 00:52:38.756625 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.128618 (* 1 = 0.128618 loss)
I1026 00:52:38.756630 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0239695 (* 1 = 0.0239695 loss)
I1026 00:52:38.756634 17176 sgd_solver.cpp:106] Iteration 15360, lr = 0.001
I1026 00:52:39.318480 17176 solver.cpp:229] Iteration 15380, loss = 0.146214
I1026 00:52:39.318512 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.102157 (* 1 = 0.102157 loss)
I1026 00:52:39.318517 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.044057 (* 1 = 0.044057 loss)
I1026 00:52:39.318521 17176 sgd_solver.cpp:106] Iteration 15380, lr = 0.001
I1026 00:52:39.870771 17176 solver.cpp:229] Iteration 15400, loss = 0.128745
I1026 00:52:39.870803 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0785056 (* 1 = 0.0785056 loss)
I1026 00:52:39.870808 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0502394 (* 1 = 0.0502394 loss)
I1026 00:52:39.870812 17176 sgd_solver.cpp:106] Iteration 15400, lr = 0.001
I1026 00:52:40.420869 17176 solver.cpp:229] Iteration 15420, loss = 0.048011
I1026 00:52:40.420898 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0256866 (* 1 = 0.0256866 loss)
I1026 00:52:40.420903 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0223243 (* 1 = 0.0223243 loss)
I1026 00:52:40.420907 17176 sgd_solver.cpp:106] Iteration 15420, lr = 0.001
I1026 00:52:40.983479 17176 solver.cpp:229] Iteration 15440, loss = 0.0404567
I1026 00:52:40.983511 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0381181 (* 1 = 0.0381181 loss)
I1026 00:52:40.983516 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00233855 (* 1 = 0.00233855 loss)
I1026 00:52:40.983520 17176 sgd_solver.cpp:106] Iteration 15440, lr = 0.001
I1026 00:52:41.533062 17176 solver.cpp:229] Iteration 15460, loss = 1.18351
I1026 00:52:41.533104 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.258325 (* 1 = 0.258325 loss)
I1026 00:52:41.533108 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.925184 (* 1 = 0.925184 loss)
I1026 00:52:41.533113 17176 sgd_solver.cpp:106] Iteration 15460, lr = 0.001
I1026 00:52:42.085337 17176 solver.cpp:229] Iteration 15480, loss = 0.145358
I1026 00:52:42.085371 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.083647 (* 1 = 0.083647 loss)
I1026 00:52:42.085376 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0617112 (* 1 = 0.0617112 loss)
I1026 00:52:42.085379 17176 sgd_solver.cpp:106] Iteration 15480, lr = 0.001
I1026 00:52:42.641202 17176 solver.cpp:229] Iteration 15500, loss = 0.335229
I1026 00:52:42.641233 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0482897 (* 1 = 0.0482897 loss)
I1026 00:52:42.641237 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.286939 (* 1 = 0.286939 loss)
I1026 00:52:42.641242 17176 sgd_solver.cpp:106] Iteration 15500, lr = 0.001
I1026 00:52:43.216836 17176 solver.cpp:229] Iteration 15520, loss = 0.0644529
I1026 00:52:43.216867 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0152395 (* 1 = 0.0152395 loss)
I1026 00:52:43.216872 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0492133 (* 1 = 0.0492133 loss)
I1026 00:52:43.216876 17176 sgd_solver.cpp:106] Iteration 15520, lr = 0.001
I1026 00:52:43.766501 17176 solver.cpp:229] Iteration 15540, loss = 0.0657679
I1026 00:52:43.766546 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.052735 (* 1 = 0.052735 loss)
I1026 00:52:43.766551 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0130329 (* 1 = 0.0130329 loss)
I1026 00:52:43.766563 17176 sgd_solver.cpp:106] Iteration 15540, lr = 0.001
I1026 00:52:44.324607 17176 solver.cpp:229] Iteration 15560, loss = 0.0488681
I1026 00:52:44.324637 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0190262 (* 1 = 0.0190262 loss)
I1026 00:52:44.324641 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0298418 (* 1 = 0.0298418 loss)
I1026 00:52:44.324645 17176 sgd_solver.cpp:106] Iteration 15560, lr = 0.001
I1026 00:52:44.889747 17176 solver.cpp:229] Iteration 15580, loss = 0.216313
I1026 00:52:44.889780 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.196169 (* 1 = 0.196169 loss)
I1026 00:52:44.889785 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0201441 (* 1 = 0.0201441 loss)
I1026 00:52:44.889789 17176 sgd_solver.cpp:106] Iteration 15580, lr = 0.001
I1026 00:52:45.461938 17176 solver.cpp:229] Iteration 15600, loss = 0.127959
I1026 00:52:45.461971 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.089706 (* 1 = 0.089706 loss)
I1026 00:52:45.461976 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0382529 (* 1 = 0.0382529 loss)
I1026 00:52:45.461979 17176 sgd_solver.cpp:106] Iteration 15600, lr = 0.001
I1026 00:52:46.010095 17176 solver.cpp:229] Iteration 15620, loss = 0.0486569
I1026 00:52:46.010125 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0366273 (* 1 = 0.0366273 loss)
I1026 00:52:46.010130 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0120296 (* 1 = 0.0120296 loss)
I1026 00:52:46.010135 17176 sgd_solver.cpp:106] Iteration 15620, lr = 0.001
I1026 00:52:46.570755 17176 solver.cpp:229] Iteration 15640, loss = 0.224734
I1026 00:52:46.570797 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.217604 (* 1 = 0.217604 loss)
I1026 00:52:46.570801 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00713046 (* 1 = 0.00713046 loss)
I1026 00:52:46.570806 17176 sgd_solver.cpp:106] Iteration 15640, lr = 0.001
I1026 00:52:47.131800 17176 solver.cpp:229] Iteration 15660, loss = 0.0358838
I1026 00:52:47.131832 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0342734 (* 1 = 0.0342734 loss)
I1026 00:52:47.131837 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00161039 (* 1 = 0.00161039 loss)
I1026 00:52:47.131842 17176 sgd_solver.cpp:106] Iteration 15660, lr = 0.001
I1026 00:52:47.699307 17176 solver.cpp:229] Iteration 15680, loss = 0.0482091
I1026 00:52:47.699339 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.041792 (* 1 = 0.041792 loss)
I1026 00:52:47.699344 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00641705 (* 1 = 0.00641705 loss)
I1026 00:52:47.699348 17176 sgd_solver.cpp:106] Iteration 15680, lr = 0.001
I1026 00:52:48.266686 17176 solver.cpp:229] Iteration 15700, loss = 0.072722
I1026 00:52:48.266718 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0584234 (* 1 = 0.0584234 loss)
I1026 00:52:48.266723 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0142986 (* 1 = 0.0142986 loss)
I1026 00:52:48.266727 17176 sgd_solver.cpp:106] Iteration 15700, lr = 0.001
I1026 00:52:48.832981 17176 solver.cpp:229] Iteration 15720, loss = 0.145142
I1026 00:52:48.833012 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.122475 (* 1 = 0.122475 loss)
I1026 00:52:48.833016 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0226669 (* 1 = 0.0226669 loss)
I1026 00:52:48.833020 17176 sgd_solver.cpp:106] Iteration 15720, lr = 0.001
I1026 00:52:49.395949 17176 solver.cpp:229] Iteration 15740, loss = 0.0734204
I1026 00:52:49.395982 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0127081 (* 1 = 0.0127081 loss)
I1026 00:52:49.395985 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0607123 (* 1 = 0.0607123 loss)
I1026 00:52:49.395990 17176 sgd_solver.cpp:106] Iteration 15740, lr = 0.001
I1026 00:52:49.961699 17176 solver.cpp:229] Iteration 15760, loss = 0.0762813
I1026 00:52:49.961730 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0603828 (* 1 = 0.0603828 loss)
I1026 00:52:49.961735 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0158986 (* 1 = 0.0158986 loss)
I1026 00:52:49.961740 17176 sgd_solver.cpp:106] Iteration 15760, lr = 0.001
I1026 00:52:50.515470 17176 solver.cpp:229] Iteration 15780, loss = 0.0545142
I1026 00:52:50.515511 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0506248 (* 1 = 0.0506248 loss)
I1026 00:52:50.515527 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00388933 (* 1 = 0.00388933 loss)
I1026 00:52:50.515530 17176 sgd_solver.cpp:106] Iteration 15780, lr = 0.001
I1026 00:52:51.068204 17176 solver.cpp:229] Iteration 15800, loss = 0.0968961
I1026 00:52:51.068235 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0813355 (* 1 = 0.0813355 loss)
I1026 00:52:51.068240 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0155606 (* 1 = 0.0155606 loss)
I1026 00:52:51.068243 17176 sgd_solver.cpp:106] Iteration 15800, lr = 0.001
I1026 00:52:51.631095 17176 solver.cpp:229] Iteration 15820, loss = 0.228332
I1026 00:52:51.631129 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0809538 (* 1 = 0.0809538 loss)
I1026 00:52:51.631134 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.147378 (* 1 = 0.147378 loss)
I1026 00:52:51.631147 17176 sgd_solver.cpp:106] Iteration 15820, lr = 0.001
I1026 00:52:52.204784 17176 solver.cpp:229] Iteration 15840, loss = 0.165379
I1026 00:52:52.204819 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0965039 (* 1 = 0.0965039 loss)
I1026 00:52:52.204824 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0688751 (* 1 = 0.0688751 loss)
I1026 00:52:52.204828 17176 sgd_solver.cpp:106] Iteration 15840, lr = 0.001
I1026 00:52:52.753494 17176 solver.cpp:229] Iteration 15860, loss = 0.180357
I1026 00:52:52.753526 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.121925 (* 1 = 0.121925 loss)
I1026 00:52:52.753531 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0584321 (* 1 = 0.0584321 loss)
I1026 00:52:52.753535 17176 sgd_solver.cpp:106] Iteration 15860, lr = 0.001
I1026 00:52:53.311528 17176 solver.cpp:229] Iteration 15880, loss = 0.315459
I1026 00:52:53.311558 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.261548 (* 1 = 0.261548 loss)
I1026 00:52:53.311563 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.053912 (* 1 = 0.053912 loss)
I1026 00:52:53.311566 17176 sgd_solver.cpp:106] Iteration 15880, lr = 0.001
I1026 00:52:53.870684 17176 solver.cpp:229] Iteration 15900, loss = 0.0524825
I1026 00:52:53.870717 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0439051 (* 1 = 0.0439051 loss)
I1026 00:52:53.870721 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00857735 (* 1 = 0.00857735 loss)
I1026 00:52:53.870725 17176 sgd_solver.cpp:106] Iteration 15900, lr = 0.001
I1026 00:52:54.424475 17176 solver.cpp:229] Iteration 15920, loss = 0.0843624
I1026 00:52:54.424506 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0560558 (* 1 = 0.0560558 loss)
I1026 00:52:54.424511 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0283066 (* 1 = 0.0283066 loss)
I1026 00:52:54.424515 17176 sgd_solver.cpp:106] Iteration 15920, lr = 0.001
I1026 00:52:54.987601 17176 solver.cpp:229] Iteration 15940, loss = 0.214434
I1026 00:52:54.987635 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0360266 (* 1 = 0.0360266 loss)
I1026 00:52:54.987639 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.178407 (* 1 = 0.178407 loss)
I1026 00:52:54.987644 17176 sgd_solver.cpp:106] Iteration 15940, lr = 0.001
I1026 00:52:55.542578 17176 solver.cpp:229] Iteration 15960, loss = 0.095208
I1026 00:52:55.542611 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0774767 (* 1 = 0.0774767 loss)
I1026 00:52:55.542616 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0177314 (* 1 = 0.0177314 loss)
I1026 00:52:55.542620 17176 sgd_solver.cpp:106] Iteration 15960, lr = 0.001
I1026 00:52:56.103977 17176 solver.cpp:229] Iteration 15980, loss = 0.0842224
I1026 00:52:56.104009 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0814773 (* 1 = 0.0814773 loss)
I1026 00:52:56.104013 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00274508 (* 1 = 0.00274508 loss)
I1026 00:52:56.104017 17176 sgd_solver.cpp:106] Iteration 15980, lr = 0.001
I1026 00:52:56.659971 17176 solver.cpp:229] Iteration 16000, loss = 0.0522177
I1026 00:52:56.660003 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0470099 (* 1 = 0.0470099 loss)
I1026 00:52:56.660008 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00520789 (* 1 = 0.00520789 loss)
I1026 00:52:56.660012 17176 sgd_solver.cpp:106] Iteration 16000, lr = 0.001
I1026 00:52:57.225105 17176 solver.cpp:229] Iteration 16020, loss = 0.111192
I1026 00:52:57.225136 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0660016 (* 1 = 0.0660016 loss)
I1026 00:52:57.225141 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0451907 (* 1 = 0.0451907 loss)
I1026 00:52:57.225145 17176 sgd_solver.cpp:106] Iteration 16020, lr = 0.001
I1026 00:52:57.783494 17176 solver.cpp:229] Iteration 16040, loss = 0.0516931
I1026 00:52:57.783527 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0439097 (* 1 = 0.0439097 loss)
I1026 00:52:57.783532 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00778334 (* 1 = 0.00778334 loss)
I1026 00:52:57.783535 17176 sgd_solver.cpp:106] Iteration 16040, lr = 0.001
I1026 00:52:58.344892 17176 solver.cpp:229] Iteration 16060, loss = 0.0692782
I1026 00:52:58.344941 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0610355 (* 1 = 0.0610355 loss)
I1026 00:52:58.344946 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0082427 (* 1 = 0.0082427 loss)
I1026 00:52:58.344950 17176 sgd_solver.cpp:106] Iteration 16060, lr = 0.001
I1026 00:52:58.911938 17176 solver.cpp:229] Iteration 16080, loss = 0.169321
I1026 00:52:58.911972 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0521577 (* 1 = 0.0521577 loss)
I1026 00:52:58.911978 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.117163 (* 1 = 0.117163 loss)
I1026 00:52:58.911981 17176 sgd_solver.cpp:106] Iteration 16080, lr = 0.001
I1026 00:52:59.473932 17176 solver.cpp:229] Iteration 16100, loss = 0.0985174
I1026 00:52:59.473964 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0760763 (* 1 = 0.0760763 loss)
I1026 00:52:59.473969 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0224411 (* 1 = 0.0224411 loss)
I1026 00:52:59.473974 17176 sgd_solver.cpp:106] Iteration 16100, lr = 0.001
I1026 00:53:00.036445 17176 solver.cpp:229] Iteration 16120, loss = 0.0389013
I1026 00:53:00.036478 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0160637 (* 1 = 0.0160637 loss)
I1026 00:53:00.036484 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0228376 (* 1 = 0.0228376 loss)
I1026 00:53:00.036489 17176 sgd_solver.cpp:106] Iteration 16120, lr = 0.001
I1026 00:53:00.590518 17176 solver.cpp:229] Iteration 16140, loss = 0.125598
I1026 00:53:00.590553 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0702155 (* 1 = 0.0702155 loss)
I1026 00:53:00.590558 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0553829 (* 1 = 0.0553829 loss)
I1026 00:53:00.590561 17176 sgd_solver.cpp:106] Iteration 16140, lr = 0.001
I1026 00:53:01.159976 17176 solver.cpp:229] Iteration 16160, loss = 0.10043
I1026 00:53:01.160010 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0901863 (* 1 = 0.0901863 loss)
I1026 00:53:01.160017 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0102436 (* 1 = 0.0102436 loss)
I1026 00:53:01.160020 17176 sgd_solver.cpp:106] Iteration 16160, lr = 0.001
I1026 00:53:01.714556 17176 solver.cpp:229] Iteration 16180, loss = 0.0659541
I1026 00:53:01.714591 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0262905 (* 1 = 0.0262905 loss)
I1026 00:53:01.714596 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0396636 (* 1 = 0.0396636 loss)
I1026 00:53:01.714601 17176 sgd_solver.cpp:106] Iteration 16180, lr = 0.001
I1026 00:53:02.282294 17176 solver.cpp:229] Iteration 16200, loss = 0.0780348
I1026 00:53:02.282328 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0658819 (* 1 = 0.0658819 loss)
I1026 00:53:02.282335 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0121529 (* 1 = 0.0121529 loss)
I1026 00:53:02.282338 17176 sgd_solver.cpp:106] Iteration 16200, lr = 0.001
I1026 00:53:02.853175 17176 solver.cpp:229] Iteration 16220, loss = 0.107078
I1026 00:53:02.853219 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0971917 (* 1 = 0.0971917 loss)
I1026 00:53:02.853235 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00988641 (* 1 = 0.00988641 loss)
I1026 00:53:02.853240 17176 sgd_solver.cpp:106] Iteration 16220, lr = 0.001
I1026 00:53:03.412957 17176 solver.cpp:229] Iteration 16240, loss = 0.0641989
I1026 00:53:03.412991 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0345648 (* 1 = 0.0345648 loss)
I1026 00:53:03.412995 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0296341 (* 1 = 0.0296341 loss)
I1026 00:53:03.412999 17176 sgd_solver.cpp:106] Iteration 16240, lr = 0.001
I1026 00:53:03.983839 17176 solver.cpp:229] Iteration 16260, loss = 0.162234
I1026 00:53:03.983870 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.136033 (* 1 = 0.136033 loss)
I1026 00:53:03.983875 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0262015 (* 1 = 0.0262015 loss)
I1026 00:53:03.983880 17176 sgd_solver.cpp:106] Iteration 16260, lr = 0.001
I1026 00:53:04.542367 17176 solver.cpp:229] Iteration 16280, loss = 1.07483
I1026 00:53:04.542399 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.492335 (* 1 = 0.492335 loss)
I1026 00:53:04.542404 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.582494 (* 1 = 0.582494 loss)
I1026 00:53:04.542408 17176 sgd_solver.cpp:106] Iteration 16280, lr = 0.001
I1026 00:53:05.095067 17176 solver.cpp:229] Iteration 16300, loss = 0.0601379
I1026 00:53:05.095099 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0432939 (* 1 = 0.0432939 loss)
I1026 00:53:05.095104 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.016844 (* 1 = 0.016844 loss)
I1026 00:53:05.095108 17176 sgd_solver.cpp:106] Iteration 16300, lr = 0.001
I1026 00:53:05.665581 17176 solver.cpp:229] Iteration 16320, loss = 0.0410708
I1026 00:53:05.665614 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0357707 (* 1 = 0.0357707 loss)
I1026 00:53:05.665619 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00530007 (* 1 = 0.00530007 loss)
I1026 00:53:05.665623 17176 sgd_solver.cpp:106] Iteration 16320, lr = 0.001
I1026 00:53:06.219439 17176 solver.cpp:229] Iteration 16340, loss = 0.260093
I1026 00:53:06.219473 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.166672 (* 1 = 0.166672 loss)
I1026 00:53:06.219478 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0934206 (* 1 = 0.0934206 loss)
I1026 00:53:06.219482 17176 sgd_solver.cpp:106] Iteration 16340, lr = 0.001
I1026 00:53:06.784996 17176 solver.cpp:229] Iteration 16360, loss = 0.169842
I1026 00:53:06.785039 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.103531 (* 1 = 0.103531 loss)
I1026 00:53:06.785044 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0663104 (* 1 = 0.0663104 loss)
I1026 00:53:06.785048 17176 sgd_solver.cpp:106] Iteration 16360, lr = 0.001
I1026 00:53:07.349915 17176 solver.cpp:229] Iteration 16380, loss = 0.0854955
I1026 00:53:07.349949 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0784083 (* 1 = 0.0784083 loss)
I1026 00:53:07.349954 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00708716 (* 1 = 0.00708716 loss)
I1026 00:53:07.349958 17176 sgd_solver.cpp:106] Iteration 16380, lr = 0.001
I1026 00:53:07.899520 17176 solver.cpp:229] Iteration 16400, loss = 0.089607
I1026 00:53:07.899552 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0423494 (* 1 = 0.0423494 loss)
I1026 00:53:07.899556 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0472576 (* 1 = 0.0472576 loss)
I1026 00:53:07.899561 17176 sgd_solver.cpp:106] Iteration 16400, lr = 0.001
I1026 00:53:08.457418 17176 solver.cpp:229] Iteration 16420, loss = 0.146405
I1026 00:53:08.457451 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0587996 (* 1 = 0.0587996 loss)
I1026 00:53:08.457456 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0876054 (* 1 = 0.0876054 loss)
I1026 00:53:08.457459 17176 sgd_solver.cpp:106] Iteration 16420, lr = 0.001
I1026 00:53:08.999339 17176 solver.cpp:229] Iteration 16440, loss = 0.06294
I1026 00:53:08.999372 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0277835 (* 1 = 0.0277835 loss)
I1026 00:53:08.999375 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0351565 (* 1 = 0.0351565 loss)
I1026 00:53:08.999379 17176 sgd_solver.cpp:106] Iteration 16440, lr = 0.001
I1026 00:53:09.562971 17176 solver.cpp:229] Iteration 16460, loss = 0.161939
I1026 00:53:09.563004 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0699162 (* 1 = 0.0699162 loss)
I1026 00:53:09.563009 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0920225 (* 1 = 0.0920225 loss)
I1026 00:53:09.563011 17176 sgd_solver.cpp:106] Iteration 16460, lr = 0.001
I1026 00:53:10.130846 17176 solver.cpp:229] Iteration 16480, loss = 0.0988102
I1026 00:53:10.130879 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.080356 (* 1 = 0.080356 loss)
I1026 00:53:10.130884 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0184542 (* 1 = 0.0184542 loss)
I1026 00:53:10.130888 17176 sgd_solver.cpp:106] Iteration 16480, lr = 0.001
I1026 00:53:10.678814 17176 solver.cpp:229] Iteration 16500, loss = 0.0676622
I1026 00:53:10.678846 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0634215 (* 1 = 0.0634215 loss)
I1026 00:53:10.678851 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00424067 (* 1 = 0.00424067 loss)
I1026 00:53:10.678855 17176 sgd_solver.cpp:106] Iteration 16500, lr = 0.001
I1026 00:53:11.243722 17176 solver.cpp:229] Iteration 16520, loss = 0.0230176
I1026 00:53:11.243755 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0151282 (* 1 = 0.0151282 loss)
I1026 00:53:11.243759 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00788941 (* 1 = 0.00788941 loss)
I1026 00:53:11.243763 17176 sgd_solver.cpp:106] Iteration 16520, lr = 0.001
I1026 00:53:11.796540 17176 solver.cpp:229] Iteration 16540, loss = 0.0255393
I1026 00:53:11.796573 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0213309 (* 1 = 0.0213309 loss)
I1026 00:53:11.796578 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00420845 (* 1 = 0.00420845 loss)
I1026 00:53:11.796582 17176 sgd_solver.cpp:106] Iteration 16540, lr = 0.001
I1026 00:53:12.365407 17176 solver.cpp:229] Iteration 16560, loss = 0.141636
I1026 00:53:12.365450 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0777853 (* 1 = 0.0777853 loss)
I1026 00:53:12.365454 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0638504 (* 1 = 0.0638504 loss)
I1026 00:53:12.365458 17176 sgd_solver.cpp:106] Iteration 16560, lr = 0.001
I1026 00:53:12.924101 17176 solver.cpp:229] Iteration 16580, loss = 0.0854992
I1026 00:53:12.924134 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0446681 (* 1 = 0.0446681 loss)
I1026 00:53:12.924139 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0408311 (* 1 = 0.0408311 loss)
I1026 00:53:12.924144 17176 sgd_solver.cpp:106] Iteration 16580, lr = 0.001
I1026 00:53:13.490006 17176 solver.cpp:229] Iteration 16600, loss = 1.02082
I1026 00:53:13.490038 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.651276 (* 1 = 0.651276 loss)
I1026 00:53:13.490042 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.369548 (* 1 = 0.369548 loss)
I1026 00:53:13.490047 17176 sgd_solver.cpp:106] Iteration 16600, lr = 0.001
I1026 00:53:14.040689 17176 solver.cpp:229] Iteration 16620, loss = 0.0737501
I1026 00:53:14.040722 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0636698 (* 1 = 0.0636698 loss)
I1026 00:53:14.040727 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0100803 (* 1 = 0.0100803 loss)
I1026 00:53:14.040730 17176 sgd_solver.cpp:106] Iteration 16620, lr = 0.001
I1026 00:53:14.599916 17176 solver.cpp:229] Iteration 16640, loss = 0.0607196
I1026 00:53:14.599959 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0313479 (* 1 = 0.0313479 loss)
I1026 00:53:14.599963 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0293716 (* 1 = 0.0293716 loss)
I1026 00:53:14.599967 17176 sgd_solver.cpp:106] Iteration 16640, lr = 0.001
I1026 00:53:15.170132 17176 solver.cpp:229] Iteration 16660, loss = 0.613366
I1026 00:53:15.170164 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.244114 (* 1 = 0.244114 loss)
I1026 00:53:15.170169 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.369252 (* 1 = 0.369252 loss)
I1026 00:53:15.170173 17176 sgd_solver.cpp:106] Iteration 16660, lr = 0.001
I1026 00:53:15.733647 17176 solver.cpp:229] Iteration 16680, loss = 0.166586
I1026 00:53:15.733680 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0770553 (* 1 = 0.0770553 loss)
I1026 00:53:15.733685 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.089531 (* 1 = 0.089531 loss)
I1026 00:53:15.733690 17176 sgd_solver.cpp:106] Iteration 16680, lr = 0.001
I1026 00:53:16.289222 17176 solver.cpp:229] Iteration 16700, loss = 0.704629
I1026 00:53:16.289254 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.339419 (* 1 = 0.339419 loss)
I1026 00:53:16.289259 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.36521 (* 1 = 0.36521 loss)
I1026 00:53:16.289263 17176 sgd_solver.cpp:106] Iteration 16700, lr = 0.001
I1026 00:53:16.856106 17176 solver.cpp:229] Iteration 16720, loss = 0.0697682
I1026 00:53:16.856138 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0472813 (* 1 = 0.0472813 loss)
I1026 00:53:16.856143 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0224869 (* 1 = 0.0224869 loss)
I1026 00:53:16.856148 17176 sgd_solver.cpp:106] Iteration 16720, lr = 0.001
I1026 00:53:17.425977 17176 solver.cpp:229] Iteration 16740, loss = 0.0432458
I1026 00:53:17.426010 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0254609 (* 1 = 0.0254609 loss)
I1026 00:53:17.426015 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0177849 (* 1 = 0.0177849 loss)
I1026 00:53:17.426019 17176 sgd_solver.cpp:106] Iteration 16740, lr = 0.001
I1026 00:53:17.996978 17176 solver.cpp:229] Iteration 16760, loss = 0.177259
I1026 00:53:17.997009 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.141212 (* 1 = 0.141212 loss)
I1026 00:53:17.997014 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.036047 (* 1 = 0.036047 loss)
I1026 00:53:17.997018 17176 sgd_solver.cpp:106] Iteration 16760, lr = 0.001
I1026 00:53:18.571669 17176 solver.cpp:229] Iteration 16780, loss = 0.0532697
I1026 00:53:18.571705 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0152635 (* 1 = 0.0152635 loss)
I1026 00:53:18.571710 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0380063 (* 1 = 0.0380063 loss)
I1026 00:53:18.571714 17176 sgd_solver.cpp:106] Iteration 16780, lr = 0.001
I1026 00:53:19.116597 17176 solver.cpp:229] Iteration 16800, loss = 0.23118
I1026 00:53:19.116631 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0620171 (* 1 = 0.0620171 loss)
I1026 00:53:19.116634 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.169163 (* 1 = 0.169163 loss)
I1026 00:53:19.116639 17176 sgd_solver.cpp:106] Iteration 16800, lr = 0.001
I1026 00:53:19.683028 17176 solver.cpp:229] Iteration 16820, loss = 0.141535
I1026 00:53:19.683060 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.125574 (* 1 = 0.125574 loss)
I1026 00:53:19.683065 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0159602 (* 1 = 0.0159602 loss)
I1026 00:53:19.683070 17176 sgd_solver.cpp:106] Iteration 16820, lr = 0.001
I1026 00:53:20.245420 17176 solver.cpp:229] Iteration 16840, loss = 0.241094
I1026 00:53:20.245452 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.14915 (* 1 = 0.14915 loss)
I1026 00:53:20.245458 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0919437 (* 1 = 0.0919437 loss)
I1026 00:53:20.245462 17176 sgd_solver.cpp:106] Iteration 16840, lr = 0.001
I1026 00:53:20.790959 17176 solver.cpp:229] Iteration 16860, loss = 0.0702985
I1026 00:53:20.790992 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0533113 (* 1 = 0.0533113 loss)
I1026 00:53:20.790997 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0169872 (* 1 = 0.0169872 loss)
I1026 00:53:20.791000 17176 sgd_solver.cpp:106] Iteration 16860, lr = 0.001
I1026 00:53:21.355561 17176 solver.cpp:229] Iteration 16880, loss = 0.0499926
I1026 00:53:21.355595 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0385842 (* 1 = 0.0385842 loss)
I1026 00:53:21.355600 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0114084 (* 1 = 0.0114084 loss)
I1026 00:53:21.355607 17176 sgd_solver.cpp:106] Iteration 16880, lr = 0.001
I1026 00:53:21.922029 17176 solver.cpp:229] Iteration 16900, loss = 0.0420779
I1026 00:53:21.922063 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0307899 (* 1 = 0.0307899 loss)
I1026 00:53:21.922068 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.011288 (* 1 = 0.011288 loss)
I1026 00:53:21.922073 17176 sgd_solver.cpp:106] Iteration 16900, lr = 0.001
I1026 00:53:22.469439 17176 solver.cpp:229] Iteration 16920, loss = 0.0708655
I1026 00:53:22.469472 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0310184 (* 1 = 0.0310184 loss)
I1026 00:53:22.469477 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0398472 (* 1 = 0.0398472 loss)
I1026 00:53:22.469481 17176 sgd_solver.cpp:106] Iteration 16920, lr = 0.001
I1026 00:53:23.039860 17176 solver.cpp:229] Iteration 16940, loss = 0.13468
I1026 00:53:23.039894 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0897982 (* 1 = 0.0897982 loss)
I1026 00:53:23.039901 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0448818 (* 1 = 0.0448818 loss)
I1026 00:53:23.039904 17176 sgd_solver.cpp:106] Iteration 16940, lr = 0.001
I1026 00:53:23.616857 17176 solver.cpp:229] Iteration 16960, loss = 0.0498822
I1026 00:53:23.616901 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0239513 (* 1 = 0.0239513 loss)
I1026 00:53:23.616907 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0259309 (* 1 = 0.0259309 loss)
I1026 00:53:23.616911 17176 sgd_solver.cpp:106] Iteration 16960, lr = 0.001
I1026 00:53:24.183959 17176 solver.cpp:229] Iteration 16980, loss = 0.0835216
I1026 00:53:24.183993 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0759605 (* 1 = 0.0759605 loss)
I1026 00:53:24.183998 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00756114 (* 1 = 0.00756114 loss)
I1026 00:53:24.184002 17176 sgd_solver.cpp:106] Iteration 16980, lr = 0.001
34 299]]
500
[[  0   0 499 333]]
433
[[195 394 288 449]
 [165 394 194 430]
 [119 384 182 437]
 [ 44 429  93 475]
 [ 20 411  97 463]
 [  9 402  99 453]
 [  4 393  88 438]
 [ 62 376 106 396]
 [210 202 263 243]
 [276 259 335 431]
 [151 151 245 254]
 [335 250 386 396]
 [372 200 415 236]
 [318 202 388 265]]
500
[[197 245 260 371]
 [228 269 247 305]
 [191 208 244 268]
 [241 148 275 178]
 [298 156 335 296]
 [332 149 359 265]]
500
[[231 186 250 216]
 [ 57 218  80 241]
 [342  79 359 104]
 [381  69 403  80]
 [428  72 448  87]]
500
[[108  88 409 327]]
500
[[ 56  61 430 371]]
500
[[ 36 226 399 274]
 [139  88 392 151]]
500
[[124   1 443 338]]
500
[[290 166 416 287]]
500
[[  0   6 216 247]
 [263  57 335 194]
 [334  10 494 234]
 [469   1 498  40]]
390
[[  6   1 126 230]
 [145   1 247 229]
 [284   0 335 220]
 [ 21 248 124 496]
 [140 254 260 493]
 [272 246 335 485]
 [334   2 386 221]
 [335 262 386 486]]
500
[[ 60   0 335 315]
 [  0   0  62 374]
 [335   1 465 373]]
500
[[101 109 334 295]
 [320  91 497 248]]
500
[[345  96 374 131]]
500
[[ 30  57 266 138]
 [ 54 207 334 261]
 [332 177 446 403]]
500
[[ 14  38 334 450]
 [334  48 497 440]]
375
[[  0  35 334 497]
 [334  96 373 126]]
363
[[  4   1 334 493]
 [335  19 357 122]]
500
[[ 14  59 334 319]
 [334 192 348 242]
 [364 238 449 314]]
500
[[ 22   0 334 273]
 [320  96 457 358]]
500
[[  3  19 121 324]
 [121   7 263 328]
 [263   6 402 310]
 [401  37 499 310]]
374
[[103 109 299 310]
 [243  30 350 142]
 [284 185 313 264]]
500
[[  0   0 240 331]
 [260   0 313 101]
 [279   0 440 166]]
500
[[230 149 394 278]]
500
[[285  96 413 292]]
500
[[151 159 226 215]
 [284 145 477 259]
 [463 289 479 330]]
500
[[ 28   2 481 467]]
500
[[ 39  99 140 243]
 [364 107 469 233]]
500
[[ 91 177 334 291]
 [334 175 368 196]]

done
Preparing training data...
done
roidb len: 1424
Output will be saved to `/home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train`
Filtered 0 roidb entries: 1424 -> 1424
RoiDataLayer: name_to_top: {'gt_boxes': 2, 'data': 0, 'im_info': 1}
Loading pretrained model weights from /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_fast_rcnn_stage1_iter_20000.caffemodel
Solving...
speed: 0.029s / iter
speed: 0.029s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
Wrote snapshot to: /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_rpn_stage2_iter_10000.caffemodel
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / itI1026 00:53:24.744362 17176 solver.cpp:229] Iteration 17000, loss = 0.075958
I1026 00:53:24.744410 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0611793 (* 1 = 0.0611793 loss)
I1026 00:53:24.744426 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0147787 (* 1 = 0.0147787 loss)
I1026 00:53:24.744431 17176 sgd_solver.cpp:106] Iteration 17000, lr = 0.001
I1026 00:53:25.306190 17176 solver.cpp:229] Iteration 17020, loss = 0.161275
I1026 00:53:25.306232 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0653012 (* 1 = 0.0653012 loss)
I1026 00:53:25.306237 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0959738 (* 1 = 0.0959738 loss)
I1026 00:53:25.306241 17176 sgd_solver.cpp:106] Iteration 17020, lr = 0.001
I1026 00:53:25.858856 17176 solver.cpp:229] Iteration 17040, loss = 0.147414
I1026 00:53:25.858897 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0756084 (* 1 = 0.0756084 loss)
I1026 00:53:25.858902 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0718055 (* 1 = 0.0718055 loss)
I1026 00:53:25.858906 17176 sgd_solver.cpp:106] Iteration 17040, lr = 0.001
I1026 00:53:26.409186 17176 solver.cpp:229] Iteration 17060, loss = 0.0618477
I1026 00:53:26.409219 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0581813 (* 1 = 0.0581813 loss)
I1026 00:53:26.409224 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0036664 (* 1 = 0.0036664 loss)
I1026 00:53:26.409229 17176 sgd_solver.cpp:106] Iteration 17060, lr = 0.001
I1026 00:53:26.954468 17176 solver.cpp:229] Iteration 17080, loss = 0.138928
I1026 00:53:26.954499 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0982263 (* 1 = 0.0982263 loss)
I1026 00:53:26.954504 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0407019 (* 1 = 0.0407019 loss)
I1026 00:53:26.954507 17176 sgd_solver.cpp:106] Iteration 17080, lr = 0.001
I1026 00:53:27.519788 17176 solver.cpp:229] Iteration 17100, loss = 0.118116
I1026 00:53:27.519821 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0485388 (* 1 = 0.0485388 loss)
I1026 00:53:27.519825 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0695767 (* 1 = 0.0695767 loss)
I1026 00:53:27.519829 17176 sgd_solver.cpp:106] Iteration 17100, lr = 0.001
I1026 00:53:28.096407 17176 solver.cpp:229] Iteration 17120, loss = 0.0726397
I1026 00:53:28.096441 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0247201 (* 1 = 0.0247201 loss)
I1026 00:53:28.096446 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0479196 (* 1 = 0.0479196 loss)
I1026 00:53:28.096462 17176 sgd_solver.cpp:106] Iteration 17120, lr = 0.001
I1026 00:53:28.646545 17176 solver.cpp:229] Iteration 17140, loss = 0.0549696
I1026 00:53:28.646579 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0320932 (* 1 = 0.0320932 loss)
I1026 00:53:28.646582 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0228764 (* 1 = 0.0228764 loss)
I1026 00:53:28.646586 17176 sgd_solver.cpp:106] Iteration 17140, lr = 0.001
I1026 00:53:29.218955 17176 solver.cpp:229] Iteration 17160, loss = 0.103535
I1026 00:53:29.218991 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0854503 (* 1 = 0.0854503 loss)
I1026 00:53:29.218996 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0180845 (* 1 = 0.0180845 loss)
I1026 00:53:29.218999 17176 sgd_solver.cpp:106] Iteration 17160, lr = 0.001
I1026 00:53:29.780813 17176 solver.cpp:229] Iteration 17180, loss = 0.0330265
I1026 00:53:29.780845 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0238245 (* 1 = 0.0238245 loss)
I1026 00:53:29.780849 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00920202 (* 1 = 0.00920202 loss)
I1026 00:53:29.780853 17176 sgd_solver.cpp:106] Iteration 17180, lr = 0.001
I1026 00:53:30.336668 17176 solver.cpp:229] Iteration 17200, loss = 0.117355
I1026 00:53:30.336700 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0653706 (* 1 = 0.0653706 loss)
I1026 00:53:30.336705 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.051984 (* 1 = 0.051984 loss)
I1026 00:53:30.336709 17176 sgd_solver.cpp:106] Iteration 17200, lr = 0.001
I1026 00:53:30.904777 17176 solver.cpp:229] Iteration 17220, loss = 0.0222823
I1026 00:53:30.904808 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.016398 (* 1 = 0.016398 loss)
I1026 00:53:30.904813 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00588439 (* 1 = 0.00588439 loss)
I1026 00:53:30.904816 17176 sgd_solver.cpp:106] Iteration 17220, lr = 0.001
I1026 00:53:31.469020 17176 solver.cpp:229] Iteration 17240, loss = 0.16489
I1026 00:53:31.469076 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.124049 (* 1 = 0.124049 loss)
I1026 00:53:31.469091 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0408419 (* 1 = 0.0408419 loss)
I1026 00:53:31.469095 17176 sgd_solver.cpp:106] Iteration 17240, lr = 0.001
I1026 00:53:32.027415 17176 solver.cpp:229] Iteration 17260, loss = 0.0743403
I1026 00:53:32.027462 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0311288 (* 1 = 0.0311288 loss)
I1026 00:53:32.027467 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0432115 (* 1 = 0.0432115 loss)
I1026 00:53:32.027470 17176 sgd_solver.cpp:106] Iteration 17260, lr = 0.001
I1026 00:53:32.583159 17176 solver.cpp:229] Iteration 17280, loss = 0.489865
I1026 00:53:32.583201 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.10201 (* 1 = 0.10201 loss)
I1026 00:53:32.583207 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.387854 (* 1 = 0.387854 loss)
I1026 00:53:32.583210 17176 sgd_solver.cpp:106] Iteration 17280, lr = 0.001
I1026 00:53:33.151149 17176 solver.cpp:229] Iteration 17300, loss = 0.0965777
I1026 00:53:33.151181 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0594322 (* 1 = 0.0594322 loss)
I1026 00:53:33.151186 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0371455 (* 1 = 0.0371455 loss)
I1026 00:53:33.151190 17176 sgd_solver.cpp:106] Iteration 17300, lr = 0.001
I1026 00:53:33.706949 17176 solver.cpp:229] Iteration 17320, loss = 0.0504231
I1026 00:53:33.706992 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.047027 (* 1 = 0.047027 loss)
I1026 00:53:33.706997 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0033961 (* 1 = 0.0033961 loss)
I1026 00:53:33.707001 17176 sgd_solver.cpp:106] Iteration 17320, lr = 0.001
I1026 00:53:34.266618 17176 solver.cpp:229] Iteration 17340, loss = 0.0906766
I1026 00:53:34.266649 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0743958 (* 1 = 0.0743958 loss)
I1026 00:53:34.266670 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0162807 (* 1 = 0.0162807 loss)
I1026 00:53:34.266674 17176 sgd_solver.cpp:106] Iteration 17340, lr = 0.001
I1026 00:53:34.827093 17176 solver.cpp:229] Iteration 17360, loss = 0.389554
I1026 00:53:34.827126 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.182098 (* 1 = 0.182098 loss)
I1026 00:53:34.827131 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.207455 (* 1 = 0.207455 loss)
I1026 00:53:34.827136 17176 sgd_solver.cpp:106] Iteration 17360, lr = 0.001
I1026 00:53:35.375619 17176 solver.cpp:229] Iteration 17380, loss = 0.185136
I1026 00:53:35.375661 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.119957 (* 1 = 0.119957 loss)
I1026 00:53:35.375666 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0651785 (* 1 = 0.0651785 loss)
I1026 00:53:35.375670 17176 sgd_solver.cpp:106] Iteration 17380, lr = 0.001
I1026 00:53:35.931278 17176 solver.cpp:229] Iteration 17400, loss = 0.136142
I1026 00:53:35.931311 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.108183 (* 1 = 0.108183 loss)
I1026 00:53:35.931316 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0279591 (* 1 = 0.0279591 loss)
I1026 00:53:35.931321 17176 sgd_solver.cpp:106] Iteration 17400, lr = 0.001
I1026 00:53:36.490972 17176 solver.cpp:229] Iteration 17420, loss = 0.0773432
I1026 00:53:36.491003 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0532925 (* 1 = 0.0532925 loss)
I1026 00:53:36.491008 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0240508 (* 1 = 0.0240508 loss)
I1026 00:53:36.491013 17176 sgd_solver.cpp:106] Iteration 17420, lr = 0.001
I1026 00:53:37.051581 17176 solver.cpp:229] Iteration 17440, loss = 0.0722218
I1026 00:53:37.051612 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0412029 (* 1 = 0.0412029 loss)
I1026 00:53:37.051617 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0310189 (* 1 = 0.0310189 loss)
I1026 00:53:37.051621 17176 sgd_solver.cpp:106] Iteration 17440, lr = 0.001
I1026 00:53:37.600735 17176 solver.cpp:229] Iteration 17460, loss = 0.141489
I1026 00:53:37.600769 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0836032 (* 1 = 0.0836032 loss)
I1026 00:53:37.600772 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0578853 (* 1 = 0.0578853 loss)
I1026 00:53:37.600792 17176 sgd_solver.cpp:106] Iteration 17460, lr = 0.001
I1026 00:53:38.152617 17176 solver.cpp:229] Iteration 17480, loss = 0.0248918
I1026 00:53:38.152648 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0163573 (* 1 = 0.0163573 loss)
I1026 00:53:38.152652 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00853454 (* 1 = 0.00853454 loss)
I1026 00:53:38.152657 17176 sgd_solver.cpp:106] Iteration 17480, lr = 0.001
I1026 00:53:38.724210 17176 solver.cpp:229] Iteration 17500, loss = 0.0631271
I1026 00:53:38.724241 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0490519 (* 1 = 0.0490519 loss)
I1026 00:53:38.724244 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0140752 (* 1 = 0.0140752 loss)
I1026 00:53:38.724249 17176 sgd_solver.cpp:106] Iteration 17500, lr = 0.001
I1026 00:53:39.278393 17176 solver.cpp:229] Iteration 17520, loss = 0.142361
I1026 00:53:39.278424 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.120719 (* 1 = 0.120719 loss)
I1026 00:53:39.278430 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0216421 (* 1 = 0.0216421 loss)
I1026 00:53:39.278434 17176 sgd_solver.cpp:106] Iteration 17520, lr = 0.001
I1026 00:53:39.841260 17176 solver.cpp:229] Iteration 17540, loss = 0.0693383
I1026 00:53:39.841294 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0457401 (* 1 = 0.0457401 loss)
I1026 00:53:39.841297 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0235983 (* 1 = 0.0235983 loss)
I1026 00:53:39.841302 17176 sgd_solver.cpp:106] Iteration 17540, lr = 0.001
I1026 00:53:40.404649 17176 solver.cpp:229] Iteration 17560, loss = 0.486974
I1026 00:53:40.404680 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0960422 (* 1 = 0.0960422 loss)
I1026 00:53:40.404685 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.390932 (* 1 = 0.390932 loss)
I1026 00:53:40.404690 17176 sgd_solver.cpp:106] Iteration 17560, lr = 0.001
I1026 00:53:40.979766 17176 solver.cpp:229] Iteration 17580, loss = 0.123853
I1026 00:53:40.979799 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.106565 (* 1 = 0.106565 loss)
I1026 00:53:40.979804 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0172888 (* 1 = 0.0172888 loss)
I1026 00:53:40.979809 17176 sgd_solver.cpp:106] Iteration 17580, lr = 0.001
I1026 00:53:41.531321 17176 solver.cpp:229] Iteration 17600, loss = 0.0620459
I1026 00:53:41.531353 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00543972 (* 1 = 0.00543972 loss)
I1026 00:53:41.531358 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0566061 (* 1 = 0.0566061 loss)
I1026 00:53:41.531363 17176 sgd_solver.cpp:106] Iteration 17600, lr = 0.001
I1026 00:53:42.094842 17176 solver.cpp:229] Iteration 17620, loss = 0.154833
I1026 00:53:42.094876 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.13132 (* 1 = 0.13132 loss)
I1026 00:53:42.094880 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0235122 (* 1 = 0.0235122 loss)
I1026 00:53:42.094884 17176 sgd_solver.cpp:106] Iteration 17620, lr = 0.001
I1026 00:53:42.649567 17176 solver.cpp:229] Iteration 17640, loss = 0.0320196
I1026 00:53:42.649600 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0272914 (* 1 = 0.0272914 loss)
I1026 00:53:42.649605 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00472817 (* 1 = 0.00472817 loss)
I1026 00:53:42.649608 17176 sgd_solver.cpp:106] Iteration 17640, lr = 0.001
I1026 00:53:43.194326 17176 solver.cpp:229] Iteration 17660, loss = 0.0864176
I1026 00:53:43.194362 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.078889 (* 1 = 0.078889 loss)
I1026 00:53:43.194367 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00752868 (* 1 = 0.00752868 loss)
I1026 00:53:43.194371 17176 sgd_solver.cpp:106] Iteration 17660, lr = 0.001
I1026 00:53:43.757045 17176 solver.cpp:229] Iteration 17680, loss = 0.271145
I1026 00:53:43.757076 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.146367 (* 1 = 0.146367 loss)
I1026 00:53:43.757081 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.124778 (* 1 = 0.124778 loss)
I1026 00:53:43.757086 17176 sgd_solver.cpp:106] Iteration 17680, lr = 0.001
I1026 00:53:44.308955 17176 solver.cpp:229] Iteration 17700, loss = 0.100961
I1026 00:53:44.308998 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0861767 (* 1 = 0.0861767 loss)
I1026 00:53:44.309003 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0147841 (* 1 = 0.0147841 loss)
I1026 00:53:44.309007 17176 sgd_solver.cpp:106] Iteration 17700, lr = 0.001
I1026 00:53:44.866376 17176 solver.cpp:229] Iteration 17720, loss = 0.144767
I1026 00:53:44.866410 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.113182 (* 1 = 0.113182 loss)
I1026 00:53:44.866420 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0315848 (* 1 = 0.0315848 loss)
I1026 00:53:44.866425 17176 sgd_solver.cpp:106] Iteration 17720, lr = 0.001
I1026 00:53:45.418071 17176 solver.cpp:229] Iteration 17740, loss = 0.0607387
I1026 00:53:45.418103 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0236068 (* 1 = 0.0236068 loss)
I1026 00:53:45.418108 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0371318 (* 1 = 0.0371318 loss)
I1026 00:53:45.418112 17176 sgd_solver.cpp:106] Iteration 17740, lr = 0.001
I1026 00:53:45.983132 17176 solver.cpp:229] Iteration 17760, loss = 0.115622
I1026 00:53:45.983175 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0512261 (* 1 = 0.0512261 loss)
I1026 00:53:45.983178 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0643963 (* 1 = 0.0643963 loss)
I1026 00:53:45.983182 17176 sgd_solver.cpp:106] Iteration 17760, lr = 0.001
I1026 00:53:46.540278 17176 solver.cpp:229] Iteration 17780, loss = 0.321337
I1026 00:53:46.540310 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.109442 (* 1 = 0.109442 loss)
I1026 00:53:46.540314 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.211895 (* 1 = 0.211895 loss)
I1026 00:53:46.540318 17176 sgd_solver.cpp:106] Iteration 17780, lr = 0.001
I1026 00:53:47.095965 17176 solver.cpp:229] Iteration 17800, loss = 0.074698
I1026 00:53:47.095999 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0575571 (* 1 = 0.0575571 loss)
I1026 00:53:47.096002 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0171409 (* 1 = 0.0171409 loss)
I1026 00:53:47.096007 17176 sgd_solver.cpp:106] Iteration 17800, lr = 0.001
I1026 00:53:47.650550 17176 solver.cpp:229] Iteration 17820, loss = 0.0499609
I1026 00:53:47.650583 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0427748 (* 1 = 0.0427748 loss)
I1026 00:53:47.650586 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00718609 (* 1 = 0.00718609 loss)
I1026 00:53:47.650593 17176 sgd_solver.cpp:106] Iteration 17820, lr = 0.001
I1026 00:53:48.222148 17176 solver.cpp:229] Iteration 17840, loss = 0.0771859
I1026 00:53:48.222180 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0414136 (* 1 = 0.0414136 loss)
I1026 00:53:48.222184 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0357723 (* 1 = 0.0357723 loss)
I1026 00:53:48.222188 17176 sgd_solver.cpp:106] Iteration 17840, lr = 0.001
I1026 00:53:48.788311 17176 solver.cpp:229] Iteration 17860, loss = 0.0865788
I1026 00:53:48.788343 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0684447 (* 1 = 0.0684447 loss)
I1026 00:53:48.788348 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0181341 (* 1 = 0.0181341 loss)
I1026 00:53:48.788352 17176 sgd_solver.cpp:106] Iteration 17860, lr = 0.001
I1026 00:53:49.357520 17176 solver.cpp:229] Iteration 17880, loss = 0.14511
I1026 00:53:49.357553 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0678055 (* 1 = 0.0678055 loss)
I1026 00:53:49.357556 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0773045 (* 1 = 0.0773045 loss)
I1026 00:53:49.357560 17176 sgd_solver.cpp:106] Iteration 17880, lr = 0.001
I1026 00:53:49.921407 17176 solver.cpp:229] Iteration 17900, loss = 0.0246143
I1026 00:53:49.921439 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0221892 (* 1 = 0.0221892 loss)
I1026 00:53:49.921444 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00242506 (* 1 = 0.00242506 loss)
I1026 00:53:49.921447 17176 sgd_solver.cpp:106] Iteration 17900, lr = 0.001
I1026 00:53:50.479960 17176 solver.cpp:229] Iteration 17920, loss = 0.0694054
I1026 00:53:50.479992 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0450744 (* 1 = 0.0450744 loss)
I1026 00:53:50.479997 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.024331 (* 1 = 0.024331 loss)
I1026 00:53:50.480001 17176 sgd_solver.cpp:106] Iteration 17920, lr = 0.001
I1026 00:53:51.048758 17176 solver.cpp:229] Iteration 17940, loss = 0.0439575
I1026 00:53:51.048791 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0369497 (* 1 = 0.0369497 loss)
I1026 00:53:51.048796 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00700782 (* 1 = 0.00700782 loss)
I1026 00:53:51.048810 17176 sgd_solver.cpp:106] Iteration 17940, lr = 0.001
I1026 00:53:51.607518 17176 solver.cpp:229] Iteration 17960, loss = 0.216942
I1026 00:53:51.607550 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.191391 (* 1 = 0.191391 loss)
I1026 00:53:51.607555 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0255505 (* 1 = 0.0255505 loss)
I1026 00:53:51.607569 17176 sgd_solver.cpp:106] Iteration 17960, lr = 0.001
I1026 00:53:52.174866 17176 solver.cpp:229] Iteration 17980, loss = 0.38824
I1026 00:53:52.174901 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.19839 (* 1 = 0.19839 loss)
I1026 00:53:52.174908 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.189851 (* 1 = 0.189851 loss)
I1026 00:53:52.174914 17176 sgd_solver.cpp:106] Iteration 17980, lr = 0.001
I1026 00:53:52.754719 17176 solver.cpp:229] Iteration 18000, loss = 0.0994438
I1026 00:53:52.754753 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0730458 (* 1 = 0.0730458 loss)
I1026 00:53:52.754761 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.026398 (* 1 = 0.026398 loss)
I1026 00:53:52.754767 17176 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I1026 00:53:53.318461 17176 solver.cpp:229] Iteration 18020, loss = 0.0483069
I1026 00:53:53.318495 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0304132 (* 1 = 0.0304132 loss)
I1026 00:53:53.318501 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0178937 (* 1 = 0.0178937 loss)
I1026 00:53:53.318508 17176 sgd_solver.cpp:106] Iteration 18020, lr = 0.001
I1026 00:53:53.886440 17176 solver.cpp:229] Iteration 18040, loss = 0.243698
I1026 00:53:53.886473 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.179152 (* 1 = 0.179152 loss)
I1026 00:53:53.886481 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0645465 (* 1 = 0.0645465 loss)
I1026 00:53:53.886487 17176 sgd_solver.cpp:106] Iteration 18040, lr = 0.001
I1026 00:53:54.450249 17176 solver.cpp:229] Iteration 18060, loss = 0.678418
I1026 00:53:54.450284 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.424327 (* 1 = 0.424327 loss)
I1026 00:53:54.450290 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.25409 (* 1 = 0.25409 loss)
I1026 00:53:54.450297 17176 sgd_solver.cpp:106] Iteration 18060, lr = 0.001
I1026 00:53:55.012014 17176 solver.cpp:229] Iteration 18080, loss = 0.516758
I1026 00:53:55.012058 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.137242 (* 1 = 0.137242 loss)
I1026 00:53:55.012073 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.379515 (* 1 = 0.379515 loss)
I1026 00:53:55.012079 17176 sgd_solver.cpp:106] Iteration 18080, lr = 0.001
I1026 00:53:55.569180 17176 solver.cpp:229] Iteration 18100, loss = 0.0865227
I1026 00:53:55.569212 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0794578 (* 1 = 0.0794578 loss)
I1026 00:53:55.569218 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00706492 (* 1 = 0.00706492 loss)
I1026 00:53:55.569222 17176 sgd_solver.cpp:106] Iteration 18100, lr = 0.001
I1026 00:53:56.128244 17176 solver.cpp:229] Iteration 18120, loss = 0.0653037
I1026 00:53:56.128278 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0469772 (* 1 = 0.0469772 loss)
I1026 00:53:56.128281 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0183265 (* 1 = 0.0183265 loss)
I1026 00:53:56.128285 17176 sgd_solver.cpp:106] Iteration 18120, lr = 0.001
I1026 00:53:56.685935 17176 solver.cpp:229] Iteration 18140, loss = 0.0610443
I1026 00:53:56.685966 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0127892 (* 1 = 0.0127892 loss)
I1026 00:53:56.685971 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0482551 (* 1 = 0.0482551 loss)
I1026 00:53:56.685976 17176 sgd_solver.cpp:106] Iteration 18140, lr = 0.001
I1026 00:53:57.257158 17176 solver.cpp:229] Iteration 18160, loss = 0.0914917
I1026 00:53:57.257190 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0832028 (* 1 = 0.0832028 loss)
I1026 00:53:57.257195 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0082889 (* 1 = 0.0082889 loss)
I1026 00:53:57.257200 17176 sgd_solver.cpp:106] Iteration 18160, lr = 0.001
I1026 00:53:57.827116 17176 solver.cpp:229] Iteration 18180, loss = 0.118347
I1026 00:53:57.827148 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0292988 (* 1 = 0.0292988 loss)
I1026 00:53:57.827153 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0890487 (* 1 = 0.0890487 loss)
I1026 00:53:57.827157 17176 sgd_solver.cpp:106] Iteration 18180, lr = 0.001
I1026 00:53:58.388182 17176 solver.cpp:229] Iteration 18200, loss = 0.229001
I1026 00:53:58.388227 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.139132 (* 1 = 0.139132 loss)
I1026 00:53:58.388233 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0898689 (* 1 = 0.0898689 loss)
I1026 00:53:58.388237 17176 sgd_solver.cpp:106] Iteration 18200, lr = 0.001
I1026 00:53:58.946646 17176 solver.cpp:229] Iteration 18220, loss = 0.195135
I1026 00:53:58.946679 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.127275 (* 1 = 0.127275 loss)
I1026 00:53:58.946684 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0678598 (* 1 = 0.0678598 loss)
I1026 00:53:58.946688 17176 sgd_solver.cpp:106] Iteration 18220, lr = 0.001
I1026 00:53:59.498154 17176 solver.cpp:229] Iteration 18240, loss = 0.0867371
I1026 00:53:59.498186 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0633608 (* 1 = 0.0633608 loss)
I1026 00:53:59.498191 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0233764 (* 1 = 0.0233764 loss)
I1026 00:53:59.498195 17176 sgd_solver.cpp:106] Iteration 18240, lr = 0.001
I1026 00:54:00.055863 17176 solver.cpp:229] Iteration 18260, loss = 0.0718628
I1026 00:54:00.055896 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0430378 (* 1 = 0.0430378 loss)
I1026 00:54:00.055902 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.028825 (* 1 = 0.028825 loss)
I1026 00:54:00.055905 17176 sgd_solver.cpp:106] Iteration 18260, lr = 0.001
I1026 00:54:00.622884 17176 solver.cpp:229] Iteration 18280, loss = 0.0759992
I1026 00:54:00.622915 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0683642 (* 1 = 0.0683642 loss)
I1026 00:54:00.622920 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.007635 (* 1 = 0.007635 loss)
I1026 00:54:00.622923 17176 sgd_solver.cpp:106] Iteration 18280, lr = 0.001
I1026 00:54:01.184082 17176 solver.cpp:229] Iteration 18300, loss = 0.0726216
I1026 00:54:01.184114 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.056101 (* 1 = 0.056101 loss)
I1026 00:54:01.184118 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0165206 (* 1 = 0.0165206 loss)
I1026 00:54:01.184123 17176 sgd_solver.cpp:106] Iteration 18300, lr = 0.001
I1026 00:54:01.746565 17176 solver.cpp:229] Iteration 18320, loss = 0.253517
I1026 00:54:01.746597 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.130755 (* 1 = 0.130755 loss)
I1026 00:54:01.746601 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.122762 (* 1 = 0.122762 loss)
I1026 00:54:01.746605 17176 sgd_solver.cpp:106] Iteration 18320, lr = 0.001
I1026 00:54:02.299420 17176 solver.cpp:229] Iteration 18340, loss = 0.184661
I1026 00:54:02.299461 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.122055 (* 1 = 0.122055 loss)
I1026 00:54:02.299466 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0626068 (* 1 = 0.0626068 loss)
I1026 00:54:02.299470 17176 sgd_solver.cpp:106] Iteration 18340, lr = 0.001
I1026 00:54:02.842063 17176 solver.cpp:229] Iteration 18360, loss = 0.617342
I1026 00:54:02.842097 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.291338 (* 1 = 0.291338 loss)
I1026 00:54:02.842102 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.326003 (* 1 = 0.326003 loss)
I1026 00:54:02.842104 17176 sgd_solver.cpp:106] Iteration 18360, lr = 0.001
I1026 00:54:03.396720 17176 solver.cpp:229] Iteration 18380, loss = 0.0373532
I1026 00:54:03.396751 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0331132 (* 1 = 0.0331132 loss)
I1026 00:54:03.396756 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00423998 (* 1 = 0.00423998 loss)
I1026 00:54:03.396760 17176 sgd_solver.cpp:106] Iteration 18380, lr = 0.001
I1026 00:54:03.973237 17176 solver.cpp:229] Iteration 18400, loss = 0.114368
I1026 00:54:03.973271 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0472339 (* 1 = 0.0472339 loss)
I1026 00:54:03.973276 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0671344 (* 1 = 0.0671344 loss)
I1026 00:54:03.973292 17176 sgd_solver.cpp:106] Iteration 18400, lr = 0.001
I1026 00:54:04.535679 17176 solver.cpp:229] Iteration 18420, loss = 0.0884093
I1026 00:54:04.535712 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0666124 (* 1 = 0.0666124 loss)
I1026 00:54:04.535715 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0217969 (* 1 = 0.0217969 loss)
I1026 00:54:04.535719 17176 sgd_solver.cpp:106] Iteration 18420, lr = 0.001
I1026 00:54:05.093777 17176 solver.cpp:229] Iteration 18440, loss = 0.102734
I1026 00:54:05.093811 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0777931 (* 1 = 0.0777931 loss)
I1026 00:54:05.093816 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0249412 (* 1 = 0.0249412 loss)
I1026 00:54:05.093821 17176 sgd_solver.cpp:106] Iteration 18440, lr = 0.001
I1026 00:54:05.662763 17176 solver.cpp:229] Iteration 18460, loss = 0.0755888
I1026 00:54:05.662796 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0693213 (* 1 = 0.0693213 loss)
I1026 00:54:05.662799 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0062675 (* 1 = 0.0062675 loss)
I1026 00:54:05.662804 17176 sgd_solver.cpp:106] Iteration 18460, lr = 0.001
I1026 00:54:06.236145 17176 solver.cpp:229] Iteration 18480, loss = 0.224696
I1026 00:54:06.236176 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.134691 (* 1 = 0.134691 loss)
I1026 00:54:06.236181 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0900055 (* 1 = 0.0900055 loss)
I1026 00:54:06.236184 17176 sgd_solver.cpp:106] Iteration 18480, lr = 0.001
I1026 00:54:06.799121 17176 solver.cpp:229] Iteration 18500, loss = 0.114188
I1026 00:54:06.799154 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0609923 (* 1 = 0.0609923 loss)
I1026 00:54:06.799159 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0531961 (* 1 = 0.0531961 loss)
I1026 00:54:06.799162 17176 sgd_solver.cpp:106] Iteration 18500, lr = 0.001
I1026 00:54:07.365520 17176 solver.cpp:229] Iteration 18520, loss = 0.0512872
I1026 00:54:07.365550 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0475069 (* 1 = 0.0475069 loss)
I1026 00:54:07.365556 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00378032 (* 1 = 0.00378032 loss)
I1026 00:54:07.365559 17176 sgd_solver.cpp:106] Iteration 18520, lr = 0.001
I1026 00:54:07.926576 17176 solver.cpp:229] Iteration 18540, loss = 0.0834235
I1026 00:54:07.926609 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0514619 (* 1 = 0.0514619 loss)
I1026 00:54:07.926614 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0319616 (* 1 = 0.0319616 loss)
I1026 00:54:07.926617 17176 sgd_solver.cpp:106] Iteration 18540, lr = 0.001
I1026 00:54:08.484207 17176 solver.cpp:229] Iteration 18560, loss = 0.0652169
I1026 00:54:08.484239 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0456467 (* 1 = 0.0456467 loss)
I1026 00:54:08.484244 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0195703 (* 1 = 0.0195703 loss)
I1026 00:54:08.484248 17176 sgd_solver.cpp:106] Iteration 18560, lr = 0.001
I1026 00:54:09.045120 17176 solver.cpp:229] Iteration 18580, loss = 0.110902
I1026 00:54:09.045153 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0350557 (* 1 = 0.0350557 loss)
I1026 00:54:09.045157 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0758461 (* 1 = 0.0758461 loss)
I1026 00:54:09.045161 17176 sgd_solver.cpp:106] Iteration 18580, lr = 0.001
I1026 00:54:09.618271 17176 solver.cpp:229] Iteration 18600, loss = 0.0675196
I1026 00:54:09.618304 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0567906 (* 1 = 0.0567906 loss)
I1026 00:54:09.618309 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.010729 (* 1 = 0.010729 loss)
I1026 00:54:09.618312 17176 sgd_solver.cpp:106] Iteration 18600, lr = 0.001
I1026 00:54:10.186513 17176 solver.cpp:229] Iteration 18620, loss = 0.0922519
I1026 00:54:10.186545 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0732706 (* 1 = 0.0732706 loss)
I1026 00:54:10.186550 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0189813 (* 1 = 0.0189813 loss)
I1026 00:54:10.186564 17176 sgd_solver.cpp:106] Iteration 18620, lr = 0.001
I1026 00:54:10.738370 17176 solver.cpp:229] Iteration 18640, loss = 0.0701772
I1026 00:54:10.738402 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0191429 (* 1 = 0.0191429 loss)
I1026 00:54:10.738407 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0510344 (* 1 = 0.0510344 loss)
I1026 00:54:10.738412 17176 sgd_solver.cpp:106] Iteration 18640, lr = 0.001
I1026 00:54:11.307142 17176 solver.cpp:229] Iteration 18660, loss = 0.0628568
I1026 00:54:11.307175 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0466861 (* 1 = 0.0466861 loss)
I1026 00:54:11.307179 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0161707 (* 1 = 0.0161707 loss)
I1026 00:54:11.307183 17176 sgd_solver.cpp:106] Iteration 18660, lr = 0.001
I1026 00:54:11.873822 17176 solver.cpp:229] Iteration 18680, loss = 0.0941089
I1026 00:54:11.873855 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0261264 (* 1 = 0.0261264 loss)
I1026 00:54:11.873859 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0679824 (* 1 = 0.0679824 loss)
I1026 00:54:11.873873 17176 sgd_solver.cpp:106] Iteration 18680, lr = 0.001
I1026 00:54:12.438849 17176 solver.cpp:229] Iteration 18700, loss = 0.881427
I1026 00:54:12.438891 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.295541 (* 1 = 0.295541 loss)
I1026 00:54:12.438896 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.585886 (* 1 = 0.585886 loss)
I1026 00:54:12.438900 17176 sgd_solver.cpp:106] Iteration 18700, lr = 0.001
I1026 00:54:12.996268 17176 solver.cpp:229] Iteration 18720, loss = 0.079422
I1026 00:54:12.996300 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.047683 (* 1 = 0.047683 loss)
I1026 00:54:12.996305 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.031739 (* 1 = 0.031739 loss)
I1026 00:54:12.996309 17176 sgd_solver.cpp:106] Iteration 18720, lr = 0.001
I1026 00:54:13.549803 17176 solver.cpp:229] Iteration 18740, loss = 0.0675578
I1026 00:54:13.549846 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.049793 (* 1 = 0.049793 loss)
I1026 00:54:13.549851 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0177648 (* 1 = 0.0177648 loss)
I1026 00:54:13.549855 17176 sgd_solver.cpp:106] Iteration 18740, lr = 0.001
I1026 00:54:14.112805 17176 solver.cpp:229] Iteration 18760, loss = 0.0500272
I1026 00:54:14.112836 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0257325 (* 1 = 0.0257325 loss)
I1026 00:54:14.112841 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0242947 (* 1 = 0.0242947 loss)
I1026 00:54:14.112846 17176 sgd_solver.cpp:106] Iteration 18760, lr = 0.001
I1026 00:54:14.669541 17176 solver.cpp:229] Iteration 18780, loss = 0.145216
I1026 00:54:14.669574 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0765597 (* 1 = 0.0765597 loss)
I1026 00:54:14.669577 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0686559 (* 1 = 0.0686559 loss)
I1026 00:54:14.669581 17176 sgd_solver.cpp:106] Iteration 18780, lr = 0.001
I1026 00:54:15.232934 17176 solver.cpp:229] Iteration 18800, loss = 0.0793188
I1026 00:54:15.232969 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0351821 (* 1 = 0.0351821 loss)
I1026 00:54:15.232975 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0441367 (* 1 = 0.0441367 loss)
I1026 00:54:15.232980 17176 sgd_solver.cpp:106] Iteration 18800, lr = 0.001
I1026 00:54:15.793242 17176 solver.cpp:229] Iteration 18820, loss = 0.0953236
I1026 00:54:15.793277 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0852745 (* 1 = 0.0852745 loss)
I1026 00:54:15.793282 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0100491 (* 1 = 0.0100491 loss)
I1026 00:54:15.793287 17176 sgd_solver.cpp:106] Iteration 18820, lr = 0.001
I1026 00:54:16.364991 17176 solver.cpp:229] Iteration 18840, loss = 0.0492342
I1026 00:54:16.365023 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0415511 (* 1 = 0.0415511 loss)
I1026 00:54:16.365030 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00768315 (* 1 = 0.00768315 loss)
I1026 00:54:16.365033 17176 sgd_solver.cpp:106] Iteration 18840, lr = 0.001
I1026 00:54:16.921284 17176 solver.cpp:229] Iteration 18860, loss = 0.0297394
I1026 00:54:16.921319 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0237624 (* 1 = 0.0237624 loss)
I1026 00:54:16.921324 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00597696 (* 1 = 0.00597696 loss)
I1026 00:54:16.921339 17176 sgd_solver.cpp:106] Iteration 18860, lr = 0.001
I1026 00:54:17.485641 17176 solver.cpp:229] Iteration 18880, loss = 0.239724
I1026 00:54:17.485676 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0982118 (* 1 = 0.0982118 loss)
I1026 00:54:17.485680 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.141512 (* 1 = 0.141512 loss)
I1026 00:54:17.485685 17176 sgd_solver.cpp:106] Iteration 18880, lr = 0.001
I1026 00:54:18.062371 17176 solver.cpp:229] Iteration 18900, loss = 0.34679
I1026 00:54:18.062404 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.167665 (* 1 = 0.167665 loss)
I1026 00:54:18.062409 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.179124 (* 1 = 0.179124 loss)
I1026 00:54:18.062414 17176 sgd_solver.cpp:106] Iteration 18900, lr = 0.001
I1026 00:54:18.638672 17176 solver.cpp:229] Iteration 18920, loss = 0.0737744
I1026 00:54:18.638705 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0573164 (* 1 = 0.0573164 loss)
I1026 00:54:18.638710 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.016458 (* 1 = 0.016458 loss)
I1026 00:54:18.638715 17176 sgd_solver.cpp:106] Iteration 18920, lr = 0.001
I1026 00:54:19.202613 17176 solver.cpp:229] Iteration 18940, loss = 0.0504414
I1026 00:54:19.202646 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0441114 (* 1 = 0.0441114 loss)
I1026 00:54:19.202652 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00633005 (* 1 = 0.00633005 loss)
I1026 00:54:19.202656 17176 sgd_solver.cpp:106] Iteration 18940, lr = 0.001
I1026 00:54:19.768030 17176 solver.cpp:229] Iteration 18960, loss = 0.025653
I1026 00:54:19.768064 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.017116 (* 1 = 0.017116 loss)
I1026 00:54:19.768069 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0085371 (* 1 = 0.0085371 loss)
I1026 00:54:19.768074 17176 sgd_solver.cpp:106] Iteration 18960, lr = 0.001
I1026 00:54:20.330283 17176 solver.cpp:229] Iteration 18980, loss = 0.0486223
I1026 00:54:20.330317 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0172497 (* 1 = 0.0172497 loss)
I1026 00:54:20.330320 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0313726 (* 1 = 0.0313726 loss)
I1026 00:54:20.330324 17176 sgd_solver.cpp:106] Iteration 18980, lr = 0.001
I1026 00:54:20.888130 17176 solver.cpp:229] Iteration 19000, loss = 0.0716043
I1026 00:54:20.888164 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0558274 (* 1 = 0.0558274 loss)
I1026 00:54:20.888169 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0157768 (* 1 = 0.0157768 loss)
I1026 00:54:20.888172 17176 sgd_solver.cpp:106] Iteration 19000, lr = 0.001
I1026 00:54:21.449340 17176 solver.cpp:229] Iteration 19020, loss = 0.285013
I1026 00:54:21.449373 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.142419 (* 1 = 0.142419 loss)
I1026 00:54:21.449378 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.142594 (* 1 = 0.142594 loss)
I1026 00:54:21.449383 17176 sgd_solver.cpp:106] Iteration 19020, lr = 0.001
I1026 00:54:21.997054 17176 solver.cpp:229] Iteration 19040, loss = 0.06165
I1026 00:54:21.997087 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0560734 (* 1 = 0.0560734 loss)
I1026 00:54:21.997090 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00557663 (* 1 = 0.00557663 loss)
I1026 00:54:21.997094 17176 sgd_solver.cpp:106] Iteration 19040, lr = 0.001
I1026 00:54:22.563995 17176 solver.cpp:229] Iteration 19060, loss = 0.134491
I1026 00:54:22.564028 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0702163 (* 1 = 0.0702163 loss)
I1026 00:54:22.564033 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0642747 (* 1 = 0.0642747 loss)
I1026 00:54:22.564038 17176 sgd_solver.cpp:106] Iteration 19060, lr = 0.001
I1026 00:54:23.125368 17176 solver.cpp:229] Iteration 19080, loss = 0.0374361
I1026 00:54:23.125401 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0318271 (* 1 = 0.0318271 loss)
I1026 00:54:23.125406 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00560902 (* 1 = 0.00560902 loss)
I1026 00:54:23.125408 17176 sgd_solver.cpp:106] Iteration 19080, lr = 0.001
I1026 00:54:23.670040 17176 solver.cpp:229] Iteration 19100, loss = 0.06417
I1026 00:54:23.670073 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0599287 (* 1 = 0.0599287 loss)
I1026 00:54:23.670078 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00424125 (* 1 = 0.00424125 loss)
I1026 00:54:23.670081 17176 sgd_solver.cpp:106] Iteration 19100, lr = 0.001
I1026 00:54:24.225098 17176 solver.cpp:229] Iteration 19120, loss = 0.136546
I1026 00:54:24.225131 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0467905 (* 1 = 0.0467905 loss)
I1026 00:54:24.225134 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0897557 (* 1 = 0.0897557 loss)
I1026 00:54:24.225138 17176 sgd_solver.cpp:106] Iteration 19120, lr = 0.001
I1026 00:54:24.792222 17176 solver.cpp:229] Iteration 19140, loss = 0.3367
I1026 00:54:24.792265 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.1103 (* 1 = 0.1103 loss)
I1026 00:54:24.792269 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.2264 (* 1 = 0.2264 loss)
I1026 00:54:24.792273 17176 sgd_solver.cpp:106] Iteration 19140, lr = 0.001
I1026 00:54:25.362781 17176 solver.cpp:229] Iteration 19160, loss = 0.134272
I1026 00:54:25.362814 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0744999 (* 1 = 0.0744999 loss)
I1026 00:54:25.362819 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0597723 (* 1 = 0.0597723 loss)
I1026 00:54:25.362823 17176 sgd_solver.cpp:106] Iteration 19160, lr = 0.001
I1026 00:54:25.914453 17176 solver.cpp:229] Iteration 19180, loss = 0.110707
I1026 00:54:25.914486 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0177951 (* 1 = 0.0177951 loss)
I1026 00:54:25.914491 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0929114 (* 1 = 0.0929114 loss)
I1026 00:54:25.914496 17176 sgd_solver.cpp:106] Iteration 19180, lr = 0.001
I1026 00:54:26.477648 17176 solver.cpp:229] Iteration 19200, loss = 0.178388
I1026 00:54:26.477682 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0307266 (* 1 = 0.0307266 loss)
I1026 00:54:26.477691 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.147662 (* 1 = 0.147662 loss)
I1026 00:54:26.477696 17176 sgd_solver.cpp:106] Iteration 19200, lr = 0.001
I1026 00:54:27.042078 17176 solver.cpp:229] Iteration 19220, loss = 0.161555
I1026 00:54:27.042111 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.140439 (* 1 = 0.140439 loss)
I1026 00:54:27.042119 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0211154 (* 1 = 0.0211154 loss)
I1026 00:54:27.042125 17176 sgd_solver.cpp:106] Iteration 19220, lr = 0.001
I1026 00:54:27.605265 17176 solver.cpp:229] Iteration 19240, loss = 0.128756
I1026 00:54:27.605301 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.118632 (* 1 = 0.118632 loss)
I1026 00:54:27.605309 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0101237 (* 1 = 0.0101237 loss)
I1026 00:54:27.605315 17176 sgd_solver.cpp:106] Iteration 19240, lr = 0.001
I1026 00:54:28.156944 17176 solver.cpp:229] Iteration 19260, loss = 0.0779988
I1026 00:54:28.156978 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0497391 (* 1 = 0.0497391 loss)
I1026 00:54:28.156986 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0282596 (* 1 = 0.0282596 loss)
I1026 00:54:28.156992 17176 sgd_solver.cpp:106] Iteration 19260, lr = 0.001
I1026 00:54:28.727337 17176 solver.cpp:229] Iteration 19280, loss = 0.0569426
I1026 00:54:28.727371 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0428184 (* 1 = 0.0428184 loss)
I1026 00:54:28.727377 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0141243 (* 1 = 0.0141243 loss)
I1026 00:54:28.727381 17176 sgd_solver.cpp:106] Iteration 19280, lr = 0.001
I1026 00:54:29.283047 17176 solver.cpp:229] Iteration 19300, loss = 0.118609
I1026 00:54:29.283082 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0995918 (* 1 = 0.0995918 loss)
I1026 00:54:29.283087 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0190171 (* 1 = 0.0190171 loss)
I1026 00:54:29.283092 17176 sgd_solver.cpp:106] Iteration 19300, lr = 0.001
I1026 00:54:29.855701 17176 solver.cpp:229] Iteration 19320, loss = 0.0732491
I1026 00:54:29.855736 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0655329 (* 1 = 0.0655329 loss)
I1026 00:54:29.855741 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00771621 (* 1 = 0.00771621 loss)
I1026 00:54:29.855746 17176 sgd_solver.cpp:106] Iteration 19320, lr = 0.001
I1026 00:54:30.420212 17176 solver.cpp:229] Iteration 19340, loss = 0.0360027
I1026 00:54:30.420244 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0233209 (* 1 = 0.0233209 loss)
I1026 00:54:30.420249 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0126817 (* 1 = 0.0126817 loss)
I1026 00:54:30.420253 17176 sgd_solver.cpp:106] Iteration 19340, lr = 0.001
I1026 00:54:30.973431 17176 solver.cpp:229] Iteration 19360, loss = 0.105162
I1026 00:54:30.973464 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0709691 (* 1 = 0.0709691 loss)
I1026 00:54:30.973469 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0341926 (* 1 = 0.0341926 loss)
I1026 00:54:30.973474 17176 sgd_solver.cpp:106] Iteration 19360, lr = 0.001
I1026 00:54:31.510125 17176 solver.cpp:229] Iteration 19380, loss = 0.267326
I1026 00:54:31.510169 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.16509 (* 1 = 0.16509 loss)
I1026 00:54:31.510174 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.102235 (* 1 = 0.102235 loss)
I1026 00:54:31.510179 17176 sgd_solver.cpp:106] Iteration 19380, lr = 0.001
I1026 00:54:32.073437 17176 solver.cpp:229] Iteration 19400, loss = 0.736181
I1026 00:54:32.073470 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.352117 (* 1 = 0.352117 loss)
I1026 00:54:32.073475 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.384064 (* 1 = 0.384064 loss)
I1026 00:54:32.073480 17176 sgd_solver.cpp:106] Iteration 19400, lr = 0.001
I1026 00:54:32.633704 17176 solver.cpp:229] Iteration 19420, loss = 0.156869
I1026 00:54:32.633735 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0803406 (* 1 = 0.0803406 loss)
I1026 00:54:32.633739 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0765286 (* 1 = 0.0765286 loss)
I1026 00:54:32.633744 17176 sgd_solver.cpp:106] Iteration 19420, lr = 0.001
I1026 00:54:33.190155 17176 solver.cpp:229] Iteration 19440, loss = 0.0745825
I1026 00:54:33.190188 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0695873 (* 1 = 0.0695873 loss)
I1026 00:54:33.190192 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00499511 (* 1 = 0.00499511 loss)
I1026 00:54:33.190196 17176 sgd_solver.cpp:106] Iteration 19440, lr = 0.001
I1026 00:54:33.754844 17176 solver.cpp:229] Iteration 19460, loss = 0.0899207
I1026 00:54:33.754878 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0548086 (* 1 = 0.0548086 loss)
I1026 00:54:33.754881 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0351122 (* 1 = 0.0351122 loss)
I1026 00:54:33.754885 17176 sgd_solver.cpp:106] Iteration 19460, lr = 0.001
I1026 00:54:34.316587 17176 solver.cpp:229] Iteration 19480, loss = 0.360181
I1026 00:54:34.316632 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.124054 (* 1 = 0.124054 loss)
I1026 00:54:34.316645 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.236127 (* 1 = 0.236127 loss)
I1026 00:54:34.316649 17176 sgd_solver.cpp:106] Iteration 19480, lr = 0.001
I1026 00:54:34.876643 17176 solver.cpp:229] Iteration 19500, loss = 0.0656768
I1026 00:54:34.876677 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.054468 (* 1 = 0.054468 loss)
I1026 00:54:34.876684 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0112088 (* 1 = 0.0112088 loss)
I1026 00:54:34.876690 17176 sgd_solver.cpp:106] Iteration 19500, lr = 0.001
I1026 00:54:35.433282 17176 solver.cpp:229] Iteration 19520, loss = 0.0754231
I1026 00:54:35.433315 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0453933 (* 1 = 0.0453933 loss)
I1026 00:54:35.433318 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0300298 (* 1 = 0.0300298 loss)
I1026 00:54:35.433322 17176 sgd_solver.cpp:106] Iteration 19520, lr = 0.001
I1026 00:54:35.992059 17176 solver.cpp:229] Iteration 19540, loss = 0.0657023
I1026 00:54:35.992091 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0347005 (* 1 = 0.0347005 loss)
I1026 00:54:35.992096 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0310017 (* 1 = 0.0310017 loss)
I1026 00:54:35.992100 17176 sgd_solver.cpp:106] Iteration 19540, lr = 0.001
I1026 00:54:36.547749 17176 solver.cpp:229] Iteration 19560, loss = 0.126084
I1026 00:54:36.547781 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.111035 (* 1 = 0.111035 loss)
I1026 00:54:36.547785 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0150482 (* 1 = 0.0150482 loss)
I1026 00:54:36.547791 17176 sgd_solver.cpp:106] Iteration 19560, lr = 0.001
I1026 00:54:37.113626 17176 solver.cpp:229] Iteration 19580, loss = 0.12928
I1026 00:54:37.113662 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0734103 (* 1 = 0.0734103 loss)
I1026 00:54:37.113669 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0558699 (* 1 = 0.0558699 loss)
I1026 00:54:37.113675 17176 sgd_solver.cpp:106] Iteration 19580, lr = 0.001
I1026 00:54:37.666632 17176 solver.cpp:229] Iteration 19600, loss = 0.089492
I1026 00:54:37.666668 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0527953 (* 1 = 0.0527953 loss)
I1026 00:54:37.666676 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0366967 (* 1 = 0.0366967 loss)
I1026 00:54:37.666692 17176 sgd_solver.cpp:106] Iteration 19600, lr = 0.001
I1026 00:54:38.223704 17176 solver.cpp:229] Iteration 19620, loss = 0.165009
I1026 00:54:38.223739 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.116409 (* 1 = 0.116409 loss)
I1026 00:54:38.223747 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0485999 (* 1 = 0.0485999 loss)
I1026 00:54:38.223753 17176 sgd_solver.cpp:106] Iteration 19620, lr = 0.001
I1026 00:54:38.798346 17176 solver.cpp:229] Iteration 19640, loss = 0.124291
I1026 00:54:38.798380 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0748754 (* 1 = 0.0748754 loss)
I1026 00:54:38.798388 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0494152 (* 1 = 0.0494152 loss)
I1026 00:54:38.798394 17176 sgd_solver.cpp:106] Iteration 19640, lr = 0.001
I1026 00:54:39.377109 17176 solver.cpp:229] Iteration 19660, loss = 0.025533
I1026 00:54:39.377143 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0199288 (* 1 = 0.0199288 loss)
I1026 00:54:39.377151 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00560417 (* 1 = 0.00560417 loss)
I1026 00:54:39.377157 17176 sgd_solver.cpp:106] Iteration 19660, lr = 0.001
I1026 00:54:39.944489 17176 solver.cpp:229] Iteration 19680, loss = 0.0256984
I1026 00:54:39.944521 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0231258 (* 1 = 0.0231258 loss)
I1026 00:54:39.944527 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00257263 (* 1 = 0.00257263 loss)
I1026 00:54:39.944531 17176 sgd_solver.cpp:106] Iteration 19680, lr = 0.001
I1026 00:54:40.497604 17176 solver.cpp:229] Iteration 19700, loss = 0.130658
I1026 00:54:40.497638 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0444078 (* 1 = 0.0444078 loss)
I1026 00:54:40.497644 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0862499 (* 1 = 0.0862499 loss)
I1026 00:54:40.497649 17176 sgd_solver.cpp:106] Iteration 19700, lr = 0.001
I1026 00:54:41.061005 17176 solver.cpp:229] Iteration 19720, loss = 0.140362
I1026 00:54:41.061038 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.120815 (* 1 = 0.120815 loss)
I1026 00:54:41.061044 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0195465 (* 1 = 0.0195465 loss)
I1026 00:54:41.061049 17176 sgd_solver.cpp:106] Iteration 19720, lr = 0.001
I1026 00:54:41.618302 17176 solver.cpp:229] Iteration 19740, loss = 0.280561
I1026 00:54:41.618336 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0976079 (* 1 = 0.0976079 loss)
I1026 00:54:41.618341 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.182953 (* 1 = 0.182953 loss)
I1026 00:54:41.618347 17176 sgd_solver.cpp:106] Iteration 19740, lr = 0.001
I1026 00:54:42.178275 17176 solver.cpp:229] Iteration 19760, loss = 0.113448
I1026 00:54:42.178310 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0970847 (* 1 = 0.0970847 loss)
I1026 00:54:42.178316 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0163631 (* 1 = 0.0163631 loss)
I1026 00:54:42.178330 17176 sgd_solver.cpp:106] Iteration 19760, lr = 0.001
I1026 00:54:42.736932 17176 solver.cpp:229] Iteration 19780, loss = 0.0981122
I1026 00:54:42.736966 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0535662 (* 1 = 0.0535662 loss)
I1026 00:54:42.736971 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0445461 (* 1 = 0.0445461 loss)
I1026 00:54:42.736975 17176 sgd_solver.cpp:106] Iteration 19780, lr = 0.001
I1026 00:54:43.316298 17176 solver.cpp:229] Iteration 19800, loss = 0.0676419
I1026 00:54:43.316330 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0472988 (* 1 = 0.0472988 loss)
I1026 00:54:43.316336 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0203431 (* 1 = 0.0203431 loss)
I1026 00:54:43.316340 17176 sgd_solver.cpp:106] Iteration 19800, lr = 0.001
I1026 00:54:43.883003 17176 solver.cpp:229] Iteration 19820, loss = 0.0267065
I1026 00:54:43.883036 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0156697 (* 1 = 0.0156697 loss)
I1026 00:54:43.883041 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0110368 (* 1 = 0.0110368 loss)
I1026 00:54:43.883045 17176 sgd_solver.cpp:106] Iteration 19820, lr = 0.001
I1026 00:54:44.440837 17176 solver.cpp:229] Iteration 19840, loss = 0.0494136
I1026 00:54:44.440871 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0347535 (* 1 = 0.0347535 loss)
I1026 00:54:44.440874 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0146601 (* 1 = 0.0146601 loss)
I1026 00:54:44.440878 17176 sgd_solver.cpp:106] Iteration 19840, lr = 0.001
I1026 00:54:44.995403 17176 solver.cpp:229] Iteration 19860, loss = 0.0961086
I1026 00:54:44.995440 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0540441 (* 1 = 0.0540441 loss)
I1026 00:54:44.995445 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0420645 (* 1 = 0.0420645 loss)
I1026 00:54:44.995448 17176 sgd_solver.cpp:106] Iteration 19860, lr = 0.001
I1026 00:54:45.556174 17176 solver.cpp:229] Iteration 19880, loss = 0.283342
I1026 00:54:45.556217 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.159986 (* 1 = 0.159986 loss)
I1026 00:54:45.556222 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.123356 (* 1 = 0.123356 loss)
I1026 00:54:45.556226 17176 sgd_solver.cpp:106] Iteration 19880, lr = 0.001
I1026 00:54:46.118010 17176 solver.cpp:229] Iteration 19900, loss = 0.876293
I1026 00:54:46.118042 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.334728 (* 1 = 0.334728 loss)
I1026 00:54:46.118047 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.541565 (* 1 = 0.541565 loss)
I1026 00:54:46.118052 17176 sgd_solver.cpp:106] Iteration 19900, lr = 0.001
I1026 00:54:46.672143 17176 solver.cpp:229] Iteration 19920, loss = 0.0862209
I1026 00:54:46.672176 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0705929 (* 1 = 0.0705929 loss)
I1026 00:54:46.672181 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.015628 (* 1 = 0.015628 loss)
I1026 00:54:46.672185 17176 sgd_solver.cpp:106] Iteration 19920, lr = 0.001
I1026 00:54:47.233032 17176 solver.cpp:229] Iteration 19940, loss = 0.123847
I1026 00:54:47.233064 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.100881 (* 1 = 0.100881 loss)
I1026 00:54:47.233069 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0229657 (* 1 = 0.0229657 loss)
I1026 00:54:47.233074 17176 sgd_solver.cpp:106] Iteration 19940, lr = 0.001
I1026 00:54:47.788106 17176 solver.cpp:229] Iteration 19960, loss = 0.06281
I1026 00:54:47.788141 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0390724 (* 1 = 0.0390724 loss)
I1026 00:54:47.788146 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0237376 (* 1 = 0.0237376 loss)
I1026 00:54:47.788151 17176 sgd_solver.cpp:106] Iteration 19960, lr = 0.001
I1026 00:54:48.340394 17176 solver.cpp:229] Iteration 19980, loss = 0.0444029
I1026 00:54:48.340435 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00897244 (* 1 = 0.00897244 loss)
I1026 00:54:48.340440 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0354304 (* 1 = 0.0354304 loss)
I1026 00:54:48.340443 17176 sgd_solver.cpp:106] Iteration 19980, lr = 0.001
I1026 00:54:49.342726 17176 solver.cpp:229] Iteration 20000, loss = 0.102395
I1026 00:54:49.342759 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0299003 (* 1 = 0.0299003 loss)
I1026 00:54:49.342764 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0724945 (* 1 = 0.0724945 loss)
I1026 00:54:49.342768 17176 sgd_solver.cpp:106] Iteration 20000, lr = 0.001
I1026 00:54:49.894980 17176 solver.cpp:229] Iteration 20020, loss = 0.0864322
I1026 00:54:49.895015 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0387408 (* 1 = 0.0387408 loss)
I1026 00:54:49.895020 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0476914 (* 1 = 0.0476914 loss)
I1026 00:54:49.895023 17176 sgd_solver.cpp:106] Iteration 20020, lr = 0.001
I1026 00:54:50.443970 17176 solver.cpp:229] Iteration 20040, loss = 0.287632
I1026 00:54:50.444002 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.193475 (* 1 = 0.193475 loss)
I1026 00:54:50.444006 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0941571 (* 1 = 0.0941571 loss)
I1026 00:54:50.444010 17176 sgd_solver.cpp:106] Iteration 20040, lr = 0.001
I1026 00:54:51.015374 17176 solver.cpp:229] Iteration 20060, loss = 0.18107
I1026 00:54:51.015408 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.138933 (* 1 = 0.138933 loss)
I1026 00:54:51.015413 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0421372 (* 1 = 0.0421372 loss)
I1026 00:54:51.015419 17176 sgd_solver.cpp:106] Iteration 20060, lr = 0.001
I1026 00:54:51.582955 17176 solver.cpp:229] Iteration 20080, loss = 0.181714
I1026 00:54:51.582988 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0945297 (* 1 = 0.0945297 loss)
I1026 00:54:51.582993 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0871842 (* 1 = 0.0871842 loss)
I1026 00:54:51.582998 17176 sgd_solver.cpp:106] Iteration 20080, lr = 0.001
I1026 00:54:52.137333 17176 solver.cpp:229] Iteration 20100, loss = 0.043848
I1026 00:54:52.137375 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.025196 (* 1 = 0.025196 loss)
I1026 00:54:52.137380 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.018652 (* 1 = 0.018652 loss)
I1026 00:54:52.137385 17176 sgd_solver.cpp:106] Iteration 20100, lr = 0.001
I1026 00:54:52.702755 17176 solver.cpp:229] Iteration 20120, loss = 0.0512472
I1026 00:54:52.702788 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0270741 (* 1 = 0.0270741 loss)
I1026 00:54:52.702793 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0241731 (* 1 = 0.0241731 loss)
I1026 00:54:52.702796 17176 sgd_solver.cpp:106] Iteration 20120, lr = 0.001
I1026 00:54:53.270911 17176 solver.cpp:229] Iteration 20140, loss = 0.268188
I1026 00:54:53.270943 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.123147 (* 1 = 0.123147 loss)
I1026 00:54:53.270948 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.14504 (* 1 = 0.14504 loss)
I1026 00:54:53.270952 17176 sgd_solver.cpp:106] Iteration 20140, lr = 0.001
I1026 00:54:53.832746 17176 solver.cpp:229] Iteration 20160, loss = 0.0695778
I1026 00:54:53.832777 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.040178 (* 1 = 0.040178 loss)
I1026 00:54:53.832780 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0293998 (* 1 = 0.0293998 loss)
I1026 00:54:53.832784 17176 sgd_solver.cpp:106] Iteration 20160, lr = 0.001
I1026 00:54:54.388067 17176 solver.cpp:229] Iteration 20180, loss = 0.0694946
I1026 00:54:54.388099 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0424254 (* 1 = 0.0424254 loss)
I1026 00:54:54.388104 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0270692 (* 1 = 0.0270692 loss)
I1026 00:54:54.388108 17176 sgd_solver.cpp:106] Iteration 20180, lr = 0.001
I1026 00:54:54.945618 17176 solver.cpp:229] Iteration 20200, loss = 0.0594358
I1026 00:54:54.945660 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0386189 (* 1 = 0.0386189 loss)
I1026 00:54:54.945665 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0208168 (* 1 = 0.0208168 loss)
I1026 00:54:54.945669 17176 sgd_solver.cpp:106] Iteration 20200, lr = 0.001
I1026 00:54:55.509724 17176 solver.cpp:229] Iteration 20220, loss = 0.0349105
I1026 00:54:55.509758 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.032778 (* 1 = 0.032778 loss)
I1026 00:54:55.509763 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00213252 (* 1 = 0.00213252 loss)
I1026 00:54:55.509776 17176 sgd_solver.cpp:106] Iteration 20220, lr = 0.001
I1026 00:54:56.069942 17176 solver.cpp:229] Iteration 20240, loss = 0.0474489
I1026 00:54:56.069973 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0181982 (* 1 = 0.0181982 loss)
I1026 00:54:56.069978 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0292507 (* 1 = 0.0292507 loss)
I1026 00:54:56.069983 17176 sgd_solver.cpp:106] Iteration 20240, lr = 0.001
I1026 00:54:56.623388 17176 solver.cpp:229] Iteration 20260, loss = 0.0397759
I1026 00:54:56.623420 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0383317 (* 1 = 0.0383317 loss)
I1026 00:54:56.623425 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00144415 (* 1 = 0.00144415 loss)
I1026 00:54:56.623428 17176 sgd_solver.cpp:106] Iteration 20260, lr = 0.001
I1026 00:54:57.182624 17176 solver.cpp:229] Iteration 20280, loss = 0.0369226
I1026 00:54:57.182658 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0162702 (* 1 = 0.0162702 loss)
I1026 00:54:57.182663 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0206524 (* 1 = 0.0206524 loss)
I1026 00:54:57.182667 17176 sgd_solver.cpp:106] Iteration 20280, lr = 0.001
I1026 00:54:57.734632 17176 solver.cpp:229] Iteration 20300, loss = 0.0744055
I1026 00:54:57.734664 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0632309 (* 1 = 0.0632309 loss)
I1026 00:54:57.734668 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0111746 (* 1 = 0.0111746 loss)
I1026 00:54:57.734673 17176 sgd_solver.cpp:106] Iteration 20300, lr = 0.001
I1026 00:54:58.289731 17176 solver.cpp:229] Iteration 20320, loss = 0.0731217
I1026 00:54:58.289763 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0621567 (* 1 = 0.0621567 loss)
I1026 00:54:58.289767 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.010965 (* 1 = 0.010965 loss)
I1026 00:54:58.289772 17176 sgd_solver.cpp:106] Iteration 20320, lr = 0.001
I1026 00:54:58.848983 17176 solver.cpp:229] Iteration 20340, loss = 0.0464135
I1026 00:54:58.849014 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0321308 (* 1 = 0.0321308 loss)
I1026 00:54:58.849020 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0142826 (* 1 = 0.0142826 loss)
I1026 00:54:58.849025 17176 sgd_solver.cpp:106] Iteration 20340, lr = 0.001
I1026 00:54:59.425655 17176 solver.cpp:229] Iteration 20360, loss = 0.0739924
I1026 00:54:59.425688 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0276216 (* 1 = 0.0276216 loss)
I1026 00:54:59.425693 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0463708 (* 1 = 0.0463708 loss)
I1026 00:54:59.425696 17176 sgd_solver.cpp:106] Iteration 20360, lr = 0.001
I1026 00:54:59.986963 17176 solver.cpp:229] Iteration 20380, loss = 0.0695501
I1026 00:54:59.986996 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.057725 (* 1 = 0.057725 loss)
I1026 00:54:59.987001 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0118251 (* 1 = 0.0118251 loss)
I1026 00:54:59.987005 17176 sgd_solver.cpp:106] Iteration 20380, lr = 0.001
I1026 00:55:00.544859 17176 solver.cpp:229] Iteration 20400, loss = 0.100115
I1026 00:55:00.544893 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0723148 (* 1 = 0.0723148 loss)
I1026 00:55:00.544898 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0278003 (* 1 = 0.0278003 loss)
I1026 00:55:00.544901 17176 sgd_solver.cpp:106] Iteration 20400, lr = 0.001
I1026 00:55:01.113382 17176 solver.cpp:229] Iteration 20420, loss = 0.0970795
I1026 00:55:01.113415 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0560087 (* 1 = 0.0560087 loss)
I1026 00:55:01.113420 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0410709 (* 1 = 0.0410709 loss)
I1026 00:55:01.113425 17176 sgd_solver.cpp:106] Iteration 20420, lr = 0.001
I1026 00:55:01.679548 17176 solver.cpp:229] Iteration 20440, loss = 0.254246
I1026 00:55:01.679589 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.103803 (* 1 = 0.103803 loss)
I1026 00:55:01.679594 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.150443 (* 1 = 0.150443 loss)
I1026 00:55:01.679599 17176 sgd_solver.cpp:106] Iteration 20440, lr = 0.001
I1026 00:55:02.240334 17176 solver.cpp:229] Iteration 20460, loss = 0.0211887
I1026 00:55:02.240366 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0186406 (* 1 = 0.0186406 loss)
I1026 00:55:02.240370 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0025481 (* 1 = 0.0025481 loss)
I1026 00:55:02.240375 17176 sgd_solver.cpp:106] Iteration 20460, lr = 0.001
I1026 00:55:02.803267 17176 solver.cpp:229] Iteration 20480, loss = 0.162677
I1026 00:55:02.803309 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.111446 (* 1 = 0.111446 loss)
I1026 00:55:02.803314 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0512317 (* 1 = 0.0512317 loss)
I1026 00:55:02.803318 17176 sgd_solver.cpp:106] Iteration 20480, lr = 0.001
I1026 00:55:03.363513 17176 solver.cpp:229] Iteration 20500, loss = 0.237958
I1026 00:55:03.363545 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.127182 (* 1 = 0.127182 loss)
I1026 00:55:03.363550 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.110776 (* 1 = 0.110776 loss)
I1026 00:55:03.363554 17176 sgd_solver.cpp:106] Iteration 20500, lr = 0.001
I1026 00:55:03.919939 17176 solver.cpp:229] Iteration 20520, loss = 0.112716
I1026 00:55:03.919970 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.069805 (* 1 = 0.069805 loss)
I1026 00:55:03.919975 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0429112 (* 1 = 0.0429112 loss)
I1026 00:55:03.919980 17176 sgd_solver.cpp:106] Iteration 20520, lr = 0.001
I1026 00:55:04.480114 17176 solver.cpp:229] Iteration 20540, loss = 0.075775
I1026 00:55:04.480145 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0570262 (* 1 = 0.0570262 loss)
I1026 00:55:04.480150 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0187488 (* 1 = 0.0187488 loss)
I1026 00:55:04.480154 17176 sgd_solver.cpp:106] Iteration 20540, lr = 0.001
I1026 00:55:05.030695 17176 solver.cpp:229] Iteration 20560, loss = 0.0328317
I1026 00:55:05.030740 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0284004 (* 1 = 0.0284004 loss)
I1026 00:55:05.030745 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00443136 (* 1 = 0.00443136 loss)
I1026 00:55:05.030748 17176 sgd_solver.cpp:106] Iteration 20560, lr = 0.001
I1026 00:55:05.595588 17176 solver.cpp:229] Iteration 20580, loss = 0.0191062
I1026 00:55:05.595630 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0179599 (* 1 = 0.0179599 loss)
I1026 00:55:05.595635 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00114633 (* 1 = 0.00114633 loss)
I1026 00:55:05.595639 17176 sgd_solver.cpp:106] Iteration 20580, lr = 0.001
I1026 00:55:06.168901 17176 solver.cpp:229] Iteration 20600, loss = 0.0778732
I1026 00:55:06.168933 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0566531 (* 1 = 0.0566531 loss)
I1026 00:55:06.168938 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0212201 (* 1 = 0.0212201 loss)
I1026 00:55:06.168942 17176 sgd_solver.cpp:106] Iteration 20600, lr = 0.001
I1026 00:55:06.739955 17176 solver.cpp:229] Iteration 20620, loss = 0.183329
I1026 00:55:06.739986 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.126632 (* 1 = 0.126632 loss)
I1026 00:55:06.739991 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0566974 (* 1 = 0.0566974 loss)
I1026 00:55:06.739995 17176 sgd_solver.cpp:106] Iteration 20620, lr = 0.001
I1026 00:55:07.296005 17176 solver.cpp:229] Iteration 20640, loss = 0.124038
I1026 00:55:07.296037 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.106422 (* 1 = 0.106422 loss)
I1026 00:55:07.296042 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0176168 (* 1 = 0.0176168 loss)
I1026 00:55:07.296046 17176 sgd_solver.cpp:106] Iteration 20640, lr = 0.001
I1026 00:55:07.845365 17176 solver.cpp:229] Iteration 20660, loss = 0.500763
I1026 00:55:07.845407 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.230348 (* 1 = 0.230348 loss)
I1026 00:55:07.845412 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.270415 (* 1 = 0.270415 loss)
I1026 00:55:07.845415 17176 sgd_solver.cpp:106] Iteration 20660, lr = 0.001
I1026 00:55:08.420863 17176 solver.cpp:229] Iteration 20680, loss = 0.257558
I1026 00:55:08.420895 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.170124 (* 1 = 0.170124 loss)
I1026 00:55:08.420899 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0874338 (* 1 = 0.0874338 loss)
I1026 00:55:08.420903 17176 sgd_solver.cpp:106] Iteration 20680, lr = 0.001
I1026 00:55:08.987921 17176 solver.cpp:229] Iteration 20700, loss = 0.103863
I1026 00:55:08.987952 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0258845 (* 1 = 0.0258845 loss)
I1026 00:55:08.987957 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0779783 (* 1 = 0.0779783 loss)
I1026 00:55:08.987962 17176 sgd_solver.cpp:106] Iteration 20700, lr = 0.001
I1026 00:55:09.541203 17176 solver.cpp:229] Iteration 20720, loss = 0.205223
I1026 00:55:09.541235 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.180245 (* 1 = 0.180245 loss)
I1026 00:55:09.541240 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.024978 (* 1 = 0.024978 loss)
I1026 00:55:09.541244 17176 sgd_solver.cpp:106] Iteration 20720, lr = 0.001
I1026 00:55:10.104157 17176 solver.cpp:229] Iteration 20740, loss = 0.0755239
I1026 00:55:10.104189 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0590077 (* 1 = 0.0590077 loss)
I1026 00:55:10.104194 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0165162 (* 1 = 0.0165162 loss)
I1026 00:55:10.104198 17176 sgd_solver.cpp:106] Iteration 20740, lr = 0.001
I1026 00:55:10.660357 17176 solver.cpp:229] Iteration 20760, loss = 0.0569787
I1026 00:55:10.660389 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0280773 (* 1 = 0.0280773 loss)
I1026 00:55:10.660393 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0289014 (* 1 = 0.0289014 loss)
I1026 00:55:10.660398 17176 sgd_solver.cpp:106] Iteration 20760, lr = 0.001
I1026 00:55:11.229423 17176 solver.cpp:229] Iteration 20780, loss = 0.215563
I1026 00:55:11.229455 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0498203 (* 1 = 0.0498203 loss)
I1026 00:55:11.229460 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.165742 (* 1 = 0.165742 loss)
I1026 00:55:11.229463 17176 sgd_solver.cpp:106] Iteration 20780, lr = 0.001
I1026 00:55:11.791564 17176 solver.cpp:229] Iteration 20800, loss = 0.167842
I1026 00:55:11.791597 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0419185 (* 1 = 0.0419185 loss)
I1026 00:55:11.791602 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.125924 (* 1 = 0.125924 loss)
I1026 00:55:11.791606 17176 sgd_solver.cpp:106] Iteration 20800, lr = 0.001
I1026 00:55:12.343219 17176 solver.cpp:229] Iteration 20820, loss = 0.0595601
I1026 00:55:12.343251 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0226315 (* 1 = 0.0226315 loss)
I1026 00:55:12.343255 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0369286 (* 1 = 0.0369286 loss)
I1026 00:55:12.343260 17176 sgd_solver.cpp:106] Iteration 20820, lr = 0.001
I1026 00:55:12.896343 17176 solver.cpp:229] Iteration 20840, loss = 0.12711
I1026 00:55:12.896376 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0951276 (* 1 = 0.0951276 loss)
I1026 00:55:12.896381 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.031982 (* 1 = 0.031982 loss)
I1026 00:55:12.896395 17176 sgd_solver.cpp:106] Iteration 20840, lr = 0.001
I1026 00:55:13.452632 17176 solver.cpp:229] Iteration 20860, loss = 0.354165
I1026 00:55:13.452664 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.12649 (* 1 = 0.12649 loss)
I1026 00:55:13.452669 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.227675 (* 1 = 0.227675 loss)
I1026 00:55:13.452673 17176 sgd_solver.cpp:106] Iteration 20860, lr = 0.001
I1026 00:55:14.019330 17176 solver.cpp:229] Iteration 20880, loss = 0.0529925
I1026 00:55:14.019363 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0287635 (* 1 = 0.0287635 loss)
I1026 00:55:14.019368 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0242291 (* 1 = 0.0242291 loss)
I1026 00:55:14.019372 17176 sgd_solver.cpp:106] Iteration 20880, lr = 0.001
I1026 00:55:14.588991 17176 solver.cpp:229] Iteration 20900, loss = 0.0588423
I1026 00:55:14.589023 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0424487 (* 1 = 0.0424487 loss)
I1026 00:55:14.589027 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0163936 (* 1 = 0.0163936 loss)
I1026 00:55:14.589031 17176 sgd_solver.cpp:106] Iteration 20900, lr = 0.001
I1026 00:55:15.141472 17176 solver.cpp:229] Iteration 20920, loss = 0.0211962
I1026 00:55:15.141507 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0174663 (* 1 = 0.0174663 loss)
I1026 00:55:15.141515 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00372984 (* 1 = 0.00372984 loss)
I1026 00:55:15.141530 17176 sgd_solver.cpp:106] Iteration 20920, lr = 0.001
I1026 00:55:15.696282 17176 solver.cpp:229] Iteration 20940, loss = 0.0633644
I1026 00:55:15.696317 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0328018 (* 1 = 0.0328018 loss)
I1026 00:55:15.696323 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0305625 (* 1 = 0.0305625 loss)
I1026 00:55:15.696328 17176 sgd_solver.cpp:106] Iteration 20940, lr = 0.001
I1026 00:55:16.259918 17176 solver.cpp:229] Iteration 20960, loss = 0.0555181
I1026 00:55:16.259953 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.043117 (* 1 = 0.043117 loss)
I1026 00:55:16.259958 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0124011 (* 1 = 0.0124011 loss)
I1026 00:55:16.259963 17176 sgd_solver.cpp:106] Iteration 20960, lr = 0.001
I1026 00:55:16.821072 17176 solver.cpp:229] Iteration 20980, loss = 0.041428
I1026 00:55:16.821105 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0360421 (* 1 = 0.0360421 loss)
I1026 00:55:16.821111 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00538594 (* 1 = 0.00538594 loss)
I1026 00:55:16.821116 17176 sgd_solver.cpp:106] Iteration 20980, lr = 0.001
I1026 00:55:17.365454 17176 solver.cpp:229] Iteration 21000, loss = 0.166945
I1026 00:55:17.365485 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0303206 (* 1 = 0.0303206 loss)
I1026 00:55:17.365489 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.136624 (* 1 = 0.136624 loss)
I1026 00:55:17.365494 17176 sgd_solver.cpp:106] Iteration 21000, lr = 0.001
I1026 00:55:17.924301 17176 solver.cpp:229] Iteration 21020, loss = 0.0883453
I1026 00:55:17.924335 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0678231 (* 1 = 0.0678231 loss)
I1026 00:55:17.924338 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0205222 (* 1 = 0.0205222 loss)
I1026 00:55:17.924342 17176 sgd_solver.cpp:106] Iteration 21020, lr = 0.001
I1026 00:55:18.475070 17176 solver.cpp:229] Iteration 21040, loss = 0.111914
I1026 00:55:18.475102 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0553627 (* 1 = 0.0553627 loss)
I1026 00:55:18.475107 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0565514 (* 1 = 0.0565514 loss)
I1026 00:55:18.475111 17176 sgd_solver.cpp:106] Iteration 21040, lr = 0.001
I1026 00:55:19.038439 17176 solver.cpp:229] Iteration 21060, loss = 0.0463608
I1026 00:55:19.038472 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0235215 (* 1 = 0.0235215 loss)
I1026 00:55:19.038477 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0228393 (* 1 = 0.0228393 loss)
I1026 00:55:19.038480 17176 sgd_solver.cpp:106] Iteration 21060, lr = 0.001
I1026 00:55:19.605367 17176 solver.cpp:229] Iteration 21080, loss = 0.194851
I1026 00:55:19.605399 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0523751 (* 1 = 0.0523751 loss)
I1026 00:55:19.605403 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.142476 (* 1 = 0.142476 loss)
I1026 00:55:19.605407 17176 sgd_solver.cpp:106] Iteration 21080, lr = 0.001
I1026 00:55:20.161448 17176 solver.cpp:229] Iteration 21100, loss = 0.0867339
I1026 00:55:20.161480 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0538228 (* 1 = 0.0538228 loss)
I1026 00:55:20.161484 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0329111 (* 1 = 0.0329111 loss)
I1026 00:55:20.161489 17176 sgd_solver.cpp:106] Iteration 21100, lr = 0.001
I1026 00:55:20.714568 17176 solver.cpp:229] Iteration 21120, loss = 0.11997
I1026 00:55:20.714601 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0951443 (* 1 = 0.0951443 loss)
I1026 00:55:20.714606 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.024826 (* 1 = 0.024826 loss)
I1026 00:55:20.714609 17176 sgd_solver.cpp:106] Iteration 21120, lr = 0.001
I1026 00:55:21.274463 17176 solver.cpp:229] Iteration 21140, loss = 0.104104
I1026 00:55:21.274497 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0967994 (* 1 = 0.0967994 loss)
I1026 00:55:21.274500 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00730481 (* 1 = 0.00730481 loss)
I1026 00:55:21.274504 17176 sgd_solver.cpp:106] Iteration 21140, lr = 0.001
I1026 00:55:21.839148 17176 solver.cpp:229] Iteration 21160, loss = 0.0756269
I1026 00:55:21.839180 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0559399 (* 1 = 0.0559399 loss)
I1026 00:55:21.839186 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.019687 (* 1 = 0.019687 loss)
I1026 00:55:21.839190 17176 sgd_solver.cpp:106] Iteration 21160, lr = 0.001
I1026 00:55:22.397912 17176 solver.cpp:229] Iteration 21180, loss = 0.044404
I1026 00:55:22.397945 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0401403 (* 1 = 0.0401403 loss)
I1026 00:55:22.397950 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00426376 (* 1 = 0.00426376 loss)
I1026 00:55:22.397954 17176 sgd_solver.cpp:106] Iteration 21180, lr = 0.001
I1026 00:55:22.953415 17176 solver.cpp:229] Iteration 21200, loss = 0.0457153
I1026 00:55:22.953446 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0414195 (* 1 = 0.0414195 loss)
I1026 00:55:22.953451 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00429572 (* 1 = 0.00429572 loss)
I1026 00:55:22.953455 17176 sgd_solver.cpp:106] Iteration 21200, lr = 0.001
I1026 00:55:23.505996 17176 solver.cpp:229] Iteration 21220, loss = 0.0584741
I1026 00:55:23.506028 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.02036 (* 1 = 0.02036 loss)
I1026 00:55:23.506033 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.038114 (* 1 = 0.038114 loss)
I1026 00:55:23.506047 17176 sgd_solver.cpp:106] Iteration 21220, lr = 0.001
I1026 00:55:24.072825 17176 solver.cpp:229] Iteration 21240, loss = 0.0648406
I1026 00:55:24.072859 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0566803 (* 1 = 0.0566803 loss)
I1026 00:55:24.072863 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00816029 (* 1 = 0.00816029 loss)
I1026 00:55:24.072867 17176 sgd_solver.cpp:106] Iteration 21240, lr = 0.001
I1026 00:55:24.615715 17176 solver.cpp:229] Iteration 21260, loss = 0.0866759
I1026 00:55:24.615747 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0487066 (* 1 = 0.0487066 loss)
I1026 00:55:24.615752 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0379694 (* 1 = 0.0379694 loss)
I1026 00:55:24.615756 17176 sgd_solver.cpp:106] Iteration 21260, lr = 0.001
I1026 00:55:25.173948 17176 solver.cpp:229] Iteration 21280, loss = 0.0317623
I1026 00:55:25.173979 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00931587 (* 1 = 0.00931587 loss)
I1026 00:55:25.173985 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0224464 (* 1 = 0.0224464 loss)
I1026 00:55:25.173988 17176 sgd_solver.cpp:106] Iteration 21280, lr = 0.001
I1026 00:55:25.723120 17176 solver.cpp:229] Iteration 21300, loss = 0.0389752
I1026 00:55:25.723153 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0335836 (* 1 = 0.0335836 loss)
I1026 00:55:25.723158 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00539155 (* 1 = 0.00539155 loss)
I1026 00:55:25.723162 17176 sgd_solver.cpp:106] Iteration 21300, lr = 0.001
I1026 00:55:26.286763 17176 solver.cpp:229] Iteration 21320, loss = 0.0751362
I1026 00:55:26.286795 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0505605 (* 1 = 0.0505605 loss)
I1026 00:55:26.286799 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0245758 (* 1 = 0.0245758 loss)
I1026 00:55:26.286803 17176 sgd_solver.cpp:106] Iteration 21320, lr = 0.001
I1026 00:55:26.853399 17176 solver.cpp:229] Iteration 21340, loss = 0.158566
I1026 00:55:26.853431 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.140098 (* 1 = 0.140098 loss)
I1026 00:55:26.853435 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0184676 (* 1 = 0.0184676 loss)
I1026 00:55:26.853440 17176 sgd_solver.cpp:106] Iteration 21340, lr = 0.001
I1026 00:55:27.415014 17176 solver.cpp:229] Iteration 21360, loss = 0.102985
I1026 00:55:27.415045 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0633692 (* 1 = 0.0633692 loss)
I1026 00:55:27.415050 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0396162 (* 1 = 0.0396162 loss)
I1026 00:55:27.415053 17176 sgd_solver.cpp:106] Iteration 21360, lr = 0.001
I1026 00:55:27.974401 17176 solver.cpp:229] Iteration 21380, loss = 0.0732452
I1026 00:55:27.974432 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0238521 (* 1 = 0.0238521 loss)
I1026 00:55:27.974437 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0493931 (* 1 = 0.0493931 loss)
I1026 00:55:27.974442 17176 sgd_solver.cpp:106] Iteration 21380, lr = 0.001
I1026 00:55:28.538609 17176 solver.cpp:229] Iteration 21400, loss = 0.101295
I1026 00:55:28.538640 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0612566 (* 1 = 0.0612566 loss)
I1026 00:55:28.538645 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0400388 (* 1 = 0.0400388 loss)
I1026 00:55:28.538648 17176 sgd_solver.cpp:106] Iteration 21400, lr = 0.001
I1026 00:55:29.098496 17176 solver.cpp:229] Iteration 21420, loss = 0.0399222
I1026 00:55:29.098528 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0297807 (* 1 = 0.0297807 loss)
I1026 00:55:29.098533 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0101415 (* 1 = 0.0101415 loss)
I1026 00:55:29.098538 17176 sgd_solver.cpp:106] Iteration 21420, lr = 0.001
I1026 00:55:29.657542 17176 solver.cpp:229] Iteration 21440, loss = 0.0568437
I1026 00:55:29.657573 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0496525 (* 1 = 0.0496525 loss)
I1026 00:55:29.657578 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00719114 (* 1 = 0.00719114 loss)
I1026 00:55:29.657582 17176 sgd_solver.cpp:106] Iteration 21440, lr = 0.001
I1026 00:55:30.213940 17176 solver.cpp:229] Iteration 21460, loss = 0.0932834
I1026 00:55:30.213973 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0363589 (* 1 = 0.0363589 loss)
I1026 00:55:30.213976 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0569245 (* 1 = 0.0569245 loss)
I1026 00:55:30.213980 17176 sgd_solver.cpp:106] Iteration 21460, lr = 0.001
I1026 00:55:30.768230 17176 solver.cpp:229] Iteration 21480, loss = 0.0287901
I1026 00:55:30.768262 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0223255 (* 1 = 0.0223255 loss)
I1026 00:55:30.768267 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00646463 (* 1 = 0.00646463 loss)
I1026 00:55:30.768271 17176 sgd_solver.cpp:106] Iteration 21480, lr = 0.001
I1026 00:55:31.326563 17176 solver.cpp:229] Iteration 21500, loss = 0.067241
I1026 00:55:31.326596 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0478664 (* 1 = 0.0478664 loss)
I1026 00:55:31.326601 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0193747 (* 1 = 0.0193747 loss)
I1026 00:55:31.326604 17176 sgd_solver.cpp:106] Iteration 21500, lr = 0.001
I1026 00:55:31.895398 17176 solver.cpp:229] Iteration 21520, loss = 0.0594744
I1026 00:55:31.895434 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0307141 (* 1 = 0.0307141 loss)
I1026 00:55:31.895439 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0287603 (* 1 = 0.0287603 loss)
I1026 00:55:31.895443 17176 sgd_solver.cpp:106] Iteration 21520, lr = 0.001
I1026 00:55:32.459524 17176 solver.cpp:229] Iteration 21540, loss = 0.158187
I1026 00:55:32.459574 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.113999 (* 1 = 0.113999 loss)
I1026 00:55:32.459583 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0441879 (* 1 = 0.0441879 loss)
I1026 00:55:32.459589 17176 sgd_solver.cpp:106] Iteration 21540, lr = 0.001
I1026 00:55:33.011440 17176 solver.cpp:229] Iteration 21560, loss = 0.111437
I1026 00:55:33.011472 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0958402 (* 1 = 0.0958402 loss)
I1026 00:55:33.011476 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0155963 (* 1 = 0.0155963 loss)
I1026 00:55:33.011481 17176 sgd_solver.cpp:106] Iteration 21560, lr = 0.001
I1026 00:55:33.569350 17176 solver.cpp:229] Iteration 21580, loss = 0.0549784
I1026 00:55:33.569382 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.031582 (* 1 = 0.031582 loss)
I1026 00:55:33.569387 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0233964 (* 1 = 0.0233964 loss)
I1026 00:55:33.569391 17176 sgd_solver.cpp:106] Iteration 21580, lr = 0.001
I1026 00:55:34.134933 17176 solver.cpp:229] Iteration 21600, loss = 0.0973714
I1026 00:55:34.134975 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0478414 (* 1 = 0.0478414 loss)
I1026 00:55:34.134981 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.04953 (* 1 = 0.04953 loss)
I1026 00:55:34.134984 17176 sgd_solver.cpp:106] Iteration 21600, lr = 0.001
I1026 00:55:34.701957 17176 solver.cpp:229] Iteration 21620, loss = 0.0612267
I1026 00:55:34.701989 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0382846 (* 1 = 0.0382846 loss)
I1026 00:55:34.701994 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0229421 (* 1 = 0.0229421 loss)
I1026 00:55:34.701998 17176 sgd_solver.cpp:106] Iteration 21620, lr = 0.001
I1026 00:55:35.265233 17176 solver.cpp:229] Iteration 21640, loss = 0.0714329
I1026 00:55:35.265264 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0510887 (* 1 = 0.0510887 loss)
I1026 00:55:35.265269 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0203442 (* 1 = 0.0203442 loss)
I1026 00:55:35.265275 17176 sgd_solver.cpp:106] Iteration 21640, lr = 0.001
I1026 00:55:35.827304 17176 solver.cpp:229] Iteration 21660, loss = 0.0649016
I1026 00:55:35.827337 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0362475 (* 1 = 0.0362475 loss)
I1026 00:55:35.827342 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0286541 (* 1 = 0.0286541 loss)
I1026 00:55:35.827345 17176 sgd_solver.cpp:106] Iteration 21660, lr = 0.001
I1026 00:55:36.387933 17176 solver.cpp:229] Iteration 21680, loss = 0.143852
I1026 00:55:36.387976 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.118879 (* 1 = 0.118879 loss)
I1026 00:55:36.387981 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0249732 (* 1 = 0.0249732 loss)
I1026 00:55:36.387984 17176 sgd_solver.cpp:106] Iteration 21680, lr = 0.001
I1026 00:55:36.953193 17176 solver.cpp:229] Iteration 21700, loss = 0.470994
I1026 00:55:36.953236 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0914654 (* 1 = 0.0914654 loss)
I1026 00:55:36.953241 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.379529 (* 1 = 0.379529 loss)
I1026 00:55:36.953255 17176 sgd_solver.cpp:106] Iteration 21700, lr = 0.001
I1026 00:55:37.505568 17176 solver.cpp:229] Iteration 21720, loss = 0.105293
I1026 00:55:37.505600 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0682452 (* 1 = 0.0682452 loss)
I1026 00:55:37.505605 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0370476 (* 1 = 0.0370476 loss)
I1026 00:55:37.505609 17176 sgd_solver.cpp:106] Iteration 21720, lr = 0.001
I1026 00:55:38.065731 17176 solver.cpp:229] Iteration 21740, loss = 0.131407
I1026 00:55:38.065762 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.119632 (* 1 = 0.119632 loss)
I1026 00:55:38.065768 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0117752 (* 1 = 0.0117752 loss)
I1026 00:55:38.065771 17176 sgd_solver.cpp:106] Iteration 21740, lr = 0.001
I1026 00:55:38.618682 17176 solver.cpp:229] Iteration 21760, loss = 0.0913241
I1026 00:55:38.618713 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0378179 (* 1 = 0.0378179 loss)
I1026 00:55:38.618718 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0535062 (* 1 = 0.0535062 loss)
I1026 00:55:38.618722 17176 sgd_solver.cpp:106] Iteration 21760, lr = 0.001
I1026 00:55:39.182766 17176 solver.cpp:229] Iteration 21780, loss = 0.0631293
I1026 00:55:39.182798 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0436916 (* 1 = 0.0436916 loss)
I1026 00:55:39.182802 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0194377 (* 1 = 0.0194377 loss)
I1026 00:55:39.182806 17176 sgd_solver.cpp:106] Iteration 21780, lr = 0.001
I1026 00:55:39.738682 17176 solver.cpp:229] Iteration 21800, loss = 0.221938
I1026 00:55:39.738713 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.166072 (* 1 = 0.166072 loss)
I1026 00:55:39.738718 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.055866 (* 1 = 0.055866 loss)
I1026 00:55:39.738721 17176 sgd_solver.cpp:106] Iteration 21800, lr = 0.001
I1026 00:55:40.291010 17176 solver.cpp:229] Iteration 21820, loss = 0.283922
I1026 00:55:40.291043 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.175309 (* 1 = 0.175309 loss)
I1026 00:55:40.291048 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.108613 (* 1 = 0.108613 loss)
I1026 00:55:40.291051 17176 sgd_solver.cpp:106] Iteration 21820, lr = 0.001
I1026 00:55:40.863864 17176 solver.cpp:229] Iteration 21840, loss = 0.0850698
I1026 00:55:40.863895 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0803969 (* 1 = 0.0803969 loss)
I1026 00:55:40.863900 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00467289 (* 1 = 0.00467289 loss)
I1026 00:55:40.863904 17176 sgd_solver.cpp:106] Iteration 21840, lr = 0.001
I1026 00:55:41.422283 17176 solver.cpp:229] Iteration 21860, loss = 0.032994
I1026 00:55:41.422317 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0304086 (* 1 = 0.0304086 loss)
I1026 00:55:41.422320 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00258533 (* 1 = 0.00258533 loss)
I1026 00:55:41.422324 17176 sgd_solver.cpp:106] Iteration 21860, lr = 0.001
I1026 00:55:41.996067 17176 solver.cpp:229] Iteration 21880, loss = 0.150733
I1026 00:55:41.996099 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0491107 (* 1 = 0.0491107 loss)
I1026 00:55:41.996104 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.101623 (* 1 = 0.101623 loss)
I1026 00:55:41.996107 17176 sgd_solver.cpp:106] Iteration 21880, lr = 0.001
I1026 00:55:42.562760 17176 solver.cpp:229] Iteration 21900, loss = 0.108312
I1026 00:55:42.562793 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0778157 (* 1 = 0.0778157 loss)
I1026 00:55:42.562798 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0304963 (* 1 = 0.0304963 loss)
I1026 00:55:42.562801 17176 sgd_solver.cpp:106] Iteration 21900, lr = 0.001
I1026 00:55:43.117933 17176 solver.cpp:229] Iteration 21920, loss = 0.108066
I1026 00:55:43.117974 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0430644 (* 1 = 0.0430644 loss)
I1026 00:55:43.117980 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0650017 (* 1 = 0.0650017 loss)
I1026 00:55:43.117983 17176 sgd_solver.cpp:106] Iteration 21920, lr = 0.001
I1026 00:55:43.678082 17176 solver.cpp:229] Iteration 21940, loss = 0.0856198
I1026 00:55:43.678124 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0512685 (* 1 = 0.0512685 loss)
I1026 00:55:43.678129 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0343513 (* 1 = 0.0343513 loss)
I1026 00:55:43.678133 17176 sgd_solver.cpp:106] Iteration 21940, lr = 0.001
I1026 00:55:44.259563 17176 solver.cpp:229] Iteration 21960, loss = 0.015492
I1026 00:55:44.259608 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0135717 (* 1 = 0.0135717 loss)
I1026 00:55:44.259613 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00192038 (* 1 = 0.00192038 loss)
I1026 00:55:44.259618 17176 sgd_solver.cpp:106] Iteration 21960, lr = 0.001
I1026 00:55:44.814213 17176 solver.cpp:229] Iteration 21980, loss = 0.0885898
I1026 00:55:44.814245 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0346282 (* 1 = 0.0346282 loss)
I1026 00:55:44.814250 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0539616 (* 1 = 0.0539616 loss)
I1026 00:55:44.814254 17176 sgd_solver.cpp:106] Iteration 21980, lr = 0.001
I1026 00:55:45.383584 17176 solver.cpp:229] Iteration 22000, loss = 0.0606642
I1026 00:55:45.383615 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0241882 (* 1 = 0.0241882 loss)
I1026 00:55:45.383620 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0364759 (* 1 = 0.0364759 loss)
I1026 00:55:45.383623 17176 sgd_solver.cpp:106] Iteration 22000, lr = 0.001
I1026 00:55:45.940980 17176 solver.cpp:229] Iteration 22020, loss = 0.0318979
I1026 00:55:45.941026 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0171827 (* 1 = 0.0171827 loss)
I1026 00:55:45.941031 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0147152 (* 1 = 0.0147152 loss)
I1026 00:55:45.941035 17176 sgd_solver.cpp:106] Iteration 22020, lr = 0.001
I1026 00:55:46.489861 17176 solver.cpp:229] Iteration 22040, loss = 0.0213679
I1026 00:55:46.489892 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0114306 (* 1 = 0.0114306 loss)
I1026 00:55:46.489897 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00993734 (* 1 = 0.00993734 loss)
I1026 00:55:46.489900 17176 sgd_solver.cpp:106] Iteration 22040, lr = 0.001
I1026 00:55:47.063621 17176 solver.cpp:229] Iteration 22060, loss = 0.0531206
I1026 00:55:47.063652 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0450018 (* 1 = 0.0450018 loss)
I1026 00:55:47.063657 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00811884 (* 1 = 0.00811884 loss)
I1026 00:55:47.063662 17176 sgd_solver.cpp:106] Iteration 22060, lr = 0.001
I1026 00:55:47.629982 17176 solver.cpp:229] Iteration 22080, loss = 0.587706
I1026 00:55:47.630024 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.245148 (* 1 = 0.245148 loss)
I1026 00:55:47.630029 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.342558 (* 1 = 0.342558 loss)
I1026 00:55:47.630033 17176 sgd_solver.cpp:106] Iteration 22080, lr = 0.001
I1026 00:55:48.191218 17176 solver.cpp:229] Iteration 22100, loss = 0.0600725
I1026 00:55:48.191262 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0286204 (* 1 = 0.0286204 loss)
I1026 00:55:48.191265 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0314521 (* 1 = 0.0314521 loss)
I1026 00:55:48.191269 17176 sgd_solver.cpp:106] Iteration 22100, lr = 0.001
I1026 00:55:48.750571 17176 solver.cpp:229] Iteration 22120, loss = 0.0494485
I1026 00:55:48.750613 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0249097 (* 1 = 0.0249097 loss)
I1026 00:55:48.750618 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0245388 (* 1 = 0.0245388 loss)
I1026 00:55:48.750622 17176 sgd_solver.cpp:106] Iteration 22120, lr = 0.001
I1026 00:55:49.305657 17176 solver.cpp:229] Iteration 22140, loss = 0.194019
I1026 00:55:49.305691 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.111729 (* 1 = 0.111729 loss)
I1026 00:55:49.305696 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0822897 (* 1 = 0.0822897 loss)
I1026 00:55:49.305701 17176 sgd_solver.cpp:106] Iteration 22140, lr = 0.001
I1026 00:55:49.872651 17176 solver.cpp:229] Iteration 22160, loss = 0.0443325
I1026 00:55:49.872684 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0199311 (* 1 = 0.0199311 loss)
I1026 00:55:49.872687 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0244014 (* 1 = 0.0244014 loss)
I1026 00:55:49.872691 17176 sgd_solver.cpp:106] Iteration 22160, lr = 0.001
I1026 00:55:50.424515 17176 solver.cpp:229] Iteration 22180, loss = 0.0713033
I1026 00:55:50.424547 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0492646 (* 1 = 0.0492646 loss)
I1026 00:55:50.424552 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0220387 (* 1 = 0.0220387 loss)
I1026 00:55:50.424556 17176 sgd_solver.cpp:106] Iteration 22180, lr = 0.001
I1026 00:55:50.974642 17176 solver.cpp:229] Iteration 22200, loss = 0.069003
I1026 00:55:50.974674 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0535259 (* 1 = 0.0535259 loss)
I1026 00:55:50.974679 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0154771 (* 1 = 0.0154771 loss)
I1026 00:55:50.974684 17176 sgd_solver.cpp:106] Iteration 22200, lr = 0.001
I1026 00:55:51.532079 17176 solver.cpp:229] Iteration 22220, loss = 0.104332
I1026 00:55:51.532119 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.090088 (* 1 = 0.090088 loss)
I1026 00:55:51.532124 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.014244 (* 1 = 0.014244 loss)
I1026 00:55:51.532130 17176 sgd_solver.cpp:106] Iteration 22220, lr = 0.001
I1026 00:55:52.101447 17176 solver.cpp:229] Iteration 22240, loss = 0.350508
I1026 00:55:52.101491 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.208063 (* 1 = 0.208063 loss)
I1026 00:55:52.101495 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.142445 (* 1 = 0.142445 loss)
I1026 00:55:52.101500 17176 sgd_solver.cpp:106] Iteration 22240, lr = 0.001
I1026 00:55:52.662561 17176 solver.cpp:229] Iteration 22260, loss = 0.0867225
I1026 00:55:52.662606 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0629343 (* 1 = 0.0629343 loss)
I1026 00:55:52.662611 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0237882 (* 1 = 0.0237882 loss)
I1026 00:55:52.662614 17176 sgd_solver.cpp:106] Iteration 22260, lr = 0.001
I1026 00:55:53.239167 17176 solver.cpp:229] Iteration 22280, loss = 0.371161
I1026 00:55:53.239200 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.306998 (* 1 = 0.306998 loss)
I1026 00:55:53.239205 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0641625 (* 1 = 0.0641625 loss)
I1026 00:55:53.239210 17176 sgd_solver.cpp:106] Iteration 22280, lr = 0.001
I1026 00:55:53.792928 17176 solver.cpp:229] Iteration 22300, loss = 0.138705
I1026 00:55:53.792961 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0888462 (* 1 = 0.0888462 loss)
I1026 00:55:53.792965 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0498589 (* 1 = 0.0498589 loss)
I1026 00:55:53.792969 17176 sgd_solver.cpp:106] Iteration 22300, lr = 0.001
I1026 00:55:54.343302 17176 solver.cpp:229] Iteration 22320, loss = 0.0422133
I1026 00:55:54.343345 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0407942 (* 1 = 0.0407942 loss)
I1026 00:55:54.343353 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00141905 (* 1 = 0.00141905 loss)
I1026 00:55:54.343367 17176 sgd_solver.cpp:106] Iteration 22320, lr = 0.001
I1026 00:55:54.900729 17176 solver.cpp:229] Iteration 22340, loss = 0.518535
I1026 00:55:54.900763 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.130823 (* 1 = 0.130823 loss)
I1026 00:55:54.900769 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.387713 (* 1 = 0.387713 loss)
I1026 00:55:54.900775 17176 sgd_solver.cpp:106] Iteration 22340, lr = 0.001
I1026 00:55:55.451313 17176 solver.cpp:229] Iteration 22360, loss = 0.0962949
I1026 00:55:55.451346 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0725204 (* 1 = 0.0725204 loss)
I1026 00:55:55.451352 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0237745 (* 1 = 0.0237745 loss)
I1026 00:55:55.451369 17176 sgd_solver.cpp:106] Iteration 22360, lr = 0.001
I1026 00:55:56.005859 17176 solver.cpp:229] Iteration 22380, loss = 0.0378831
I1026 00:55:56.005892 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0239786 (* 1 = 0.0239786 loss)
I1026 00:55:56.005898 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0139045 (* 1 = 0.0139045 loss)
I1026 00:55:56.005903 17176 sgd_solver.cpp:106] Iteration 22380, lr = 0.001
I1026 00:55:56.567219 17176 solver.cpp:229] Iteration 22400, loss = 0.121187
I1026 00:55:56.567260 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.106922 (* 1 = 0.106922 loss)
I1026 00:55:56.567265 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0142649 (* 1 = 0.0142649 loss)
I1026 00:55:56.567270 17176 sgd_solver.cpp:106] Iteration 22400, lr = 0.001
I1026 00:55:57.147719 17176 solver.cpp:229] Iteration 22420, loss = 0.0401726
I1026 00:55:57.147759 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0355329 (* 1 = 0.0355329 loss)
I1026 00:55:57.147764 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00463979 (* 1 = 0.00463979 loss)
I1026 00:55:57.147768 17176 sgd_solver.cpp:106] Iteration 22420, lr = 0.001
I1026 00:55:57.714669 17176 solver.cpp:229] Iteration 22440, loss = 0.140476
I1026 00:55:57.714701 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.134997 (* 1 = 0.134997 loss)
I1026 00:55:57.714706 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00547914 (* 1 = 0.00547914 loss)
I1026 00:55:57.714711 17176 sgd_solver.cpp:106] Iteration 22440, lr = 0.001
I1026 00:55:58.269656 17176 solver.cpp:229] Iteration 22460, loss = 0.648949
I1026 00:55:58.269700 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.21261 (* 1 = 0.21261 loss)
I1026 00:55:58.269703 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.436339 (* 1 = 0.436339 loss)
I1026 00:55:58.269708 17176 sgd_solver.cpp:106] Iteration 22460, lr = 0.001
I1026 00:55:58.838040 17176 solver.cpp:229] Iteration 22480, loss = 0.044643
I1026 00:55:58.838071 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0166826 (* 1 = 0.0166826 loss)
I1026 00:55:58.838076 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0279605 (* 1 = 0.0279605 loss)
I1026 00:55:58.838081 17176 sgd_solver.cpp:106] Iteration 22480, lr = 0.001
I1026 00:55:59.399027 17176 solver.cpp:229] Iteration 22500, loss = 0.121525
I1026 00:55:59.399058 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0549435 (* 1 = 0.0549435 loss)
I1026 00:55:59.399063 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0665817 (* 1 = 0.0665817 loss)
I1026 00:55:59.399066 17176 sgd_solver.cpp:106] Iteration 22500, lr = 0.001
I1026 00:55:59.955374 17176 solver.cpp:229] Iteration 22520, loss = 0.0549931
I1026 00:55:59.955404 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0323374 (* 1 = 0.0323374 loss)
I1026 00:55:59.955410 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0226557 (* 1 = 0.0226557 loss)
I1026 00:55:59.955412 17176 sgd_solver.cpp:106] Iteration 22520, lr = 0.001
I1026 00:56:00.514758 17176 solver.cpp:229] Iteration 22540, loss = 0.251334
I1026 00:56:00.514791 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.101204 (* 1 = 0.101204 loss)
I1026 00:56:00.514796 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.15013 (* 1 = 0.15013 loss)
I1026 00:56:00.514799 17176 sgd_solver.cpp:106] Iteration 22540, lr = 0.001
I1026 00:56:01.072870 17176 solver.cpp:229] Iteration 22560, loss = 0.0420357
I1026 00:56:01.072912 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0389251 (* 1 = 0.0389251 loss)
I1026 00:56:01.072917 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00311059 (* 1 = 0.00311059 loss)
I1026 00:56:01.072921 17176 sgd_solver.cpp:106] Iteration 22560, lr = 0.001
I1026 00:56:01.629359 17176 solver.cpp:229] Iteration 22580, loss = 0.0489717
I1026 00:56:01.629390 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0457842 (* 1 = 0.0457842 loss)
I1026 00:56:01.629395 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00318744 (* 1 = 0.00318744 loss)
I1026 00:56:01.629398 17176 sgd_solver.cpp:106] Iteration 22580, lr = 0.001
I1026 00:56:02.178824 17176 solver.cpp:229] Iteration 22600, loss = 0.0617835
I1026 00:56:02.178856 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0431915 (* 1 = 0.0431915 loss)
I1026 00:56:02.178860 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.018592 (* 1 = 0.018592 loss)
I1026 00:56:02.178865 17176 sgd_solver.cpp:106] Iteration 22600, lr = 0.001
I1026 00:56:02.734386 17176 solver.cpp:229] Iteration 22620, loss = 0.0390304
I1026 00:56:02.734418 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.034256 (* 1 = 0.034256 loss)
I1026 00:56:02.734423 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00477445 (* 1 = 0.00477445 loss)
I1026 00:56:02.734428 17176 sgd_solver.cpp:106] Iteration 22620, lr = 0.001
I1026 00:56:03.298104 17176 solver.cpp:229] Iteration 22640, loss = 0.195116
I1026 00:56:03.298146 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0461215 (* 1 = 0.0461215 loss)
I1026 00:56:03.298151 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.148994 (* 1 = 0.148994 loss)
I1026 00:56:03.298156 17176 sgd_solver.cpp:106] Iteration 22640, lr = 0.001
I1026 00:56:03.853652 17176 solver.cpp:229] Iteration 22660, loss = 0.31658
I1026 00:56:03.853682 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.122201 (* 1 = 0.122201 loss)
I1026 00:56:03.853688 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.19438 (* 1 = 0.19438 loss)
I1026 00:56:03.853691 17176 sgd_solver.cpp:106] Iteration 22660, lr = 0.001
I1026 00:56:04.420179 17176 solver.cpp:229] Iteration 22680, loss = 0.176233
I1026 00:56:04.420210 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0493664 (* 1 = 0.0493664 loss)
I1026 00:56:04.420215 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.126867 (* 1 = 0.126867 loss)
I1026 00:56:04.420219 17176 sgd_solver.cpp:106] Iteration 22680, lr = 0.001
I1026 00:56:04.977836 17176 solver.cpp:229] Iteration 22700, loss = 0.312886
I1026 00:56:04.977877 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.183752 (* 1 = 0.183752 loss)
I1026 00:56:04.977882 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.129134 (* 1 = 0.129134 loss)
I1026 00:56:04.977885 17176 sgd_solver.cpp:106] Iteration 22700, lr = 0.001
I1026 00:56:05.529886 17176 solver.cpp:229] Iteration 22720, loss = 0.0281174
I1026 00:56:05.529917 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.02617 (* 1 = 0.02617 loss)
I1026 00:56:05.529922 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00194742 (* 1 = 0.00194742 loss)
I1026 00:56:05.529927 17176 sgd_solver.cpp:106] Iteration 22720, lr = 0.001
I1026 00:56:06.085906 17176 solver.cpp:229] Iteration 22740, loss = 0.0239893
I1026 00:56:06.085939 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0189082 (* 1 = 0.0189082 loss)
I1026 00:56:06.085944 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00508109 (* 1 = 0.00508109 loss)
I1026 00:56:06.085948 17176 sgd_solver.cpp:106] Iteration 22740, lr = 0.001
I1026 00:56:06.644578 17176 solver.cpp:229] Iteration 22760, loss = 0.0978068
I1026 00:56:06.644608 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0754335 (* 1 = 0.0754335 loss)
I1026 00:56:06.644611 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0223733 (* 1 = 0.0223733 loss)
I1026 00:56:06.644615 17176 sgd_solver.cpp:106] Iteration 22760, lr = 0.001
I1026 00:56:07.210042 17176 solver.cpp:229] Iteration 22780, loss = 0.021918
I1026 00:56:07.210074 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0135664 (* 1 = 0.0135664 loss)
I1026 00:56:07.210079 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00835165 (* 1 = 0.00835165 loss)
I1026 00:56:07.210083 17176 sgd_solver.cpp:106] Iteration 22780, lr = 0.001
I1026 00:56:07.759598 17176 solver.cpp:229] Iteration 22800, loss = 0.103718
I1026 00:56:07.759632 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0776149 (* 1 = 0.0776149 loss)
I1026 00:56:07.759636 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0261033 (* 1 = 0.0261033 loss)
I1026 00:56:07.759640 17176 sgd_solver.cpp:106] Iteration 22800, lr = 0.001
I1026 00:56:08.316123 17176 solver.cpp:229] Iteration 22820, loss = 0.104714
I1026 00:56:08.316165 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0498014 (* 1 = 0.0498014 loss)
I1026 00:56:08.316171 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0549126 (* 1 = 0.0549126 loss)
I1026 00:56:08.316175 17176 sgd_solver.cpp:106] Iteration 22820, lr = 0.001
I1026 00:56:08.877069 17176 solver.cpp:229] Iteration 22840, loss = 0.603532
I1026 00:56:08.877111 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.235306 (* 1 = 0.235306 loss)
I1026 00:56:08.877116 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.368226 (* 1 = 0.368226 loss)
I1026 00:56:08.877130 17176 sgd_solver.cpp:106] Iteration 22840, lr = 0.001
I1026 00:56:09.445888 17176 solver.cpp:229] Iteration 22860, loss = 0.0652451
I1026 00:56:09.445920 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0393678 (* 1 = 0.0393678 loss)
I1026 00:56:09.445925 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0258773 (* 1 = 0.0258773 loss)
I1026 00:56:09.445930 17176 sgd_solver.cpp:106] Iteration 22860, lr = 0.001
I1026 00:56:10.016822 17176 solver.cpp:229] Iteration 22880, loss = 0.0704271
I1026 00:56:10.016855 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0607602 (* 1 = 0.0607602 loss)
I1026 00:56:10.016860 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00966686 (* 1 = 0.00966686 loss)
I1026 00:56:10.016863 17176 sgd_solver.cpp:106] Iteration 22880, lr = 0.001
I1026 00:56:10.586985 17176 solver.cpp:229] Iteration 22900, loss = 0.0365729
I1026 00:56:10.587018 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0105282 (* 1 = 0.0105282 loss)
I1026 00:56:10.587023 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0260447 (* 1 = 0.0260447 loss)
I1026 00:56:10.587026 17176 sgd_solver.cpp:106] Iteration 22900, lr = 0.001
I1026 00:56:11.152603 17176 solver.cpp:229] Iteration 22920, loss = 0.135886
I1026 00:56:11.152636 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0574247 (* 1 = 0.0574247 loss)
I1026 00:56:11.152639 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0784615 (* 1 = 0.0784615 loss)
I1026 00:56:11.152643 17176 sgd_solver.cpp:106] Iteration 22920, lr = 0.001
I1026 00:56:11.719660 17176 solver.cpp:229] Iteration 22940, loss = 0.0974082
I1026 00:56:11.719691 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0338675 (* 1 = 0.0338675 loss)
I1026 00:56:11.719696 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0635408 (* 1 = 0.0635408 loss)
I1026 00:56:11.719699 17176 sgd_solver.cpp:106] Iteration 22940, lr = 0.001
I1026 00:56:12.275956 17176 solver.cpp:229] Iteration 22960, loss = 0.156541
I1026 00:56:12.275988 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0516419 (* 1 = 0.0516419 loss)
I1026 00:56:12.275993 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.1049 (* 1 = 0.1049 loss)
I1026 00:56:12.275998 17176 sgd_solver.cpp:106] Iteration 22960, lr = 0.001
I1026 00:56:12.843508 17176 solver.cpp:229] Iteration 22980, loss = 0.127387
I1026 00:56:12.843540 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0775452 (* 1 = 0.0775452 loss)
I1026 00:56:12.843545 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0498415 (* 1 = 0.0498415 loss)
I1026 00:56:12.843550 17176 sgd_solver.cpp:106] Iteration 22980, lr = 0.001
I1026 00:56:13.407791 17176 solver.cpp:229] Iteration 23000, loss = 0.115286
I1026 00:56:13.407821 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.100171 (* 1 = 0.100171 loss)
I1026 00:56:13.407826 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0151144 (* 1 = 0.0151144 loss)
I1026 00:56:13.407831 17176 sgd_solver.cpp:106] Iteration 23000, lr = 0.001
I1026 00:56:13.956802 17176 solver.cpp:229] Iteration 23020, loss = 0.0904468
I1026 00:56:13.956845 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0672022 (* 1 = 0.0672022 loss)
I1026 00:56:13.956850 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0232446 (* 1 = 0.0232446 loss)
I1026 00:56:13.956853 17176 sgd_solver.cpp:106] Iteration 23020, lr = 0.001
I1026 00:56:14.507827 17176 solver.cpp:229] Iteration 23040, loss = 0.431922
I1026 00:56:14.507860 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.123113 (* 1 = 0.123113 loss)
I1026 00:56:14.507865 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.308809 (* 1 = 0.308809 loss)
I1026 00:56:14.507869 17176 sgd_solver.cpp:106] Iteration 23040, lr = 0.001
I1026 00:56:15.073875 17176 solver.cpp:229] Iteration 23060, loss = 0.146798
I1026 00:56:15.073917 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0403793 (* 1 = 0.0403793 loss)
I1026 00:56:15.073922 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.106419 (* 1 = 0.106419 loss)
I1026 00:56:15.073926 17176 sgd_solver.cpp:106] Iteration 23060, lr = 0.001
I1026 00:56:15.626675 17176 solver.cpp:229] Iteration 23080, loss = 0.0814901
I1026 00:56:15.626716 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0554114 (* 1 = 0.0554114 loss)
I1026 00:56:15.626721 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0260787 (* 1 = 0.0260787 loss)
I1026 00:56:15.626725 17176 sgd_solver.cpp:106] Iteration 23080, lr = 0.001
I1026 00:56:16.188664 17176 solver.cpp:229] Iteration 23100, loss = 0.0478122
I1026 00:56:16.188695 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0413285 (* 1 = 0.0413285 loss)
I1026 00:56:16.188700 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00648365 (* 1 = 0.00648365 loss)
I1026 00:56:16.188705 17176 sgd_solver.cpp:106] Iteration 23100, lr = 0.001
I1026 00:56:16.752141 17176 solver.cpp:229] Iteration 23120, loss = 0.0151631
I1026 00:56:16.752173 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00958557 (* 1 = 0.00958557 loss)
I1026 00:56:16.752178 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00557752 (* 1 = 0.00557752 loss)
I1026 00:56:16.752182 17176 sgd_solver.cpp:106] Iteration 23120, lr = 0.001
I1026 00:56:17.308125 17176 solver.cpp:229] Iteration 23140, loss = 0.043916
I1026 00:56:17.308156 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0408174 (* 1 = 0.0408174 loss)
I1026 00:56:17.308161 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0030986 (* 1 = 0.0030986 loss)
I1026 00:56:17.308164 17176 sgd_solver.cpp:106] Iteration 23140, lr = 0.001
I1026 00:56:17.867218 17176 solver.cpp:229] Iteration 23160, loss = 0.0570948
I1026 00:56:17.867251 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.012179 (* 1 = 0.012179 loss)
I1026 00:56:17.867255 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0449158 (* 1 = 0.0449158 loss)
I1026 00:56:17.867259 17176 sgd_solver.cpp:106] Iteration 23160, lr = 0.001
I1026 00:56:18.411365 17176 solver.cpp:229] Iteration 23180, loss = 0.0577059
I1026 00:56:18.411397 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.048291 (* 1 = 0.048291 loss)
I1026 00:56:18.411402 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00941481 (* 1 = 0.00941481 loss)
I1026 00:56:18.411406 17176 sgd_solver.cpp:106] Iteration 23180, lr = 0.001
I1026 00:56:18.976680 17176 solver.cpp:229] Iteration 23200, loss = 0.0543001
I1026 00:56:18.976713 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0504817 (* 1 = 0.0504817 loss)
I1026 00:56:18.976718 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00381834 (* 1 = 0.00381834 loss)
I1026 00:56:18.976722 17176 sgd_solver.cpp:106] Iteration 23200, lr = 0.001
I1026 00:56:19.543386 17176 solver.cpp:229] Iteration 23220, loss = 0.122991
I1026 00:56:19.543423 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.101959 (* 1 = 0.101959 loss)
I1026 00:56:19.543433 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0210319 (* 1 = 0.0210319 loss)
I1026 00:56:19.543452 17176 sgd_solver.cpp:106] Iteration 23220, lr = 0.001
I1026 00:56:20.103256 17176 solver.cpp:229] Iteration 23240, loss = 0.0117337
I1026 00:56:20.103288 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00984678 (* 1 = 0.00984678 loss)
I1026 00:56:20.103293 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0018869 (* 1 = 0.0018869 loss)
I1026 00:56:20.103297 17176 sgd_solver.cpp:106] Iteration 23240, lr = 0.001
I1026 00:56:20.668174 17176 solver.cpp:229] Iteration 23260, loss = 0.120744
I1026 00:56:20.668205 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0362458 (* 1 = 0.0362458 loss)
I1026 00:56:20.668208 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0844982 (* 1 = 0.0844982 loss)
I1026 00:56:20.668212 17176 sgd_solver.cpp:106] Iteration 23260, lr = 0.001
I1026 00:56:21.225486 17176 solver.cpp:229] Iteration 23280, loss = 0.0284731
I1026 00:56:21.225517 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0263532 (* 1 = 0.0263532 loss)
I1026 00:56:21.225522 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00211993 (* 1 = 0.00211993 loss)
I1026 00:56:21.225528 17176 sgd_solver.cpp:106] Iteration 23280, lr = 0.001
I1026 00:56:21.795083 17176 solver.cpp:229] Iteration 23300, loss = 0.0900365
I1026 00:56:21.795115 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0589982 (* 1 = 0.0589982 loss)
I1026 00:56:21.795120 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0310383 (* 1 = 0.0310383 loss)
I1026 00:56:21.795133 17176 sgd_solver.cpp:106] Iteration 23300, lr = 0.001
I1026 00:56:22.348711 17176 solver.cpp:229] Iteration 23320, loss = 0.0391149
I1026 00:56:22.348752 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0158719 (* 1 = 0.0158719 loss)
I1026 00:56:22.348757 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0232431 (* 1 = 0.0232431 loss)
I1026 00:56:22.348762 17176 sgd_solver.cpp:106] Iteration 23320, lr = 0.001
I1026 00:56:22.901897 17176 solver.cpp:229] Iteration 23340, loss = 0.0406575
I1026 00:56:22.901929 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0248601 (* 1 = 0.0248601 loss)
I1026 00:56:22.901933 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0157974 (* 1 = 0.0157974 loss)
I1026 00:56:22.901947 17176 sgd_solver.cpp:106] Iteration 23340, lr = 0.001
I1026 00:56:23.453320 17176 solver.cpp:229] Iteration 23360, loss = 0.296017
I1026 00:56:23.453353 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.131236 (* 1 = 0.131236 loss)
I1026 00:56:23.453358 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.164781 (* 1 = 0.164781 loss)
I1026 00:56:23.453362 17176 sgd_solver.cpp:106] Iteration 23360, lr = 0.001
I1026 00:56:24.003482 17176 solver.cpp:229] Iteration 23380, loss = 0.03566
I1026 00:56:24.003525 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0241507 (* 1 = 0.0241507 loss)
I1026 00:56:24.003530 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0115093 (* 1 = 0.0115093 loss)
I1026 00:56:24.003535 17176 sgd_solver.cpp:106] Iteration 23380, lr = 0.001
I1026 00:56:24.568068 17176 solver.cpp:229] Iteration 23400, loss = 0.0778876
I1026 00:56:24.568101 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0425714 (* 1 = 0.0425714 loss)
I1026 00:56:24.568106 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0353162 (* 1 = 0.0353162 loss)
I1026 00:56:24.568110 17176 sgd_solver.cpp:106] Iteration 23400, lr = 0.001
I1026 00:56:25.117254 17176 solver.cpp:229] Iteration 23420, loss = 0.163008
I1026 00:56:25.117287 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0518952 (* 1 = 0.0518952 loss)
I1026 00:56:25.117292 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.111113 (* 1 = 0.111113 loss)
I1026 00:56:25.117296 17176 sgd_solver.cpp:106] Iteration 23420, lr = 0.001
I1026 00:56:25.680573 17176 solver.cpp:229] Iteration 23440, loss = 0.122749
I1026 00:56:25.680603 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0902277 (* 1 = 0.0902277 loss)
I1026 00:56:25.680608 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.032521 (* 1 = 0.032521 loss)
I1026 00:56:25.680613 17176 sgd_solver.cpp:106] Iteration 23440, lr = 0.001
I1026 00:56:26.228579 17176 solver.cpp:229] Iteration 23460, loss = 0.258507
I1026 00:56:26.228622 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.11191 (* 1 = 0.11191 loss)
I1026 00:56:26.228627 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.146597 (* 1 = 0.146597 loss)
I1026 00:56:26.228631 17176 sgd_solver.cpp:106] Iteration 23460, lr = 0.001
I1026 00:56:26.786846 17176 solver.cpp:229] Iteration 23480, loss = 0.0591612
I1026 00:56:26.786878 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0524303 (* 1 = 0.0524303 loss)
I1026 00:56:26.786883 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0067309 (* 1 = 0.0067309 loss)
I1026 00:56:26.786887 17176 sgd_solver.cpp:106] Iteration 23480, lr = 0.001
I1026 00:56:27.339730 17176 solver.cpp:229] Iteration 23500, loss = 0.0401926
I1026 00:56:27.339761 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.018984 (* 1 = 0.018984 loss)
I1026 00:56:27.339766 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0212086 (* 1 = 0.0212086 loss)
I1026 00:56:27.339771 17176 sgd_solver.cpp:106] Iteration 23500, lr = 0.001
I1026 00:56:27.903883 17176 solver.cpp:229] Iteration 23520, loss = 0.181997
I1026 00:56:27.903915 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0339263 (* 1 = 0.0339263 loss)
I1026 00:56:27.903920 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.148071 (* 1 = 0.148071 loss)
I1026 00:56:27.903924 17176 sgd_solver.cpp:106] Iteration 23520, lr = 0.001
I1026 00:56:28.460505 17176 solver.cpp:229] Iteration 23540, loss = 0.041706
I1026 00:56:28.460549 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0316008 (* 1 = 0.0316008 loss)
I1026 00:56:28.460553 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0101052 (* 1 = 0.0101052 loss)
I1026 00:56:28.460557 17176 sgd_solver.cpp:106] Iteration 23540, lr = 0.001
I1026 00:56:29.022362 17176 solver.cpp:229] Iteration 23560, loss = 0.196431
I1026 00:56:29.022394 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.127402 (* 1 = 0.127402 loss)
I1026 00:56:29.022399 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0690289 (* 1 = 0.0690289 loss)
I1026 00:56:29.022404 17176 sgd_solver.cpp:106] Iteration 23560, lr = 0.001
I1026 00:56:29.582008 17176 solver.cpp:229] Iteration 23580, loss = 0.0895636
I1026 00:56:29.582052 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0646174 (* 1 = 0.0646174 loss)
I1026 00:56:29.582056 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0249462 (* 1 = 0.0249462 loss)
I1026 00:56:29.582062 17176 sgd_solver.cpp:106] Iteration 23580, lr = 0.001
I1026 00:56:30.141690 17176 solver.cpp:229] Iteration 23600, loss = 0.055287
I1026 00:56:30.141722 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0435807 (* 1 = 0.0435807 loss)
I1026 00:56:30.141726 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0117063 (* 1 = 0.0117063 loss)
I1026 00:56:30.141731 17176 sgd_solver.cpp:106] Iteration 23600, lr = 0.001
I1026 00:56:30.694556 17176 solver.cpp:229] Iteration 23620, loss = 0.255848
I1026 00:56:30.694587 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.194538 (* 1 = 0.194538 loss)
I1026 00:56:30.694592 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0613107 (* 1 = 0.0613107 loss)
I1026 00:56:30.694597 17176 sgd_solver.cpp:106] Iteration 23620, lr = 0.001
I1026 00:56:31.262006 17176 solver.cpp:229] Iteration 23640, loss = 0.0393409
I1026 00:56:31.262049 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0217075 (* 1 = 0.0217075 loss)
I1026 00:56:31.262054 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0176334 (* 1 = 0.0176334 loss)
I1026 00:56:31.262058 17176 sgd_solver.cpp:106] Iteration 23640, lr = 0.001
I1026 00:56:31.827080 17176 solver.cpp:229] Iteration 23660, loss = 0.0988702
I1026 00:56:31.827111 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0525949 (* 1 = 0.0525949 loss)
I1026 00:56:31.827116 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0462752 (* 1 = 0.0462752 loss)
I1026 00:56:31.827118 17176 sgd_solver.cpp:106] Iteration 23660, lr = 0.001
I1026 00:56:32.385807 17176 solver.cpp:229] Iteration 23680, loss = 0.412553
I1026 00:56:32.385838 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.172715 (* 1 = 0.172715 loss)
I1026 00:56:32.385843 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.239838 (* 1 = 0.239838 loss)
I1026 00:56:32.385845 17176 sgd_solver.cpp:106] Iteration 23680, lr = 0.001
I1026 00:56:32.927012 17176 solver.cpp:229] Iteration 23700, loss = 0.0607547
I1026 00:56:32.927045 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0333818 (* 1 = 0.0333818 loss)
I1026 00:56:32.927049 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0273729 (* 1 = 0.0273729 loss)
I1026 00:56:32.927053 17176 sgd_solver.cpp:106] Iteration 23700, lr = 0.001
I1026 00:56:33.479876 17176 solver.cpp:229] Iteration 23720, loss = 0.0449298
I1026 00:56:33.479907 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0300353 (* 1 = 0.0300353 loss)
I1026 00:56:33.479912 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0148945 (* 1 = 0.0148945 loss)
I1026 00:56:33.479914 17176 sgd_solver.cpp:106] Iteration 23720, lr = 0.001
I1026 00:56:34.040426 17176 solver.cpp:229] Iteration 23740, loss = 0.0371942
I1026 00:56:34.040458 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.032722 (* 1 = 0.032722 loss)
I1026 00:56:34.040463 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00447216 (* 1 = 0.00447216 loss)
I1026 00:56:34.040468 17176 sgd_solver.cpp:106] Iteration 23740, lr = 0.001
I1026 00:56:34.603860 17176 solver.cpp:229] Iteration 23760, loss = 0.0441202
I1026 00:56:34.603893 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0212652 (* 1 = 0.0212652 loss)
I1026 00:56:34.603896 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.022855 (* 1 = 0.022855 loss)
I1026 00:56:34.603900 17176 sgd_solver.cpp:106] Iteration 23760, lr = 0.001
I1026 00:56:35.150434 17176 solver.cpp:229] Iteration 23780, loss = 0.0625261
I1026 00:56:35.150467 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0358262 (* 1 = 0.0358262 loss)
I1026 00:56:35.150471 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0266999 (* 1 = 0.0266999 loss)
I1026 00:56:35.150475 17176 sgd_solver.cpp:106] Iteration 23780, lr = 0.001
I1026 00:56:35.706461 17176 solver.cpp:229] Iteration 23800, loss = 0.131125
I1026 00:56:35.706504 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0810732 (* 1 = 0.0810732 loss)
I1026 00:56:35.706509 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0500519 (* 1 = 0.0500519 loss)
I1026 00:56:35.706513 17176 sgd_solver.cpp:106] Iteration 23800, lr = 0.001
I1026 00:56:36.264679 17176 solver.cpp:229] Iteration 23820, loss = 0.084822
I1026 00:56:36.264711 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0608601 (* 1 = 0.0608601 loss)
I1026 00:56:36.264716 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0239619 (* 1 = 0.0239619 loss)
I1026 00:56:36.264720 17176 sgd_solver.cpp:106] Iteration 23820, lr = 0.001
I1026 00:56:36.831794 17176 solver.cpp:229] Iteration 23840, loss = 0.0978461
I1026 00:56:36.831825 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0319032 (* 1 = 0.0319032 loss)
I1026 00:56:36.831830 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0659429 (* 1 = 0.0659429 loss)
I1026 00:56:36.831835 17176 sgd_solver.cpp:106] Iteration 23840, lr = 0.001
I1026 00:56:37.385922 17176 solver.cpp:229] Iteration 23860, loss = 0.0680551
I1026 00:56:37.385954 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0509375 (* 1 = 0.0509375 loss)
I1026 00:56:37.385958 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0171176 (* 1 = 0.0171176 loss)
I1026 00:56:37.385963 17176 sgd_solver.cpp:106] Iteration 23860, lr = 0.001
I1026 00:56:37.948930 17176 solver.cpp:229] Iteration 23880, loss = 0.107071
I1026 00:56:37.948963 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0372233 (* 1 = 0.0372233 loss)
I1026 00:56:37.948967 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0698478 (* 1 = 0.0698478 loss)
I1026 00:56:37.948972 17176 sgd_solver.cpp:106] Iteration 23880, lr = 0.001
I1026 00:56:38.523223 17176 solver.cpp:229] Iteration 23900, loss = 0.10595
I1026 00:56:38.523255 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0956286 (* 1 = 0.0956286 loss)
I1026 00:56:38.523259 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0103211 (* 1 = 0.0103211 loss)
I1026 00:56:38.523264 17176 sgd_solver.cpp:106] Iteration 23900, lr = 0.001
I1026 00:56:39.090590 17176 solver.cpp:229] Iteration 23920, loss = 0.0556359
I1026 00:56:39.090620 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0402331 (* 1 = 0.0402331 loss)
I1026 00:56:39.090626 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0154028 (* 1 = 0.0154028 loss)
I1026 00:56:39.090629 17176 sgd_solver.cpp:106] Iteration 23920, lr = 0.001
I1026 00:56:39.642344 17176 solver.cpp:229] Iteration 23940, loss = 0.0398391
I1026 00:56:39.642375 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.032142 (* 1 = 0.032142 loss)
I1026 00:56:39.642380 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00769706 (* 1 = 0.00769706 loss)
I1026 00:56:39.642384 17176 sgd_solver.cpp:106] Iteration 23940, lr = 0.001
I1026 00:56:40.214620 17176 solver.cpp:229] Iteration 23960, loss = 0.0606006
I1026 00:56:40.214651 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0557612 (* 1 = 0.0557612 loss)
I1026 00:56:40.214656 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00483943 (* 1 = 0.00483943 loss)
I1026 00:56:40.214660 17176 sgd_solver.cpp:106] Iteration 23960, lr = 0.001
I1026 00:56:40.770782 17176 solver.cpp:229] Iteration 23980, loss = 0.0691909
I1026 00:56:40.770813 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0634008 (* 1 = 0.0634008 loss)
I1026 00:56:40.770818 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00579007 (* 1 = 0.00579007 loss)
I1026 00:56:40.770823 17176 sgd_solver.cpp:106] Iteration 23980, lr = 0.001
I1026 00:56:41.343114 17176 solver.cpp:229] Iteration 24000, loss = 0.904558
I1026 00:56:41.343152 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.345193 (* 1 = 0.345193 loss)
I1026 00:56:41.343156 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.559365 (* 1 = 0.559365 loss)
I1026 00:56:41.343160 17176 sgd_solver.cpp:106] Iteration 24000, lr = 0.001
I1026 00:56:41.894181 17176 solver.cpp:229] Iteration 24020, loss = 0.110314
I1026 00:56:41.894223 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0945813 (* 1 = 0.0945813 loss)
I1026 00:56:41.894228 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.015733 (* 1 = 0.015733 loss)
I1026 00:56:41.894232 17176 sgd_solver.cpp:106] Iteration 24020, lr = 0.001
I1026 00:56:42.455356 17176 solver.cpp:229] Iteration 24040, loss = 0.0467576
I1026 00:56:42.455407 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0332455 (* 1 = 0.0332455 loss)
I1026 00:56:42.455411 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0135121 (* 1 = 0.0135121 loss)
I1026 00:56:42.455415 17176 sgd_solver.cpp:106] Iteration 24040, lr = 0.001
I1026 00:56:43.013381 17176 solver.cpp:229] Iteration 24060, loss = 0.072515
I1026 00:56:43.013422 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0355286 (* 1 = 0.0355286 loss)
I1026 00:56:43.013427 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0369863 (* 1 = 0.0369863 loss)
I1026 00:56:43.013432 17176 sgd_solver.cpp:106] Iteration 24060, lr = 0.001
I1026 00:56:43.574178 17176 solver.cpp:229] Iteration 24080, loss = 0.10624
I1026 00:56:43.574211 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0616834 (* 1 = 0.0616834 loss)
I1026 00:56:43.574216 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0445563 (* 1 = 0.0445563 loss)
I1026 00:56:43.574220 17176 sgd_solver.cpp:106] Iteration 24080, lr = 0.001
I1026 00:56:44.149276 17176 solver.cpp:229] Iteration 24100, loss = 0.109871
I1026 00:56:44.149317 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0945873 (* 1 = 0.0945873 loss)
I1026 00:56:44.149322 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.015284 (* 1 = 0.015284 loss)
I1026 00:56:44.149325 17176 sgd_solver.cpp:106] Iteration 24100, lr = 0.001
I1026 00:56:44.703297 17176 solver.cpp:229] Iteration 24120, loss = 0.0574815
I1026 00:56:44.703337 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0400637 (* 1 = 0.0400637 loss)
I1026 00:56:44.703342 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0174177 (* 1 = 0.0174177 loss)
I1026 00:56:44.703346 17176 sgd_solver.cpp:106] Iteration 24120, lr = 0.001
I1026 00:56:45.260232 17176 solver.cpp:229] Iteration 24140, loss = 0.120307
I1026 00:56:45.260259 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0854192 (* 1 = 0.0854192 loss)
I1026 00:56:45.260264 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0348875 (* 1 = 0.0348875 loss)
I1026 00:56:45.260267 17176 sgd_solver.cpp:106] Iteration 24140, lr = 0.001
I1026 00:56:45.819036 17176 solver.cpp:229] Iteration 24160, loss = 0.0414121
I1026 00:56:45.819068 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0353464 (* 1 = 0.0353464 loss)
I1026 00:56:45.819073 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00606566 (* 1 = 0.00606566 loss)
I1026 00:56:45.819077 17176 sgd_solver.cpp:106] Iteration 24160, lr = 0.001
I1026 00:56:46.370404 17176 solver.cpp:229] Iteration 24180, loss = 0.927409
I1026 00:56:46.370436 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.43518 (* 1 = 0.43518 loss)
I1026 00:56:46.370440 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.492229 (* 1 = 0.492229 loss)
I1026 00:56:46.370445 17176 sgd_solver.cpp:106] Iteration 24180, lr = 0.001
I1026 00:56:46.947149 17176 solver.cpp:229] Iteration 24200, loss = 0.0717101
I1026 00:56:46.947180 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0520809 (* 1 = 0.0520809 loss)
I1026 00:56:46.947185 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0196292 (* 1 = 0.0196292 loss)
I1026 00:56:46.947190 17176 sgd_solver.cpp:106] Iteration 24200, lr = 0.001
I1026 00:56:47.501305 17176 solver.cpp:229] Iteration 24220, loss = 0.0493079
I1026 00:56:47.501338 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0363496 (* 1 = 0.0363496 loss)
I1026 00:56:47.501341 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0129583 (* 1 = 0.0129583 loss)
I1026 00:56:47.501346 17176 sgd_solver.cpp:106] Iteration 24220, lr = 0.001
I1026 00:56:48.056545 17176 solver.cpp:229] Iteration 24240, loss = 0.29851
I1026 00:56:48.056576 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.214189 (* 1 = 0.214189 loss)
I1026 00:56:48.056581 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0843217 (* 1 = 0.0843217 loss)
I1026 00:56:48.056586 17176 sgd_solver.cpp:106] Iteration 24240, lr = 0.001
I1026 00:56:48.622318 17176 solver.cpp:229] Iteration 24260, loss = 0.123393
I1026 00:56:48.622349 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0787965 (* 1 = 0.0787965 loss)
I1026 00:56:48.622354 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.044597 (* 1 = 0.044597 loss)
I1026 00:56:48.622359 17176 sgd_solver.cpp:106] Iteration 24260, lr = 0.001
I1026 00:56:49.181504 17176 solver.cpp:229] Iteration 24280, loss = 0.0791652
I1026 00:56:49.181538 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0528974 (* 1 = 0.0528974 loss)
I1026 00:56:49.181543 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0262678 (* 1 = 0.0262678 loss)
I1026 00:56:49.181560 17176 sgd_solver.cpp:106] Iteration 24280, lr = 0.001
I1026 00:56:49.739713 17176 solver.cpp:229] Iteration 24300, loss = 0.176212
I1026 00:56:49.739745 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.1289 (* 1 = 0.1289 loss)
I1026 00:56:49.739750 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0473121 (* 1 = 0.0473121 loss)
I1026 00:56:49.739754 17176 sgd_solver.cpp:106] Iteration 24300, lr = 0.001
I1026 00:56:50.295189 17176 solver.cpp:229] Iteration 24320, loss = 0.102784
I1026 00:56:50.295231 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0608197 (* 1 = 0.0608197 loss)
I1026 00:56:50.295236 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0419646 (* 1 = 0.0419646 loss)
I1026 00:56:50.295241 17176 sgd_solver.cpp:106] Iteration 24320, lr = 0.001
I1026 00:56:50.843983 17176 solver.cpp:229] Iteration 24340, loss = 0.0356399
I1026 00:56:50.844017 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0157728 (* 1 = 0.0157728 loss)
I1026 00:56:50.844022 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0198671 (* 1 = 0.0198671 loss)
I1026 00:56:50.844025 17176 sgd_solver.cpp:106] Iteration 24340, lr = 0.001
I1026 00:56:51.402102 17176 solver.cpp:229] Iteration 24360, loss = 0.418247
I1026 00:56:51.402134 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.243895 (* 1 = 0.243895 loss)
I1026 00:56:51.402138 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.174352 (* 1 = 0.174352 loss)
I1026 00:56:51.402143 17176 sgd_solver.cpp:106] Iteration 24360, lr = 0.001
I1026 00:56:51.969557 17176 solver.cpp:229] Iteration 24380, loss = 0.0602923
I1026 00:56:51.969589 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0316163 (* 1 = 0.0316163 loss)
I1026 00:56:51.969594 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0286761 (* 1 = 0.0286761 loss)
I1026 00:56:51.969599 17176 sgd_solver.cpp:106] Iteration 24380, lr = 0.001
I1026 00:56:52.534821 17176 solver.cpp:229] Iteration 24400, loss = 0.140542
I1026 00:56:52.534864 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0747114 (* 1 = 0.0747114 loss)
I1026 00:56:52.534878 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0658306 (* 1 = 0.0658306 loss)
I1026 00:56:52.534883 17176 sgd_solver.cpp:106] Iteration 24400, lr = 0.001
I1026 00:56:53.090945 17176 solver.cpp:229] Iteration 24420, loss = 0.149453
I1026 00:56:53.090986 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.134847 (* 1 = 0.134847 loss)
I1026 00:56:53.090991 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0146059 (* 1 = 0.0146059 loss)
I1026 00:56:53.090996 17176 sgd_solver.cpp:106] Iteration 24420, lr = 0.001
I1026 00:56:53.638221 17176 solver.cpp:229] Iteration 24440, loss = 0.0606298
I1026 00:56:53.638252 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0268585 (* 1 = 0.0268585 loss)
I1026 00:56:53.638257 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0337713 (* 1 = 0.0337713 loss)
I1026 00:56:53.638262 17176 sgd_solver.cpp:106] Iteration 24440, lr = 0.001
I1026 00:56:54.193382 17176 solver.cpp:229] Iteration 24460, loss = 0.041933
I1026 00:56:54.193414 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0358748 (* 1 = 0.0358748 loss)
I1026 00:56:54.193419 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00605818 (* 1 = 0.00605818 loss)
I1026 00:56:54.193423 17176 sgd_solver.cpp:106] Iteration 24460, lr = 0.001
I1026 00:56:54.752451 17176 solver.cpp:229] Iteration 24480, loss = 0.155737
I1026 00:56:54.752483 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.115635 (* 1 = 0.115635 loss)
I1026 00:56:54.752488 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0401019 (* 1 = 0.0401019 loss)
I1026 00:56:54.752493 17176 sgd_solver.cpp:106] Iteration 24480, lr = 0.001
I1026 00:56:55.323328 17176 solver.cpp:229] Iteration 24500, loss = 0.0775207
I1026 00:56:55.323376 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0466291 (* 1 = 0.0466291 loss)
I1026 00:56:55.323390 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0308916 (* 1 = 0.0308916 loss)
I1026 00:56:55.323395 17176 sgd_solver.cpp:106] Iteration 24500, lr = 0.001
I1026 00:56:55.877316 17176 solver.cpp:229] Iteration 24520, loss = 0.0859122
I1026 00:56:55.877348 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.055535 (* 1 = 0.055535 loss)
I1026 00:56:55.877353 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0303772 (* 1 = 0.0303772 loss)
I1026 00:56:55.877357 17176 sgd_solver.cpp:106] Iteration 24520, lr = 0.001
I1026 00:56:56.437376 17176 solver.cpp:229] Iteration 24540, loss = 0.10834
I1026 00:56:56.437415 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.104383 (* 1 = 0.104383 loss)
I1026 00:56:56.437429 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00395691 (* 1 = 0.00395691 loss)
I1026 00:56:56.437433 17176 sgd_solver.cpp:106] Iteration 24540, lr = 0.001
I1026 00:56:56.992008 17176 solver.cpp:229] Iteration 24560, loss = 0.272938
I1026 00:56:56.992041 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.111466 (* 1 = 0.111466 loss)
I1026 00:56:56.992046 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.161472 (* 1 = 0.161472 loss)
I1026 00:56:56.992050 17176 sgd_solver.cpp:106] Iteration 24560, lr = 0.001
I1026 00:56:57.561975 17176 solver.cpp:229] Iteration 24580, loss = 0.0651156
I1026 00:56:57.562006 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0583263 (* 1 = 0.0583263 loss)
I1026 00:56:57.562011 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00678933 (* 1 = 0.00678933 loss)
I1026 00:56:57.562016 17176 sgd_solver.cpp:106] Iteration 24580, lr = 0.001
I1026 00:56:58.122066 17176 solver.cpp:229] Iteration 24600, loss = 0.0426198
I1026 00:56:58.122108 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0257394 (* 1 = 0.0257394 loss)
I1026 00:56:58.122113 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0168804 (* 1 = 0.0168804 loss)
I1026 00:56:58.122117 17176 sgd_solver.cpp:106] Iteration 24600, lr = 0.001
I1026 00:56:58.685860 17176 solver.cpp:229] Iteration 24620, loss = 0.0193321
I1026 00:56:58.685904 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0147404 (* 1 = 0.0147404 loss)
I1026 00:56:58.685909 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00459176 (* 1 = 0.00459176 loss)
I1026 00:56:58.685914 17176 sgd_solver.cpp:106] Iteration 24620, lr = 0.001
I1026 00:56:59.253432 17176 solver.cpp:229] Iteration 24640, loss = 0.0813006
I1026 00:56:59.253474 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0208821 (* 1 = 0.0208821 loss)
I1026 00:56:59.253479 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0604185 (* 1 = 0.0604185 loss)
I1026 00:56:59.253482 17176 sgd_solver.cpp:106] Iteration 24640, lr = 0.001
I1026 00:56:59.807060 17176 solver.cpp:229] Iteration 24660, loss = 0.0599009
I1026 00:56:59.807093 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0296091 (* 1 = 0.0296091 loss)
I1026 00:56:59.807098 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0302919 (* 1 = 0.0302919 loss)
I1026 00:56:59.807102 17176 sgd_solver.cpp:106] Iteration 24660, lr = 0.001
I1026 00:57:00.372625 17176 solver.cpp:229] Iteration 24680, loss = 0.303177
I1026 00:57:00.372658 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.130684 (* 1 = 0.130684 loss)
I1026 00:57:00.372661 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.172494 (* 1 = 0.172494 loss)
I1026 00:57:00.372665 17176 sgd_solver.cpp:106] Iteration 24680, lr = 0.001
I1026 00:57:00.936298 17176 solver.cpp:229] Iteration 24700, loss = 0.269375
I1026 00:57:00.936338 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0903736 (* 1 = 0.0903736 loss)
I1026 00:57:00.936343 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.179002 (* 1 = 0.179002 loss)
I1026 00:57:00.936347 17176 sgd_solver.cpp:106] Iteration 24700, lr = 0.001
I1026 00:57:01.490396 17176 solver.cpp:229] Iteration 24720, loss = 0.0314401
I1026 00:57:01.490428 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0293954 (* 1 = 0.0293954 loss)
I1026 00:57:01.490433 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00204472 (* 1 = 0.00204472 loss)
I1026 00:57:01.490437 17176 sgd_solver.cpp:106] Iteration 24720, lr = 0.001
I1026 00:57:02.056192 17176 solver.cpp:229] Iteration 24740, loss = 0.30565
I1026 00:57:02.056224 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.105756 (* 1 = 0.105756 loss)
I1026 00:57:02.056228 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.199894 (* 1 = 0.199894 loss)
I1026 00:57:02.056233 17176 sgd_solver.cpp:106] Iteration 24740, lr = 0.001
I1026 00:57:02.610152 17176 solver.cpp:229] Iteration 24760, loss = 0.14214
I1026 00:57:02.610184 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0333387 (* 1 = 0.0333387 loss)
I1026 00:57:02.610189 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.108801 (* 1 = 0.108801 loss)
I1026 00:57:02.610193 17176 sgd_solver.cpp:106] Iteration 24760, lr = 0.001
I1026 00:57:03.164119 17176 solver.cpp:229] Iteration 24780, loss = 0.0313939
I1026 00:57:03.164150 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.028118 (* 1 = 0.028118 loss)
I1026 00:57:03.164155 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00327588 (* 1 = 0.00327588 loss)
I1026 00:57:03.164160 17176 sgd_solver.cpp:106] Iteration 24780, lr = 0.001
I1026 00:57:03.708964 17176 solver.cpp:229] Iteration 24800, loss = 0.0389474
I1026 00:57:03.708997 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0332567 (* 1 = 0.0332567 loss)
I1026 00:57:03.709002 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00569066 (* 1 = 0.00569066 loss)
I1026 00:57:03.709007 17176 sgd_solver.cpp:106] Iteration 24800, lr = 0.001
I1026 00:57:04.263424 17176 solver.cpp:229] Iteration 24820, loss = 0.0852753
I1026 00:57:04.263461 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0666663 (* 1 = 0.0666663 loss)
I1026 00:57:04.263466 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.018609 (* 1 = 0.018609 loss)
I1026 00:57:04.263470 17176 sgd_solver.cpp:106] Iteration 24820, lr = 0.001
I1026 00:57:04.819701 17176 solver.cpp:229] Iteration 24840, loss = 0.0746705
I1026 00:57:04.819732 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0436947 (* 1 = 0.0436947 loss)
I1026 00:57:04.819737 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0309758 (* 1 = 0.0309758 loss)
I1026 00:57:04.819742 17176 sgd_solver.cpp:106] Iteration 24840, lr = 0.001
I1026 00:57:05.375988 17176 solver.cpp:229] Iteration 24860, loss = 0.188729
I1026 00:57:05.376019 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.132268 (* 1 = 0.132268 loss)
I1026 00:57:05.376025 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0564611 (* 1 = 0.0564611 loss)
I1026 00:57:05.376029 17176 sgd_solver.cpp:106] Iteration 24860, lr = 0.001
I1026 00:57:05.933176 17176 solver.cpp:229] Iteration 24880, loss = 0.0503818
I1026 00:57:05.933210 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0356101 (* 1 = 0.0356101 loss)
I1026 00:57:05.933215 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0147717 (* 1 = 0.0147717 loss)
I1026 00:57:05.933219 17176 sgd_solver.cpp:106] Iteration 24880, lr = 0.001
I1026 00:57:06.488627 17176 solver.cpp:229] Iteration 24900, loss = 0.0445268
I1026 00:57:06.488659 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0229779 (* 1 = 0.0229779 loss)
I1026 00:57:06.488664 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0215489 (* 1 = 0.0215489 loss)
I1026 00:57:06.488669 17176 sgd_solver.cpp:106] Iteration 24900, lr = 0.001
I1026 00:57:07.052554 17176 solver.cpp:229] Iteration 24920, loss = 0.0244065
I1026 00:57:07.052587 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0190578 (* 1 = 0.0190578 loss)
I1026 00:57:07.052592 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00534868 (* 1 = 0.00534868 loss)
I1026 00:57:07.052597 17176 sgd_solver.cpp:106] Iteration 24920, lr = 0.001
I1026 00:57:07.612511 17176 solver.cpp:229] Iteration 24940, loss = 0.0546339
I1026 00:57:07.612543 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.048145 (* 1 = 0.048145 loss)
I1026 00:57:07.612547 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00648894 (* 1 = 0.00648894 loss)
I1026 00:57:07.612552 17176 sgd_solver.cpp:106] Iteration 24940, lr = 0.001
I1026 00:57:08.189481 17176 solver.cpp:229] Iteration 24960, loss = 0.078464
I1026 00:57:08.189523 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0442791 (* 1 = 0.0442791 loss)
I1026 00:57:08.189527 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0341848 (* 1 = 0.0341848 loss)
I1026 00:57:08.189532 17176 sgd_solver.cpp:106] Iteration 24960, lr = 0.001
I1026 00:57:08.740816 17176 solver.cpp:229] Iteration 24980, loss = 0.115062
I1026 00:57:08.740847 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0959185 (* 1 = 0.0959185 loss)
I1026 00:57:08.740852 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0191439 (* 1 = 0.0191439 loss)
I1026 00:57:08.740856 17176 sgd_solver.cpp:106] Iteration 24980, lr = 0.001
I1026 00:57:09.291275 17176 solver.cpp:229] Iteration 25000, loss = 0.0611581
I1026 00:57:09.291308 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0352626 (* 1 = 0.0352626 loss)
I1026 00:57:09.291313 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0258955 (* 1 = 0.0258955 loss)
I1026 00:57:09.291317 17176 sgd_solver.cpp:106] Iteration 25000, lr = 0.001
I1026 00:57:09.842396 17176 solver.cpp:229] Iteration 25020, loss = 0.08643
I1026 00:57:09.842427 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0247211 (* 1 = 0.0247211 loss)
I1026 00:57:09.842432 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0617089 (* 1 = 0.0617089 loss)
I1026 00:57:09.842435 17176 sgd_solver.cpp:106] Iteration 25020, lr = 0.001
I1026 00:57:10.418731 17176 solver.cpp:229] Iteration 25040, loss = 0.0349393
I1026 00:57:10.418763 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0331955 (* 1 = 0.0331955 loss)
I1026 00:57:10.418768 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00174377 (* 1 = 0.00174377 loss)
I1026 00:57:10.418772 17176 sgd_solver.cpp:106] Iteration 25040, lr = 0.001
I1026 00:57:10.990772 17176 solver.cpp:229] Iteration 25060, loss = 0.228946
I1026 00:57:10.990803 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.145831 (* 1 = 0.145831 loss)
I1026 00:57:10.990808 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0831147 (* 1 = 0.0831147 loss)
I1026 00:57:10.990813 17176 sgd_solver.cpp:106] Iteration 25060, lr = 0.001
I1026 00:57:11.559073 17176 solver.cpp:229] Iteration 25080, loss = 0.565857
I1026 00:57:11.559108 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.16841 (* 1 = 0.16841 loss)
I1026 00:57:11.559113 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.397447 (* 1 = 0.397447 loss)
I1026 00:57:11.559118 17176 sgd_solver.cpp:106] Iteration 25080, lr = 0.001
I1026 00:57:12.119712 17176 solver.cpp:229] Iteration 25100, loss = 0.0277635
I1026 00:57:12.119743 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0264581 (* 1 = 0.0264581 loss)
I1026 00:57:12.119748 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00130546 (* 1 = 0.00130546 loss)
I1026 00:57:12.119752 17176 sgd_solver.cpp:106] Iteration 25100, lr = 0.001
I1026 00:57:12.687750 17176 solver.cpp:229] Iteration 25120, loss = 0.0646132
I1026 00:57:12.687783 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0552369 (* 1 = 0.0552369 loss)
I1026 00:57:12.687788 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00937632 (* 1 = 0.00937632 loss)
I1026 00:57:12.687791 17176 sgd_solver.cpp:106] Iteration 25120, lr = 0.001
I1026 00:57:13.264794 17176 solver.cpp:229] Iteration 25140, loss = 0.0440344
I1026 00:57:13.264827 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.036393 (* 1 = 0.036393 loss)
I1026 00:57:13.264832 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0076414 (* 1 = 0.0076414 loss)
I1026 00:57:13.264837 17176 sgd_solver.cpp:106] Iteration 25140, lr = 0.001
I1026 00:57:13.820832 17176 solver.cpp:229] Iteration 25160, loss = 0.0677485
I1026 00:57:13.820864 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0465138 (* 1 = 0.0465138 loss)
I1026 00:57:13.820868 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0212346 (* 1 = 0.0212346 loss)
I1026 00:57:13.820873 17176 sgd_solver.cpp:106] Iteration 25160, lr = 0.001
I1026 00:57:14.369143 17176 solver.cpp:229] Iteration 25180, loss = 0.0451361
I1026 00:57:14.369174 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0271159 (* 1 = 0.0271159 loss)
I1026 00:57:14.369179 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0180202 (* 1 = 0.0180202 loss)
I1026 00:57:14.369182 17176 sgd_solver.cpp:106] Iteration 25180, lr = 0.001
I1026 00:57:14.918256 17176 solver.cpp:229] Iteration 25200, loss = 0.0588651
I1026 00:57:14.918288 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0310914 (* 1 = 0.0310914 loss)
I1026 00:57:14.918292 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0277736 (* 1 = 0.0277736 loss)
I1026 00:57:14.918296 17176 sgd_solver.cpp:106] Iteration 25200, lr = 0.001
I1026 00:57:15.467977 17176 solver.cpp:229] Iteration 25220, loss = 0.0565352
I1026 00:57:15.468019 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0424524 (* 1 = 0.0424524 loss)
I1026 00:57:15.468024 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0140828 (* 1 = 0.0140828 loss)
I1026 00:57:15.468026 17176 sgd_solver.cpp:106] Iteration 25220, lr = 0.001
I1026 00:57:16.025200 17176 solver.cpp:229] Iteration 25240, loss = 0.0464018
I1026 00:57:16.025233 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0318856 (* 1 = 0.0318856 loss)
I1026 00:57:16.025236 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0145163 (* 1 = 0.0145163 loss)
I1026 00:57:16.025240 17176 sgd_solver.cpp:106] Iteration 25240, lr = 0.001
I1026 00:57:16.587204 17176 solver.cpp:229] Iteration 25260, loss = 0.0639035
I1026 00:57:16.587236 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0485565 (* 1 = 0.0485565 loss)
I1026 00:57:16.587241 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0153471 (* 1 = 0.0153471 loss)
I1026 00:57:16.587245 17176 sgd_solver.cpp:106] Iteration 25260, lr = 0.001
I1026 00:57:17.137487 17176 solver.cpp:229] Iteration 25280, loss = 0.0737783
I1026 00:57:17.137519 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0610808 (* 1 = 0.0610808 loss)
I1026 00:57:17.137524 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0126976 (* 1 = 0.0126976 loss)
I1026 00:57:17.137542 17176 sgd_solver.cpp:106] Iteration 25280, lr = 0.001
I1026 00:57:17.692404 17176 solver.cpp:229] Iteration 25300, loss = 0.0945276
I1026 00:57:17.692437 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0581025 (* 1 = 0.0581025 loss)
I1026 00:57:17.692442 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0364251 (* 1 = 0.0364251 loss)
I1026 00:57:17.692446 17176 sgd_solver.cpp:106] Iteration 25300, lr = 0.001
I1026 00:57:18.246814 17176 solver.cpp:229] Iteration 25320, loss = 0.0907958
I1026 00:57:18.246846 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0330525 (* 1 = 0.0330525 loss)
I1026 00:57:18.246851 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0577433 (* 1 = 0.0577433 loss)
I1026 00:57:18.246855 17176 sgd_solver.cpp:106] Iteration 25320, lr = 0.001
I1026 00:57:18.803665 17176 solver.cpp:229] Iteration 25340, loss = 0.0743598
I1026 00:57:18.803699 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0416139 (* 1 = 0.0416139 loss)
I1026 00:57:18.803704 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0327459 (* 1 = 0.0327459 loss)
I1026 00:57:18.803707 17176 sgd_solver.cpp:106] Iteration 25340, lr = 0.001
I1026 00:57:19.363874 17176 solver.cpp:229] Iteration 25360, loss = 0.0494658
I1026 00:57:19.363906 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0447159 (* 1 = 0.0447159 loss)
I1026 00:57:19.363911 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00474992 (* 1 = 0.00474992 loss)
I1026 00:57:19.363915 17176 sgd_solver.cpp:106] Iteration 25360, lr = 0.001
I1026 00:57:19.937376 17176 solver.cpp:229] Iteration 25380, loss = 0.135329
I1026 00:57:19.937407 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.110202 (* 1 = 0.110202 loss)
I1026 00:57:19.937412 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0251271 (* 1 = 0.0251271 loss)
I1026 00:57:19.937415 17176 sgd_solver.cpp:106] Iteration 25380, lr = 0.001
I1026 00:57:20.497634 17176 solver.cpp:229] Iteration 25400, loss = 0.0676294
I1026 00:57:20.497668 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0404122 (* 1 = 0.0404122 loss)
I1026 00:57:20.497671 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0272172 (* 1 = 0.0272172 loss)
I1026 00:57:20.497675 17176 sgd_solver.cpp:106] Iteration 25400, lr = 0.001
I1026 00:57:21.057509 17176 solver.cpp:229] Iteration 25420, loss = 0.105428
I1026 00:57:21.057543 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0962427 (* 1 = 0.0962427 loss)
I1026 00:57:21.057548 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00918574 (* 1 = 0.00918574 loss)
I1026 00:57:21.057551 17176 sgd_solver.cpp:106] Iteration 25420, lr = 0.001
I1026 00:57:21.620759 17176 solver.cpp:229] Iteration 25440, loss = 0.0477519
I1026 00:57:21.620790 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.031843 (* 1 = 0.031843 loss)
I1026 00:57:21.620795 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0159088 (* 1 = 0.0159088 loss)
I1026 00:57:21.620798 17176 sgd_solver.cpp:106] Iteration 25440, lr = 0.001
I1026 00:57:22.177907 17176 solver.cpp:229] Iteration 25460, loss = 0.384511
I1026 00:57:22.177938 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.112189 (* 1 = 0.112189 loss)
I1026 00:57:22.177943 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.272322 (* 1 = 0.272322 loss)
I1026 00:57:22.177947 17176 sgd_solver.cpp:106] Iteration 25460, lr = 0.001
I1026 00:57:22.738649 17176 solver.cpp:229] Iteration 25480, loss = 0.0911343
I1026 00:57:22.738690 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0667228 (* 1 = 0.0667228 loss)
I1026 00:57:22.738695 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0244115 (* 1 = 0.0244115 loss)
I1026 00:57:22.738699 17176 sgd_solver.cpp:106] Iteration 25480, lr = 0.001
I1026 00:57:23.311381 17176 solver.cpp:229] Iteration 25500, loss = 0.0457213
I1026 00:57:23.311424 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0393168 (* 1 = 0.0393168 loss)
I1026 00:57:23.311434 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00640453 (* 1 = 0.00640453 loss)
I1026 00:57:23.311447 17176 sgd_solver.cpp:106] Iteration 25500, lr = 0.001
I1026 00:57:23.863087 17176 solver.cpp:229] Iteration 25520, loss = 0.0376148
I1026 00:57:23.863121 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.034576 (* 1 = 0.034576 loss)
I1026 00:57:23.863126 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00303884 (* 1 = 0.00303884 loss)
I1026 00:57:23.863129 17176 sgd_solver.cpp:106] Iteration 25520, lr = 0.001
I1026 00:57:24.421969 17176 solver.cpp:229] Iteration 25540, loss = 0.086003
I1026 00:57:24.422003 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0808012 (* 1 = 0.0808012 loss)
I1026 00:57:24.422008 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00520184 (* 1 = 0.00520184 loss)
I1026 00:57:24.422011 17176 sgd_solver.cpp:106] Iteration 25540, lr = 0.001
I1026 00:57:24.982208 17176 solver.cpp:229] Iteration 25560, loss = 0.0713011
I1026 00:57:24.982240 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0236695 (* 1 = 0.0236695 loss)
I1026 00:57:24.982245 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0476316 (* 1 = 0.0476316 loss)
I1026 00:57:24.982249 17176 sgd_solver.cpp:106] Iteration 25560, lr = 0.001
I1026 00:57:25.548990 17176 solver.cpp:229] Iteration 25580, loss = 0.0519033
I1026 00:57:25.549022 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0129242 (* 1 = 0.0129242 loss)
I1026 00:57:25.549026 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0389791 (* 1 = 0.0389791 loss)
I1026 00:57:25.549031 17176 sgd_solver.cpp:106] Iteration 25580, lr = 0.001
I1026 00:57:26.115669 17176 solver.cpp:229] Iteration 25600, loss = 0.0634813
I1026 00:57:26.115720 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0330112 (* 1 = 0.0330112 loss)
I1026 00:57:26.115725 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0304701 (* 1 = 0.0304701 loss)
I1026 00:57:26.115728 17176 sgd_solver.cpp:106] Iteration 25600, lr = 0.001
I1026 00:57:26.681004 17176 solver.cpp:229] Iteration 25620, loss = 0.0386571
I1026 00:57:26.681035 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0323325 (* 1 = 0.0323325 loss)
I1026 00:57:26.681041 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00632458 (* 1 = 0.00632458 loss)
I1026 00:57:26.681044 17176 sgd_solver.cpp:106] Iteration 25620, lr = 0.001
I1026 00:57:27.246948 17176 solver.cpp:229] Iteration 25640, loss = 0.043887
I1026 00:57:27.246990 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0154927 (* 1 = 0.0154927 loss)
I1026 00:57:27.246995 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0283943 (* 1 = 0.0283943 loss)
I1026 00:57:27.246999 17176 sgd_solver.cpp:106] Iteration 25640, lr = 0.001
I1026 00:57:27.809028 17176 solver.cpp:229] Iteration 25660, loss = 0.0705941
I1026 00:57:27.809061 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0220547 (* 1 = 0.0220547 loss)
I1026 00:57:27.809064 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0485394 (* 1 = 0.0485394 loss)
I1026 00:57:27.809068 17176 sgd_solver.cpp:106] Iteration 25660, lr = 0.001
I1026 00:57:28.366031 17176 solver.cpp:229] Iteration 25680, loss = 0.0534639
I1026 00:57:28.366065 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0185888 (* 1 = 0.0185888 loss)
I1026 00:57:28.366070 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0348751 (* 1 = 0.0348751 loss)
I1026 00:57:28.366073 17176 sgd_solver.cpp:106] Iteration 25680, lr = 0.001
I1026 00:57:28.929839 17176 solver.cpp:229] Iteration 25700, loss = 0.0595554
I1026 00:57:28.929882 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0348074 (* 1 = 0.0348074 loss)
I1026 00:57:28.929886 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0247481 (* 1 = 0.0247481 loss)
I1026 00:57:28.929890 17176 sgd_solver.cpp:106] Iteration 25700, lr = 0.001
I1026 00:57:29.490378 17176 solver.cpp:229] Iteration 25720, loss = 0.0447148
I1026 00:57:29.490411 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0399526 (* 1 = 0.0399526 loss)
I1026 00:57:29.490414 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00476216 (* 1 = 0.00476216 loss)
I1026 00:57:29.490419 17176 sgd_solver.cpp:106] Iteration 25720, lr = 0.001
I1026 00:57:30.052415 17176 solver.cpp:229] Iteration 25740, loss = 0.0581045
I1026 00:57:30.052448 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0298253 (* 1 = 0.0298253 loss)
I1026 00:57:30.052462 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0282793 (* 1 = 0.0282793 loss)
I1026 00:57:30.052466 17176 sgd_solver.cpp:106] Iteration 25740, lr = 0.001
I1026 00:57:30.598815 17176 solver.cpp:229] Iteration 25760, loss = 1.02237
I1026 00:57:30.598847 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.311016 (* 1 = 0.311016 loss)
I1026 00:57:30.598851 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.711355 (* 1 = 0.711355 loss)
I1026 00:57:30.598856 17176 sgd_solver.cpp:106] Iteration 25760, lr = 0.001
I1026 00:57:31.167115 17176 solver.cpp:229] Iteration 25780, loss = 0.0268126
I1026 00:57:31.167146 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.015599 (* 1 = 0.015599 loss)
I1026 00:57:31.167151 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0112135 (* 1 = 0.0112135 loss)
I1026 00:57:31.167156 17176 sgd_solver.cpp:106] Iteration 25780, lr = 0.001
I1026 00:57:31.730123 17176 solver.cpp:229] Iteration 25800, loss = 0.0116356
I1026 00:57:31.730170 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00720377 (* 1 = 0.00720377 loss)
I1026 00:57:31.730183 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00443182 (* 1 = 0.00443182 loss)
I1026 00:57:31.730187 17176 sgd_solver.cpp:106] Iteration 25800, lr = 0.001
I1026 00:57:32.298261 17176 solver.cpp:229] Iteration 25820, loss = 0.0860501
I1026 00:57:32.298295 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0819194 (* 1 = 0.0819194 loss)
I1026 00:57:32.298300 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00413063 (* 1 = 0.00413063 loss)
I1026 00:57:32.298305 17176 sgd_solver.cpp:106] Iteration 25820, lr = 0.001
I1026 00:57:32.868587 17176 solver.cpp:229] Iteration 25840, loss = 0.161056
I1026 00:57:32.868618 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0413069 (* 1 = 0.0413069 loss)
I1026 00:57:32.868623 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.119749 (* 1 = 0.119749 loss)
I1026 00:57:32.868626 17176 sgd_solver.cpp:106] Iteration 25840, lr = 0.001
I1026 00:57:33.428510 17176 solver.cpp:229] Iteration 25860, loss = 0.0476594
I1026 00:57:33.428544 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0425295 (* 1 = 0.0425295 loss)
I1026 00:57:33.428547 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00512988 (* 1 = 0.00512988 loss)
I1026 00:57:33.428553 17176 sgd_solver.cpp:106] Iteration 25860, lr = 0.001
I1026 00:57:33.980989 17176 solver.cpp:229] Iteration 25880, loss = 0.0510465
I1026 00:57:33.981019 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0366301 (* 1 = 0.0366301 loss)
I1026 00:57:33.981024 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0144165 (* 1 = 0.0144165 loss)
I1026 00:57:33.981027 17176 sgd_solver.cpp:106] Iteration 25880, lr = 0.001
I1026 00:57:34.535950 17176 solver.cpp:229] Iteration 25900, loss = 0.178193
I1026 00:57:34.535992 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0634961 (* 1 = 0.0634961 loss)
I1026 00:57:34.535997 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.114697 (* 1 = 0.114697 loss)
I1026 00:57:34.536001 17176 sgd_solver.cpp:106] Iteration 25900, lr = 0.001
I1026 00:57:35.084693 17176 solver.cpp:229] Iteration 25920, loss = 0.04841
I1026 00:57:35.084727 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0401084 (* 1 = 0.0401084 loss)
I1026 00:57:35.084748 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00830154 (* 1 = 0.00830154 loss)
I1026 00:57:35.084751 17176 sgd_solver.cpp:106] Iteration 25920, lr = 0.001
I1026 00:57:35.645368 17176 solver.cpp:229] Iteration 25940, loss = 0.0430152
I1026 00:57:35.645401 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0393797 (* 1 = 0.0393797 loss)
I1026 00:57:35.645406 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00363545 (* 1 = 0.00363545 loss)
I1026 00:57:35.645409 17176 sgd_solver.cpp:106] Iteration 25940, lr = 0.001
I1026 00:57:36.200225 17176 solver.cpp:229] Iteration 25960, loss = 0.118194
I1026 00:57:36.200258 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.108821 (* 1 = 0.108821 loss)
I1026 00:57:36.200263 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00937331 (* 1 = 0.00937331 loss)
I1026 00:57:36.200266 17176 sgd_solver.cpp:106] Iteration 25960, lr = 0.001
I1026 00:57:36.755547 17176 solver.cpp:229] Iteration 25980, loss = 0.0468903
I1026 00:57:36.755574 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0227345 (* 1 = 0.0227345 loss)
I1026 00:57:36.755579 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0241558 (* 1 = 0.0241558 loss)
I1026 00:57:36.755584 17176 sgd_solver.cpp:106] Iteration 25980, lr = 0.001
I1026 00:57:37.298215 17176 solver.cpp:229] Iteration 26000, loss = 0.0227451
I1026 00:57:37.298249 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0120468 (* 1 = 0.0120468 loss)
I1026 00:57:37.298254 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0106983 (* 1 = 0.0106983 loss)
I1026 00:57:37.298257 17176 sgd_solver.cpp:106] Iteration 26000, lr = 0.001
I1026 00:57:37.871273 17176 solver.cpp:229] Iteration 26020, loss = 0.021256
I1026 00:57:37.871304 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0197889 (* 1 = 0.0197889 loss)
I1026 00:57:37.871309 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00146716 (* 1 = 0.00146716 loss)
I1026 00:57:37.871314 17176 sgd_solver.cpp:106] Iteration 26020, lr = 0.001
I1026 00:57:38.429797 17176 solver.cpp:229] Iteration 26040, loss = 0.0814582
I1026 00:57:38.429841 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0447038 (* 1 = 0.0447038 loss)
I1026 00:57:38.429844 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0367544 (* 1 = 0.0367544 loss)
I1026 00:57:38.429848 17176 sgd_solver.cpp:106] Iteration 26040, lr = 0.001
I1026 00:57:38.984189 17176 solver.cpp:229] Iteration 26060, loss = 0.0331617
I1026 00:57:38.984231 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0172313 (* 1 = 0.0172313 loss)
I1026 00:57:38.984236 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0159304 (* 1 = 0.0159304 loss)
I1026 00:57:38.984239 17176 sgd_solver.cpp:106] Iteration 26060, lr = 0.001
I1026 00:57:39.543684 17176 solver.cpp:229] Iteration 26080, loss = 0.0786502
I1026 00:57:39.543716 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0520101 (* 1 = 0.0520101 loss)
I1026 00:57:39.543720 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.02664 (* 1 = 0.02664 loss)
I1026 00:57:39.543725 17176 sgd_solver.cpp:106] Iteration 26080, lr = 0.001
I1026 00:57:40.101341 17176 solver.cpp:229] Iteration 26100, loss = 0.183239
I1026 00:57:40.101372 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.160406 (* 1 = 0.160406 loss)
I1026 00:57:40.101377 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.022833 (* 1 = 0.022833 loss)
I1026 00:57:40.101380 17176 sgd_solver.cpp:106] Iteration 26100, lr = 0.001
I1026 00:57:40.665738 17176 solver.cpp:229] Iteration 26120, loss = 0.0653539
I1026 00:57:40.665771 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.026107 (* 1 = 0.026107 loss)
I1026 00:57:40.665774 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0392468 (* 1 = 0.0392468 loss)
I1026 00:57:40.665781 17176 sgd_solver.cpp:106] Iteration 26120, lr = 0.001
I1026 00:57:41.228353 17176 solver.cpp:229] Iteration 26140, loss = 0.0673293
I1026 00:57:41.228386 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0406781 (* 1 = 0.0406781 loss)
I1026 00:57:41.228391 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0266512 (* 1 = 0.0266512 loss)
I1026 00:57:41.228394 17176 sgd_solver.cpp:106] Iteration 26140, lr = 0.001
I1026 00:57:41.780268 17176 solver.cpp:229] Iteration 26160, loss = 0.152266
I1026 00:57:41.780300 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0724484 (* 1 = 0.0724484 loss)
I1026 00:57:41.780305 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0798171 (* 1 = 0.0798171 loss)
I1026 00:57:41.780309 17176 sgd_solver.cpp:106] Iteration 26160, lr = 0.001
I1026 00:57:42.339715 17176 solver.cpp:229] Iteration 26180, loss = 0.0661098
I1026 00:57:42.339750 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0419605 (* 1 = 0.0419605 loss)
I1026 00:57:42.339756 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0241493 (* 1 = 0.0241493 loss)
I1026 00:57:42.339761 17176 sgd_solver.cpp:106] Iteration 26180, lr = 0.001
I1026 00:57:42.906795 17176 solver.cpp:229] Iteration 26200, loss = 0.052353
I1026 00:57:42.906831 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0447247 (* 1 = 0.0447247 loss)
I1026 00:57:42.906837 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00762831 (* 1 = 0.00762831 loss)
I1026 00:57:42.906843 17176 sgd_solver.cpp:106] Iteration 26200, lr = 0.001
I1026 00:57:43.457516 17176 solver.cpp:229] Iteration 26220, loss = 0.178755
I1026 00:57:43.457551 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0580879 (* 1 = 0.0580879 loss)
I1026 00:57:43.457558 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.120667 (* 1 = 0.120667 loss)
I1026 00:57:43.457563 17176 sgd_solver.cpp:106] Iteration 26220, lr = 0.001
I1026 00:57:44.029933 17176 solver.cpp:229] Iteration 26240, loss = 0.0289997
I1026 00:57:44.029965 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0245786 (* 1 = 0.0245786 loss)
I1026 00:57:44.029971 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00442105 (* 1 = 0.00442105 loss)
I1026 00:57:44.029975 17176 sgd_solver.cpp:106] Iteration 26240, lr = 0.001
I1026 00:57:44.592010 17176 solver.cpp:229] Iteration 26260, loss = 0.090473
I1026 00:57:44.592042 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0571232 (* 1 = 0.0571232 loss)
I1026 00:57:44.592047 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0333498 (* 1 = 0.0333498 loss)
I1026 00:57:44.592051 17176 sgd_solver.cpp:106] Iteration 26260, lr = 0.001
I1026 00:57:45.153676 17176 solver.cpp:229] Iteration 26280, loss = 0.0179957
I1026 00:57:45.153719 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0111594 (* 1 = 0.0111594 loss)
I1026 00:57:45.153724 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00683623 (* 1 = 0.00683623 loss)
I1026 00:57:45.153728 17176 sgd_solver.cpp:106] Iteration 26280, lr = 0.001
I1026 00:57:45.700749 17176 solver.cpp:229] Iteration 26300, loss = 0.0508401
I1026 00:57:45.700783 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.045658 (* 1 = 0.045658 loss)
I1026 00:57:45.700788 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00518205 (* 1 = 0.00518205 loss)
I1026 00:57:45.700791 17176 sgd_solver.cpp:106] Iteration 26300, lr = 0.001
I1026 00:57:46.263083 17176 solver.cpp:229] Iteration 26320, loss = 0.0454194
I1026 00:57:46.263114 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0192988 (* 1 = 0.0192988 loss)
I1026 00:57:46.263119 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0261206 (* 1 = 0.0261206 loss)
I1026 00:57:46.263123 17176 sgd_solver.cpp:106] Iteration 26320, lr = 0.001
I1026 00:57:46.828935 17176 solver.cpp:229] Iteration 26340, loss = 0.0388233
I1026 00:57:46.828969 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0200873 (* 1 = 0.0200873 loss)
I1026 00:57:46.828974 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0187361 (* 1 = 0.0187361 loss)
I1026 00:57:46.828976 17176 sgd_solver.cpp:106] Iteration 26340, lr = 0.001
I1026 00:57:47.379807 17176 solver.cpp:229] Iteration 26360, loss = 0.0614079
I1026 00:57:47.379840 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0449795 (* 1 = 0.0449795 loss)
I1026 00:57:47.379845 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0164284 (* 1 = 0.0164284 loss)
I1026 00:57:47.379848 17176 sgd_solver.cpp:106] Iteration 26360, lr = 0.001
I1026 00:57:47.938381 17176 solver.cpp:229] Iteration 26380, loss = 0.0620187
I1026 00:57:47.938415 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.024526 (* 1 = 0.024526 loss)
I1026 00:57:47.938419 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0374926 (* 1 = 0.0374926 loss)
I1026 00:57:47.938423 17176 sgd_solver.cpp:106] Iteration 26380, lr = 0.001
I1026 00:57:48.497994 17176 solver.cpp:229] Iteration 26400, loss = 0.24984
I1026 00:57:48.498037 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.172156 (* 1 = 0.172156 loss)
I1026 00:57:48.498042 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0776837 (* 1 = 0.0776837 loss)
I1026 00:57:48.498047 17176 sgd_solver.cpp:106] Iteration 26400, lr = 0.001
I1026 00:57:49.064164 17176 solver.cpp:229] Iteration 26420, loss = 0.0159215
I1026 00:57:49.064196 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00860364 (* 1 = 0.00860364 loss)
I1026 00:57:49.064201 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00731789 (* 1 = 0.00731789 loss)
I1026 00:57:49.064206 17176 sgd_solver.cpp:106] Iteration 26420, lr = 0.001
I1026 00:57:49.623761 17176 solver.cpp:229] Iteration 26440, loss = 0.0703451
I1026 00:57:49.623795 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0650752 (* 1 = 0.0650752 loss)
I1026 00:57:49.623800 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00526992 (* 1 = 0.00526992 loss)
I1026 00:57:49.623805 17176 sgd_solver.cpp:106] Iteration 26440, lr = 0.001
I1026 00:57:50.171810 17176 solver.cpp:229] Iteration 26460, loss = 0.116013
I1026 00:57:50.171844 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0495321 (* 1 = 0.0495321 loss)
I1026 00:57:50.171849 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0664805 (* 1 = 0.0664805 loss)
I1026 00:57:50.171852 17176 sgd_solver.cpp:106] Iteration 26460, lr = 0.001
I1026 00:57:50.728746 17176 solver.cpp:229] Iteration 26480, loss = 0.090833
I1026 00:57:50.728780 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0806945 (* 1 = 0.0806945 loss)
I1026 00:57:50.728785 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0101385 (* 1 = 0.0101385 loss)
I1026 00:57:50.728788 17176 sgd_solver.cpp:106] Iteration 26480, lr = 0.001
I1026 00:57:51.282368 17176 solver.cpp:229] Iteration 26500, loss = 0.0713676
I1026 00:57:51.282400 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0364422 (* 1 = 0.0364422 loss)
I1026 00:57:51.282404 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0349254 (* 1 = 0.0349254 loss)
I1026 00:57:51.282408 17176 sgd_solver.cpp:106] Iteration 26500, lr = 0.001
I1026 00:57:51.837601 17176 solver.cpp:229] Iteration 26520, loss = 0.0330965
I1026 00:57:51.837643 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0289926 (* 1 = 0.0289926 loss)
I1026 00:57:51.837647 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00410385 (* 1 = 0.00410385 loss)
I1026 00:57:51.837651 17176 sgd_solver.cpp:106] Iteration 26520, lr = 0.001
I1026 00:57:52.395025 17176 solver.cpp:229] Iteration 26540, loss = 0.0656606
I1026 00:57:52.395056 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.021207 (* 1 = 0.021207 loss)
I1026 00:57:52.395061 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0444536 (* 1 = 0.0444536 loss)
I1026 00:57:52.395064 17176 sgd_solver.cpp:106] Iteration 26540, lr = 0.001
I1026 00:57:52.955500 17176 solver.cpp:229] Iteration 26560, loss = 0.0615782
I1026 00:57:52.955533 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0192931 (* 1 = 0.0192931 loss)
I1026 00:57:52.955538 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0422851 (* 1 = 0.0422851 loss)
I1026 00:57:52.955540 17176 sgd_solver.cpp:106] Iteration 26560, lr = 0.001
I1026 00:57:53.511692 17176 solver.cpp:229] Iteration 26580, loss = 0.04258
I1026 00:57:53.511723 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0357469 (* 1 = 0.0357469 loss)
I1026 00:57:53.511728 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0068331 (* 1 = 0.0068331 loss)
I1026 00:57:53.511731 17176 sgd_solver.cpp:106] Iteration 26580, lr = 0.001
I1026 00:57:54.058112 17176 solver.cpp:229] Iteration 26600, loss = 0.055467
I1026 00:57:54.058145 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0143936 (* 1 = 0.0143936 loss)
I1026 00:57:54.058151 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0410734 (* 1 = 0.0410734 loss)
I1026 00:57:54.058166 17176 sgd_solver.cpp:106] Iteration 26600, lr = 0.001
I1026 00:57:54.624253 17176 solver.cpp:229] Iteration 26620, loss = 0.0467946
I1026 00:57:54.624285 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0247091 (* 1 = 0.0247091 loss)
I1026 00:57:54.624289 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0220855 (* 1 = 0.0220855 loss)
I1026 00:57:54.624294 17176 sgd_solver.cpp:106] Iteration 26620, lr = 0.001
I1026 00:57:55.174212 17176 solver.cpp:229] Iteration 26640, loss = 0.035237
I1026 00:57:55.174244 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.012897 (* 1 = 0.012897 loss)
I1026 00:57:55.174248 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.02234 (* 1 = 0.02234 loss)
I1026 00:57:55.174252 17176 sgd_solver.cpp:106] Iteration 26640, lr = 0.001
I1026 00:57:55.731600 17176 solver.cpp:229] Iteration 26660, loss = 0.0857882
I1026 00:57:55.731631 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0594531 (* 1 = 0.0594531 loss)
I1026 00:57:55.731637 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0263351 (* 1 = 0.0263351 loss)
I1026 00:57:55.731639 17176 sgd_solver.cpp:106] Iteration 26660, lr = 0.001
I1026 00:57:56.302003 17176 solver.cpp:229] Iteration 26680, loss = 0.0207409
I1026 00:57:56.302037 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0157442 (* 1 = 0.0157442 loss)
I1026 00:57:56.302040 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00499674 (* 1 = 0.00499674 loss)
I1026 00:57:56.302044 17176 sgd_solver.cpp:106] Iteration 26680, lr = 0.001
I1026 00:57:56.874114 17176 solver.cpp:229] Iteration 26700, loss = 0.0750865
I1026 00:57:56.874146 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0317552 (* 1 = 0.0317552 loss)
I1026 00:57:56.874150 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0433313 (* 1 = 0.0433313 loss)
I1026 00:57:56.874155 17176 sgd_solver.cpp:106] Iteration 26700, lr = 0.001
I1026 00:57:57.435729 17176 solver.cpp:229] Iteration 26720, loss = 0.119548
I1026 00:57:57.435761 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.103214 (* 1 = 0.103214 loss)
I1026 00:57:57.435766 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.016334 (* 1 = 0.016334 loss)
I1026 00:57:57.435770 17176 sgd_solver.cpp:106] Iteration 26720, lr = 0.001
I1026 00:57:57.989383 17176 solver.cpp:229] Iteration 26740, loss = 0.154593
I1026 00:57:57.989415 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.103904 (* 1 = 0.103904 loss)
I1026 00:57:57.989420 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0506892 (* 1 = 0.0506892 loss)
I1026 00:57:57.989424 17176 sgd_solver.cpp:106] Iteration 26740, lr = 0.001
I1026 00:57:58.546344 17176 solver.cpp:229] Iteration 26760, loss = 0.0864919
I1026 00:57:58.546376 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.053931 (* 1 = 0.053931 loss)
I1026 00:57:58.546381 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0325609 (* 1 = 0.0325609 loss)
I1026 00:57:58.546386 17176 sgd_solver.cpp:106] Iteration 26760, lr = 0.001
I1026 00:57:59.108897 17176 solver.cpp:229] Iteration 26780, loss = 0.0863692
I1026 00:57:59.108939 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0430916 (* 1 = 0.0430916 loss)
I1026 00:57:59.108944 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0432776 (* 1 = 0.0432776 loss)
I1026 00:57:59.108948 17176 sgd_solver.cpp:106] Iteration 26780, lr = 0.001
I1026 00:57:59.672660 17176 solver.cpp:229] Iteration 26800, loss = 0.0595889
I1026 00:57:59.672703 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0227808 (* 1 = 0.0227808 loss)
I1026 00:57:59.672708 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0368081 (* 1 = 0.0368081 loss)
I1026 00:57:59.672713 17176 sgd_solver.cpp:106] Iteration 26800, lr = 0.001
I1026 00:58:00.237840 17176 solver.cpp:229] Iteration 26820, loss = 0.0469531
I1026 00:58:00.237872 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.025777 (* 1 = 0.025777 loss)
I1026 00:58:00.237877 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0211761 (* 1 = 0.0211761 loss)
I1026 00:58:00.237881 17176 sgd_solver.cpp:106] Iteration 26820, lr = 0.001
I1026 00:58:00.787459 17176 solver.cpp:229] Iteration 26840, loss = 0.0648075
I1026 00:58:00.787490 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0587876 (* 1 = 0.0587876 loss)
I1026 00:58:00.787494 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00601994 (* 1 = 0.00601994 loss)
I1026 00:58:00.787498 17176 sgd_solver.cpp:106] Iteration 26840, lr = 0.001
I1026 00:58:01.345304 17176 solver.cpp:229] Iteration 26860, loss = 0.0988702
I1026 00:58:01.345336 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0725704 (* 1 = 0.0725704 loss)
I1026 00:58:01.345341 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0262998 (* 1 = 0.0262998 loss)
I1026 00:58:01.345346 17176 sgd_solver.cpp:106] Iteration 26860, lr = 0.001
I1026 00:58:01.913022 17176 solver.cpp:229] Iteration 26880, loss = 0.136059
I1026 00:58:01.913053 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0546595 (* 1 = 0.0546595 loss)
I1026 00:58:01.913058 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0813996 (* 1 = 0.0813996 loss)
I1026 00:58:01.913063 17176 sgd_solver.cpp:106] Iteration 26880, lr = 0.001
I1026 00:58:02.470707 17176 solver.cpp:229] Iteration 26900, loss = 0.0412034
I1026 00:58:02.470739 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0116677 (* 1 = 0.0116677 loss)
I1026 00:58:02.470744 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0295358 (* 1 = 0.0295358 loss)
I1026 00:58:02.470749 17176 sgd_solver.cpp:106] Iteration 26900, lr = 0.001
I1026 00:58:03.032135 17176 solver.cpp:229] Iteration 26920, loss = 0.0914054
I1026 00:58:03.032178 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0549106 (* 1 = 0.0549106 loss)
I1026 00:58:03.032183 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0364948 (* 1 = 0.0364948 loss)
I1026 00:58:03.032187 17176 sgd_solver.cpp:106] Iteration 26920, lr = 0.001
I1026 00:58:03.595626 17176 solver.cpp:229] Iteration 26940, loss = 0.0513712
I1026 00:58:03.595659 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0359928 (* 1 = 0.0359928 loss)
I1026 00:58:03.595664 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0153784 (* 1 = 0.0153784 loss)
I1026 00:58:03.595669 17176 sgd_solver.cpp:106] Iteration 26940, lr = 0.001
I1026 00:58:04.168643 17176 solver.cpp:229] Iteration 26960, loss = 0.140037
I1026 00:58:04.168685 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0439075 (* 1 = 0.0439075 loss)
I1026 00:58:04.168699 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0961296 (* 1 = 0.0961296 loss)
I1026 00:58:04.168704 17176 sgd_solver.cpp:106] Iteration 26960, lr = 0.001
I1026 00:58:04.726878 17176 solver.cpp:229] Iteration 26980, loss = 0.136414
I1026 00:58:04.726910 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0955043 (* 1 = 0.0955043 loss)
I1026 00:58:04.726933 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0409093 (* 1 = 0.0409093 loss)
I1026 00:58:04.726936 17176 sgd_solver.cpp:106] Iteration 26980, lr = 0.001
I1026 00:58:05.279371 17176 solver.cpp:229] Iteration 27000, loss = 0.264422
I1026 00:58:05.279402 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.183935 (* 1 = 0.183935 loss)
I1026 00:58:05.279407 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0804864 (* 1 = 0.0804864 loss)
I1026 00:58:05.279412 17176 sgd_solver.cpp:106] Iteration 27000, lr = 0.001
I1026 00:58:05.842324 17176 solver.cpp:229] Iteration 27020, loss = 0.0562453
I1026 00:58:05.842358 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0298964 (* 1 = 0.0298964 loss)
I1026 00:58:05.842365 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0263489 (* 1 = 0.0263489 loss)
I1026 00:58:05.842368 17176 sgd_solver.cpp:106] Iteration 27020, lr = 0.001
I1026 00:58:06.398685 17176 solver.cpp:229] Iteration 27040, loss = 0.280565
I1026 00:58:06.398715 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0964625 (* 1 = 0.0964625 loss)
I1026 00:58:06.398718 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.184102 (* 1 = 0.184102 loss)
I1026 00:58:06.398722 17176 sgd_solver.cpp:106] Iteration 27040, lr = 0.001
I1026 00:58:06.965432 17176 solver.cpp:229] Iteration 27060, loss = 0.0400879
I1026 00:58:06.965466 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0137564 (* 1 = 0.0137564 loss)
I1026 00:58:06.965471 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0263315 (* 1 = 0.0263315 loss)
I1026 00:58:06.965474 17176 sgd_solver.cpp:106] Iteration 27060, lr = 0.001
I1026 00:58:07.531972 17176 solver.cpp:229] Iteration 27080, loss = 0.0537799
I1026 00:58:07.532004 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0400053 (* 1 = 0.0400053 loss)
I1026 00:58:07.532009 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0137746 (* 1 = 0.0137746 loss)
I1026 00:58:07.532014 17176 sgd_solver.cpp:106] Iteration 27080, lr = 0.001
I1026 00:58:08.099244 17176 solver.cpp:229] Iteration 27100, loss = 0.0777091
I1026 00:58:08.099277 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.015159 (* 1 = 0.015159 loss)
I1026 00:58:08.099282 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0625501 (* 1 = 0.0625501 loss)
I1026 00:58:08.099285 17176 sgd_solver.cpp:106] Iteration 27100, lr = 0.001
I1026 00:58:08.660867 17176 solver.cpp:229] Iteration 27120, loss = 0.0233337
I1026 00:58:08.660900 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0152566 (* 1 = 0.0152566 loss)
I1026 00:58:08.660904 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00807709 (* 1 = 0.00807709 loss)
I1026 00:58:08.660908 17176 sgd_solver.cpp:106] Iteration 27120, lr = 0.001
I1026 00:58:09.224272 17176 solver.cpp:229] Iteration 27140, loss = 0.254477
I1026 00:58:09.224324 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0894427 (* 1 = 0.0894427 loss)
I1026 00:58:09.224331 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.165035 (* 1 = 0.165035 loss)
I1026 00:58:09.224336 17176 sgd_solver.cpp:106] Iteration 27140, lr = 0.001
I1026 00:58:09.791085 17176 solver.cpp:229] Iteration 27160, loss = 0.0742579
I1026 00:58:09.791116 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0477285 (* 1 = 0.0477285 loss)
I1026 00:58:09.791121 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0265293 (* 1 = 0.0265293 loss)
I1026 00:58:09.791126 17176 sgd_solver.cpp:106] Iteration 27160, lr = 0.001
I1026 00:58:10.342139 17176 solver.cpp:229] Iteration 27180, loss = 0.0925715
I1026 00:58:10.342170 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0635419 (* 1 = 0.0635419 loss)
I1026 00:58:10.342175 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0290297 (* 1 = 0.0290297 loss)
I1026 00:58:10.342180 17176 sgd_solver.cpp:106] Iteration 27180, lr = 0.001
I1026 00:58:10.904837 17176 solver.cpp:229] Iteration 27200, loss = 0.295587
I1026 00:58:10.904870 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.103885 (* 1 = 0.103885 loss)
I1026 00:58:10.904875 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.191703 (* 1 = 0.191703 loss)
I1026 00:58:10.904878 17176 sgd_solver.cpp:106] Iteration 27200, lr = 0.001
I1026 00:58:11.473836 17176 solver.cpp:229] Iteration 27220, loss = 0.0476397
I1026 00:58:11.473868 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0213861 (* 1 = 0.0213861 loss)
I1026 00:58:11.473873 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0262537 (* 1 = 0.0262537 loss)
I1026 00:58:11.473878 17176 sgd_solver.cpp:106] Iteration 27220, lr = 0.001
I1026 00:58:12.023591 17176 solver.cpp:229] Iteration 27240, loss = 0.0703483
I1026 00:58:12.023623 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0481215 (* 1 = 0.0481215 loss)
I1026 00:58:12.023628 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0222268 (* 1 = 0.0222268 loss)
I1026 00:58:12.023633 17176 sgd_solver.cpp:106] Iteration 27240, lr = 0.001
I1026 00:58:12.577435 17176 solver.cpp:229] Iteration 27260, loss = 0.064762
I1026 00:58:12.577467 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0393366 (* 1 = 0.0393366 loss)
I1026 00:58:12.577472 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0254253 (* 1 = 0.0254253 loss)
I1026 00:58:12.577476 17176 sgd_solver.cpp:106] Iteration 27260, lr = 0.001
I1026 00:58:13.142259 17176 solver.cpp:229] Iteration 27280, loss = 0.679855
I1026 00:58:13.142292 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.319161 (* 1 = 0.319161 loss)
I1026 00:58:13.142295 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.360694 (* 1 = 0.360694 loss)
I1026 00:58:13.142299 17176 sgd_solver.cpp:106] Iteration 27280, lr = 0.001
I1026 00:58:13.706029 17176 solver.cpp:229] Iteration 27300, loss = 0.0957938
I1026 00:58:13.706063 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0741941 (* 1 = 0.0741941 loss)
I1026 00:58:13.706070 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0215997 (* 1 = 0.0215997 loss)
I1026 00:58:13.706075 17176 sgd_solver.cpp:106] Iteration 27300, lr = 0.001
I1026 00:58:14.268537 17176 solver.cpp:229] Iteration 27320, loss = 0.0403795
I1026 00:58:14.268569 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0346591 (* 1 = 0.0346591 loss)
I1026 00:58:14.268573 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00572046 (* 1 = 0.00572046 loss)
I1026 00:58:14.268577 17176 sgd_solver.cpp:106] Iteration 27320, lr = 0.001
I1026 00:58:14.828382 17176 solver.cpp:229] Iteration 27340, loss = 0.0709127
I1026 00:58:14.828414 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0459341 (* 1 = 0.0459341 loss)
I1026 00:58:14.828419 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0249786 (* 1 = 0.0249786 loss)
I1026 00:58:14.828423 17176 sgd_solver.cpp:106] Iteration 27340, lr = 0.001
I1026 00:58:15.383878 17176 solver.cpp:229] Iteration 27360, loss = 0.0226931
I1026 00:58:15.383910 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0206946 (* 1 = 0.0206946 loss)
I1026 00:58:15.383916 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00199852 (* 1 = 0.00199852 loss)
I1026 00:58:15.383919 17176 sgd_solver.cpp:106] Iteration 27360, lr = 0.001
I1026 00:58:15.943676 17176 solver.cpp:229] Iteration 27380, loss = 0.0411637
I1026 00:58:15.943708 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0174307 (* 1 = 0.0174307 loss)
I1026 00:58:15.943713 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.023733 (* 1 = 0.023733 loss)
I1026 00:58:15.943719 17176 sgd_solver.cpp:106] Iteration 27380, lr = 0.001
I1026 00:58:16.492770 17176 solver.cpp:229] Iteration 27400, loss = 0.121484
I1026 00:58:16.492805 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0875783 (* 1 = 0.0875783 loss)
I1026 00:58:16.492810 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0339059 (* 1 = 0.0339059 loss)
I1026 00:58:16.492813 17176 sgd_solver.cpp:106] Iteration 27400, lr = 0.001
I1026 00:58:17.060809 17176 solver.cpp:229] Iteration 27420, loss = 0.149692
I1026 00:58:17.060842 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.08871 (* 1 = 0.08871 loss)
I1026 00:58:17.060847 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0609822 (* 1 = 0.0609822 loss)
I1026 00:58:17.060852 17176 sgd_solver.cpp:106] Iteration 27420, lr = 0.001
I1026 00:58:17.616405 17176 solver.cpp:229] Iteration 27440, loss = 0.0671944
I1026 00:58:17.616451 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0635455 (* 1 = 0.0635455 loss)
I1026 00:58:17.616456 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00364894 (* 1 = 0.00364894 loss)
I1026 00:58:17.616459 17176 sgd_solver.cpp:106] Iteration 27440, lr = 0.001
I1026 00:58:18.188688 17176 solver.cpp:229] Iteration 27460, loss = 0.0482956
I1026 00:58:18.188721 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0297648 (* 1 = 0.0297648 loss)
I1026 00:58:18.188725 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0185307 (* 1 = 0.0185307 loss)
I1026 00:58:18.188730 17176 sgd_solver.cpp:106] Iteration 27460, lr = 0.001
I1026 00:58:18.737040 17176 solver.cpp:229] Iteration 27480, loss = 0.0575182
I1026 00:58:18.737072 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0245701 (* 1 = 0.0245701 loss)
I1026 00:58:18.737077 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0329481 (* 1 = 0.0329481 loss)
I1026 00:58:18.737082 17176 sgd_solver.cpp:106] Iteration 27480, lr = 0.001
I1026 00:58:19.312330 17176 solver.cpp:229] Iteration 27500, loss = 0.0423965
I1026 00:58:19.312372 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00842494 (* 1 = 0.00842494 loss)
I1026 00:58:19.312377 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0339715 (* 1 = 0.0339715 loss)
I1026 00:58:19.312381 17176 sgd_solver.cpp:106] Iteration 27500, lr = 0.001
I1026 00:58:19.861423 17176 solver.cpp:229] Iteration 27520, loss = 0.0931018
I1026 00:58:19.861456 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0444659 (* 1 = 0.0444659 loss)
I1026 00:58:19.861460 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0486358 (* 1 = 0.0486358 loss)
I1026 00:58:19.861464 17176 sgd_solver.cpp:106] Iteration 27520, lr = 0.001
I1026 00:58:20.415381 17176 solver.cpp:229] Iteration 27540, loss = 0.0531683
I1026 00:58:20.415413 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0371656 (* 1 = 0.0371656 loss)
I1026 00:58:20.415418 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0160027 (* 1 = 0.0160027 loss)
I1026 00:58:20.415422 17176 sgd_solver.cpp:106] Iteration 27540, lr = 0.001
I1026 00:58:20.969434 17176 solver.cpp:229] Iteration 27560, loss = 0.0577878
I1026 00:58:20.969465 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0332174 (* 1 = 0.0332174 loss)
I1026 00:58:20.969470 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0245704 (* 1 = 0.0245704 loss)
I1026 00:58:20.969475 17176 sgd_solver.cpp:106] Iteration 27560, lr = 0.001
I1026 00:58:21.528533 17176 solver.cpp:229] Iteration 27580, loss = 0.264109
I1026 00:58:21.528565 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0989152 (* 1 = 0.0989152 loss)
I1026 00:58:21.528570 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.165194 (* 1 = 0.165194 loss)
I1026 00:58:21.528584 17176 sgd_solver.cpp:106] Iteration 27580, lr = 0.001
I1026 00:58:22.093677 17176 solver.cpp:229] Iteration 27600, loss = 0.083524
I1026 00:58:22.093708 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0506458 (* 1 = 0.0506458 loss)
I1026 00:58:22.093713 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0328782 (* 1 = 0.0328782 loss)
I1026 00:58:22.093718 17176 sgd_solver.cpp:106] Iteration 27600, lr = 0.001
I1026 00:58:22.656756 17176 solver.cpp:229] Iteration 27620, loss = 0.155903
I1026 00:58:22.656798 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0765808 (* 1 = 0.0765808 loss)
I1026 00:58:22.656802 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0793218 (* 1 = 0.0793218 loss)
I1026 00:58:22.656806 17176 sgd_solver.cpp:106] Iteration 27620, lr = 0.001
I1026 00:58:23.216331 17176 solver.cpp:229] Iteration 27640, loss = 0.406762
I1026 00:58:23.216362 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.138931 (* 1 = 0.138931 loss)
I1026 00:58:23.216367 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.267831 (* 1 = 0.267831 loss)
I1026 00:58:23.216370 17176 sgd_solver.cpp:106] Iteration 27640, lr = 0.001
I1026 00:58:23.782099 17176 solver.cpp:229] Iteration 27660, loss = 0.0818012
I1026 00:58:23.782131 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0218099 (* 1 = 0.0218099 loss)
I1026 00:58:23.782136 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0599913 (* 1 = 0.0599913 loss)
I1026 00:58:23.782140 17176 sgd_solver.cpp:106] Iteration 27660, lr = 0.001
I1026 00:58:24.341364 17176 solver.cpp:229] Iteration 27680, loss = 0.153969
I1026 00:58:24.341397 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0881536 (* 1 = 0.0881536 loss)
I1026 00:58:24.341401 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0658159 (* 1 = 0.0658159 loss)
I1026 00:58:24.341415 17176 sgd_solver.cpp:106] Iteration 27680, lr = 0.001
I1026 00:58:24.893865 17176 solver.cpp:229] Iteration 27700, loss = 0.198353
I1026 00:58:24.893909 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.150037 (* 1 = 0.150037 loss)
I1026 00:58:24.893914 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0483151 (* 1 = 0.0483151 loss)
I1026 00:58:24.893918 17176 sgd_solver.cpp:106] Iteration 27700, lr = 0.001
I1026 00:58:25.459179 17176 solver.cpp:229] Iteration 27720, loss = 0.0860437
I1026 00:58:25.459211 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0366794 (* 1 = 0.0366794 loss)
I1026 00:58:25.459216 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0493643 (* 1 = 0.0493643 loss)
I1026 00:58:25.459220 17176 sgd_solver.cpp:106] Iteration 27720, lr = 0.001
I1026 00:58:26.007993 17176 solver.cpp:229] Iteration 27740, loss = 0.0693782
I1026 00:58:26.008024 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0212866 (* 1 = 0.0212866 loss)
I1026 00:58:26.008028 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0480916 (* 1 = 0.0480916 loss)
I1026 00:58:26.008033 17176 sgd_solver.cpp:106] Iteration 27740, lr = 0.001
I1026 00:58:26.567970 17176 solver.cpp:229] Iteration 27760, loss = 0.0905903
I1026 00:58:26.568002 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0860878 (* 1 = 0.0860878 loss)
I1026 00:58:26.568007 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00450245 (* 1 = 0.00450245 loss)
I1026 00:58:26.568012 17176 sgd_solver.cpp:106] Iteration 27760, lr = 0.001
I1026 00:58:27.118932 17176 solver.cpp:229] Iteration 27780, loss = 0.0609761
I1026 00:58:27.118965 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.055413 (* 1 = 0.055413 loss)
I1026 00:58:27.118970 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00556307 (* 1 = 0.00556307 loss)
I1026 00:58:27.118974 17176 sgd_solver.cpp:106] Iteration 27780, lr = 0.001
I1026 00:58:27.678197 17176 solver.cpp:229] Iteration 27800, loss = 0.0230236
I1026 00:58:27.678228 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0200882 (* 1 = 0.0200882 loss)
I1026 00:58:27.678233 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0029354 (* 1 = 0.0029354 loss)
I1026 00:58:27.678237 17176 sgd_solver.cpp:106] Iteration 27800, lr = 0.001
I1026 00:58:28.236141 17176 solver.cpp:229] Iteration 27820, loss = 0.066338
I1026 00:58:28.236172 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0545647 (* 1 = 0.0545647 loss)
I1026 00:58:28.236176 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0117733 (* 1 = 0.0117733 loss)
I1026 00:58:28.236181 17176 sgd_solver.cpp:106] Iteration 27820, lr = 0.001
I1026 00:58:28.789886 17176 solver.cpp:229] Iteration 27840, loss = 0.0395149
I1026 00:58:28.789918 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0345314 (* 1 = 0.0345314 loss)
I1026 00:58:28.789923 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00498349 (* 1 = 0.00498349 loss)
I1026 00:58:28.789927 17176 sgd_solver.cpp:106] Iteration 27840, lr = 0.001
I1026 00:58:29.350555 17176 solver.cpp:229] Iteration 27860, loss = 0.077817
I1026 00:58:29.350600 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0537696 (* 1 = 0.0537696 loss)
I1026 00:58:29.350605 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0240475 (* 1 = 0.0240475 loss)
I1026 00:58:29.350610 17176 sgd_solver.cpp:106] Iteration 27860, lr = 0.001
I1026 00:58:29.930717 17176 solver.cpp:229] Iteration 27880, loss = 0.0814652
I1026 00:58:29.930752 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.049857 (* 1 = 0.049857 loss)
I1026 00:58:29.930758 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0316082 (* 1 = 0.0316082 loss)
I1026 00:58:29.930764 17176 sgd_solver.cpp:106] Iteration 27880, lr = 0.001
I1026 00:58:30.487553 17176 solver.cpp:229] Iteration 27900, loss = 0.0515275
I1026 00:58:30.487587 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0394107 (* 1 = 0.0394107 loss)
I1026 00:58:30.487594 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0121168 (* 1 = 0.0121168 loss)
I1026 00:58:30.487599 17176 sgd_solver.cpp:106] Iteration 27900, lr = 0.001
I1026 00:58:31.045573 17176 solver.cpp:229] Iteration 27920, loss = 0.153479
I1026 00:58:31.045605 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0461338 (* 1 = 0.0461338 loss)
I1026 00:58:31.045627 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.107345 (* 1 = 0.107345 loss)
I1026 00:58:31.045632 17176 sgd_solver.cpp:106] Iteration 27920, lr = 0.001
I1026 00:58:31.603672 17176 solver.cpp:229] Iteration 27940, loss = 0.055054
I1026 00:58:31.603705 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0308239 (* 1 = 0.0308239 loss)
I1026 00:58:31.603710 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0242301 (* 1 = 0.0242301 loss)
I1026 00:58:31.603713 17176 sgd_solver.cpp:106] Iteration 27940, lr = 0.001
I1026 00:58:32.157558 17176 solver.cpp:229] Iteration 27960, loss = 0.0912127
I1026 00:58:32.157591 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0337599 (* 1 = 0.0337599 loss)
I1026 00:58:32.157596 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0574528 (* 1 = 0.0574528 loss)
I1026 00:58:32.157599 17176 sgd_solver.cpp:106] Iteration 27960, lr = 0.001
I1026 00:58:32.715579 17176 solver.cpp:229] Iteration 27980, loss = 0.10437
I1026 00:58:32.715611 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0815534 (* 1 = 0.0815534 loss)
I1026 00:58:32.715615 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0228161 (* 1 = 0.0228161 loss)
I1026 00:58:32.715620 17176 sgd_solver.cpp:106] Iteration 27980, lr = 0.001
I1026 00:58:33.278872 17176 solver.cpp:229] Iteration 28000, loss = 0.0541128
I1026 00:58:33.278914 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0358392 (* 1 = 0.0358392 loss)
I1026 00:58:33.278919 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0182736 (* 1 = 0.0182736 loss)
I1026 00:58:33.278923 17176 sgd_solver.cpp:106] Iteration 28000, lr = 0.001
I1026 00:58:33.854655 17176 solver.cpp:229] Iteration 28020, loss = 0.0566374
I1026 00:58:33.854686 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.024813 (* 1 = 0.024813 loss)
I1026 00:58:33.854691 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0318243 (* 1 = 0.0318243 loss)
I1026 00:58:33.854696 17176 sgd_solver.cpp:106] Iteration 28020, lr = 0.001
I1026 00:58:34.409381 17176 solver.cpp:229] Iteration 28040, loss = 0.0605407
I1026 00:58:34.409409 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0123524 (* 1 = 0.0123524 loss)
I1026 00:58:34.409415 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0481883 (* 1 = 0.0481883 loss)
I1026 00:58:34.409418 17176 sgd_solver.cpp:106] Iteration 28040, lr = 0.001
I1026 00:58:34.974113 17176 solver.cpp:229] Iteration 28060, loss = 0.0645636
I1026 00:58:34.974148 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0511955 (* 1 = 0.0511955 loss)
I1026 00:58:34.974155 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0133682 (* 1 = 0.0133682 loss)
I1026 00:58:34.974160 17176 sgd_solver.cpp:106] Iteration 28060, lr = 0.001
I1026 00:58:35.532990 17176 solver.cpp:229] Iteration 28080, loss = 0.0987326
I1026 00:58:35.533022 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0855812 (* 1 = 0.0855812 loss)
I1026 00:58:35.533026 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0131513 (* 1 = 0.0131513 loss)
I1026 00:58:35.533031 17176 sgd_solver.cpp:106] Iteration 28080, lr = 0.001
I1026 00:58:36.080618 17176 solver.cpp:229] Iteration 28100, loss = 0.0530074
I1026 00:58:36.080662 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0359019 (* 1 = 0.0359019 loss)
I1026 00:58:36.080665 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0171056 (* 1 = 0.0171056 loss)
I1026 00:58:36.080669 17176 sgd_solver.cpp:106] Iteration 28100, lr = 0.001
I1026 00:58:36.640859 17176 solver.cpp:229] Iteration 28120, loss = 0.0733806
I1026 00:58:36.640888 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0633891 (* 1 = 0.0633891 loss)
I1026 00:58:36.640893 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00999155 (* 1 = 0.00999155 loss)
I1026 00:58:36.640897 17176 sgd_solver.cpp:106] Iteration 28120, lr = 0.001
I1026 00:58:37.197543 17176 solver.cpp:229] Iteration 28140, loss = 0.0652374
I1026 00:58:37.197576 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0526197 (* 1 = 0.0526197 loss)
I1026 00:58:37.197580 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0126177 (* 1 = 0.0126177 loss)
I1026 00:58:37.197584 17176 sgd_solver.cpp:106] Iteration 28140, lr = 0.001
I1026 00:58:37.766443 17176 solver.cpp:229] Iteration 28160, loss = 0.0697094
I1026 00:58:37.766479 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0478378 (* 1 = 0.0478378 loss)
I1026 00:58:37.766484 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0218716 (* 1 = 0.0218716 loss)
I1026 00:58:37.766486 17176 sgd_solver.cpp:106] Iteration 28160, lr = 0.001
I1026 00:58:38.327862 17176 solver.cpp:229] Iteration 28180, loss = 0.239283
I1026 00:58:38.327893 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.102363 (* 1 = 0.102363 loss)
I1026 00:58:38.327898 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.13692 (* 1 = 0.13692 loss)
I1026 00:58:38.327903 17176 sgd_solver.cpp:106] Iteration 28180, lr = 0.001
I1026 00:58:38.878367 17176 solver.cpp:229] Iteration 28200, loss = 0.0547983
I1026 00:58:38.878399 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0111249 (* 1 = 0.0111249 loss)
I1026 00:58:38.878404 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0436734 (* 1 = 0.0436734 loss)
I1026 00:58:38.878408 17176 sgd_solver.cpp:106] Iteration 28200, lr = 0.001
I1026 00:58:39.448740 17176 solver.cpp:229] Iteration 28220, loss = 0.100637
I1026 00:58:39.448771 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0396012 (* 1 = 0.0396012 loss)
I1026 00:58:39.448776 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0610357 (* 1 = 0.0610357 loss)
I1026 00:58:39.448781 17176 sgd_solver.cpp:106] Iteration 28220, lr = 0.001
I1026 00:58:40.015116 17176 solver.cpp:229] Iteration 28240, loss = 0.768581
I1026 00:58:40.015148 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.248702 (* 1 = 0.248702 loss)
I1026 00:58:40.015153 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.519879 (* 1 = 0.519879 loss)
I1026 00:58:40.015156 17176 sgd_solver.cpp:106] Iteration 28240, lr = 0.001
I1026 00:58:40.579277 17176 solver.cpp:229] Iteration 28260, loss = 0.0495627
I1026 00:58:40.579308 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0379913 (* 1 = 0.0379913 loss)
I1026 00:58:40.579313 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0115715 (* 1 = 0.0115715 loss)
I1026 00:58:40.579316 17176 sgd_solver.cpp:106] Iteration 28260, lr = 0.001
I1026 00:58:41.132423 17176 solver.cpp:229] Iteration 28280, loss = 0.0794845
I1026 00:58:41.132455 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0668094 (* 1 = 0.0668094 loss)
I1026 00:58:41.132460 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0126751 (* 1 = 0.0126751 loss)
I1026 00:58:41.132464 17176 sgd_solver.cpp:106] Iteration 28280, lr = 0.001
I1026 00:58:41.682881 17176 solver.cpp:229] Iteration 28300, loss = 0.0370147
I1026 00:58:41.682914 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0287989 (* 1 = 0.0287989 loss)
I1026 00:58:41.682919 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00821581 (* 1 = 0.00821581 loss)
I1026 00:58:41.682924 17176 sgd_solver.cpp:106] Iteration 28300, lr = 0.001
I1026 00:58:42.241276 17176 solver.cpp:229] Iteration 28320, loss = 0.0873394
I1026 00:58:42.241308 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0531212 (* 1 = 0.0531212 loss)
I1026 00:58:42.241312 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0342182 (* 1 = 0.0342182 loss)
I1026 00:58:42.241317 17176 sgd_solver.cpp:106] Iteration 28320, lr = 0.001
I1026 00:58:42.803323 17176 solver.cpp:229] Iteration 28340, loss = 0.0610713
I1026 00:58:42.803354 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0479904 (* 1 = 0.0479904 loss)
I1026 00:58:42.803374 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0130809 (* 1 = 0.0130809 loss)
I1026 00:58:42.803377 17176 sgd_solver.cpp:106] Iteration 28340, lr = 0.001
I1026 00:58:43.353343 17176 solver.cpp:229] Iteration 28360, loss = 0.0520329
I1026 00:58:43.353374 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0257185 (* 1 = 0.0257185 loss)
I1026 00:58:43.353377 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0263145 (* 1 = 0.0263145 loss)
I1026 00:58:43.353381 17176 sgd_solver.cpp:106] Iteration 28360, lr = 0.001
I1026 00:58:43.910826 17176 solver.cpp:229] Iteration 28380, loss = 0.0471242
I1026 00:58:43.910858 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0296728 (* 1 = 0.0296728 loss)
I1026 00:58:43.910862 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0174513 (* 1 = 0.0174513 loss)
I1026 00:58:43.910866 17176 sgd_solver.cpp:106] Iteration 28380, lr = 0.001
I1026 00:58:44.454771 17176 solver.cpp:229] Iteration 28400, loss = 0.125523
I1026 00:58:44.454802 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0573905 (* 1 = 0.0573905 loss)
I1026 00:58:44.454807 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0681323 (* 1 = 0.0681323 loss)
I1026 00:58:44.454812 17176 sgd_solver.cpp:106] Iteration 28400, lr = 0.001
I1026 00:58:45.017197 17176 solver.cpp:229] Iteration 28420, loss = 0.0642803
I1026 00:58:45.017238 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0507318 (* 1 = 0.0507318 loss)
I1026 00:58:45.017243 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0135486 (* 1 = 0.0135486 loss)
I1026 00:58:45.017247 17176 sgd_solver.cpp:106] Iteration 28420, lr = 0.001
I1026 00:58:45.574591 17176 solver.cpp:229] Iteration 28440, loss = 0.756561
I1026 00:58:45.574623 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.135146 (* 1 = 0.135146 loss)
I1026 00:58:45.574628 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.621415 (* 1 = 0.621415 loss)
I1026 00:58:45.574633 17176 sgd_solver.cpp:106] Iteration 28440, lr = 0.001
I1026 00:58:46.149147 17176 solver.cpp:229] Iteration 28460, loss = 0.0544084
I1026 00:58:46.149180 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0478225 (* 1 = 0.0478225 loss)
I1026 00:58:46.149186 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00658594 (* 1 = 0.00658594 loss)
I1026 00:58:46.149190 17176 sgd_solver.cpp:106] Iteration 28460, lr = 0.001
I1026 00:58:46.703414 17176 solver.cpp:229] Iteration 28480, loss = 0.0798033
I1026 00:58:46.703451 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0744035 (* 1 = 0.0744035 loss)
I1026 00:58:46.703459 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00539985 (* 1 = 0.00539985 loss)
I1026 00:58:46.703462 17176 sgd_solver.cpp:106] Iteration 28480, lr = 0.001
I1026 00:58:47.265410 17176 solver.cpp:229] Iteration 28500, loss = 0.0749412
I1026 00:58:47.265442 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0680984 (* 1 = 0.0680984 loss)
I1026 00:58:47.265447 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00684287 (* 1 = 0.00684287 loss)
I1026 00:58:47.265450 17176 sgd_solver.cpp:106] Iteration 28500, lr = 0.001
I1026 00:58:47.827112 17176 solver.cpp:229] Iteration 28520, loss = 0.0899589
I1026 00:58:47.827147 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0126283 (* 1 = 0.0126283 loss)
I1026 00:58:47.827150 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0773306 (* 1 = 0.0773306 loss)
I1026 00:58:47.827155 17176 sgd_solver.cpp:106] Iteration 28520, lr = 0.001
I1026 00:58:48.397313 17176 solver.cpp:229] Iteration 28540, loss = 0.130207
I1026 00:58:48.397346 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0772723 (* 1 = 0.0772723 loss)
I1026 00:58:48.397351 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0529348 (* 1 = 0.0529348 loss)
I1026 00:58:48.397354 17176 sgd_solver.cpp:106] Iteration 28540, lr = 0.001
I1026 00:58:48.953093 17176 solver.cpp:229] Iteration 28560, loss = 0.0577189
I1026 00:58:48.953125 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0450006 (* 1 = 0.0450006 loss)
I1026 00:58:48.953130 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0127183 (* 1 = 0.0127183 loss)
I1026 00:58:48.953135 17176 sgd_solver.cpp:106] Iteration 28560, lr = 0.001
I1026 00:58:49.503585 17176 solver.cpp:229] Iteration 28580, loss = 0.126571
I1026 00:58:49.503618 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.048215 (* 1 = 0.048215 loss)
I1026 00:58:49.503623 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.078356 (* 1 = 0.078356 loss)
I1026 00:58:49.503626 17176 sgd_solver.cpp:106] Iteration 28580, lr = 0.001
I1026 00:58:50.053244 17176 solver.cpp:229] Iteration 28600, loss = 0.10164
I1026 00:58:50.053277 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0931331 (* 1 = 0.0931331 loss)
I1026 00:58:50.053282 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00850715 (* 1 = 0.00850715 loss)
I1026 00:58:50.053285 17176 sgd_solver.cpp:106] Iteration 28600, lr = 0.001
I1026 00:58:50.609321 17176 solver.cpp:229] Iteration 28620, loss = 0.175215
I1026 00:58:50.609354 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.106307 (* 1 = 0.106307 loss)
I1026 00:58:50.609359 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0689074 (* 1 = 0.0689074 loss)
I1026 00:58:50.609362 17176 sgd_solver.cpp:106] Iteration 28620, lr = 0.001
I1026 00:58:51.163552 17176 solver.cpp:229] Iteration 28640, loss = 0.0651768
I1026 00:58:51.163583 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0572438 (* 1 = 0.0572438 loss)
I1026 00:58:51.163588 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00793298 (* 1 = 0.00793298 loss)
I1026 00:58:51.163591 17176 sgd_solver.cpp:106] Iteration 28640, lr = 0.001
I1026 00:58:51.721745 17176 solver.cpp:229] Iteration 28660, loss = 0.0670044
I1026 00:58:51.721777 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0558647 (* 1 = 0.0558647 loss)
I1026 00:58:51.721781 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0111397 (* 1 = 0.0111397 loss)
I1026 00:58:51.721786 17176 sgd_solver.cpp:106] Iteration 28660, lr = 0.001
I1026 00:58:52.281883 17176 solver.cpp:229] Iteration 28680, loss = 0.0775263
I1026 00:58:52.281915 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0675946 (* 1 = 0.0675946 loss)
I1026 00:58:52.281920 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00993172 (* 1 = 0.00993172 loss)
I1026 00:58:52.281924 17176 sgd_solver.cpp:106] Iteration 28680, lr = 0.001
I1026 00:58:52.848120 17176 solver.cpp:229] Iteration 28700, loss = 0.0390687
I1026 00:58:52.848151 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0292198 (* 1 = 0.0292198 loss)
I1026 00:58:52.848156 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00984883 (* 1 = 0.00984883 loss)
I1026 00:58:52.848160 17176 sgd_solver.cpp:106] Iteration 28700, lr = 0.001
I1026 00:58:53.400909 17176 solver.cpp:229] Iteration 28720, loss = 0.144316
I1026 00:58:53.400943 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0972644 (* 1 = 0.0972644 loss)
I1026 00:58:53.400948 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0470515 (* 1 = 0.0470515 loss)
I1026 00:58:53.400952 17176 sgd_solver.cpp:106] Iteration 28720, lr = 0.001
I1026 00:58:53.951530 17176 solver.cpp:229] Iteration 28740, loss = 0.0318337
I1026 00:58:53.951562 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0169237 (* 1 = 0.0169237 loss)
I1026 00:58:53.951567 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.01491 (* 1 = 0.01491 loss)
I1026 00:58:53.951571 17176 sgd_solver.cpp:106] Iteration 28740, lr = 0.001
I1026 00:58:54.527186 17176 solver.cpp:229] Iteration 28760, loss = 0.314705
I1026 00:58:54.527218 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.154231 (* 1 = 0.154231 loss)
I1026 00:58:54.527223 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.160474 (* 1 = 0.160474 loss)
I1026 00:58:54.527226 17176 sgd_solver.cpp:106] Iteration 28760, lr = 0.001
I1026 00:58:55.085183 17176 solver.cpp:229] Iteration 28780, loss = 0.129069
I1026 00:58:55.085216 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0632183 (* 1 = 0.0632183 loss)
I1026 00:58:55.085222 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0658504 (* 1 = 0.0658504 loss)
I1026 00:58:55.085225 17176 sgd_solver.cpp:106] Iteration 28780, lr = 0.001
I1026 00:58:55.653031 17176 solver.cpp:229] Iteration 28800, loss = 0.103426
I1026 00:58:55.653065 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0689644 (* 1 = 0.0689644 loss)
I1026 00:58:55.653070 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0344616 (* 1 = 0.0344616 loss)
I1026 00:58:55.653074 17176 sgd_solver.cpp:106] Iteration 28800, lr = 0.001
I1026 00:58:56.202752 17176 solver.cpp:229] Iteration 28820, loss = 0.0702128
I1026 00:58:56.202785 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0365867 (* 1 = 0.0365867 loss)
I1026 00:58:56.202790 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0336261 (* 1 = 0.0336261 loss)
I1026 00:58:56.202795 17176 sgd_solver.cpp:106] Iteration 28820, lr = 0.001
I1026 00:58:56.760445 17176 solver.cpp:229] Iteration 28840, loss = 0.062184
I1026 00:58:56.760478 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0471785 (* 1 = 0.0471785 loss)
I1026 00:58:56.760483 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0150055 (* 1 = 0.0150055 loss)
I1026 00:58:56.760486 17176 sgd_solver.cpp:106] Iteration 28840, lr = 0.001
I1026 00:58:57.326676 17176 solver.cpp:229] Iteration 28860, loss = 0.0244362
I1026 00:58:57.326709 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0133466 (* 1 = 0.0133466 loss)
I1026 00:58:57.326714 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0110896 (* 1 = 0.0110896 loss)
I1026 00:58:57.326719 17176 sgd_solver.cpp:106] Iteration 28860, lr = 0.001
I1026 00:58:57.885639 17176 solver.cpp:229] Iteration 28880, loss = 0.224543
I1026 00:58:57.885681 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.129031 (* 1 = 0.129031 loss)
I1026 00:58:57.885689 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0955121 (* 1 = 0.0955121 loss)
I1026 00:58:57.885691 17176 sgd_solver.cpp:106] Iteration 28880, lr = 0.001
I1026 00:58:58.442342 17176 solver.cpp:229] Iteration 28900, loss = 0.0443372
I1026 00:58:58.442383 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0262092 (* 1 = 0.0262092 loss)
I1026 00:58:58.442386 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.018128 (* 1 = 0.018128 loss)
I1026 00:58:58.442390 17176 sgd_solver.cpp:106] Iteration 28900, lr = 0.001
I1026 00:58:59.000586 17176 solver.cpp:229] Iteration 28920, loss = 0.103852
I1026 00:58:59.000617 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0623783 (* 1 = 0.0623783 loss)
I1026 00:58:59.000622 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0414735 (* 1 = 0.0414735 loss)
I1026 00:58:59.000627 17176 sgd_solver.cpp:106] Iteration 28920, lr = 0.001
I1026 00:58:59.562961 17176 solver.cpp:229] Iteration 28940, loss = 0.117169
I1026 00:58:59.562994 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.086595 (* 1 = 0.086595 loss)
I1026 00:58:59.562999 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0305745 (* 1 = 0.0305745 loss)
I1026 00:58:59.563004 17176 sgd_solver.cpp:106] Iteration 28940, lr = 0.001
I1026 00:59:00.119726 17176 solver.cpp:229] Iteration 28960, loss = 0.214921
I1026 00:59:00.119757 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.136221 (* 1 = 0.136221 loss)
I1026 00:59:00.119762 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0787002 (* 1 = 0.0787002 loss)
I1026 00:59:00.119765 17176 sgd_solver.cpp:106] Iteration 28960, lr = 0.001
I1026 00:59:00.670110 17176 solver.cpp:229] Iteration 28980, loss = 0.0714804
I1026 00:59:00.670143 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0281689 (* 1 = 0.0281689 loss)
I1026 00:59:00.670148 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0433115 (* 1 = 0.0433115 loss)
I1026 00:59:00.670152 17176 sgd_solver.cpp:106] Iteration 28980, lr = 0.001
I1026 00:59:01.232995 17176 solver.cpp:229] Iteration 29000, loss = 0.050457
I1026 00:59:01.233027 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0196487 (* 1 = 0.0196487 loss)
I1026 00:59:01.233032 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0308083 (* 1 = 0.0308083 loss)
I1026 00:59:01.233037 17176 sgd_solver.cpp:106] Iteration 29000, lr = 0.001
I1026 00:59:01.789716 17176 solver.cpp:229] Iteration 29020, loss = 0.0739936
I1026 00:59:01.789759 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0254308 (* 1 = 0.0254308 loss)
I1026 00:59:01.789764 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0485628 (* 1 = 0.0485628 loss)
I1026 00:59:01.789768 17176 sgd_solver.cpp:106] Iteration 29020, lr = 0.001
I1026 00:59:02.348624 17176 solver.cpp:229] Iteration 29040, loss = 0.011023
I1026 00:59:02.348666 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00817307 (* 1 = 0.00817307 loss)
I1026 00:59:02.348671 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00284989 (* 1 = 0.00284989 loss)
I1026 00:59:02.348675 17176 sgd_solver.cpp:106] Iteration 29040, lr = 0.001
I1026 00:59:02.902545 17176 solver.cpp:229] Iteration 29060, loss = 0.113183
I1026 00:59:02.902578 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.076273 (* 1 = 0.076273 loss)
I1026 00:59:02.902583 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0369097 (* 1 = 0.0369097 loss)
I1026 00:59:02.902587 17176 sgd_solver.cpp:106] Iteration 29060, lr = 0.001
I1026 00:59:03.461402 17176 solver.cpp:229] Iteration 29080, loss = 0.0141813
I1026 00:59:03.461433 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00912147 (* 1 = 0.00912147 loss)
I1026 00:59:03.461437 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00505983 (* 1 = 0.00505983 loss)
I1026 00:59:03.461441 17176 sgd_solver.cpp:106] Iteration 29080, lr = 0.001
I1026 00:59:04.024976 17176 solver.cpp:229] Iteration 29100, loss = 0.053193
I1026 00:59:04.025008 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0331797 (* 1 = 0.0331797 loss)
I1026 00:59:04.025012 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0200133 (* 1 = 0.0200133 loss)
I1026 00:59:04.025017 17176 sgd_solver.cpp:106] Iteration 29100, lr = 0.001
I1026 00:59:04.588764 17176 solver.cpp:229] Iteration 29120, loss = 0.0484511
I1026 00:59:04.588796 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0301857 (* 1 = 0.0301857 loss)
I1026 00:59:04.588801 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0182654 (* 1 = 0.0182654 loss)
I1026 00:59:04.588804 17176 sgd_solver.cpp:106] Iteration 29120, lr = 0.001
I1026 00:59:05.146064 17176 solver.cpp:229] Iteration 29140, loss = 0.0511161
I1026 00:59:05.146096 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0371544 (* 1 = 0.0371544 loss)
I1026 00:59:05.146100 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0139617 (* 1 = 0.0139617 loss)
I1026 00:59:05.146105 17176 sgd_solver.cpp:106] Iteration 29140, lr = 0.001
I1026 00:59:05.700186 17176 solver.cpp:229] Iteration 29160, loss = 0.120905
I1026 00:59:05.700229 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.11555 (* 1 = 0.11555 loss)
I1026 00:59:05.700235 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00535506 (* 1 = 0.00535506 loss)
I1026 00:59:05.700239 17176 sgd_solver.cpp:106] Iteration 29160, lr = 0.001
I1026 00:59:06.248754 17176 solver.cpp:229] Iteration 29180, loss = 0.0356689
I1026 00:59:06.248796 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0293213 (* 1 = 0.0293213 loss)
I1026 00:59:06.248801 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00634764 (* 1 = 0.00634764 loss)
I1026 00:59:06.248805 17176 sgd_solver.cpp:106] Iteration 29180, lr = 0.001
I1026 00:59:06.820706 17176 solver.cpp:229] Iteration 29200, loss = 0.340675
I1026 00:59:06.820739 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.105218 (* 1 = 0.105218 loss)
I1026 00:59:06.820742 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.235456 (* 1 = 0.235456 loss)
I1026 00:59:06.820747 17176 sgd_solver.cpp:106] Iteration 29200, lr = 0.001
I1026 00:59:07.385051 17176 solver.cpp:229] Iteration 29220, loss = 0.0206192
I1026 00:59:07.385083 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0133741 (* 1 = 0.0133741 loss)
I1026 00:59:07.385089 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00724505 (* 1 = 0.00724505 loss)
I1026 00:59:07.385094 17176 sgd_solver.cpp:106] Iteration 29220, lr = 0.001
I1026 00:59:07.942958 17176 solver.cpp:229] Iteration 29240, loss = 0.107141
I1026 00:59:07.942992 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.103533 (* 1 = 0.103533 loss)
I1026 00:59:07.942997 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00360847 (* 1 = 0.00360847 loss)
I1026 00:59:07.943001 17176 sgd_solver.cpp:106] Iteration 29240, lr = 0.001
I1026 00:59:08.499351 17176 solver.cpp:229] Iteration 29260, loss = 0.0525079
I1026 00:59:08.499383 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0281425 (* 1 = 0.0281425 loss)
I1026 00:59:08.499388 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0243654 (* 1 = 0.0243654 loss)
I1026 00:59:08.499402 17176 sgd_solver.cpp:106] Iteration 29260, lr = 0.001
I1026 00:59:09.055630 17176 solver.cpp:229] Iteration 29280, loss = 0.244774
I1026 00:59:09.055660 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.214593 (* 1 = 0.214593 loss)
I1026 00:59:09.055665 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0301811 (* 1 = 0.0301811 loss)
I1026 00:59:09.055670 17176 sgd_solver.cpp:106] Iteration 29280, lr = 0.001
I1026 00:59:09.615252 17176 solver.cpp:229] Iteration 29300, loss = 0.0786205
I1026 00:59:09.615283 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0589082 (* 1 = 0.0589082 loss)
I1026 00:59:09.615288 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0197123 (* 1 = 0.0197123 loss)
I1026 00:59:09.615291 17176 sgd_solver.cpp:106] Iteration 29300, lr = 0.001
I1026 00:59:10.165541 17176 solver.cpp:229] Iteration 29320, loss = 0.0165327
I1026 00:59:10.165573 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0114406 (* 1 = 0.0114406 loss)
I1026 00:59:10.165578 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00509208 (* 1 = 0.00509208 loss)
I1026 00:59:10.165583 17176 sgd_solver.cpp:106] Iteration 29320, lr = 0.001
I1026 00:59:10.720461 17176 solver.cpp:229] Iteration 29340, loss = 0.0596138
I1026 00:59:10.720494 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0421774 (* 1 = 0.0421774 loss)
I1026 00:59:10.720499 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0174364 (* 1 = 0.0174364 loss)
I1026 00:59:10.720501 17176 sgd_solver.cpp:106] Iteration 29340, lr = 0.001
I1026 00:59:11.268697 17176 solver.cpp:229] Iteration 29360, loss = 0.0508387
I1026 00:59:11.268728 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0433011 (* 1 = 0.0433011 loss)
I1026 00:59:11.268733 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00753764 (* 1 = 0.00753764 loss)
I1026 00:59:11.268738 17176 sgd_solver.cpp:106] Iteration 29360, lr = 0.001
I1026 00:59:11.822145 17176 solver.cpp:229] Iteration 29380, loss = 0.0707901
I1026 00:59:11.822177 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0678915 (* 1 = 0.0678915 loss)
I1026 00:59:11.822182 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00289863 (* 1 = 0.00289863 loss)
I1026 00:59:11.822186 17176 sgd_solver.cpp:106] Iteration 29380, lr = 0.001
I1026 00:59:12.382551 17176 solver.cpp:229] Iteration 29400, loss = 0.136103
I1026 00:59:12.382583 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0757771 (* 1 = 0.0757771 loss)
I1026 00:59:12.382588 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.060326 (* 1 = 0.060326 loss)
I1026 00:59:12.382592 17176 sgd_solver.cpp:106] Iteration 29400, lr = 0.001
I1026 00:59:12.947981 17176 solver.cpp:229] Iteration 29420, loss = 0.00850454
I1026 00:59:12.948015 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00517288 (* 1 = 0.00517288 loss)
I1026 00:59:12.948020 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00333166 (* 1 = 0.00333166 loss)
I1026 00:59:12.948024 17176 sgd_solver.cpp:106] Iteration 29420, lr = 0.001
I1026 00:59:13.518436 17176 solver.cpp:229] Iteration 29440, loss = 1.00795
I1026 00:59:13.518470 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.394944 (* 1 = 0.394944 loss)
I1026 00:59:13.518474 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.613007 (* 1 = 0.613007 loss)
I1026 00:59:13.518478 17176 sgd_solver.cpp:106] Iteration 29440, lr = 0.001
I1026 00:59:14.075667 17176 solver.cpp:229] Iteration 29460, loss = 0.050685
I1026 00:59:14.075709 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0409939 (* 1 = 0.0409939 loss)
I1026 00:59:14.075714 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00969107 (* 1 = 0.00969107 loss)
I1026 00:59:14.075718 17176 sgd_solver.cpp:106] Iteration 29460, lr = 0.001
I1026 00:59:14.646381 17176 solver.cpp:229] Iteration 29480, loss = 0.0413545
I1026 00:59:14.646410 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0289233 (* 1 = 0.0289233 loss)
I1026 00:59:14.646415 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0124312 (* 1 = 0.0124312 loss)
I1026 00:59:14.646420 17176 sgd_solver.cpp:106] Iteration 29480, lr = 0.001
I1026 00:59:15.218101 17176 solver.cpp:229] Iteration 29500, loss = 0.0536666
I1026 00:59:15.218135 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0397875 (* 1 = 0.0397875 loss)
I1026 00:59:15.218140 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.013879 (* 1 = 0.013879 loss)
I1026 00:59:15.218144 17176 sgd_solver.cpp:106] Iteration 29500, lr = 0.001
I1026 00:59:15.783040 17176 solver.cpp:229] Iteration 29520, loss = 0.0661762
I1026 00:59:15.783071 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0116959 (* 1 = 0.0116959 loss)
I1026 00:59:15.783077 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0544803 (* 1 = 0.0544803 loss)
I1026 00:59:15.783082 17176 sgd_solver.cpp:106] Iteration 29520, lr = 0.001
I1026 00:59:16.345149 17176 solver.cpp:229] Iteration 29540, loss = 0.0760525
I1026 00:59:16.345180 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0325351 (* 1 = 0.0325351 loss)
I1026 00:59:16.345183 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0435174 (* 1 = 0.0435174 loss)
I1026 00:59:16.345187 17176 sgd_solver.cpp:106] Iteration 29540, lr = 0.001
I1026 00:59:16.910842 17176 solver.cpp:229] Iteration 29560, loss = 0.148827
I1026 00:59:16.910887 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0863109 (* 1 = 0.0863109 loss)
I1026 00:59:16.910892 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0625159 (* 1 = 0.0625159 loss)
I1026 00:59:16.910895 17176 sgd_solver.cpp:106] Iteration 29560, lr = 0.001
I1026 00:59:17.467927 17176 solver.cpp:229] Iteration 29580, loss = 0.131043
I1026 00:59:17.467962 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0431064 (* 1 = 0.0431064 loss)
I1026 00:59:17.467969 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0879368 (* 1 = 0.0879368 loss)
I1026 00:59:17.467974 17176 sgd_solver.cpp:106] Iteration 29580, lr = 0.001
I1026 00:59:18.029130 17176 solver.cpp:229] Iteration 29600, loss = 0.156399
I1026 00:59:18.029161 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.111042 (* 1 = 0.111042 loss)
I1026 00:59:18.029166 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0453566 (* 1 = 0.0453566 loss)
I1026 00:59:18.029170 17176 sgd_solver.cpp:106] Iteration 29600, lr = 0.001
I1026 00:59:18.586760 17176 solver.cpp:229] Iteration 29620, loss = 0.483629
I1026 00:59:18.586791 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.213392 (* 1 = 0.213392 loss)
I1026 00:59:18.586796 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.270237 (* 1 = 0.270237 loss)
I1026 00:59:18.586799 17176 sgd_solver.cpp:106] Iteration 29620, lr = 0.001
I1026 00:59:19.139786 17176 solver.cpp:229] Iteration 29640, loss = 0.0864458
I1026 00:59:19.139818 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0361309 (* 1 = 0.0361309 loss)
I1026 00:59:19.139822 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0503149 (* 1 = 0.0503149 loss)
I1026 00:59:19.139827 17176 sgd_solver.cpp:106] Iteration 29640, lr = 0.001
I1026 00:59:19.692544 17176 solver.cpp:229] Iteration 29660, loss = 0.0532964
I1026 00:59:19.692576 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0418043 (* 1 = 0.0418043 loss)
I1026 00:59:19.692581 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0114922 (* 1 = 0.0114922 loss)
I1026 00:59:19.692584 17176 sgd_solver.cpp:106] Iteration 29660, lr = 0.001
I1026 00:59:20.255738 17176 solver.cpp:229] Iteration 29680, loss = 0.0876772
I1026 00:59:20.255770 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0657484 (* 1 = 0.0657484 loss)
I1026 00:59:20.255774 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0219287 (* 1 = 0.0219287 loss)
I1026 00:59:20.255779 17176 sgd_solver.cpp:106] Iteration 29680, lr = 0.001
I1026 00:59:20.816565 17176 solver.cpp:229] Iteration 29700, loss = 0.0956456
I1026 00:59:20.816597 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0436697 (* 1 = 0.0436697 loss)
I1026 00:59:20.816602 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0519758 (* 1 = 0.0519758 loss)
I1026 00:59:20.816606 17176 sgd_solver.cpp:106] Iteration 29700, lr = 0.001
I1026 00:59:21.387246 17176 solver.cpp:229] Iteration 29720, loss = 0.0678295
I1026 00:59:21.387279 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0453924 (* 1 = 0.0453924 loss)
I1026 00:59:21.387282 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0224371 (* 1 = 0.0224371 loss)
I1026 00:59:21.387286 17176 sgd_solver.cpp:106] Iteration 29720, lr = 0.001
I1026 00:59:21.939960 17176 solver.cpp:229] Iteration 29740, loss = 0.102011
I1026 00:59:21.939990 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.070301 (* 1 = 0.070301 loss)
I1026 00:59:21.939996 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0317103 (* 1 = 0.0317103 loss)
I1026 00:59:21.939999 17176 sgd_solver.cpp:106] Iteration 29740, lr = 0.001
I1026 00:59:22.507347 17176 solver.cpp:229] Iteration 29760, loss = 0.0391056
I1026 00:59:22.507380 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0139294 (* 1 = 0.0139294 loss)
I1026 00:59:22.507385 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0251762 (* 1 = 0.0251762 loss)
I1026 00:59:22.507388 17176 sgd_solver.cpp:106] Iteration 29760, lr = 0.001
I1026 00:59:23.061128 17176 solver.cpp:229] Iteration 29780, loss = 0.0495657
I1026 00:59:23.061161 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0328261 (* 1 = 0.0328261 loss)
I1026 00:59:23.061166 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0167395 (* 1 = 0.0167395 loss)
I1026 00:59:23.061168 17176 sgd_solver.cpp:106] Iteration 29780, lr = 0.001
I1026 00:59:23.635814 17176 solver.cpp:229] Iteration 29800, loss = 0.0638573
I1026 00:59:23.635846 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.045007 (* 1 = 0.045007 loss)
I1026 00:59:23.635851 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0188503 (* 1 = 0.0188503 loss)
I1026 00:59:23.635854 17176 sgd_solver.cpp:106] Iteration 29800, lr = 0.001
I1026 00:59:24.199425 17176 solver.cpp:229] Iteration 29820, loss = 0.15921
I1026 00:59:24.199475 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0723307 (* 1 = 0.0723307 loss)
I1026 00:59:24.199489 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0868795 (* 1 = 0.0868795 loss)
I1026 00:59:24.199493 17176 sgd_solver.cpp:106] Iteration 29820, lr = 0.001
I1026 00:59:24.769194 17176 solver.cpp:229] Iteration 29840, loss = 0.0324436
I1026 00:59:24.769227 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0236834 (* 1 = 0.0236834 loss)
I1026 00:59:24.769232 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0087602 (* 1 = 0.0087602 loss)
I1026 00:59:24.769235 17176 sgd_solver.cpp:106] Iteration 29840, lr = 0.001
I1026 00:59:25.310487 17176 solver.cpp:229] Iteration 29860, loss = 0.0506642
I1026 00:59:25.310523 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0391732 (* 1 = 0.0391732 loss)
I1026 00:59:25.310528 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.011491 (* 1 = 0.011491 loss)
I1026 00:59:25.310533 17176 sgd_solver.cpp:106] Iteration 29860, lr = 0.001
I1026 00:59:25.872463 17176 solver.cpp:229] Iteration 29880, loss = 0.0576514
I1026 00:59:25.872496 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0431409 (* 1 = 0.0431409 loss)
I1026 00:59:25.872501 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0145104 (* 1 = 0.0145104 loss)
I1026 00:59:25.872505 17176 sgd_solver.cpp:106] Iteration 29880, lr = 0.001
I1026 00:59:26.424367 17176 solver.cpp:229] Iteration 29900, loss = 0.0961016
I1026 00:59:26.424399 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0668396 (* 1 = 0.0668396 loss)
I1026 00:59:26.424404 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0292621 (* 1 = 0.0292621 loss)
I1026 00:59:26.424408 17176 sgd_solver.cpp:106] Iteration 29900, lr = 0.001
I1026 00:59:26.993161 17176 solver.cpp:229] Iteration 29920, loss = 0.0452311
I1026 00:59:26.993193 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0278862 (* 1 = 0.0278862 loss)
I1026 00:59:26.993198 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0173449 (* 1 = 0.0173449 loss)
I1026 00:59:26.993202 17176 sgd_solver.cpp:106] Iteration 29920, lr = 0.001
I1026 00:59:27.543833 17176 solver.cpp:229] Iteration 29940, loss = 0.075507
I1026 00:59:27.543865 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0269024 (* 1 = 0.0269024 loss)
I1026 00:59:27.543870 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0486046 (* 1 = 0.0486046 loss)
I1026 00:59:27.543874 17176 sgd_solver.cpp:106] Iteration 29940, lr = 0.001
I1026 00:59:28.094071 17176 solver.cpp:229] Iteration 29960, loss = 0.043858
I1026 00:59:28.094104 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0314156 (* 1 = 0.0314156 loss)
I1026 00:59:28.094108 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0124424 (* 1 = 0.0124424 loss)
I1026 00:59:28.094113 17176 sgd_solver.cpp:106] Iteration 29960, lr = 0.001
I1026 00:59:28.650818 17176 solver.cpp:229] Iteration 29980, loss = 0.0567936
I1026 00:59:28.650851 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0277406 (* 1 = 0.0277406 loss)
I1026 00:59:28.650856 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.029053 (* 1 = 0.029053 loss)
I1026 00:59:28.650859 17176 sgd_solver.cpp:106] Iteration 29980, lr = 0.001
I1026 00:59:29.636301 17176 solver.cpp:229] Iteration 30000, loss = 0.0381388
I1026 00:59:29.636345 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0250496 (* 1 = 0.0250496 loss)
I1026 00:59:29.636353 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0130892 (* 1 = 0.0130892 loss)
I1026 00:59:29.636358 17176 sgd_solver.cpp:106] Iteration 30000, lr = 0.001
I1026 00:59:30.203389 17176 solver.cpp:229] Iteration 30020, loss = 0.0592463
I1026 00:59:30.203423 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0262769 (* 1 = 0.0262769 loss)
I1026 00:59:30.203428 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0329695 (* 1 = 0.0329695 loss)
I1026 00:59:30.203438 17176 sgd_solver.cpp:106] Iteration 30020, lr = 0.001
I1026 00:59:30.755143 17176 solver.cpp:229] Iteration 30040, loss = 0.0561479
I1026 00:59:30.755175 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0395179 (* 1 = 0.0395179 loss)
I1026 00:59:30.755180 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0166301 (* 1 = 0.0166301 loss)
I1026 00:59:30.755184 17176 sgd_solver.cpp:106] Iteration 30040, lr = 0.001
I1026 00:59:31.319245 17176 solver.cpp:229] Iteration 30060, loss = 0.0245085
I1026 00:59:31.319279 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0169124 (* 1 = 0.0169124 loss)
I1026 00:59:31.319285 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00759613 (* 1 = 0.00759613 loss)
I1026 00:59:31.319289 17176 sgd_solver.cpp:106] Iteration 30060, lr = 0.001
I1026 00:59:31.882401 17176 solver.cpp:229] Iteration 30080, loss = 0.0911863
I1026 00:59:31.882436 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0077821 (* 1 = 0.0077821 loss)
I1026 00:59:31.882441 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0834042 (* 1 = 0.0834042 loss)
I1026 00:59:31.882444 17176 sgd_solver.cpp:106] Iteration 30080, lr = 0.001
I1026 00:59:32.445149 17176 solver.cpp:229] Iteration 30100, loss = 0.02891
I1026 00:59:32.445183 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0255012 (* 1 = 0.0255012 loss)
I1026 00:59:32.445188 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00340882 (* 1 = 0.00340882 loss)
I1026 00:59:32.445191 17176 sgd_solver.cpp:106] Iteration 30100, lr = 0.001
I1026 00:59:33.012581 17176 solver.cpp:229] Iteration 30120, loss = 0.116421
I1026 00:59:33.012614 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.108015 (* 1 = 0.108015 loss)
I1026 00:59:33.012620 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00840584 (* 1 = 0.00840584 loss)
I1026 00:59:33.012625 17176 sgd_solver.cpp:106] Iteration 30120, lr = 0.001
I1026 00:59:33.568768 17176 solver.cpp:229] Iteration 30140, loss = 0.0603481
I1026 00:59:33.568810 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.049193 (* 1 = 0.049193 loss)
I1026 00:59:33.568825 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0111551 (* 1 = 0.0111551 loss)
I1026 00:59:33.568828 17176 sgd_solver.cpp:106] Iteration 30140, lr = 0.001
I1026 00:59:34.144229 17176 solver.cpp:229] Iteration 30160, loss = 0.0417744
I1026 00:59:34.144261 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0348378 (* 1 = 0.0348378 loss)
I1026 00:59:34.144266 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00693664 (* 1 = 0.00693664 loss)
I1026 00:59:34.144270 17176 sgd_solver.cpp:106] Iteration 30160, lr = 0.001
I1026 00:59:34.704390 17176 solver.cpp:229] Iteration 30180, loss = 0.0174268
I1026 00:59:34.704421 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0159262 (* 1 = 0.0159262 loss)
I1026 00:59:34.704427 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00150059 (* 1 = 0.00150059 loss)
I1026 00:59:34.704430 17176 sgd_solver.cpp:106] Iteration 30180, lr = 0.001
I1026 00:59:35.265594 17176 solver.cpp:229] Iteration 30200, loss = 0.060344
I1026 00:59:35.265625 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.03888 (* 1 = 0.03888 loss)
I1026 00:59:35.265630 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0214639 (* 1 = 0.0214639 loss)
I1026 00:59:35.265635 17176 sgd_solver.cpp:106] Iteration 30200, lr = 0.001
I1026 00:59:35.837489 17176 solver.cpp:229] Iteration 30220, loss = 0.0945195
I1026 00:59:35.837522 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0542326 (* 1 = 0.0542326 loss)
I1026 00:59:35.837527 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0402869 (* 1 = 0.0402869 loss)
I1026 00:59:35.837530 17176 sgd_solver.cpp:106] Iteration 30220, lr = 0.001
I1026 00:59:36.399852 17176 solver.cpp:229] Iteration 30240, loss = 0.128607
I1026 00:59:36.399883 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0839373 (* 1 = 0.0839373 loss)
I1026 00:59:36.399888 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0446701 (* 1 = 0.0446701 loss)
I1026 00:59:36.399893 17176 sgd_solver.cpp:106] Iteration 30240, lr = 0.001
I1026 00:59:36.957532 17176 solver.cpp:229] Iteration 30260, loss = 0.0231412
I1026 00:59:36.957564 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0135831 (* 1 = 0.0135831 loss)
I1026 00:59:36.957569 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0095581 (* 1 = 0.0095581 loss)
I1026 00:59:36.957574 17176 sgd_solver.cpp:106] Iteration 30260, lr = 0.001
I1026 00:59:37.525861 17176 solver.cpp:229] Iteration 30280, loss = 0.0357751
I1026 00:59:37.525893 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0184039 (* 1 = 0.0184039 loss)
I1026 00:59:37.525898 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0173712 (* 1 = 0.0173712 loss)
I1026 00:59:37.525902 17176 sgd_solver.cpp:106] Iteration 30280, lr = 0.001
I1026 00:59:38.074988 17176 solver.cpp:229] Iteration 30300, loss = 0.0685256
I1026 00:59:38.075019 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0279142 (* 1 = 0.0279142 loss)
I1026 00:59:38.075024 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0406114 (* 1 = 0.0406114 loss)
I1026 00:59:38.075028 17176 sgd_solver.cpp:106] Iteration 30300, lr = 0.001
I1026 00:59:38.622483 17176 solver.cpp:229] Iteration 30320, loss = 0.0365521
I1026 00:59:38.622517 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00821041 (* 1 = 0.00821041 loss)
I1026 00:59:38.622522 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0283417 (* 1 = 0.0283417 loss)
I1026 00:59:38.622526 17176 sgd_solver.cpp:106] Iteration 30320, lr = 0.001
I1026 00:59:39.175534 17176 solver.cpp:229] Iteration 30340, loss = 0.0304448
I1026 00:59:39.175566 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0091061 (* 1 = 0.0091061 loss)
I1026 00:59:39.175571 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0213387 (* 1 = 0.0213387 loss)
I1026 00:59:39.175575 17176 sgd_solver.cpp:106] Iteration 30340, lr = 0.001
I1026 00:59:39.729774 17176 solver.cpp:229] Iteration 30360, loss = 0.0238621
I1026 00:59:39.729807 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0205372 (* 1 = 0.0205372 loss)
I1026 00:59:39.729812 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00332494 (* 1 = 0.00332494 loss)
I1026 00:59:39.729816 17176 sgd_solver.cpp:106] Iteration 30360, lr = 0.001
I1026 00:59:40.292985 17176 solver.cpp:229] Iteration 30380, loss = 0.0444585
I1026 00:59:40.293017 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0412674 (* 1 = 0.0412674 loss)
I1026 00:59:40.293022 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0031911 (* 1 = 0.0031911 loss)
I1026 00:59:40.293026 17176 sgd_solver.cpp:106] Iteration 30380, lr = 0.001
I1026 00:59:40.852447 17176 solver.cpp:229] Iteration 30400, loss = 0.0777211
I1026 00:59:40.852478 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0560534 (* 1 = 0.0560534 loss)
I1026 00:59:40.852483 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0216678 (* 1 = 0.0216678 loss)
I1026 00:59:40.852486 17176 sgd_solver.cpp:106] Iteration 30400, lr = 0.001
I1026 00:59:41.416131 17176 solver.cpp:229] Iteration 30420, loss = 0.264788
I1026 00:59:41.416162 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.125567 (* 1 = 0.125567 loss)
I1026 00:59:41.416167 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.139221 (* 1 = 0.139221 loss)
I1026 00:59:41.416169 17176 sgd_solver.cpp:106] Iteration 30420, lr = 0.001
I1026 00:59:41.979485 17176 solver.cpp:229] Iteration 30440, loss = 0.106326
I1026 00:59:41.979518 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0287226 (* 1 = 0.0287226 loss)
I1026 00:59:41.979523 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0776032 (* 1 = 0.0776032 loss)
I1026 00:59:41.979528 17176 sgd_solver.cpp:106] Iteration 30440, lr = 0.001
I1026 00:59:42.551777 17176 solver.cpp:229] Iteration 30460, loss = 0.0348729
I1026 00:59:42.551808 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0265579 (* 1 = 0.0265579 loss)
I1026 00:59:42.551813 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00831503 (* 1 = 0.00831503 loss)
I1026 00:59:42.551818 17176 sgd_solver.cpp:106] Iteration 30460, lr = 0.001
I1026 00:59:43.105484 17176 solver.cpp:229] Iteration 30480, loss = 0.0215648
I1026 00:59:43.105517 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0190152 (* 1 = 0.0190152 loss)
I1026 00:59:43.105522 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00254966 (* 1 = 0.00254966 loss)
I1026 00:59:43.105537 17176 sgd_solver.cpp:106] Iteration 30480, lr = 0.001
I1026 00:59:43.680690 17176 solver.cpp:229] Iteration 30500, loss = 0.715944
I1026 00:59:43.680721 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.190937 (* 1 = 0.190937 loss)
I1026 00:59:43.680727 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.525007 (* 1 = 0.525007 loss)
I1026 00:59:43.680742 17176 sgd_solver.cpp:106] Iteration 30500, lr = 0.001
I1026 00:59:44.241838 17176 solver.cpp:229] Iteration 30520, loss = 0.0893436
I1026 00:59:44.241871 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0413116 (* 1 = 0.0413116 loss)
I1026 00:59:44.241878 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.048032 (* 1 = 0.048032 loss)
I1026 00:59:44.241892 17176 sgd_solver.cpp:106] Iteration 30520, lr = 0.001
I1026 00:59:44.817993 17176 solver.cpp:229] Iteration 30540, loss = 0.0758808
I1026 00:59:44.818027 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0709447 (* 1 = 0.0709447 loss)
I1026 00:59:44.818032 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00493607 (* 1 = 0.00493607 loss)
I1026 00:59:44.818035 17176 sgd_solver.cpp:106] Iteration 30540, lr = 0.001
I1026 00:59:45.392506 17176 solver.cpp:229] Iteration 30560, loss = 0.119029
I1026 00:59:45.392539 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.105847 (* 1 = 0.105847 loss)
I1026 00:59:45.392544 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0131817 (* 1 = 0.0131817 loss)
I1026 00:59:45.392560 17176 sgd_solver.cpp:106] Iteration 30560, lr = 0.001
I1026 00:59:45.956737 17176 solver.cpp:229] Iteration 30580, loss = 0.0874136
I1026 00:59:45.956770 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.064812 (* 1 = 0.064812 loss)
I1026 00:59:45.956776 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0226016 (* 1 = 0.0226016 loss)
I1026 00:59:45.956790 17176 sgd_solver.cpp:106] Iteration 30580, lr = 0.001
I1026 00:59:46.528540 17176 solver.cpp:229] Iteration 30600, loss = 0.125644
I1026 00:59:46.528574 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0736484 (* 1 = 0.0736484 loss)
I1026 00:59:46.528580 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0519957 (* 1 = 0.0519957 loss)
I1026 00:59:46.528595 17176 sgd_solver.cpp:106] Iteration 30600, lr = 0.001
I1026 00:59:47.091706 17176 solver.cpp:229] Iteration 30620, loss = 0.0634725
I1026 00:59:47.091740 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0526577 (* 1 = 0.0526577 loss)
I1026 00:59:47.091747 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0108148 (* 1 = 0.0108148 loss)
I1026 00:59:47.091750 17176 sgd_solver.cpp:106] Iteration 30620, lr = 0.001
I1026 00:59:47.661685 17176 solver.cpp:229] Iteration 30640, loss = 0.0541576
I1026 00:59:47.661728 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.038918 (* 1 = 0.038918 loss)
I1026 00:59:47.661733 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0152396 (* 1 = 0.0152396 loss)
I1026 00:59:47.661738 17176 sgd_solver.cpp:106] Iteration 30640, lr = 0.001
I1026 00:59:48.219339 17176 solver.cpp:229] Iteration 30660, loss = 0.0436161
I1026 00:59:48.219383 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0125723 (* 1 = 0.0125723 loss)
I1026 00:59:48.219388 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0310438 (* 1 = 0.0310438 loss)
I1026 00:59:48.219391 17176 sgd_solver.cpp:106] Iteration 30660, lr = 0.001
I1026 00:59:48.772455 17176 solver.cpp:229] Iteration 30680, loss = 0.151837
I1026 00:59:48.772488 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0723356 (* 1 = 0.0723356 loss)
I1026 00:59:48.772493 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0795014 (* 1 = 0.0795014 loss)
I1026 00:59:48.772497 17176 sgd_solver.cpp:106] Iteration 30680, lr = 0.001
I1026 00:59:49.333405 17176 solver.cpp:229] Iteration 30700, loss = 0.0328907
I1026 00:59:49.333437 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0282231 (* 1 = 0.0282231 loss)
I1026 00:59:49.333442 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00466762 (* 1 = 0.00466762 loss)
I1026 00:59:49.333446 17176 sgd_solver.cpp:106] Iteration 30700, lr = 0.001
I1026 00:59:49.898123 17176 solver.cpp:229] Iteration 30720, loss = 0.0377538
I1026 00:59:49.898155 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0217629 (* 1 = 0.0217629 loss)
I1026 00:59:49.898160 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0159909 (* 1 = 0.0159909 loss)
I1026 00:59:49.898165 17176 sgd_solver.cpp:106] Iteration 30720, lr = 0.001
I1026 00:59:50.451789 17176 solver.cpp:229] Iteration 30740, loss = 0.123437
I1026 00:59:50.451820 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0203443 (* 1 = 0.0203443 loss)
I1026 00:59:50.451825 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.103093 (* 1 = 0.103093 loss)
I1026 00:59:50.451829 17176 sgd_solver.cpp:106] Iteration 30740, lr = 0.001
I1026 00:59:51.002045 17176 solver.cpp:229] Iteration 30760, loss = 0.0339347
I1026 00:59:51.002079 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0312085 (* 1 = 0.0312085 loss)
I1026 00:59:51.002085 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00272624 (* 1 = 0.00272624 loss)
I1026 00:59:51.002089 17176 sgd_solver.cpp:106] Iteration 30760, lr = 0.001
I1026 00:59:51.566368 17176 solver.cpp:229] Iteration 30780, loss = 0.785054
I1026 00:59:51.566401 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.4027 (* 1 = 0.4027 loss)
I1026 00:59:51.566406 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.382354 (* 1 = 0.382354 loss)
I1026 00:59:51.566412 17176 sgd_solver.cpp:106] Iteration 30780, lr = 0.001
I1026 00:59:52.136042 17176 solver.cpp:229] Iteration 30800, loss = 0.0775939
I1026 00:59:52.136075 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0392696 (* 1 = 0.0392696 loss)
I1026 00:59:52.136082 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0383243 (* 1 = 0.0383243 loss)
I1026 00:59:52.136087 17176 sgd_solver.cpp:106] Iteration 30800, lr = 0.001
I1026 00:59:52.709643 17176 solver.cpp:229] Iteration 30820, loss = 0.0400851
I1026 00:59:52.709676 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0318747 (* 1 = 0.0318747 loss)
I1026 00:59:52.709681 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00821042 (* 1 = 0.00821042 loss)
I1026 00:59:52.709686 17176 sgd_solver.cpp:106] Iteration 30820, lr = 0.001
I1026 00:59:53.276185 17176 solver.cpp:229] Iteration 30840, loss = 0.0599671
I1026 00:59:53.276228 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0404202 (* 1 = 0.0404202 loss)
I1026 00:59:53.276234 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0195469 (* 1 = 0.0195469 loss)
I1026 00:59:53.276238 17176 sgd_solver.cpp:106] Iteration 30840, lr = 0.001
I1026 00:59:53.843911 17176 solver.cpp:229] Iteration 30860, loss = 0.0290461
I1026 00:59:53.843952 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0267057 (* 1 = 0.0267057 loss)
I1026 00:59:53.843957 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00234037 (* 1 = 0.00234037 loss)
I1026 00:59:53.843961 17176 sgd_solver.cpp:106] Iteration 30860, lr = 0.001
I1026 00:59:54.408753 17176 solver.cpp:229] Iteration 30880, loss = 0.0486351
I1026 00:59:54.408787 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0381485 (* 1 = 0.0381485 loss)
I1026 00:59:54.408792 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0104866 (* 1 = 0.0104866 loss)
I1026 00:59:54.408795 17176 sgd_solver.cpp:106] Iteration 30880, lr = 0.001
I1026 00:59:54.978363 17176 solver.cpp:229] Iteration 30900, loss = 0.132339
I1026 00:59:54.978396 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0316087 (* 1 = 0.0316087 loss)
I1026 00:59:54.978402 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.10073 (* 1 = 0.10073 loss)
I1026 00:59:54.978407 17176 sgd_solver.cpp:106] Iteration 30900, lr = 0.001
I1026 00:59:55.542213 17176 solver.cpp:229] Iteration 30920, loss = 0.0264981
I1026 00:59:55.542255 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0167369 (* 1 = 0.0167369 loss)
I1026 00:59:55.542260 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00976122 (* 1 = 0.00976122 loss)
I1026 00:59:55.542265 17176 sgd_solver.cpp:106] Iteration 30920, lr = 0.001
I1026 00:59:56.104449 17176 solver.cpp:229] Iteration 30940, loss = 0.0560951
I1026 00:59:56.104480 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0142463 (* 1 = 0.0142463 loss)
I1026 00:59:56.104485 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0418488 (* 1 = 0.0418488 loss)
I1026 00:59:56.104490 17176 sgd_solver.cpp:106] Iteration 30940, lr = 0.001
I1026 00:59:56.661020 17176 solver.cpp:229] Iteration 30960, loss = 0.0276756
I1026 00:59:56.661052 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0201894 (* 1 = 0.0201894 loss)
I1026 00:59:56.661057 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00748621 (* 1 = 0.00748621 loss)
I1026 00:59:56.661062 17176 sgd_solver.cpp:106] Iteration 30960, lr = 0.001
I1026 00:59:57.237906 17176 solver.cpp:229] Iteration 30980, loss = 0.0473408
I1026 00:59:57.237939 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.016979 (* 1 = 0.016979 loss)
I1026 00:59:57.237944 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0303618 (* 1 = 0.0303618 loss)
I1026 00:59:57.237948 17176 sgd_solver.cpp:106] Iteration 30980, lr = 0.001
I1026 00:59:57.799084 17176 solver.cpp:229] Iteration 31000, loss = 0.0230681
I1026 00:59:57.799126 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0202715 (* 1 = 0.0202715 loss)
I1026 00:59:57.799132 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00279659 (* 1 = 0.00279659 loss)
I1026 00:59:57.799137 17176 sgd_solver.cpp:106] Iteration 31000, lr = 0.001
I1026 00:59:58.359268 17176 solver.cpp:229] Iteration 31020, loss = 0.0137712
I1026 00:59:58.359313 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00765964 (* 1 = 0.00765964 loss)
I1026 00:59:58.359318 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00611159 (* 1 = 0.00611159 loss)
I1026 00:59:58.359321 17176 sgd_solver.cpp:106] Iteration 31020, lr = 0.001
I1026 00:59:58.929402 17176 solver.cpp:229] Iteration 31040, loss = 0.0721219
I1026 00:59:58.929445 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0646994 (* 1 = 0.0646994 loss)
I1026 00:59:58.929450 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00742256 (* 1 = 0.00742256 loss)
I1026 00:59:58.929455 17176 sgd_solver.cpp:106] Iteration 31040, lr = 0.001
I1026 00:59:59.498226 17176 solver.cpp:229] Iteration 31060, loss = 0.0526821
I1026 00:59:59.498270 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0183167 (* 1 = 0.0183167 loss)
I1026 00:59:59.498275 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0343654 (* 1 = 0.0343654 loss)
I1026 00:59:59.498278 17176 sgd_solver.cpp:106] Iteration 31060, lr = 0.001
I1026 01:00:00.062075 17176 solver.cpp:229] Iteration 31080, loss = 0.12097
I1026 01:00:00.062119 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0627626 (* 1 = 0.0627626 loss)
I1026 01:00:00.062124 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0582069 (* 1 = 0.0582069 loss)
I1026 01:00:00.062129 17176 sgd_solver.cpp:106] Iteration 31080, lr = 0.001
I1026 01:00:00.627050 17176 solver.cpp:229] Iteration 31100, loss = 0.138326
I1026 01:00:00.627094 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0875207 (* 1 = 0.0875207 loss)
I1026 01:00:00.627099 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0508053 (* 1 = 0.0508053 loss)
I1026 01:00:00.627104 17176 sgd_solver.cpp:106] Iteration 31100, lr = 0.001
I1026 01:00:01.183879 17176 solver.cpp:229] Iteration 31120, loss = 0.0709805
I1026 01:00:01.183914 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0361273 (* 1 = 0.0361273 loss)
I1026 01:00:01.183919 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0348532 (* 1 = 0.0348532 loss)
I1026 01:00:01.183924 17176 sgd_solver.cpp:106] Iteration 31120, lr = 0.001
I1026 01:00:01.735395 17176 solver.cpp:229] Iteration 31140, loss = 0.0343648
I1026 01:00:01.735433 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0176394 (* 1 = 0.0176394 loss)
I1026 01:00:01.735440 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0167254 (* 1 = 0.0167254 loss)
I1026 01:00:01.735445 17176 sgd_solver.cpp:106] Iteration 31140, lr = 0.001
I1026 01:00:02.296447 17176 solver.cpp:229] Iteration 31160, loss = 0.0622457
I1026 01:00:02.296490 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0287099 (* 1 = 0.0287099 loss)
I1026 01:00:02.296495 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0335358 (* 1 = 0.0335358 loss)
I1026 01:00:02.296499 17176 sgd_solver.cpp:106] Iteration 31160, lr = 0.001
I1026 01:00:02.871824 17176 solver.cpp:229] Iteration 31180, loss = 0.10011
I1026 01:00:02.871868 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0322067 (* 1 = 0.0322067 loss)
I1026 01:00:02.871873 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.067903 (* 1 = 0.067903 loss)
I1026 01:00:02.871878 17176 sgd_solver.cpp:106] Iteration 31180, lr = 0.001
I1026 01:00:03.430213 17176 solver.cpp:229] Iteration 31200, loss = 0.0344659
I1026 01:00:03.430245 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0286684 (* 1 = 0.0286684 loss)
I1026 01:00:03.430250 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00579752 (* 1 = 0.00579752 loss)
I1026 01:00:03.430255 17176 sgd_solver.cpp:106] Iteration 31200, lr = 0.001
I1026 01:00:04.002643 17176 solver.cpp:229] Iteration 31220, loss = 0.036241
I1026 01:00:04.002676 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0272115 (* 1 = 0.0272115 loss)
I1026 01:00:04.002681 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0090295 (* 1 = 0.0090295 loss)
I1026 01:00:04.002684 17176 sgd_solver.cpp:106] Iteration 31220, lr = 0.001
I1026 01:00:04.559801 17176 solver.cpp:229] Iteration 31240, loss = 0.0350805
I1026 01:00:04.559834 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00808863 (* 1 = 0.00808863 loss)
I1026 01:00:04.559840 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0269919 (* 1 = 0.0269919 loss)
I1026 01:00:04.559844 17176 sgd_solver.cpp:106] Iteration 31240, lr = 0.001
I1026 01:00:05.128012 17176 solver.cpp:229] Iteration 31260, loss = 0.0834862
I1026 01:00:05.128044 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0722257 (* 1 = 0.0722257 loss)
I1026 01:00:05.128049 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0112605 (* 1 = 0.0112605 loss)
I1026 01:00:05.128054 17176 sgd_solver.cpp:106] Iteration 31260, lr = 0.001
I1026 01:00:05.698894 17176 solver.cpp:229] Iteration 31280, loss = 0.0514694
I1026 01:00:05.698925 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.04623 (* 1 = 0.04623 loss)
I1026 01:00:05.698930 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00523936 (* 1 = 0.00523936 loss)
I1026 01:00:05.698935 17176 sgd_solver.cpp:106] Iteration 31280, lr = 0.001
I1026 01:00:06.264914 17176 solver.cpp:229] Iteration 31300, loss = 0.0591974
I1026 01:00:06.264946 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00920835 (* 1 = 0.00920835 loss)
I1026 01:00:06.264951 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.049989 (* 1 = 0.049989 loss)
I1026 01:00:06.265100 17176 sgd_solver.cpp:106] Iteration 31300, lr = 0.001
I1026 01:00:06.832756 17176 solver.cpp:229] Iteration 31320, loss = 0.0185732
I1026 01:00:06.832788 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0114039 (* 1 = 0.0114039 loss)
I1026 01:00:06.832793 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00716935 (* 1 = 0.00716935 loss)
I1026 01:00:06.832798 17176 sgd_solver.cpp:106] Iteration 31320, lr = 0.001
I1026 01:00:07.399621 17176 solver.cpp:229] Iteration 31340, loss = 0.0589899
I1026 01:00:07.399652 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.042602 (* 1 = 0.042602 loss)
I1026 01:00:07.399657 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0163879 (* 1 = 0.0163879 loss)
I1026 01:00:07.399660 17176 sgd_solver.cpp:106] Iteration 31340, lr = 0.001
I1026 01:00:07.961710 17176 solver.cpp:229] Iteration 31360, loss = 0.187519
I1026 01:00:07.961743 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0753698 (* 1 = 0.0753698 loss)
I1026 01:00:07.961748 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.112149 (* 1 = 0.112149 loss)
I1026 01:00:07.961752 17176 sgd_solver.cpp:106] Iteration 31360, lr = 0.001
I1026 01:00:08.511713 17176 solver.cpp:229] Iteration 31380, loss = 0.0223274
I1026 01:00:08.511745 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0160675 (* 1 = 0.0160675 loss)
I1026 01:00:08.511750 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00625985 (* 1 = 0.00625985 loss)
I1026 01:00:08.511754 17176 sgd_solver.cpp:106] Iteration 31380, lr = 0.001
I1026 01:00:09.067286 17176 solver.cpp:229] Iteration 31400, loss = 0.13988
I1026 01:00:09.067318 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0261388 (* 1 = 0.0261388 loss)
I1026 01:00:09.067323 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.113741 (* 1 = 0.113741 loss)
I1026 01:00:09.067337 17176 sgd_solver.cpp:106] Iteration 31400, lr = 0.001
I1026 01:00:09.640717 17176 solver.cpp:229] Iteration 31420, loss = 0.268774
I1026 01:00:09.640749 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.100748 (* 1 = 0.100748 loss)
I1026 01:00:09.640754 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.168026 (* 1 = 0.168026 loss)
I1026 01:00:09.640758 17176 sgd_solver.cpp:106] Iteration 31420, lr = 0.001
I1026 01:00:10.204303 17176 solver.cpp:229] Iteration 31440, loss = 0.0262941
I1026 01:00:10.204334 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.015561 (* 1 = 0.015561 loss)
I1026 01:00:10.204339 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0107331 (* 1 = 0.0107331 loss)
I1026 01:00:10.204342 17176 sgd_solver.cpp:106] Iteration 31440, lr = 0.001
I1026 01:00:10.762895 17176 solver.cpp:229] Iteration 31460, loss = 0.0768346
I1026 01:00:10.762928 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0423091 (* 1 = 0.0423091 loss)
I1026 01:00:10.762933 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0345255 (* 1 = 0.0345255 loss)
I1026 01:00:10.762938 17176 sgd_solver.cpp:106] Iteration 31460, lr = 0.001
I1026 01:00:11.311240 17176 solver.cpp:229] Iteration 31480, loss = 0.143707
I1026 01:00:11.311272 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0333962 (* 1 = 0.0333962 loss)
I1026 01:00:11.311276 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.11031 (* 1 = 0.11031 loss)
I1026 01:00:11.311281 17176 sgd_solver.cpp:106] Iteration 31480, lr = 0.001
I1026 01:00:11.875125 17176 solver.cpp:229] Iteration 31500, loss = 0.153418
I1026 01:00:11.875162 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0627598 (* 1 = 0.0627598 loss)
I1026 01:00:11.875170 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0906582 (* 1 = 0.0906582 loss)
I1026 01:00:11.875176 17176 sgd_solver.cpp:106] Iteration 31500, lr = 0.001
I1026 01:00:12.428774 17176 solver.cpp:229] Iteration 31520, loss = 0.0750333
I1026 01:00:12.428810 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0171627 (* 1 = 0.0171627 loss)
I1026 01:00:12.428818 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0578705 (* 1 = 0.0578705 loss)
I1026 01:00:12.428824 17176 sgd_solver.cpp:106] Iteration 31520, lr = 0.001
I1026 01:00:12.971262 17176 solver.cpp:229] Iteration 31540, loss = 0.115604
I1026 01:00:12.971299 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0587033 (* 1 = 0.0587033 loss)
I1026 01:00:12.971308 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0569003 (* 1 = 0.0569003 loss)
I1026 01:00:12.971315 17176 sgd_solver.cpp:106] Iteration 31540, lr = 0.001
I1026 01:00:13.528281 17176 solver.cpp:229] Iteration 31560, loss = 0.138944
I1026 01:00:13.528316 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0262453 (* 1 = 0.0262453 loss)
I1026 01:00:13.528324 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.112699 (* 1 = 0.112699 loss)
I1026 01:00:13.528331 17176 sgd_solver.cpp:106] Iteration 31560, lr = 0.001
I1026 01:00:14.100459 17176 solver.cpp:229] Iteration 31580, loss = 0.0401941
I1026 01:00:14.100493 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0308555 (* 1 = 0.0308555 loss)
I1026 01:00:14.100512 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00933859 (* 1 = 0.00933859 loss)
I1026 01:00:14.100517 17176 sgd_solver.cpp:106] Iteration 31580, lr = 0.001
I1026 01:00:14.666745 17176 solver.cpp:229] Iteration 31600, loss = 0.0531671
I1026 01:00:14.666781 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0434948 (* 1 = 0.0434948 loss)
I1026 01:00:14.666790 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00967233 (* 1 = 0.00967233 loss)
I1026 01:00:14.666795 17176 sgd_solver.cpp:106] Iteration 31600, lr = 0.001
I1026 01:00:15.241320 17176 solver.cpp:229] Iteration 31620, loss = 0.868549
I1026 01:00:15.241353 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.187621 (* 1 = 0.187621 loss)
I1026 01:00:15.241361 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.680927 (* 1 = 0.680927 loss)
I1026 01:00:15.241367 17176 sgd_solver.cpp:106] Iteration 31620, lr = 0.001
I1026 01:00:15.790045 17176 solver.cpp:229] Iteration 31640, loss = 0.0377028
I1026 01:00:15.790081 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.026233 (* 1 = 0.026233 loss)
I1026 01:00:15.790089 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0114698 (* 1 = 0.0114698 loss)
I1026 01:00:15.790094 17176 sgd_solver.cpp:106] Iteration 31640, lr = 0.001
I1026 01:00:16.345165 17176 solver.cpp:229] Iteration 31660, loss = 0.0661441
I1026 01:00:16.345199 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0536994 (* 1 = 0.0536994 loss)
I1026 01:00:16.345207 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0124447 (* 1 = 0.0124447 loss)
I1026 01:00:16.345213 17176 sgd_solver.cpp:106] Iteration 31660, lr = 0.001
I1026 01:00:16.912006 17176 solver.cpp:229] Iteration 31680, loss = 0.0858765
I1026 01:00:16.912042 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0632064 (* 1 = 0.0632064 loss)
I1026 01:00:16.912050 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0226701 (* 1 = 0.0226701 loss)
I1026 01:00:16.912055 17176 sgd_solver.cpp:106] Iteration 31680, lr = 0.001
I1026 01:00:17.465464 17176 solver.cpp:229] Iteration 31700, loss = 0.0207537
I1026 01:00:17.465498 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0195888 (* 1 = 0.0195888 loss)
I1026 01:00:17.465507 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00116492 (* 1 = 0.00116492 loss)
I1026 01:00:17.465514 17176 sgd_solver.cpp:106] Iteration 31700, lr = 0.001
I1026 01:00:18.037741 17176 solver.cpp:229] Iteration 31720, loss = 0.052358
I1026 01:00:18.037777 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0434812 (* 1 = 0.0434812 loss)
I1026 01:00:18.037786 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00887674 (* 1 = 0.00887674 loss)
I1026 01:00:18.037791 17176 sgd_solver.cpp:106] Iteration 31720, lr = 0.001
I1026 01:00:18.586557 17176 solver.cpp:229] Iteration 31740, loss = 0.12725
I1026 01:00:18.586594 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0848303 (* 1 = 0.0848303 loss)
I1026 01:00:18.586601 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0424198 (* 1 = 0.0424198 loss)
I1026 01:00:18.586607 17176 sgd_solver.cpp:106] Iteration 31740, lr = 0.001
I1026 01:00:19.142397 17176 solver.cpp:229] Iteration 31760, loss = 0.0550432
I1026 01:00:19.142433 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0272166 (* 1 = 0.0272166 loss)
I1026 01:00:19.142442 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0278266 (* 1 = 0.0278266 loss)
I1026 01:00:19.142448 17176 sgd_solver.cpp:106] Iteration 31760, lr = 0.001
I1026 01:00:19.708303 17176 solver.cpp:229] Iteration 31780, loss = 0.0366042
I1026 01:00:19.708338 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0314629 (* 1 = 0.0314629 loss)
I1026 01:00:19.708348 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00514128 (* 1 = 0.00514128 loss)
I1026 01:00:19.708353 17176 sgd_solver.cpp:106] Iteration 31780, lr = 0.001
I1026 01:00:20.275449 17176 solver.cpp:229] Iteration 31800, loss = 0.0528084
I1026 01:00:20.275486 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0409256 (* 1 = 0.0409256 loss)
I1026 01:00:20.275492 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0118828 (* 1 = 0.0118828 loss)
I1026 01:00:20.275496 17176 sgd_solver.cpp:106] Iteration 31800, lr = 0.001
I1026 01:00:20.843617 17176 solver.cpp:229] Iteration 31820, loss = 0.0384585
I1026 01:00:20.843652 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0302839 (* 1 = 0.0302839 loss)
I1026 01:00:20.843657 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00817464 (* 1 = 0.00817464 loss)
I1026 01:00:20.843662 17176 sgd_solver.cpp:106] Iteration 31820, lr = 0.001
I1026 01:00:21.397770 17176 solver.cpp:229] Iteration 31840, loss = 0.0285703
I1026 01:00:21.397804 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0091866 (* 1 = 0.0091866 loss)
I1026 01:00:21.397810 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0193837 (* 1 = 0.0193837 loss)
I1026 01:00:21.397814 17176 sgd_solver.cpp:106] Iteration 31840, lr = 0.001
I1026 01:00:21.963693 17176 solver.cpp:229] Iteration 31860, loss = 0.0391487
I1026 01:00:21.963726 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0213332 (* 1 = 0.0213332 loss)
I1026 01:00:21.963732 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0178155 (* 1 = 0.0178155 loss)
I1026 01:00:21.963735 17176 sgd_solver.cpp:106] Iteration 31860, lr = 0.001
I1026 01:00:22.524933 17176 solver.cpp:229] Iteration 31880, loss = 0.0468269
I1026 01:00:22.524965 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0226985 (* 1 = 0.0226985 loss)
I1026 01:00:22.524969 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0241284 (* 1 = 0.0241284 loss)
I1026 01:00:22.524973 17176 sgd_solver.cpp:106] Iteration 31880, lr = 0.001
I1026 01:00:23.072727 17176 solver.cpp:229] Iteration 31900, loss = 0.478
I1026 01:00:23.072759 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0903043 (* 1 = 0.0903043 loss)
I1026 01:00:23.072764 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.387695 (* 1 = 0.387695 loss)
I1026 01:00:23.072768 17176 sgd_solver.cpp:106] Iteration 31900, lr = 0.001
I1026 01:00:23.635756 17176 solver.cpp:229] Iteration 31920, loss = 0.0526431
I1026 01:00:23.635788 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0367763 (* 1 = 0.0367763 loss)
I1026 01:00:23.635794 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0158668 (* 1 = 0.0158668 loss)
I1026 01:00:23.635797 17176 sgd_solver.cpp:106] Iteration 31920, lr = 0.001
I1026 01:00:24.203866 17176 solver.cpp:229] Iteration 31940, loss = 0.0212565
I1026 01:00:24.203899 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0194819 (* 1 = 0.0194819 loss)
I1026 01:00:24.203904 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0017746 (* 1 = 0.0017746 loss)
I1026 01:00:24.203908 17176 sgd_solver.cpp:106] Iteration 31940, lr = 0.001
I1026 01:00:24.761983 17176 solver.cpp:229] Iteration 31960, loss = 0.0704918
I1026 01:00:24.762017 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0339916 (* 1 = 0.0339916 loss)
I1026 01:00:24.762020 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0365001 (* 1 = 0.0365001 loss)
I1026 01:00:24.762025 17176 sgd_solver.cpp:106] Iteration 31960, lr = 0.001
I1026 01:00:25.325248 17176 solver.cpp:229] Iteration 31980, loss = 0.0616157
I1026 01:00:25.325283 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0251095 (* 1 = 0.0251095 loss)
I1026 01:00:25.325287 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0365062 (* 1 = 0.0365062 loss)
I1026 01:00:25.325291 17176 sgd_solver.cpp:106] Iteration 31980, lr = 0.001
I1026 01:00:25.898438 17176 solver.cpp:229] Iteration 32000, loss = 0.152153
I1026 01:00:25.898473 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.129663 (* 1 = 0.129663 loss)
I1026 01:00:25.898478 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0224899 (* 1 = 0.0224899 loss)
I1026 01:00:25.898484 17176 sgd_solver.cpp:106] Iteration 32000, lr = 0.001
I1026 01:00:26.465834 17176 solver.cpp:229] Iteration 32020, loss = 0.189961
I1026 01:00:26.465865 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.133263 (* 1 = 0.133263 loss)
I1026 01:00:26.465872 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0566975 (* 1 = 0.0566975 loss)
I1026 01:00:26.465876 17176 sgd_solver.cpp:106] Iteration 32020, lr = 0.001
I1026 01:00:27.030050 17176 solver.cpp:229] Iteration 32040, loss = 0.061142
I1026 01:00:27.030084 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0206892 (* 1 = 0.0206892 loss)
I1026 01:00:27.030089 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0404528 (* 1 = 0.0404528 loss)
I1026 01:00:27.030095 17176 sgd_solver.cpp:106] Iteration 32040, lr = 0.001
I1026 01:00:27.585963 17176 solver.cpp:229] Iteration 32060, loss = 0.030034
I1026 01:00:27.585997 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00841704 (* 1 = 0.00841704 loss)
I1026 01:00:27.586002 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.021617 (* 1 = 0.021617 loss)
I1026 01:00:27.586007 17176 sgd_solver.cpp:106] Iteration 32060, lr = 0.001
I1026 01:00:28.134950 17176 solver.cpp:229] Iteration 32080, loss = 0.020922
I1026 01:00:28.134982 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0165545 (* 1 = 0.0165545 loss)
I1026 01:00:28.135004 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00436754 (* 1 = 0.00436754 loss)
I1026 01:00:28.135009 17176 sgd_solver.cpp:106] Iteration 32080, lr = 0.001
I1026 01:00:28.694408 17176 solver.cpp:229] Iteration 32100, loss = 0.0460337
I1026 01:00:28.694442 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0230097 (* 1 = 0.0230097 loss)
I1026 01:00:28.694447 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.023024 (* 1 = 0.023024 loss)
I1026 01:00:28.694452 17176 sgd_solver.cpp:106] Iteration 32100, lr = 0.001
I1026 01:00:29.248486 17176 solver.cpp:229] Iteration 32120, loss = 0.0184433
I1026 01:00:29.248519 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0127828 (* 1 = 0.0127828 loss)
I1026 01:00:29.248525 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00566047 (* 1 = 0.00566047 loss)
I1026 01:00:29.248530 17176 sgd_solver.cpp:106] Iteration 32120, lr = 0.001
I1026 01:00:29.823946 17176 solver.cpp:229] Iteration 32140, loss = 0.0290657
I1026 01:00:29.823981 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0248223 (* 1 = 0.0248223 loss)
I1026 01:00:29.824002 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00424343 (* 1 = 0.00424343 loss)
I1026 01:00:29.824007 17176 sgd_solver.cpp:106] Iteration 32140, lr = 0.001
I1026 01:00:30.390070 17176 solver.cpp:229] Iteration 32160, loss = 0.0379772
I1026 01:00:30.390103 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0197189 (* 1 = 0.0197189 loss)
I1026 01:00:30.390108 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0182583 (* 1 = 0.0182583 loss)
I1026 01:00:30.390112 17176 sgd_solver.cpp:106] Iteration 32160, lr = 0.001
I1026 01:00:30.960549 17176 solver.cpp:229] Iteration 32180, loss = 0.0542575
I1026 01:00:30.960582 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0364938 (* 1 = 0.0364938 loss)
I1026 01:00:30.960587 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0177638 (* 1 = 0.0177638 loss)
I1026 01:00:30.960592 17176 sgd_solver.cpp:106] Iteration 32180, lr = 0.001
I1026 01:00:31.518785 17176 solver.cpp:229] Iteration 32200, loss = 0.153025
I1026 01:00:31.518829 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0804688 (* 1 = 0.0804688 loss)
I1026 01:00:31.518834 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0725561 (* 1 = 0.0725561 loss)
I1026 01:00:31.518838 17176 sgd_solver.cpp:106] Iteration 32200, lr = 0.001
I1026 01:00:32.087342 17176 solver.cpp:229] Iteration 32220, loss = 0.011548
I1026 01:00:32.087386 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.010015 (* 1 = 0.010015 loss)
I1026 01:00:32.087393 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00153292 (* 1 = 0.00153292 loss)
I1026 01:00:32.087398 17176 sgd_solver.cpp:106] Iteration 32220, lr = 0.001
I1026 01:00:32.644610 17176 solver.cpp:229] Iteration 32240, loss = 0.0711651
I1026 01:00:32.644644 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0633071 (* 1 = 0.0633071 loss)
I1026 01:00:32.644649 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00785801 (* 1 = 0.00785801 loss)
I1026 01:00:32.644654 17176 sgd_solver.cpp:106] Iteration 32240, lr = 0.001
I1026 01:00:33.206725 17176 solver.cpp:229] Iteration 32260, loss = 0.321286
I1026 01:00:33.206771 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0513389 (* 1 = 0.0513389 loss)
I1026 01:00:33.206776 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.269947 (* 1 = 0.269947 loss)
I1026 01:00:33.206781 17176 sgd_solver.cpp:106] Iteration 32260, lr = 0.001
I1026 01:00:33.775270 17176 solver.cpp:229] Iteration 32280, loss = 0.0518661
I1026 01:00:33.775315 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0323986 (* 1 = 0.0323986 loss)
I1026 01:00:33.775319 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0194675 (* 1 = 0.0194675 loss)
I1026 01:00:33.775324 17176 sgd_solver.cpp:106] Iteration 32280, lr = 0.001
I1026 01:00:34.345057 17176 solver.cpp:229] Iteration 32300, loss = 0.0569922
I1026 01:00:34.345101 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0208999 (* 1 = 0.0208999 loss)
I1026 01:00:34.345108 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0360923 (* 1 = 0.0360923 loss)
I1026 01:00:34.345113 17176 sgd_solver.cpp:106] Iteration 32300, lr = 0.001
I1026 01:00:34.918063 17176 solver.cpp:229] Iteration 32320, loss = 0.0977829
I1026 01:00:34.918107 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0598352 (* 1 = 0.0598352 loss)
I1026 01:00:34.918113 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0379477 (* 1 = 0.0379477 loss)
I1026 01:00:34.918118 17176 sgd_solver.cpp:106] Iteration 32320, lr = 0.001
I1026 01:00:35.476403 17176 solver.cpp:229] Iteration 32340, loss = 0.530256
I1026 01:00:35.476445 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0713871 (* 1 = 0.0713871 loss)
I1026 01:00:35.476450 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.458869 (* 1 = 0.458869 loss)
I1026 01:00:35.476454 17176 sgd_solver.cpp:106] Iteration 32340, lr = 0.001
I1026 01:00:36.036573 17176 solver.cpp:229] Iteration 32360, loss = 0.0444596
I1026 01:00:36.036605 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0197567 (* 1 = 0.0197567 loss)
I1026 01:00:36.036610 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0247029 (* 1 = 0.0247029 loss)
I1026 01:00:36.036614 17176 sgd_solver.cpp:106] Iteration 32360, lr = 0.001
I1026 01:00:36.609746 17176 solver.cpp:229] Iteration 32380, loss = 0.0897484
I1026 01:00:36.609800 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0615366 (* 1 = 0.0615366 loss)
I1026 01:00:36.609805 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0282118 (* 1 = 0.0282118 loss)
I1026 01:00:36.609819 17176 sgd_solver.cpp:106] Iteration 32380, lr = 0.001
I1026 01:00:37.180264 17176 solver.cpp:229] Iteration 32400, loss = 0.0930497
I1026 01:00:37.180310 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0821168 (* 1 = 0.0821168 loss)
I1026 01:00:37.180315 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0109329 (* 1 = 0.0109329 loss)
I1026 01:00:37.180318 17176 sgd_solver.cpp:106] Iteration 32400, lr = 0.001
I1026 01:00:37.748212 17176 solver.cpp:229] Iteration 32420, loss = 0.131473
I1026 01:00:37.748245 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0527221 (* 1 = 0.0527221 loss)
I1026 01:00:37.748250 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0787511 (* 1 = 0.0787511 loss)
I1026 01:00:37.748255 17176 sgd_solver.cpp:106] Iteration 32420, lr = 0.001
I1026 01:00:38.312947 17176 solver.cpp:229] Iteration 32440, loss = 0.143725
I1026 01:00:38.312978 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0841413 (* 1 = 0.0841413 loss)
I1026 01:00:38.312984 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0595837 (* 1 = 0.0595837 loss)
I1026 01:00:38.312988 17176 sgd_solver.cpp:106] Iteration 32440, lr = 0.001
I1026 01:00:38.877038 17176 solver.cpp:229] Iteration 32460, loss = 0.0452672
I1026 01:00:38.877070 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0209978 (* 1 = 0.0209978 loss)
I1026 01:00:38.877075 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0242694 (* 1 = 0.0242694 loss)
I1026 01:00:38.877079 17176 sgd_solver.cpp:106] Iteration 32460, lr = 0.001
I1026 01:00:39.449028 17176 solver.cpp:229] Iteration 32480, loss = 0.0437112
I1026 01:00:39.449069 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0238603 (* 1 = 0.0238603 loss)
I1026 01:00:39.449074 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0198509 (* 1 = 0.0198509 loss)
I1026 01:00:39.449079 17176 sgd_solver.cpp:106] Iteration 32480, lr = 0.001
I1026 01:00:40.017894 17176 solver.cpp:229] Iteration 32500, loss = 0.0523976
I1026 01:00:40.017938 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0470121 (* 1 = 0.0470121 loss)
I1026 01:00:40.017945 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00538554 (* 1 = 0.00538554 loss)
I1026 01:00:40.017949 17176 sgd_solver.cpp:106] Iteration 32500, lr = 0.001
I1026 01:00:40.595881 17176 solver.cpp:229] Iteration 32520, loss = 0.0499743
I1026 01:00:40.595913 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0343393 (* 1 = 0.0343393 loss)
I1026 01:00:40.595919 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.015635 (* 1 = 0.015635 loss)
I1026 01:00:40.595923 17176 sgd_solver.cpp:106] Iteration 32520, lr = 0.001
I1026 01:00:41.166204 17176 solver.cpp:229] Iteration 32540, loss = 0.0681699
I1026 01:00:41.166246 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0302454 (* 1 = 0.0302454 loss)
I1026 01:00:41.166251 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0379245 (* 1 = 0.0379245 loss)
I1026 01:00:41.166256 17176 sgd_solver.cpp:106] Iteration 32540, lr = 0.001
I1026 01:00:41.739573 17176 solver.cpp:229] Iteration 32560, loss = 0.0599905
I1026 01:00:41.739608 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0531172 (* 1 = 0.0531172 loss)
I1026 01:00:41.739612 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00687336 (* 1 = 0.00687336 loss)
I1026 01:00:41.739619 17176 sgd_solver.cpp:106] Iteration 32560, lr = 0.001
I1026 01:00:42.307715 17176 solver.cpp:229] Iteration 32580, loss = 0.109219
I1026 01:00:42.307747 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0208555 (* 1 = 0.0208555 loss)
I1026 01:00:42.307754 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0883635 (* 1 = 0.0883635 loss)
I1026 01:00:42.307757 17176 sgd_solver.cpp:106] Iteration 32580, lr = 0.001
I1026 01:00:42.874956 17176 solver.cpp:229] Iteration 32600, loss = 0.0771938
I1026 01:00:42.874989 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0573178 (* 1 = 0.0573178 loss)
I1026 01:00:42.874995 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.019876 (* 1 = 0.019876 loss)
I1026 01:00:42.875000 17176 sgd_solver.cpp:106] Iteration 32600, lr = 0.001
I1026 01:00:43.435528 17176 solver.cpp:229] Iteration 32620, loss = 0.102598
I1026 01:00:43.435562 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0208261 (* 1 = 0.0208261 loss)
I1026 01:00:43.435567 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0817723 (* 1 = 0.0817723 loss)
I1026 01:00:43.435571 17176 sgd_solver.cpp:106] Iteration 32620, lr = 0.001
I1026 01:00:44.009485 17176 solver.cpp:229] Iteration 32640, loss = 0.0232801
I1026 01:00:44.009518 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0203432 (* 1 = 0.0203432 loss)
I1026 01:00:44.009523 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00293686 (* 1 = 0.00293686 loss)
I1026 01:00:44.009528 17176 sgd_solver.cpp:106] Iteration 32640, lr = 0.001
I1026 01:00:44.572309 17176 solver.cpp:229] Iteration 32660, loss = 0.045796
I1026 01:00:44.572342 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0390253 (* 1 = 0.0390253 loss)
I1026 01:00:44.572348 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00677074 (* 1 = 0.00677074 loss)
I1026 01:00:44.572353 17176 sgd_solver.cpp:106] Iteration 32660, lr = 0.001
I1026 01:00:45.129029 17176 solver.cpp:229] Iteration 32680, loss = 0.0390585
I1026 01:00:45.129060 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0256373 (* 1 = 0.0256373 loss)
I1026 01:00:45.129065 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0134211 (* 1 = 0.0134211 loss)
I1026 01:00:45.129068 17176 sgd_solver.cpp:106] Iteration 32680, lr = 0.001
I1026 01:00:45.674196 17176 solver.cpp:229] Iteration 32700, loss = 0.0264103
I1026 01:00:45.674227 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0238502 (* 1 = 0.0238502 loss)
I1026 01:00:45.674232 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00256017 (* 1 = 0.00256017 loss)
I1026 01:00:45.674237 17176 sgd_solver.cpp:106] Iteration 32700, lr = 0.001
I1026 01:00:46.235060 17176 solver.cpp:229] Iteration 32720, loss = 0.0332054
I1026 01:00:46.235092 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0198569 (* 1 = 0.0198569 loss)
I1026 01:00:46.235097 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0133485 (* 1 = 0.0133485 loss)
I1026 01:00:46.235101 17176 sgd_solver.cpp:106] Iteration 32720, lr = 0.001
I1026 01:00:46.812505 17176 solver.cpp:229] Iteration 32740, loss = 0.0457614
I1026 01:00:46.812536 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0341961 (* 1 = 0.0341961 loss)
I1026 01:00:46.812541 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0115653 (* 1 = 0.0115653 loss)
I1026 01:00:46.812544 17176 sgd_solver.cpp:106] Iteration 32740, lr = 0.001
I1026 01:00:47.370460 17176 solver.cpp:229] Iteration 32760, loss = 0.0315821
I1026 01:00:47.370491 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0109361 (* 1 = 0.0109361 loss)
I1026 01:00:47.370496 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.020646 (* 1 = 0.020646 loss)
I1026 01:00:47.370501 17176 sgd_solver.cpp:106] Iteration 32760, lr = 0.001
I1026 01:00:47.937134 17176 solver.cpp:229] Iteration 32780, loss = 0.0364025
I1026 01:00:47.937165 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0223337 (* 1 = 0.0223337 loss)
I1026 01:00:47.937170 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0140688 (* 1 = 0.0140688 loss)
I1026 01:00:47.937175 17176 sgd_solver.cpp:106] Iteration 32780, lr = 0.001
I1026 01:00:48.496063 17176 solver.cpp:229] Iteration 32800, loss = 0.928746
I1026 01:00:48.496096 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.440473 (* 1 = 0.440473 loss)
I1026 01:00:48.496101 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.488272 (* 1 = 0.488272 loss)
I1026 01:00:48.496106 17176 sgd_solver.cpp:106] Iteration 32800, lr = 0.001
I1026 01:00:49.055459 17176 solver.cpp:229] Iteration 32820, loss = 0.0309407
I1026 01:00:49.055490 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0234757 (* 1 = 0.0234757 loss)
I1026 01:00:49.055495 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00746498 (* 1 = 0.00746498 loss)
I1026 01:00:49.055500 17176 sgd_solver.cpp:106] Iteration 32820, lr = 0.001
I1026 01:00:49.634716 17176 solver.cpp:229] Iteration 32840, loss = 0.170696
I1026 01:00:49.634748 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.019839 (* 1 = 0.019839 loss)
I1026 01:00:49.634753 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.150858 (* 1 = 0.150858 loss)
I1026 01:00:49.634758 17176 sgd_solver.cpp:106] Iteration 32840, lr = 0.001
I1026 01:00:50.193506 17176 solver.cpp:229] Iteration 32860, loss = 0.030867
I1026 01:00:50.193538 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0249466 (* 1 = 0.0249466 loss)
I1026 01:00:50.193542 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00592038 (* 1 = 0.00592038 loss)
I1026 01:00:50.193547 17176 sgd_solver.cpp:106] Iteration 32860, lr = 0.001
I1026 01:00:50.754395 17176 solver.cpp:229] Iteration 32880, loss = 0.00775359
I1026 01:00:50.754429 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00332635 (* 1 = 0.00332635 loss)
I1026 01:00:50.754437 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00442723 (* 1 = 0.00442723 loss)
I1026 01:00:50.754453 17176 sgd_solver.cpp:106] Iteration 32880, lr = 0.001
I1026 01:00:51.322962 17176 solver.cpp:229] Iteration 32900, loss = 0.0633203
I1026 01:00:51.322998 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0466251 (* 1 = 0.0466251 loss)
I1026 01:00:51.323005 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0166952 (* 1 = 0.0166952 loss)
I1026 01:00:51.323011 17176 sgd_solver.cpp:106] Iteration 32900, lr = 0.001
I1026 01:00:51.874555 17176 solver.cpp:229] Iteration 32920, loss = 0.0750166
I1026 01:00:51.874590 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0280678 (* 1 = 0.0280678 loss)
I1026 01:00:51.874598 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0469488 (* 1 = 0.0469488 loss)
I1026 01:00:51.874603 17176 sgd_solver.cpp:106] Iteration 32920, lr = 0.001
I1026 01:00:52.434308 17176 solver.cpp:229] Iteration 32940, loss = 0.248842
I1026 01:00:52.434343 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0239202 (* 1 = 0.0239202 loss)
I1026 01:00:52.434350 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.224922 (* 1 = 0.224922 loss)
I1026 01:00:52.434356 17176 sgd_solver.cpp:106] Iteration 32940, lr = 0.001
I1026 01:00:53.001554 17176 solver.cpp:229] Iteration 32960, loss = 0.0320782
I1026 01:00:53.001588 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0263233 (* 1 = 0.0263233 loss)
I1026 01:00:53.001596 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00575488 (* 1 = 0.00575488 loss)
I1026 01:00:53.001603 17176 sgd_solver.cpp:106] Iteration 32960, lr = 0.001
I1026 01:00:53.575901 17176 solver.cpp:229] Iteration 32980, loss = 0.0671259
I1026 01:00:53.575937 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.061502 (* 1 = 0.061502 loss)
I1026 01:00:53.575945 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00562389 (* 1 = 0.00562389 loss)
I1026 01:00:53.575951 17176 sgd_solver.cpp:106] Iteration 32980, lr = 0.001
I1026 01:00:54.129063 17176 solver.cpp:229] Iteration 33000, loss = 0.0718058
I1026 01:00:54.129099 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0432363 (* 1 = 0.0432363 loss)
I1026 01:00:54.129108 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0285695 (* 1 = 0.0285695 loss)
I1026 01:00:54.129114 17176 sgd_solver.cpp:106] Iteration 33000, lr = 0.001
I1026 01:00:54.673615 17176 solver.cpp:229] Iteration 33020, loss = 0.0167405
I1026 01:00:54.673650 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0148955 (* 1 = 0.0148955 loss)
I1026 01:00:54.673658 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00184504 (* 1 = 0.00184504 loss)
I1026 01:00:54.673665 17176 sgd_solver.cpp:106] Iteration 33020, lr = 0.001
I1026 01:00:55.232771 17176 solver.cpp:229] Iteration 33040, loss = 0.0525952
I1026 01:00:55.232807 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0238701 (* 1 = 0.0238701 loss)
I1026 01:00:55.232815 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0287251 (* 1 = 0.0287251 loss)
I1026 01:00:55.232821 17176 sgd_solver.cpp:106] Iteration 33040, lr = 0.001
I1026 01:00:55.801765 17176 solver.cpp:229] Iteration 33060, loss = 0.0806679
I1026 01:00:55.801800 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0291746 (* 1 = 0.0291746 loss)
I1026 01:00:55.801808 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0514933 (* 1 = 0.0514933 loss)
I1026 01:00:55.801813 17176 sgd_solver.cpp:106] Iteration 33060, lr = 0.001
I1026 01:00:56.366895 17176 solver.cpp:229] Iteration 33080, loss = 0.361653
I1026 01:00:56.366928 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.252501 (* 1 = 0.252501 loss)
I1026 01:00:56.366936 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.109152 (* 1 = 0.109152 loss)
I1026 01:00:56.366942 17176 sgd_solver.cpp:106] Iteration 33080, lr = 0.001
I1026 01:00:56.934692 17176 solver.cpp:229] Iteration 33100, loss = 0.058039
I1026 01:00:56.934728 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0407517 (* 1 = 0.0407517 loss)
I1026 01:00:56.934736 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0172873 (* 1 = 0.0172873 loss)
I1026 01:00:56.934742 17176 sgd_solver.cpp:106] Iteration 33100, lr = 0.001
I1026 01:00:57.496754 17176 solver.cpp:229] Iteration 33120, loss = 0.0474269
I1026 01:00:57.496786 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0107952 (* 1 = 0.0107952 loss)
I1026 01:00:57.496791 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0366317 (* 1 = 0.0366317 loss)
I1026 01:00:57.496795 17176 sgd_solver.cpp:106] Iteration 33120, lr = 0.001
I1026 01:00:58.066934 17176 solver.cpp:229] Iteration 33140, loss = 0.0611541
I1026 01:00:58.066967 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0426833 (* 1 = 0.0426833 loss)
I1026 01:00:58.066972 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0184708 (* 1 = 0.0184708 loss)
I1026 01:00:58.066977 17176 sgd_solver.cpp:106] Iteration 33140, lr = 0.001
I1026 01:00:58.625546 17176 solver.cpp:229] Iteration 33160, loss = 0.147676
I1026 01:00:58.625579 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0756049 (* 1 = 0.0756049 loss)
I1026 01:00:58.625584 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0720716 (* 1 = 0.0720716 loss)
I1026 01:00:58.625589 17176 sgd_solver.cpp:106] Iteration 33160, lr = 0.001
I1026 01:00:59.188668 17176 solver.cpp:229] Iteration 33180, loss = 0.128852
I1026 01:00:59.188702 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00293718 (* 1 = 0.00293718 loss)
I1026 01:00:59.188709 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.125915 (* 1 = 0.125915 loss)
I1026 01:00:59.188714 17176 sgd_solver.cpp:106] Iteration 33180, lr = 0.001
I1026 01:00:59.759862 17176 solver.cpp:229] Iteration 33200, loss = 0.140801
I1026 01:00:59.759896 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0642034 (* 1 = 0.0642034 loss)
I1026 01:00:59.759902 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0765978 (* 1 = 0.0765978 loss)
I1026 01:00:59.759907 17176 sgd_solver.cpp:106] Iteration 33200, lr = 0.001
I1026 01:01:00.321573 17176 solver.cpp:229] Iteration 33220, loss = 0.0245957
I1026 01:01:00.321609 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0226915 (* 1 = 0.0226915 loss)
I1026 01:01:00.321614 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00190422 (* 1 = 0.00190422 loss)
I1026 01:01:00.321619 17176 sgd_solver.cpp:106] Iteration 33220, lr = 0.001
I1026 01:01:00.883872 17176 solver.cpp:229] Iteration 33240, loss = 0.0546744
I1026 01:01:00.883905 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0466897 (* 1 = 0.0466897 loss)
I1026 01:01:00.883913 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00798467 (* 1 = 0.00798467 loss)
I1026 01:01:00.883926 17176 sgd_solver.cpp:106] Iteration 33240, lr = 0.001
I1026 01:01:01.451453 17176 solver.cpp:229] Iteration 33260, loss = 0.0554929
I1026 01:01:01.451486 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0433312 (* 1 = 0.0433312 loss)
I1026 01:01:01.451491 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0121617 (* 1 = 0.0121617 loss)
I1026 01:01:01.451496 17176 sgd_solver.cpp:106] Iteration 33260, lr = 0.001
I1026 01:01:02.022398 17176 solver.cpp:229] Iteration 33280, loss = 0.138836
I1026 01:01:02.022433 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.108255 (* 1 = 0.108255 loss)
I1026 01:01:02.022439 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0305812 (* 1 = 0.0305812 loss)
I1026 01:01:02.022444 17176 sgd_solver.cpp:106] Iteration 33280, lr = 0.001
I1026 01:01:02.586871 17176 solver.cpp:229] Iteration 33300, loss = 0.197561
I1026 01:01:02.586905 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0358959 (* 1 = 0.0358959 loss)
I1026 01:01:02.586910 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.161665 (* 1 = 0.161665 loss)
I1026 01:01:02.586915 17176 sgd_solver.cpp:106] Iteration 33300, lr = 0.001
I1026 01:01:03.157678 17176 solver.cpp:229] Iteration 33320, loss = 0.0435692
I1026 01:01:03.157711 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0238634 (* 1 = 0.0238634 loss)
I1026 01:01:03.157717 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0197057 (* 1 = 0.0197057 loss)
I1026 01:01:03.157721 17176 sgd_solver.cpp:106] Iteration 33320, lr = 0.001
I1026 01:01:03.713636 17176 solver.cpp:229] Iteration 33340, loss = 1.02026
I1026 01:01:03.713670 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.258017 (* 1 = 0.258017 loss)
I1026 01:01:03.713676 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.762244 (* 1 = 0.762244 loss)
I1026 01:01:03.713680 17176 sgd_solver.cpp:106] Iteration 33340, lr = 0.001
I1026 01:01:04.264693 17176 solver.cpp:229] Iteration 33360, loss = 0.0863763
I1026 01:01:04.264726 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.07284 (* 1 = 0.07284 loss)
I1026 01:01:04.264732 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0135363 (* 1 = 0.0135363 loss)
I1026 01:01:04.264737 17176 sgd_solver.cpp:106] Iteration 33360, lr = 0.001
I1026 01:01:04.833210 17176 solver.cpp:229] Iteration 33380, loss = 0.0295839
I1026 01:01:04.833253 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0272343 (* 1 = 0.0272343 loss)
I1026 01:01:04.833259 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00234957 (* 1 = 0.00234957 loss)
I1026 01:01:04.833264 17176 sgd_solver.cpp:106] Iteration 33380, lr = 0.001
I1026 01:01:05.403286 17176 solver.cpp:229] Iteration 33400, loss = 0.0472384
I1026 01:01:05.403331 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0402034 (* 1 = 0.0402034 loss)
I1026 01:01:05.403337 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00703501 (* 1 = 0.00703501 loss)
I1026 01:01:05.403340 17176 sgd_solver.cpp:106] Iteration 33400, lr = 0.001
I1026 01:01:05.960070 17176 solver.cpp:229] Iteration 33420, loss = 0.0210162
I1026 01:01:05.960114 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0173472 (* 1 = 0.0173472 loss)
I1026 01:01:05.960119 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00366901 (* 1 = 0.00366901 loss)
I1026 01:01:05.960124 17176 sgd_solver.cpp:106] Iteration 33420, lr = 0.001
I1026 01:01:06.529769 17176 solver.cpp:229] Iteration 33440, loss = 0.0209095
I1026 01:01:06.529803 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0183383 (* 1 = 0.0183383 loss)
I1026 01:01:06.529809 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00257116 (* 1 = 0.00257116 loss)
I1026 01:01:06.529814 17176 sgd_solver.cpp:106] Iteration 33440, lr = 0.001
I1026 01:01:07.089561 17176 solver.cpp:229] Iteration 33460, loss = 0.0962937
I1026 01:01:07.089596 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0169386 (* 1 = 0.0169386 loss)
I1026 01:01:07.089601 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0793551 (* 1 = 0.0793551 loss)
I1026 01:01:07.089606 17176 sgd_solver.cpp:106] Iteration 33460, lr = 0.001
I1026 01:01:07.650176 17176 solver.cpp:229] Iteration 33480, loss = 0.0691745
I1026 01:01:07.650207 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0577688 (* 1 = 0.0577688 loss)
I1026 01:01:07.650212 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0114058 (* 1 = 0.0114058 loss)
I1026 01:01:07.650216 17176 sgd_solver.cpp:106] Iteration 33480, lr = 0.001
I1026 01:01:08.220069 17176 solver.cpp:229] Iteration 33500, loss = 0.0344846
I1026 01:01:08.220103 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0227567 (* 1 = 0.0227567 loss)
I1026 01:01:08.220108 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0117279 (* 1 = 0.0117279 loss)
I1026 01:01:08.220111 17176 sgd_solver.cpp:106] Iteration 33500, lr = 0.001
I1026 01:01:08.771828 17176 solver.cpp:229] Iteration 33520, loss = 1.67292
I1026 01:01:08.771870 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.217921 (* 1 = 0.217921 loss)
I1026 01:01:08.771875 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 1.455 (* 1 = 1.455 loss)
I1026 01:01:08.771879 17176 sgd_solver.cpp:106] Iteration 33520, lr = 0.001
I1026 01:01:09.327682 17176 solver.cpp:229] Iteration 33540, loss = 0.016487
I1026 01:01:09.327723 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.015442 (* 1 = 0.015442 loss)
I1026 01:01:09.327728 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00104495 (* 1 = 0.00104495 loss)
I1026 01:01:09.327741 17176 sgd_solver.cpp:106] Iteration 33540, lr = 0.001
I1026 01:01:09.890125 17176 solver.cpp:229] Iteration 33560, loss = 0.0697077
I1026 01:01:09.890157 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0570225 (* 1 = 0.0570225 loss)
I1026 01:01:09.890163 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0126852 (* 1 = 0.0126852 loss)
I1026 01:01:09.890167 17176 sgd_solver.cpp:106] Iteration 33560, lr = 0.001
I1026 01:01:10.462769 17176 solver.cpp:229] Iteration 33580, loss = 0.0786035
I1026 01:01:10.462802 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0634866 (* 1 = 0.0634866 loss)
I1026 01:01:10.462807 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.015117 (* 1 = 0.015117 loss)
I1026 01:01:10.462813 17176 sgd_solver.cpp:106] Iteration 33580, lr = 0.001
I1026 01:01:11.027360 17176 solver.cpp:229] Iteration 33600, loss = 0.0254109
I1026 01:01:11.027405 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0110213 (* 1 = 0.0110213 loss)
I1026 01:01:11.027411 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0143896 (* 1 = 0.0143896 loss)
I1026 01:01:11.027415 17176 sgd_solver.cpp:106] Iteration 33600, lr = 0.001
I1026 01:01:11.607408 17176 solver.cpp:229] Iteration 33620, loss = 1.42165
I1026 01:01:11.607445 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.189481 (* 1 = 0.189481 loss)
I1026 01:01:11.607450 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 1.23217 (* 1 = 1.23217 loss)
I1026 01:01:11.607455 17176 sgd_solver.cpp:106] Iteration 33620, lr = 0.001
I1026 01:01:12.173264 17176 solver.cpp:229] Iteration 33640, loss = 0.0680624
I1026 01:01:12.173297 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0445725 (* 1 = 0.0445725 loss)
I1026 01:01:12.173303 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0234899 (* 1 = 0.0234899 loss)
I1026 01:01:12.173307 17176 sgd_solver.cpp:106] Iteration 33640, lr = 0.001
I1026 01:01:12.732566 17176 solver.cpp:229] Iteration 33660, loss = 0.134636
I1026 01:01:12.732600 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0822361 (* 1 = 0.0822361 loss)
I1026 01:01:12.732606 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0524004 (* 1 = 0.0524004 loss)
I1026 01:01:12.732610 17176 sgd_solver.cpp:106] Iteration 33660, lr = 0.001
I1026 01:01:13.298071 17176 solver.cpp:229] Iteration 33680, loss = 0.0516838
I1026 01:01:13.298105 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0222811 (* 1 = 0.0222811 loss)
I1026 01:01:13.298110 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0294027 (* 1 = 0.0294027 loss)
I1026 01:01:13.298115 17176 sgd_solver.cpp:106] Iteration 33680, lr = 0.001
I1026 01:01:13.855311 17176 solver.cpp:229] Iteration 33700, loss = 0.0336351
I1026 01:01:13.855343 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0163366 (* 1 = 0.0163366 loss)
I1026 01:01:13.855348 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0172986 (* 1 = 0.0172986 loss)
I1026 01:01:13.855353 17176 sgd_solver.cpp:106] Iteration 33700, lr = 0.001
I1026 01:01:14.423773 17176 solver.cpp:229] Iteration 33720, loss = 0.0312173
I1026 01:01:14.423806 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0198071 (* 1 = 0.0198071 loss)
I1026 01:01:14.423812 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0114101 (* 1 = 0.0114101 loss)
I1026 01:01:14.423816 17176 sgd_solver.cpp:106] Iteration 33720, lr = 0.001
I1026 01:01:14.993338 17176 solver.cpp:229] Iteration 33740, loss = 0.886821
I1026 01:01:14.993371 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.177475 (* 1 = 0.177475 loss)
I1026 01:01:14.993377 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.709345 (* 1 = 0.709345 loss)
I1026 01:01:14.993381 17176 sgd_solver.cpp:106] Iteration 33740, lr = 0.001
I1026 01:01:15.536821 17176 solver.cpp:229] Iteration 33760, loss = 0.219578
I1026 01:01:15.536870 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0517994 (* 1 = 0.0517994 loss)
I1026 01:01:15.536875 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.167779 (* 1 = 0.167779 loss)
I1026 01:01:15.536880 17176 sgd_solver.cpp:106] Iteration 33760, lr = 0.001
I1026 01:01:16.105191 17176 solver.cpp:229] Iteration 33780, loss = 0.0210865
I1026 01:01:16.105226 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0160299 (* 1 = 0.0160299 loss)
I1026 01:01:16.105231 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00505656 (* 1 = 0.00505656 loss)
I1026 01:01:16.105237 17176 sgd_solver.cpp:106] Iteration 33780, lr = 0.001
I1026 01:01:16.682191 17176 solver.cpp:229] Iteration 33800, loss = 0.136637
I1026 01:01:16.682225 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0976906 (* 1 = 0.0976906 loss)
I1026 01:01:16.682230 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0389466 (* 1 = 0.0389466 loss)
I1026 01:01:16.682235 17176 sgd_solver.cpp:106] Iteration 33800, lr = 0.001
I1026 01:01:17.260315 17176 solver.cpp:229] Iteration 33820, loss = 0.0279919
I1026 01:01:17.260349 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0134662 (* 1 = 0.0134662 loss)
I1026 01:01:17.260354 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0145257 (* 1 = 0.0145257 loss)
I1026 01:01:17.260359 17176 sgd_solver.cpp:106] Iteration 33820, lr = 0.001
I1026 01:01:17.813596 17176 solver.cpp:229] Iteration 33840, loss = 0.0397701
I1026 01:01:17.813629 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0181756 (* 1 = 0.0181756 loss)
I1026 01:01:17.813634 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0215945 (* 1 = 0.0215945 loss)
I1026 01:01:17.813638 17176 sgd_solver.cpp:106] Iteration 33840, lr = 0.001
I1026 01:01:18.379248 17176 solver.cpp:229] Iteration 33860, loss = 0.0139357
I1026 01:01:18.379281 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0079268 (* 1 = 0.0079268 loss)
I1026 01:01:18.379287 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00600894 (* 1 = 0.00600894 loss)
I1026 01:01:18.379292 17176 sgd_solver.cpp:106] Iteration 33860, lr = 0.001
I1026 01:01:18.931707 17176 solver.cpp:229] Iteration 33880, loss = 0.447452
I1026 01:01:18.931740 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.170852 (* 1 = 0.170852 loss)
I1026 01:01:18.931743 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.2766 (* 1 = 0.2766 loss)
I1026 01:01:18.931747 17176 sgd_solver.cpp:106] Iteration 33880, lr = 0.001
I1026 01:01:19.497436 17176 solver.cpp:229] Iteration 33900, loss = 0.0352353
I1026 01:01:19.497467 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0160267 (* 1 = 0.0160267 loss)
I1026 01:01:19.497473 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0192086 (* 1 = 0.0192086 loss)
I1026 01:01:19.497478 17176 sgd_solver.cpp:106] Iteration 33900, lr = 0.001
I1026 01:01:20.066179 17176 solver.cpp:229] Iteration 33920, loss = 0.072244
I1026 01:01:20.066212 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0678661 (* 1 = 0.0678661 loss)
I1026 01:01:20.066234 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00437793 (* 1 = 0.00437793 loss)
I1026 01:01:20.066239 17176 sgd_solver.cpp:106] Iteration 33920, lr = 0.001
I1026 01:01:20.639019 17176 solver.cpp:229] Iteration 33940, loss = 0.0684984
I1026 01:01:20.639051 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0510147 (* 1 = 0.0510147 loss)
I1026 01:01:20.639058 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0174837 (* 1 = 0.0174837 loss)
I1026 01:01:20.639062 17176 sgd_solver.cpp:106] Iteration 33940, lr = 0.001
I1026 01:01:21.207808 17176 solver.cpp:229] Iteration 33960, loss = 0.353944
I1026 01:01:21.207842 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.137819 (* 1 = 0.137819 loss)
I1026 01:01:21.207847 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.216124 (* 1 = 0.216124 loss)
I1026 01:01:21.207851 17176 sgd_solver.cpp:106] Iteration 33960, lr = 0.001
I1026 01:01:21.764819 17176 solver.cpp:229] Iteration 33980, loss = 0.0395886
I1026 01:01:21.764863 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.011807 (* 1 = 0.011807 loss)
I1026 01:01:21.764868 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0277815 (* 1 = 0.0277815 loss)
I1026 01:01:21.764871 17176 sgd_solver.cpp:106] Iteration 33980, lr = 0.001
I1026 01:01:22.328330 17176 solver.cpp:229] Iteration 34000, loss = 0.0620581
I1026 01:01:22.328364 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0279109 (* 1 = 0.0279109 loss)
I1026 01:01:22.328369 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0341472 (* 1 = 0.0341472 loss)
I1026 01:01:22.328373 17176 sgd_solver.cpp:106] Iteration 34000, lr = 0.001
I1026 01:01:22.879091 17176 solver.cpp:229] Iteration 34020, loss = 0.049026
I1026 01:01:22.879124 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0383413 (* 1 = 0.0383413 loss)
I1026 01:01:22.879129 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0106846 (* 1 = 0.0106846 loss)
I1026 01:01:22.879134 17176 sgd_solver.cpp:106] Iteration 34020, lr = 0.001
I1026 01:01:23.432423 17176 solver.cpp:229] Iteration 34040, loss = 0.0251991
I1026 01:01:23.432467 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0093153 (* 1 = 0.0093153 loss)
I1026 01:01:23.432473 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0158838 (* 1 = 0.0158838 loss)
I1026 01:01:23.432477 17176 sgd_solver.cpp:106] Iteration 34040, lr = 0.001
I1026 01:01:24.007264 17176 solver.cpp:229] Iteration 34060, loss = 0.043626
I1026 01:01:24.007298 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0306464 (* 1 = 0.0306464 loss)
I1026 01:01:24.007303 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0129796 (* 1 = 0.0129796 loss)
I1026 01:01:24.007308 17176 sgd_solver.cpp:106] Iteration 34060, lr = 0.001
I1026 01:01:24.553601 17176 solver.cpp:229] Iteration 34080, loss = 0.0828165
I1026 01:01:24.553634 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0637345 (* 1 = 0.0637345 loss)
I1026 01:01:24.553639 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.019082 (* 1 = 0.019082 loss)
I1026 01:01:24.553643 17176 sgd_solver.cpp:106] Iteration 34080, lr = 0.001
I1026 01:01:25.106287 17176 solver.cpp:229] Iteration 34100, loss = 0.208081
I1026 01:01:25.106319 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.121477 (* 1 = 0.121477 loss)
I1026 01:01:25.106325 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0866033 (* 1 = 0.0866033 loss)
I1026 01:01:25.106329 17176 sgd_solver.cpp:106] Iteration 34100, lr = 0.001
I1026 01:01:25.670044 17176 solver.cpp:229] Iteration 34120, loss = 0.0840824
I1026 01:01:25.670078 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0800112 (* 1 = 0.0800112 loss)
I1026 01:01:25.670083 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00407129 (* 1 = 0.00407129 loss)
I1026 01:01:25.670087 17176 sgd_solver.cpp:106] Iteration 34120, lr = 0.001
I1026 01:01:26.236027 17176 solver.cpp:229] Iteration 34140, loss = 0.0405669
I1026 01:01:26.236059 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0231815 (* 1 = 0.0231815 loss)
I1026 01:01:26.236064 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0173854 (* 1 = 0.0173854 loss)
I1026 01:01:26.236069 17176 sgd_solver.cpp:106] Iteration 34140, lr = 0.001
I1026 01:01:26.805811 17176 solver.cpp:229] Iteration 34160, loss = 0.112758
I1026 01:01:26.805845 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0599058 (* 1 = 0.0599058 loss)
I1026 01:01:26.805850 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0528521 (* 1 = 0.0528521 loss)
I1026 01:01:26.805853 17176 sgd_solver.cpp:106] Iteration 34160, lr = 0.001
I1026 01:01:27.371392 17176 solver.cpp:229] Iteration 34180, loss = 0.166807
I1026 01:01:27.371451 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0899937 (* 1 = 0.0899937 loss)
I1026 01:01:27.371457 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0768128 (* 1 = 0.0768128 loss)
I1026 01:01:27.371472 17176 sgd_solver.cpp:106] Iteration 34180, lr = 0.001
I1026 01:01:27.935286 17176 solver.cpp:229] Iteration 34200, loss = 0.0225745
I1026 01:01:27.935320 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0202133 (* 1 = 0.0202133 loss)
I1026 01:01:27.935326 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00236113 (* 1 = 0.00236113 loss)
I1026 01:01:27.935333 17176 sgd_solver.cpp:106] Iteration 34200, lr = 0.001
I1026 01:01:28.494283 17176 solver.cpp:229] Iteration 34220, loss = 0.239776
I1026 01:01:28.494328 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.195139 (* 1 = 0.195139 loss)
I1026 01:01:28.494333 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0446368 (* 1 = 0.0446368 loss)
I1026 01:01:28.494338 17176 sgd_solver.cpp:106] Iteration 34220, lr = 0.001
I1026 01:01:29.049229 17176 solver.cpp:229] Iteration 34240, loss = 0.0161989
I1026 01:01:29.049262 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00692636 (* 1 = 0.00692636 loss)
I1026 01:01:29.049266 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00927251 (* 1 = 0.00927251 loss)
I1026 01:01:29.049270 17176 sgd_solver.cpp:106] Iteration 34240, lr = 0.001
I1026 01:01:29.616816 17176 solver.cpp:229] Iteration 34260, loss = 0.112773
I1026 01:01:29.616849 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0531261 (* 1 = 0.0531261 loss)
I1026 01:01:29.616854 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0596467 (* 1 = 0.0596467 loss)
I1026 01:01:29.616859 17176 sgd_solver.cpp:106] Iteration 34260, lr = 0.001
I1026 01:01:30.174423 17176 solver.cpp:229] Iteration 34280, loss = 0.253559
I1026 01:01:30.174466 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.141048 (* 1 = 0.141048 loss)
I1026 01:01:30.174471 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.112511 (* 1 = 0.112511 loss)
I1026 01:01:30.174475 17176 sgd_solver.cpp:106] Iteration 34280, lr = 0.001
I1026 01:01:30.715409 17176 solver.cpp:229] Iteration 34300, loss = 0.0240135
I1026 01:01:30.715461 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0183569 (* 1 = 0.0183569 loss)
I1026 01:01:30.715467 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00565658 (* 1 = 0.00565658 loss)
I1026 01:01:30.715471 17176 sgd_solver.cpp:106] Iteration 34300, lr = 0.001
I1026 01:01:31.278751 17176 solver.cpp:229] Iteration 34320, loss = 0.0398648
I1026 01:01:31.278784 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0155703 (* 1 = 0.0155703 loss)
I1026 01:01:31.278789 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0242946 (* 1 = 0.0242946 loss)
I1026 01:01:31.278794 17176 sgd_solver.cpp:106] Iteration 34320, lr = 0.001
I1026 01:01:31.843335 17176 solver.cpp:229] Iteration 34340, loss = 0.0589052
I1026 01:01:31.843369 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0350781 (* 1 = 0.0350781 loss)
I1026 01:01:31.843374 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0238271 (* 1 = 0.0238271 loss)
I1026 01:01:31.843377 17176 sgd_solver.cpp:106] Iteration 34340, lr = 0.001
I1026 01:01:32.405328 17176 solver.cpp:229] Iteration 34360, loss = 0.11289
I1026 01:01:32.405360 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0179158 (* 1 = 0.0179158 loss)
I1026 01:01:32.405365 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0949739 (* 1 = 0.0949739 loss)
I1026 01:01:32.405370 17176 sgd_solver.cpp:106] Iteration 34360, lr = 0.001
I1026 01:01:32.972262 17176 solver.cpp:229] Iteration 34380, loss = 0.076109
I1026 01:01:32.972295 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0485766 (* 1 = 0.0485766 loss)
I1026 01:01:32.972301 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0275324 (* 1 = 0.0275324 loss)
I1026 01:01:32.972304 17176 sgd_solver.cpp:106] Iteration 34380, lr = 0.001
I1026 01:01:33.537825 17176 solver.cpp:229] Iteration 34400, loss = 0.0989308
I1026 01:01:33.537856 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0307691 (* 1 = 0.0307691 loss)
I1026 01:01:33.537861 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0681618 (* 1 = 0.0681618 loss)
I1026 01:01:33.537865 17176 sgd_solver.cpp:106] Iteration 34400, lr = 0.001
I1026 01:01:34.099678 17176 solver.cpp:229] Iteration 34420, loss = 0.113328
I1026 01:01:34.099710 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0280451 (* 1 = 0.0280451 loss)
I1026 01:01:34.099714 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.085283 (* 1 = 0.085283 loss)
I1026 01:01:34.099720 17176 sgd_solver.cpp:106] Iteration 34420, lr = 0.001
I1026 01:01:34.660063 17176 solver.cpp:229] Iteration 34440, loss = 0.0605112
I1026 01:01:34.660095 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0499082 (* 1 = 0.0499082 loss)
I1026 01:01:34.660100 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.010603 (* 1 = 0.010603 loss)
I1026 01:01:34.660104 17176 sgd_solver.cpp:106] Iteration 34440, lr = 0.001
I1026 01:01:35.222532 17176 solver.cpp:229] Iteration 34460, loss = 0.0252188
I1026 01:01:35.222563 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0233228 (* 1 = 0.0233228 loss)
I1026 01:01:35.222579 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00189601 (* 1 = 0.00189601 loss)
I1026 01:01:35.222584 17176 sgd_solver.cpp:106] Iteration 34460, lr = 0.001
I1026 01:01:35.790710 17176 solver.cpp:229] Iteration 34480, loss = 0.217442
I1026 01:01:35.790742 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.167747 (* 1 = 0.167747 loss)
I1026 01:01:35.790748 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0496949 (* 1 = 0.0496949 loss)
I1026 01:01:35.790752 17176 sgd_solver.cpp:106] Iteration 34480, lr = 0.001
I1026 01:01:36.352825 17176 solver.cpp:229] Iteration 34500, loss = 0.0154117
I1026 01:01:36.352856 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00511739 (* 1 = 0.00511739 loss)
I1026 01:01:36.352861 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0102943 (* 1 = 0.0102943 loss)
I1026 01:01:36.352866 17176 sgd_solver.cpp:106] Iteration 34500, lr = 0.001
I1026 01:01:36.907812 17176 solver.cpp:229] Iteration 34520, loss = 0.0205806
I1026 01:01:36.907845 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0178015 (* 1 = 0.0178015 loss)
I1026 01:01:36.907850 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00277913 (* 1 = 0.00277913 loss)
I1026 01:01:36.907855 17176 sgd_solver.cpp:106] Iteration 34520, lr = 0.001
I1026 01:01:37.462061 17176 solver.cpp:229] Iteration 34540, loss = 0.0446365
I1026 01:01:37.462091 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.039235 (* 1 = 0.039235 loss)
I1026 01:01:37.462096 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00540153 (* 1 = 0.00540153 loss)
I1026 01:01:37.462100 17176 sgd_solver.cpp:106] Iteration 34540, lr = 0.001
I1026 01:01:38.025342 17176 solver.cpp:229] Iteration 34560, loss = 0.558815
I1026 01:01:38.025374 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.189719 (* 1 = 0.189719 loss)
I1026 01:01:38.025379 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.369096 (* 1 = 0.369096 loss)
I1026 01:01:38.025383 17176 sgd_solver.cpp:106] Iteration 34560, lr = 0.001
I1026 01:01:38.592703 17176 solver.cpp:229] Iteration 34580, loss = 0.0495095
I1026 01:01:38.592736 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0412917 (* 1 = 0.0412917 loss)
I1026 01:01:38.592741 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00821785 (* 1 = 0.00821785 loss)
I1026 01:01:38.592746 17176 sgd_solver.cpp:106] Iteration 34580, lr = 0.001
I1026 01:01:39.164186 17176 solver.cpp:229] Iteration 34600, loss = 0.106951
I1026 01:01:39.164218 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0906209 (* 1 = 0.0906209 loss)
I1026 01:01:39.164223 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0163301 (* 1 = 0.0163301 loss)
I1026 01:01:39.164227 17176 sgd_solver.cpp:106] Iteration 34600, lr = 0.001
I1026 01:01:39.736098 17176 solver.cpp:229] Iteration 34620, loss = 0.0263552
I1026 01:01:39.736130 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0189855 (* 1 = 0.0189855 loss)
I1026 01:01:39.736135 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00736964 (* 1 = 0.00736964 loss)
I1026 01:01:39.736140 17176 sgd_solver.cpp:106] Iteration 34620, lr = 0.001
I1026 01:01:40.309481 17176 solver.cpp:229] Iteration 34640, loss = 0.0617915
I1026 01:01:40.309514 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.01324 (* 1 = 0.01324 loss)
I1026 01:01:40.309520 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0485514 (* 1 = 0.0485514 loss)
I1026 01:01:40.309525 17176 sgd_solver.cpp:106] Iteration 34640, lr = 0.001
I1026 01:01:40.877115 17176 solver.cpp:229] Iteration 34660, loss = 0.100834
I1026 01:01:40.877149 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0665899 (* 1 = 0.0665899 loss)
I1026 01:01:40.877154 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0342444 (* 1 = 0.0342444 loss)
I1026 01:01:40.877169 17176 sgd_solver.cpp:106] Iteration 34660, lr = 0.001
I1026 01:01:41.442677 17176 solver.cpp:229] Iteration 34680, loss = 0.115587
I1026 01:01:41.442709 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0255636 (* 1 = 0.0255636 loss)
I1026 01:01:41.442714 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0900237 (* 1 = 0.0900237 loss)
I1026 01:01:41.442718 17176 sgd_solver.cpp:106] Iteration 34680, lr = 0.001
I1026 01:01:42.007967 17176 solver.cpp:229] Iteration 34700, loss = 0.18677
I1026 01:01:42.008002 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0578117 (* 1 = 0.0578117 loss)
I1026 01:01:42.008008 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.128959 (* 1 = 0.128959 loss)
I1026 01:01:42.008013 17176 sgd_solver.cpp:106] Iteration 34700, lr = 0.001
I1026 01:01:42.565832 17176 solver.cpp:229] Iteration 34720, loss = 0.0399605
I1026 01:01:42.565865 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0158907 (* 1 = 0.0158907 loss)
I1026 01:01:42.565871 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0240698 (* 1 = 0.0240698 loss)
I1026 01:01:42.565876 17176 sgd_solver.cpp:106] Iteration 34720, lr = 0.001
I1026 01:01:43.132133 17176 solver.cpp:229] Iteration 34740, loss = 0.0183283
I1026 01:01:43.132177 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0108489 (* 1 = 0.0108489 loss)
I1026 01:01:43.132182 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00747936 (* 1 = 0.00747936 loss)
I1026 01:01:43.132187 17176 sgd_solver.cpp:106] Iteration 34740, lr = 0.001
I1026 01:01:43.694768 17176 solver.cpp:229] Iteration 34760, loss = 0.0690753
I1026 01:01:43.694803 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0458037 (* 1 = 0.0458037 loss)
I1026 01:01:43.694808 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0232716 (* 1 = 0.0232716 loss)
I1026 01:01:43.694813 17176 sgd_solver.cpp:106] Iteration 34760, lr = 0.001
I1026 01:01:44.261132 17176 solver.cpp:229] Iteration 34780, loss = 0.0230323
I1026 01:01:44.261163 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0196795 (* 1 = 0.0196795 loss)
I1026 01:01:44.261168 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00335284 (* 1 = 0.00335284 loss)
I1026 01:01:44.261173 17176 sgd_solver.cpp:106] Iteration 34780, lr = 0.001
I1026 01:01:44.824937 17176 solver.cpp:229] Iteration 34800, loss = 0.268643
I1026 01:01:44.824970 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0898853 (* 1 = 0.0898853 loss)
I1026 01:01:44.824976 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.178757 (* 1 = 0.178757 loss)
I1026 01:01:44.824980 17176 sgd_solver.cpp:106] Iteration 34800, lr = 0.001
I1026 01:01:45.388208 17176 solver.cpp:229] Iteration 34820, loss = 0.0681255
I1026 01:01:45.388243 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0300235 (* 1 = 0.0300235 loss)
I1026 01:01:45.388248 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0381019 (* 1 = 0.0381019 loss)
I1026 01:01:45.388253 17176 sgd_solver.cpp:106] Iteration 34820, lr = 0.001
I1026 01:01:45.942931 17176 solver.cpp:229] Iteration 34840, loss = 0.373995
I1026 01:01:45.942965 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0993186 (* 1 = 0.0993186 loss)
I1026 01:01:45.942986 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.274676 (* 1 = 0.274676 loss)
I1026 01:01:45.942991 17176 sgd_solver.cpp:106] Iteration 34840, lr = 0.001
I1026 01:01:46.504966 17176 solver.cpp:229] Iteration 34860, loss = 0.114899
I1026 01:01:46.504997 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0839258 (* 1 = 0.0839258 loss)
I1026 01:01:46.505003 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0309734 (* 1 = 0.0309734 loss)
I1026 01:01:46.505008 17176 sgd_solver.cpp:106] Iteration 34860, lr = 0.001
I1026 01:01:47.072135 17176 solver.cpp:229] Iteration 34880, loss = 0.0813237
I1026 01:01:47.072167 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0693363 (* 1 = 0.0693363 loss)
I1026 01:01:47.072172 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0119875 (* 1 = 0.0119875 loss)
I1026 01:01:47.072176 17176 sgd_solver.cpp:106] Iteration 34880, lr = 0.001
I1026 01:01:47.626488 17176 solver.cpp:229] Iteration 34900, loss = 0.0634296
I1026 01:01:47.626521 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0227432 (* 1 = 0.0227432 loss)
I1026 01:01:47.626525 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0406863 (* 1 = 0.0406863 loss)
I1026 01:01:47.626530 17176 sgd_solver.cpp:106] Iteration 34900, lr = 0.001
I1026 01:01:48.185519 17176 solver.cpp:229] Iteration 34920, loss = 0.403911
I1026 01:01:48.185561 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.14492 (* 1 = 0.14492 loss)
I1026 01:01:48.185567 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.258991 (* 1 = 0.258991 loss)
I1026 01:01:48.185570 17176 sgd_solver.cpp:106] Iteration 34920, lr = 0.001
I1026 01:01:48.743297 17176 solver.cpp:229] Iteration 34940, loss = 0.0614668
I1026 01:01:48.743340 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0371879 (* 1 = 0.0371879 loss)
I1026 01:01:48.743345 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0242789 (* 1 = 0.0242789 loss)
I1026 01:01:48.743348 17176 sgd_solver.cpp:106] Iteration 34940, lr = 0.001
I1026 01:01:49.310163 17176 solver.cpp:229] Iteration 34960, loss = 0.0478478
I1026 01:01:49.310195 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0228727 (* 1 = 0.0228727 loss)
I1026 01:01:49.310199 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0249751 (* 1 = 0.0249751 loss)
I1026 01:01:49.310204 17176 sgd_solver.cpp:106] Iteration 34960, lr = 0.001
I1026 01:01:49.861080 17176 solver.cpp:229] Iteration 34980, loss = 0.0596714
I1026 01:01:49.861124 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0412773 (* 1 = 0.0412773 loss)
I1026 01:01:49.861129 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0183941 (* 1 = 0.0183941 loss)
I1026 01:01:49.861132 17176 sgd_solver.cpp:106] Iteration 34980, lr = 0.001
I1026 01:01:50.431975 17176 solver.cpp:229] Iteration 35000, loss = 0.0477901
I1026 01:01:50.432009 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0399425 (* 1 = 0.0399425 loss)
I1026 01:01:50.432014 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00784758 (* 1 = 0.00784758 loss)
I1026 01:01:50.432019 17176 sgd_solver.cpp:106] Iteration 35000, lr = 0.001
I1026 01:01:50.992331 17176 solver.cpp:229] Iteration 35020, loss = 0.0497418
I1026 01:01:50.992363 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0155759 (* 1 = 0.0155759 loss)
I1026 01:01:50.992368 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0341659 (* 1 = 0.0341659 loss)
I1026 01:01:50.992372 17176 sgd_solver.cpp:106] Iteration 35020, lr = 0.001
I1026 01:01:51.554029 17176 solver.cpp:229] Iteration 35040, loss = 0.02028
I1026 01:01:51.554061 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00981262 (* 1 = 0.00981262 loss)
I1026 01:01:51.554067 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0104674 (* 1 = 0.0104674 loss)
I1026 01:01:51.554071 17176 sgd_solver.cpp:106] Iteration 35040, lr = 0.001
I1026 01:01:52.120221 17176 solver.cpp:229] Iteration 35060, loss = 0.07267
I1026 01:01:52.120255 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0502771 (* 1 = 0.0502771 loss)
I1026 01:01:52.120261 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0223929 (* 1 = 0.0223929 loss)
I1026 01:01:52.120265 17176 sgd_solver.cpp:106] Iteration 35060, lr = 0.001
I1026 01:01:52.686301 17176 solver.cpp:229] Iteration 35080, loss = 0.0419938
I1026 01:01:52.686336 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0181781 (* 1 = 0.0181781 loss)
I1026 01:01:52.686341 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0238156 (* 1 = 0.0238156 loss)
I1026 01:01:52.686345 17176 sgd_solver.cpp:106] Iteration 35080, lr = 0.001
I1026 01:01:53.257199 17176 solver.cpp:229] Iteration 35100, loss = 0.124756
I1026 01:01:53.257232 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0672496 (* 1 = 0.0672496 loss)
I1026 01:01:53.257238 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0575062 (* 1 = 0.0575062 loss)
I1026 01:01:53.257242 17176 sgd_solver.cpp:106] Iteration 35100, lr = 0.001
I1026 01:01:53.823362 17176 solver.cpp:229] Iteration 35120, loss = 0.118745
I1026 01:01:53.823396 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0445298 (* 1 = 0.0445298 loss)
I1026 01:01:53.823401 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0742157 (* 1 = 0.0742157 loss)
I1026 01:01:53.823405 17176 sgd_solver.cpp:106] Iteration 35120, lr = 0.001
I1026 01:01:54.383317 17176 solver.cpp:229] Iteration 35140, loss = 0.251734
I1026 01:01:54.383350 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0544729 (* 1 = 0.0544729 loss)
I1026 01:01:54.383357 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.197261 (* 1 = 0.197261 loss)
I1026 01:01:54.383361 17176 sgd_solver.cpp:106] Iteration 35140, lr = 0.001
I1026 01:01:54.944892 17176 solver.cpp:229] Iteration 35160, loss = 0.0571429
I1026 01:01:54.944936 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0460528 (* 1 = 0.0460528 loss)
I1026 01:01:54.944941 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0110901 (* 1 = 0.0110901 loss)
I1026 01:01:54.944944 17176 sgd_solver.cpp:106] Iteration 35160, lr = 0.001
I1026 01:01:55.515821 17176 solver.cpp:229] Iteration 35180, loss = 0.01222
I1026 01:01:55.515864 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00861843 (* 1 = 0.00861843 loss)
I1026 01:01:55.515871 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00360156 (* 1 = 0.00360156 loss)
I1026 01:01:55.515875 17176 sgd_solver.cpp:106] Iteration 35180, lr = 0.001
I1026 01:01:56.073151 17176 solver.cpp:229] Iteration 35200, loss = 0.1724
I1026 01:01:56.073202 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0806613 (* 1 = 0.0806613 loss)
I1026 01:01:56.073207 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.091739 (* 1 = 0.091739 loss)
I1026 01:01:56.073212 17176 sgd_solver.cpp:106] Iteration 35200, lr = 0.001
I1026 01:01:56.641182 17176 solver.cpp:229] Iteration 35220, loss = 0.0318854
I1026 01:01:56.641211 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0112383 (* 1 = 0.0112383 loss)
I1026 01:01:56.641216 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0206471 (* 1 = 0.0206471 loss)
I1026 01:01:56.641219 17176 sgd_solver.cpp:106] Iteration 35220, lr = 0.001
I1026 01:01:57.203291 17176 solver.cpp:229] Iteration 35240, loss = 0.121405
I1026 01:01:57.203326 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0481985 (* 1 = 0.0481985 loss)
I1026 01:01:57.203332 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0732068 (* 1 = 0.0732068 loss)
I1026 01:01:57.203336 17176 sgd_solver.cpp:106] Iteration 35240, lr = 0.001
I1026 01:01:57.752755 17176 solver.cpp:229] Iteration 35260, loss = 0.0207771
I1026 01:01:57.752789 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0187251 (* 1 = 0.0187251 loss)
I1026 01:01:57.752794 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.002052 (* 1 = 0.002052 loss)
I1026 01:01:57.752799 17176 sgd_solver.cpp:106] Iteration 35260, lr = 0.001
I1026 01:01:58.309973 17176 solver.cpp:229] Iteration 35280, loss = 0.07616
I1026 01:01:58.310006 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0647831 (* 1 = 0.0647831 loss)
I1026 01:01:58.310011 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0113768 (* 1 = 0.0113768 loss)
I1026 01:01:58.310016 17176 sgd_solver.cpp:106] Iteration 35280, lr = 0.001
I1026 01:01:58.879322 17176 solver.cpp:229] Iteration 35300, loss = 0.0665628
I1026 01:01:58.879354 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0523331 (* 1 = 0.0523331 loss)
I1026 01:01:58.879361 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0142297 (* 1 = 0.0142297 loss)
I1026 01:01:58.879365 17176 sgd_solver.cpp:106] Iteration 35300, lr = 0.001
I1026 01:01:59.429649 17176 solver.cpp:229] Iteration 35320, loss = 0.0219944
I1026 01:01:59.429697 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0197834 (* 1 = 0.0197834 loss)
I1026 01:01:59.429702 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00221105 (* 1 = 0.00221105 loss)
I1026 01:01:59.429707 17176 sgd_solver.cpp:106] Iteration 35320, lr = 0.001
I1026 01:01:59.995784 17176 solver.cpp:229] Iteration 35340, loss = 0.126369
I1026 01:01:59.995817 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0670697 (* 1 = 0.0670697 loss)
I1026 01:01:59.995822 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0592995 (* 1 = 0.0592995 loss)
I1026 01:01:59.995827 17176 sgd_solver.cpp:106] Iteration 35340, lr = 0.001
I1026 01:02:00.572383 17176 solver.cpp:229] Iteration 35360, loss = 0.121639
I1026 01:02:00.572414 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0518602 (* 1 = 0.0518602 loss)
I1026 01:02:00.572419 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0697792 (* 1 = 0.0697792 loss)
I1026 01:02:00.572424 17176 sgd_solver.cpp:106] Iteration 35360, lr = 0.001
I1026 01:02:01.145275 17176 solver.cpp:229] Iteration 35380, loss = 0.0386956
I1026 01:02:01.145309 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0180635 (* 1 = 0.0180635 loss)
I1026 01:02:01.145314 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0206321 (* 1 = 0.0206321 loss)
I1026 01:02:01.145318 17176 sgd_solver.cpp:106] Iteration 35380, lr = 0.001
I1026 01:02:01.694989 17176 solver.cpp:229] Iteration 35400, loss = 0.0733028
I1026 01:02:01.695022 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0157636 (* 1 = 0.0157636 loss)
I1026 01:02:01.695027 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0575391 (* 1 = 0.0575391 loss)
I1026 01:02:01.695031 17176 sgd_solver.cpp:106] Iteration 35400, lr = 0.001
I1026 01:02:02.253873 17176 solver.cpp:229] Iteration 35420, loss = 0.032787
I1026 01:02:02.253906 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0268898 (* 1 = 0.0268898 loss)
I1026 01:02:02.253911 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00589722 (* 1 = 0.00589722 loss)
I1026 01:02:02.253916 17176 sgd_solver.cpp:106] Iteration 35420, lr = 0.001
I1026 01:02:02.818997 17176 solver.cpp:229] Iteration 35440, loss = 0.0665312
I1026 01:02:02.819031 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0574734 (* 1 = 0.0574734 loss)
I1026 01:02:02.819037 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00905778 (* 1 = 0.00905778 loss)
I1026 01:02:02.819041 17176 sgd_solver.cpp:106] Iteration 35440, lr = 0.001
I1026 01:02:03.371354 17176 solver.cpp:229] Iteration 35460, loss = 0.0916452
I1026 01:02:03.371387 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0746537 (* 1 = 0.0746537 loss)
I1026 01:02:03.371392 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0169916 (* 1 = 0.0169916 loss)
I1026 01:02:03.371395 17176 sgd_solver.cpp:106] Iteration 35460, lr = 0.001
I1026 01:02:03.945163 17176 solver.cpp:229] Iteration 35480, loss = 0.0570366
I1026 01:02:03.945206 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0187624 (* 1 = 0.0187624 loss)
I1026 01:02:03.945211 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0382742 (* 1 = 0.0382742 loss)
I1026 01:02:03.945216 17176 sgd_solver.cpp:106] Iteration 35480, lr = 0.001
I1026 01:02:04.500814 17176 solver.cpp:229] Iteration 35500, loss = 0.0440704
I1026 01:02:04.500846 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0389062 (* 1 = 0.0389062 loss)
I1026 01:02:04.500851 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00516416 (* 1 = 0.00516416 loss)
I1026 01:02:04.500855 17176 sgd_solver.cpp:106] Iteration 35500, lr = 0.001
I1026 01:02:05.051970 17176 solver.cpp:229] Iteration 35520, loss = 0.042742
I1026 01:02:05.052001 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0265478 (* 1 = 0.0265478 loss)
I1026 01:02:05.052006 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0161942 (* 1 = 0.0161942 loss)
I1026 01:02:05.052009 17176 sgd_solver.cpp:106] Iteration 35520, lr = 0.001
I1026 01:02:05.612879 17176 solver.cpp:229] Iteration 35540, loss = 0.0212291
I1026 01:02:05.612910 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0198734 (* 1 = 0.0198734 loss)
I1026 01:02:05.612915 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00135572 (* 1 = 0.00135572 loss)
I1026 01:02:05.612918 17176 sgd_solver.cpp:106] Iteration 35540, lr = 0.001
I1026 01:02:06.165498 17176 solver.cpp:229] Iteration 35560, loss = 0.0741979
I1026 01:02:06.165530 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0463666 (* 1 = 0.0463666 loss)
I1026 01:02:06.165534 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0278313 (* 1 = 0.0278313 loss)
I1026 01:02:06.165539 17176 sgd_solver.cpp:106] Iteration 35560, lr = 0.001
I1026 01:02:06.732494 17176 solver.cpp:229] Iteration 35580, loss = 0.0598528
I1026 01:02:06.732527 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0175583 (* 1 = 0.0175583 loss)
I1026 01:02:06.732532 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0422945 (* 1 = 0.0422945 loss)
I1026 01:02:06.732535 17176 sgd_solver.cpp:106] Iteration 35580, lr = 0.001
I1026 01:02:07.292417 17176 solver.cpp:229] Iteration 35600, loss = 0.0837585
I1026 01:02:07.292449 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0283739 (* 1 = 0.0283739 loss)
I1026 01:02:07.292454 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0553846 (* 1 = 0.0553846 loss)
I1026 01:02:07.292459 17176 sgd_solver.cpp:106] Iteration 35600, lr = 0.001
I1026 01:02:07.844291 17176 solver.cpp:229] Iteration 35620, loss = 0.212007
I1026 01:02:07.844343 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.131757 (* 1 = 0.131757 loss)
I1026 01:02:07.844348 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0802497 (* 1 = 0.0802497 loss)
I1026 01:02:07.844352 17176 sgd_solver.cpp:106] Iteration 35620, lr = 0.001
I1026 01:02:08.398790 17176 solver.cpp:229] Iteration 35640, loss = 0.149385
I1026 01:02:08.398823 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0988063 (* 1 = 0.0988063 loss)
I1026 01:02:08.398828 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0505782 (* 1 = 0.0505782 loss)
I1026 01:02:08.398833 17176 sgd_solver.cpp:106] Iteration 35640, lr = 0.001
I1026 01:02:08.958181 17176 solver.cpp:229] Iteration 35660, loss = 0.0568662
I1026 01:02:08.958212 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0219436 (* 1 = 0.0219436 loss)
I1026 01:02:08.958217 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0349226 (* 1 = 0.0349226 loss)
I1026 01:02:08.958221 17176 sgd_solver.cpp:106] Iteration 35660, lr = 0.001
I1026 01:02:09.507684 17176 solver.cpp:229] Iteration 35680, loss = 0.110625
I1026 01:02:09.507727 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0654979 (* 1 = 0.0654979 loss)
I1026 01:02:09.507732 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0451271 (* 1 = 0.0451271 loss)
I1026 01:02:09.507736 17176 sgd_solver.cpp:106] Iteration 35680, lr = 0.001
I1026 01:02:10.075482 17176 solver.cpp:229] Iteration 35700, loss = 0.0301877
I1026 01:02:10.075513 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0174777 (* 1 = 0.0174777 loss)
I1026 01:02:10.075518 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.01271 (* 1 = 0.01271 loss)
I1026 01:02:10.075522 17176 sgd_solver.cpp:106] Iteration 35700, lr = 0.001
I1026 01:02:10.631718 17176 solver.cpp:229] Iteration 35720, loss = 0.346951
I1026 01:02:10.631747 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0321149 (* 1 = 0.0321149 loss)
I1026 01:02:10.631752 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.314836 (* 1 = 0.314836 loss)
I1026 01:02:10.631757 17176 sgd_solver.cpp:106] Iteration 35720, lr = 0.001
I1026 01:02:11.190548 17176 solver.cpp:229] Iteration 35740, loss = 0.115728
I1026 01:02:11.190582 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0412178 (* 1 = 0.0412178 loss)
I1026 01:02:11.190587 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0745102 (* 1 = 0.0745102 loss)
I1026 01:02:11.190590 17176 sgd_solver.cpp:106] Iteration 35740, lr = 0.001
I1026 01:02:11.749790 17176 solver.cpp:229] Iteration 35760, loss = 0.154878
I1026 01:02:11.749826 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0713419 (* 1 = 0.0713419 loss)
I1026 01:02:11.749835 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0835357 (* 1 = 0.0835357 loss)
I1026 01:02:11.749840 17176 sgd_solver.cpp:106] Iteration 35760, lr = 0.001
I1026 01:02:12.317898 17176 solver.cpp:229] Iteration 35780, loss = 0.0190019
I1026 01:02:12.317932 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0160593 (* 1 = 0.0160593 loss)
I1026 01:02:12.317940 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00294265 (* 1 = 0.00294265 loss)
I1026 01:02:12.317946 17176 sgd_solver.cpp:106] Iteration 35780, lr = 0.001
I1026 01:02:12.879667 17176 solver.cpp:229] Iteration 35800, loss = 0.114388
I1026 01:02:12.879703 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0314655 (* 1 = 0.0314655 loss)
I1026 01:02:12.879711 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0829224 (* 1 = 0.0829224 loss)
I1026 01:02:12.879717 17176 sgd_solver.cpp:106] Iteration 35800, lr = 0.001
I1026 01:02:13.450263 17176 solver.cpp:229] Iteration 35820, loss = 0.0429301
I1026 01:02:13.450299 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0266672 (* 1 = 0.0266672 loss)
I1026 01:02:13.450307 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.016263 (* 1 = 0.016263 loss)
I1026 01:02:13.450323 17176 sgd_solver.cpp:106] Iteration 35820, lr = 0.001
I1026 01:02:14.007663 17176 solver.cpp:229] Iteration 35840, loss = 0.0271501
I1026 01:02:14.007699 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00957111 (* 1 = 0.00957111 loss)
I1026 01:02:14.007707 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.017579 (* 1 = 0.017579 loss)
I1026 01:02:14.007714 17176 sgd_solver.cpp:106] Iteration 35840, lr = 0.001
I1026 01:02:14.568035 17176 solver.cpp:229] Iteration 35860, loss = 0.0365538
I1026 01:02:14.568070 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0284107 (* 1 = 0.0284107 loss)
I1026 01:02:14.568079 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00814308 (* 1 = 0.00814308 loss)
I1026 01:02:14.568085 17176 sgd_solver.cpp:106] Iteration 35860, lr = 0.001
I1026 01:02:15.137267 17176 solver.cpp:229] Iteration 35880, loss = 0.607954
I1026 01:02:15.137302 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.211332 (* 1 = 0.211332 loss)
I1026 01:02:15.137310 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.396622 (* 1 = 0.396622 loss)
I1026 01:02:15.137315 17176 sgd_solver.cpp:106] Iteration 35880, lr = 0.001
I1026 01:02:15.704255 17176 solver.cpp:229] Iteration 35900, loss = 0.0196865
I1026 01:02:15.704291 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0125287 (* 1 = 0.0125287 loss)
I1026 01:02:15.704299 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00715779 (* 1 = 0.00715779 loss)
I1026 01:02:15.704305 17176 sgd_solver.cpp:106] Iteration 35900, lr = 0.001
I1026 01:02:16.271004 17176 solver.cpp:229] Iteration 35920, loss = 0.0783104
I1026 01:02:16.271039 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0749462 (* 1 = 0.0749462 loss)
I1026 01:02:16.271049 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00336426 (* 1 = 0.00336426 loss)
I1026 01:02:16.271054 17176 sgd_solver.cpp:106] Iteration 35920, lr = 0.001
I1026 01:02:16.839143 17176 solver.cpp:229] Iteration 35940, loss = 0.0311625
I1026 01:02:16.839179 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0218017 (* 1 = 0.0218017 loss)
I1026 01:02:16.839187 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00936073 (* 1 = 0.00936073 loss)
I1026 01:02:16.839203 17176 sgd_solver.cpp:106] Iteration 35940, lr = 0.001
I1026 01:02:17.400715 17176 solver.cpp:229] Iteration 35960, loss = 0.055765
I1026 01:02:17.400751 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0368256 (* 1 = 0.0368256 loss)
I1026 01:02:17.400759 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0189393 (* 1 = 0.0189393 loss)
I1026 01:02:17.400765 17176 sgd_solver.cpp:106] Iteration 35960, lr = 0.001
I1026 01:02:17.951107 17176 solver.cpp:229] Iteration 35980, loss = 0.118152
I1026 01:02:17.951143 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.080876 (* 1 = 0.080876 loss)
I1026 01:02:17.951151 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0372763 (* 1 = 0.0372763 loss)
I1026 01:02:17.951158 17176 sgd_solver.cpp:106] Iteration 35980, lr = 0.001
I1026 01:02:18.519490 17176 solver.cpp:229] Iteration 36000, loss = 0.051005
I1026 01:02:18.519526 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0286066 (* 1 = 0.0286066 loss)
I1026 01:02:18.519533 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0223985 (* 1 = 0.0223985 loss)
I1026 01:02:18.519538 17176 sgd_solver.cpp:106] Iteration 36000, lr = 0.001
I1026 01:02:19.080605 17176 solver.cpp:229] Iteration 36020, loss = 0.0669485
I1026 01:02:19.080639 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0427509 (* 1 = 0.0427509 loss)
I1026 01:02:19.080647 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0241976 (* 1 = 0.0241976 loss)
I1026 01:02:19.080652 17176 sgd_solver.cpp:106] Iteration 36020, lr = 0.001
I1026 01:02:19.641376 17176 solver.cpp:229] Iteration 36040, loss = 0.0990766
I1026 01:02:19.641408 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0371517 (* 1 = 0.0371517 loss)
I1026 01:02:19.641413 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0619248 (* 1 = 0.0619248 loss)
I1026 01:02:19.641417 17176 sgd_solver.cpp:106] Iteration 36040, lr = 0.001
I1026 01:02:20.189230 17176 solver.cpp:229] Iteration 36060, loss = 0.0505831
I1026 01:02:20.189262 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0138014 (* 1 = 0.0138014 loss)
I1026 01:02:20.189266 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0367817 (* 1 = 0.0367817 loss)
I1026 01:02:20.189281 17176 sgd_solver.cpp:106] Iteration 36060, lr = 0.001
I1026 01:02:20.762287 17176 solver.cpp:229] Iteration 36080, loss = 0.0808188
I1026 01:02:20.762320 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0654234 (* 1 = 0.0654234 loss)
I1026 01:02:20.762325 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0153954 (* 1 = 0.0153954 loss)
I1026 01:02:20.762329 17176 sgd_solver.cpp:106] Iteration 36080, lr = 0.001
I1026 01:02:21.321583 17176 solver.cpp:229] Iteration 36100, loss = 0.0404654
I1026 01:02:21.321614 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0170489 (* 1 = 0.0170489 loss)
I1026 01:02:21.321619 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0234165 (* 1 = 0.0234165 loss)
I1026 01:02:21.321624 17176 sgd_solver.cpp:106] Iteration 36100, lr = 0.001
I1026 01:02:21.882185 17176 solver.cpp:229] Iteration 36120, loss = 0.0726335
I1026 01:02:21.882226 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.044104 (* 1 = 0.044104 loss)
I1026 01:02:21.882231 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0285295 (* 1 = 0.0285295 loss)
I1026 01:02:21.882236 17176 sgd_solver.cpp:106] Iteration 36120, lr = 0.001
I1026 01:02:22.442446 17176 solver.cpp:229] Iteration 36140, loss = 0.0959015
I1026 01:02:22.442479 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0787046 (* 1 = 0.0787046 loss)
I1026 01:02:22.442483 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0171968 (* 1 = 0.0171968 loss)
I1026 01:02:22.442487 17176 sgd_solver.cpp:106] Iteration 36140, lr = 0.001
I1026 01:02:22.998255 17176 solver.cpp:229] Iteration 36160, loss = 0.352587
I1026 01:02:22.998288 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.235961 (* 1 = 0.235961 loss)
I1026 01:02:22.998294 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.116626 (* 1 = 0.116626 loss)
I1026 01:02:22.998298 17176 sgd_solver.cpp:106] Iteration 36160, lr = 0.001
I1026 01:02:23.557551 17176 solver.cpp:229] Iteration 36180, loss = 0.0570279
I1026 01:02:23.557586 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0472711 (* 1 = 0.0472711 loss)
I1026 01:02:23.557592 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0097568 (* 1 = 0.0097568 loss)
I1026 01:02:23.557596 17176 sgd_solver.cpp:106] Iteration 36180, lr = 0.001
I1026 01:02:24.109061 17176 solver.cpp:229] Iteration 36200, loss = 0.236434
I1026 01:02:24.109092 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.191788 (* 1 = 0.191788 loss)
I1026 01:02:24.109098 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0446462 (* 1 = 0.0446462 loss)
I1026 01:02:24.109103 17176 sgd_solver.cpp:106] Iteration 36200, lr = 0.001
I1026 01:02:24.669816 17176 solver.cpp:229] Iteration 36220, loss = 0.0365521
I1026 01:02:24.669849 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0194706 (* 1 = 0.0194706 loss)
I1026 01:02:24.669854 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0170815 (* 1 = 0.0170815 loss)
I1026 01:02:24.669860 17176 sgd_solver.cpp:106] Iteration 36220, lr = 0.001
I1026 01:02:25.232607 17176 solver.cpp:229] Iteration 36240, loss = 0.106191
I1026 01:02:25.232642 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0520757 (* 1 = 0.0520757 loss)
I1026 01:02:25.232648 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0541152 (* 1 = 0.0541152 loss)
I1026 01:02:25.232652 17176 sgd_solver.cpp:106] Iteration 36240, lr = 0.001
I1026 01:02:25.801213 17176 solver.cpp:229] Iteration 36260, loss = 0.154045
I1026 01:02:25.801245 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.113162 (* 1 = 0.113162 loss)
I1026 01:02:25.801250 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0408827 (* 1 = 0.0408827 loss)
I1026 01:02:25.801254 17176 sgd_solver.cpp:106] Iteration 36260, lr = 0.001
I1026 01:02:26.369794 17176 solver.cpp:229] Iteration 36280, loss = 0.14864
I1026 01:02:26.369830 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0688335 (* 1 = 0.0688335 loss)
I1026 01:02:26.369835 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0798065 (* 1 = 0.0798065 loss)
I1026 01:02:26.369840 17176 sgd_solver.cpp:106] Iteration 36280, lr = 0.001
I1026 01:02:26.933631 17176 solver.cpp:229] Iteration 36300, loss = 0.0429481
I1026 01:02:26.933665 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0374721 (* 1 = 0.0374721 loss)
I1026 01:02:26.933670 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00547594 (* 1 = 0.00547594 loss)
I1026 01:02:26.933675 17176 sgd_solver.cpp:106] Iteration 36300, lr = 0.001
I1026 01:02:27.495574 17176 solver.cpp:229] Iteration 36320, loss = 0.138053
I1026 01:02:27.495609 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0927153 (* 1 = 0.0927153 loss)
I1026 01:02:27.495614 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0453377 (* 1 = 0.0453377 loss)
I1026 01:02:27.495628 17176 sgd_solver.cpp:106] Iteration 36320, lr = 0.001
I1026 01:02:28.058845 17176 solver.cpp:229] Iteration 36340, loss = 0.0258128
I1026 01:02:28.058878 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0154203 (* 1 = 0.0154203 loss)
I1026 01:02:28.058883 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0103925 (* 1 = 0.0103925 loss)
I1026 01:02:28.058888 17176 sgd_solver.cpp:106] Iteration 36340, lr = 0.001
I1026 01:02:28.617322 17176 solver.cpp:229] Iteration 36360, loss = 0.102623
I1026 01:02:28.617355 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.064517 (* 1 = 0.064517 loss)
I1026 01:02:28.617360 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0381057 (* 1 = 0.0381057 loss)
I1026 01:02:28.617365 17176 sgd_solver.cpp:106] Iteration 36360, lr = 0.001
I1026 01:02:29.166611 17176 solver.cpp:229] Iteration 36380, loss = 0.035679
I1026 01:02:29.166645 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00857622 (* 1 = 0.00857622 loss)
I1026 01:02:29.166651 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0271027 (* 1 = 0.0271027 loss)
I1026 01:02:29.166656 17176 sgd_solver.cpp:106] Iteration 36380, lr = 0.001
I1026 01:02:29.724494 17176 solver.cpp:229] Iteration 36400, loss = 0.0662213
I1026 01:02:29.724542 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0129795 (* 1 = 0.0129795 loss)
I1026 01:02:29.724547 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0532418 (* 1 = 0.0532418 loss)
I1026 01:02:29.724552 17176 sgd_solver.cpp:106] Iteration 36400, lr = 0.001
I1026 01:02:30.287909 17176 solver.cpp:229] Iteration 36420, loss = 0.0771181
I1026 01:02:30.287942 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0445824 (* 1 = 0.0445824 loss)
I1026 01:02:30.287947 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0325357 (* 1 = 0.0325357 loss)
I1026 01:02:30.287953 17176 sgd_solver.cpp:106] Iteration 36420, lr = 0.001
I1026 01:02:30.849578 17176 solver.cpp:229] Iteration 36440, loss = 0.0388412
I1026 01:02:30.849612 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0213129 (* 1 = 0.0213129 loss)
I1026 01:02:30.849618 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0175283 (* 1 = 0.0175283 loss)
I1026 01:02:30.849623 17176 sgd_solver.cpp:106] Iteration 36440, lr = 0.001
I1026 01:02:31.412930 17176 solver.cpp:229] Iteration 36460, loss = 0.028261
I1026 01:02:31.412962 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0182173 (* 1 = 0.0182173 loss)
I1026 01:02:31.412969 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0100437 (* 1 = 0.0100437 loss)
I1026 01:02:31.412976 17176 sgd_solver.cpp:106] Iteration 36460, lr = 0.001
I1026 01:02:31.968422 17176 solver.cpp:229] Iteration 36480, loss = 0.0655395
I1026 01:02:31.968456 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0539711 (* 1 = 0.0539711 loss)
I1026 01:02:31.968462 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0115684 (* 1 = 0.0115684 loss)
I1026 01:02:31.968466 17176 sgd_solver.cpp:106] Iteration 36480, lr = 0.001
I1026 01:02:32.542021 17176 solver.cpp:229] Iteration 36500, loss = 0.0422359
I1026 01:02:32.542054 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00983709 (* 1 = 0.00983709 loss)
I1026 01:02:32.542059 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0323988 (* 1 = 0.0323988 loss)
I1026 01:02:32.542064 17176 sgd_solver.cpp:106] Iteration 36500, lr = 0.001
I1026 01:02:33.107938 17176 solver.cpp:229] Iteration 36520, loss = 0.0328158
I1026 01:02:33.107971 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0227466 (* 1 = 0.0227466 loss)
I1026 01:02:33.107977 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0100692 (* 1 = 0.0100692 loss)
I1026 01:02:33.107981 17176 sgd_solver.cpp:106] Iteration 36520, lr = 0.001
I1026 01:02:33.667212 17176 solver.cpp:229] Iteration 36540, loss = 0.142821
I1026 01:02:33.667244 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0552378 (* 1 = 0.0552378 loss)
I1026 01:02:33.667249 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0875835 (* 1 = 0.0875835 loss)
I1026 01:02:33.667253 17176 sgd_solver.cpp:106] Iteration 36540, lr = 0.001
I1026 01:02:34.234809 17176 solver.cpp:229] Iteration 36560, loss = 0.0304289
I1026 01:02:34.234853 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0252042 (* 1 = 0.0252042 loss)
I1026 01:02:34.234859 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00522471 (* 1 = 0.00522471 loss)
I1026 01:02:34.234863 17176 sgd_solver.cpp:106] Iteration 36560, lr = 0.001
I1026 01:02:34.796319 17176 solver.cpp:229] Iteration 36580, loss = 0.0327552
I1026 01:02:34.796351 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0204134 (* 1 = 0.0204134 loss)
I1026 01:02:34.796356 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0123417 (* 1 = 0.0123417 loss)
I1026 01:02:34.796361 17176 sgd_solver.cpp:106] Iteration 36580, lr = 0.001
I1026 01:02:35.351440 17176 solver.cpp:229] Iteration 36600, loss = 0.0253883
I1026 01:02:35.351474 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0119913 (* 1 = 0.0119913 loss)
I1026 01:02:35.351480 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0133971 (* 1 = 0.0133971 loss)
I1026 01:02:35.351495 17176 sgd_solver.cpp:106] Iteration 36600, lr = 0.001
I1026 01:02:35.916383 17176 solver.cpp:229] Iteration 36620, loss = 0.235957
I1026 01:02:35.916417 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0856563 (* 1 = 0.0856563 loss)
I1026 01:02:35.916424 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.150301 (* 1 = 0.150301 loss)
I1026 01:02:35.916427 17176 sgd_solver.cpp:106] Iteration 36620, lr = 0.001
I1026 01:02:36.480170 17176 solver.cpp:229] Iteration 36640, loss = 0.161519
I1026 01:02:36.480213 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0553297 (* 1 = 0.0553297 loss)
I1026 01:02:36.480219 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.106189 (* 1 = 0.106189 loss)
I1026 01:02:36.480224 17176 sgd_solver.cpp:106] Iteration 36640, lr = 0.001
I1026 01:02:37.050252 17176 solver.cpp:229] Iteration 36660, loss = 0.0690866
I1026 01:02:37.050287 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0554099 (* 1 = 0.0554099 loss)
I1026 01:02:37.050292 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0136768 (* 1 = 0.0136768 loss)
I1026 01:02:37.050297 17176 sgd_solver.cpp:106] Iteration 36660, lr = 0.001
I1026 01:02:37.603116 17176 solver.cpp:229] Iteration 36680, loss = 0.0481521
I1026 01:02:37.603149 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0241044 (* 1 = 0.0241044 loss)
I1026 01:02:37.603154 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0240476 (* 1 = 0.0240476 loss)
I1026 01:02:37.603158 17176 sgd_solver.cpp:106] Iteration 36680, lr = 0.001
I1026 01:02:38.155961 17176 solver.cpp:229] Iteration 36700, loss = 0.0743498
I1026 01:02:38.155993 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0460877 (* 1 = 0.0460877 loss)
I1026 01:02:38.155998 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0282622 (* 1 = 0.0282622 loss)
I1026 01:02:38.156003 17176 sgd_solver.cpp:106] Iteration 36700, lr = 0.001
I1026 01:02:38.723616 17176 solver.cpp:229] Iteration 36720, loss = 0.0776249
I1026 01:02:38.723659 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.026653 (* 1 = 0.026653 loss)
I1026 01:02:38.723665 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0509719 (* 1 = 0.0509719 loss)
I1026 01:02:38.723670 17176 sgd_solver.cpp:106] Iteration 36720, lr = 0.001
I1026 01:02:39.303231 17176 solver.cpp:229] Iteration 36740, loss = 0.0699951
I1026 01:02:39.303274 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0272329 (* 1 = 0.0272329 loss)
I1026 01:02:39.303279 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0427621 (* 1 = 0.0427621 loss)
I1026 01:02:39.303284 17176 sgd_solver.cpp:106] Iteration 36740, lr = 0.001
I1026 01:02:39.877676 17176 solver.cpp:229] Iteration 36760, loss = 0.349659
I1026 01:02:39.877708 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.258375 (* 1 = 0.258375 loss)
I1026 01:02:39.877715 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.091284 (* 1 = 0.091284 loss)
I1026 01:02:39.877730 17176 sgd_solver.cpp:106] Iteration 36760, lr = 0.001
I1026 01:02:40.444826 17176 solver.cpp:229] Iteration 36780, loss = 0.0893415
I1026 01:02:40.444871 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.057951 (* 1 = 0.057951 loss)
I1026 01:02:40.444886 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0313905 (* 1 = 0.0313905 loss)
I1026 01:02:40.444891 17176 sgd_solver.cpp:106] Iteration 36780, lr = 0.001
I1026 01:02:41.011904 17176 solver.cpp:229] Iteration 36800, loss = 0.0275016
I1026 01:02:41.011937 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0192609 (* 1 = 0.0192609 loss)
I1026 01:02:41.011942 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00824076 (* 1 = 0.00824076 loss)
I1026 01:02:41.011947 17176 sgd_solver.cpp:106] Iteration 36800, lr = 0.001
I1026 01:02:41.570147 17176 solver.cpp:229] Iteration 36820, loss = 0.03849
I1026 01:02:41.570179 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0306654 (* 1 = 0.0306654 loss)
I1026 01:02:41.570184 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00782467 (* 1 = 0.00782467 loss)
I1026 01:02:41.570188 17176 sgd_solver.cpp:106] Iteration 36820, lr = 0.001
I1026 01:02:42.138532 17176 solver.cpp:229] Iteration 36840, loss = 0.137194
I1026 01:02:42.138564 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0270192 (* 1 = 0.0270192 loss)
I1026 01:02:42.138569 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.110175 (* 1 = 0.110175 loss)
I1026 01:02:42.138586 17176 sgd_solver.cpp:106] Iteration 36840, lr = 0.001
I1026 01:02:42.706343 17176 solver.cpp:229] Iteration 36860, loss = 0.0440568
I1026 01:02:42.706375 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0343973 (* 1 = 0.0343973 loss)
I1026 01:02:42.706380 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0096595 (* 1 = 0.0096595 loss)
I1026 01:02:42.706384 17176 sgd_solver.cpp:106] Iteration 36860, lr = 0.001
I1026 01:02:43.279374 17176 solver.cpp:229] Iteration 36880, loss = 0.0177348
I1026 01:02:43.279407 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00560233 (* 1 = 0.00560233 loss)
I1026 01:02:43.279412 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0121324 (* 1 = 0.0121324 loss)
I1026 01:02:43.279417 17176 sgd_solver.cpp:106] Iteration 36880, lr = 0.001
I1026 01:02:43.841454 17176 solver.cpp:229] Iteration 36900, loss = 0.0540665
I1026 01:02:43.841486 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0417921 (* 1 = 0.0417921 loss)
I1026 01:02:43.841491 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0122744 (* 1 = 0.0122744 loss)
I1026 01:02:43.841495 17176 sgd_solver.cpp:106] Iteration 36900, lr = 0.001
I1026 01:02:44.419929 17176 solver.cpp:229] Iteration 36920, loss = 0.0968398
I1026 01:02:44.419962 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0239755 (* 1 = 0.0239755 loss)
I1026 01:02:44.419967 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0728643 (* 1 = 0.0728643 loss)
I1026 01:02:44.419971 17176 sgd_solver.cpp:106] Iteration 36920, lr = 0.001
I1026 01:02:44.986505 17176 solver.cpp:229] Iteration 36940, loss = 0.16503
I1026 01:02:44.986539 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0468694 (* 1 = 0.0468694 loss)
I1026 01:02:44.986544 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.11816 (* 1 = 0.11816 loss)
I1026 01:02:44.986549 17176 sgd_solver.cpp:106] Iteration 36940, lr = 0.001
I1026 01:02:45.549109 17176 solver.cpp:229] Iteration 36960, loss = 0.0382939
I1026 01:02:45.549141 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0343415 (* 1 = 0.0343415 loss)
I1026 01:02:45.549147 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00395237 (* 1 = 0.00395237 loss)
I1026 01:02:45.549151 17176 sgd_solver.cpp:106] Iteration 36960, lr = 0.001
I1026 01:02:46.124321 17176 solver.cpp:229] Iteration 36980, loss = 0.0261348
I1026 01:02:46.124363 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0181933 (* 1 = 0.0181933 loss)
I1026 01:02:46.124368 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00794146 (* 1 = 0.00794146 loss)
I1026 01:02:46.124373 17176 sgd_solver.cpp:106] Iteration 36980, lr = 0.001
I1026 01:02:46.686554 17176 solver.cpp:229] Iteration 37000, loss = 0.050404
I1026 01:02:46.686588 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0350097 (* 1 = 0.0350097 loss)
I1026 01:02:46.686592 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0153944 (* 1 = 0.0153944 loss)
I1026 01:02:46.686596 17176 sgd_solver.cpp:106] Iteration 37000, lr = 0.001
I1026 01:02:47.254071 17176 solver.cpp:229] Iteration 37020, loss = 0.249847
I1026 01:02:47.254106 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0275063 (* 1 = 0.0275063 loss)
I1026 01:02:47.254112 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.222341 (* 1 = 0.222341 loss)
I1026 01:02:47.254117 17176 sgd_solver.cpp:106] Iteration 37020, lr = 0.001
I1026 01:02:47.811017 17176 solver.cpp:229] Iteration 37040, loss = 0.0988408
I1026 01:02:47.811049 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0346695 (* 1 = 0.0346695 loss)
I1026 01:02:47.811055 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0641713 (* 1 = 0.0641713 loss)
I1026 01:02:47.811059 17176 sgd_solver.cpp:106] Iteration 37040, lr = 0.001
I1026 01:02:48.373397 17176 solver.cpp:229] Iteration 37060, loss = 0.0765693
I1026 01:02:48.373432 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0226134 (* 1 = 0.0226134 loss)
I1026 01:02:48.373437 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0539559 (* 1 = 0.0539559 loss)
I1026 01:02:48.373442 17176 sgd_solver.cpp:106] Iteration 37060, lr = 0.001
I1026 01:02:48.945411 17176 solver.cpp:229] Iteration 37080, loss = 0.0837423
I1026 01:02:48.945446 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0629751 (* 1 = 0.0629751 loss)
I1026 01:02:48.945451 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0207672 (* 1 = 0.0207672 loss)
I1026 01:02:48.945454 17176 sgd_solver.cpp:106] Iteration 37080, lr = 0.001
I1026 01:02:49.517942 17176 solver.cpp:229] Iteration 37100, loss = 0.0740507
I1026 01:02:49.517987 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.040201 (* 1 = 0.040201 loss)
I1026 01:02:49.517992 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0338497 (* 1 = 0.0338497 loss)
I1026 01:02:49.517997 17176 sgd_solver.cpp:106] Iteration 37100, lr = 0.001
I1026 01:02:50.087767 17176 solver.cpp:229] Iteration 37120, loss = 0.060725
I1026 01:02:50.087802 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0351396 (* 1 = 0.0351396 loss)
I1026 01:02:50.087808 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0255854 (* 1 = 0.0255854 loss)
I1026 01:02:50.087812 17176 sgd_solver.cpp:106] Iteration 37120, lr = 0.001
I1026 01:02:50.648227 17176 solver.cpp:229] Iteration 37140, loss = 0.112638
I1026 01:02:50.648259 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0334656 (* 1 = 0.0334656 loss)
I1026 01:02:50.648264 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.079172 (* 1 = 0.079172 loss)
I1026 01:02:50.648268 17176 sgd_solver.cpp:106] Iteration 37140, lr = 0.001
I1026 01:02:51.210470 17176 solver.cpp:229] Iteration 37160, loss = 0.0946737
I1026 01:02:51.210502 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0897079 (* 1 = 0.0897079 loss)
I1026 01:02:51.210508 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00496586 (* 1 = 0.00496586 loss)
I1026 01:02:51.210512 17176 sgd_solver.cpp:106] Iteration 37160, lr = 0.001
I1026 01:02:51.758546 17176 solver.cpp:229] Iteration 37180, loss = 0.0123619
I1026 01:02:51.758579 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.010787 (* 1 = 0.010787 loss)
I1026 01:02:51.758584 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00157488 (* 1 = 0.00157488 loss)
I1026 01:02:51.758587 17176 sgd_solver.cpp:106] Iteration 37180, lr = 0.001
I1026 01:02:52.323910 17176 solver.cpp:229] Iteration 37200, loss = 0.081634
I1026 01:02:52.323942 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.020331 (* 1 = 0.020331 loss)
I1026 01:02:52.323947 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.061303 (* 1 = 0.061303 loss)
I1026 01:02:52.323951 17176 sgd_solver.cpp:106] Iteration 37200, lr = 0.001
I1026 01:02:52.893285 17176 solver.cpp:229] Iteration 37220, loss = 0.0138294
I1026 01:02:52.893316 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00557496 (* 1 = 0.00557496 loss)
I1026 01:02:52.893319 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00825439 (* 1 = 0.00825439 loss)
I1026 01:02:52.893323 17176 sgd_solver.cpp:106] Iteration 37220, lr = 0.001
I1026 01:02:53.450013 17176 solver.cpp:229] Iteration 37240, loss = 0.0345781
I1026 01:02:53.450045 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00792888 (* 1 = 0.00792888 loss)
I1026 01:02:53.450049 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0266492 (* 1 = 0.0266492 loss)
I1026 01:02:53.450054 17176 sgd_solver.cpp:106] Iteration 37240, lr = 0.001
I1026 01:02:54.005234 17176 solver.cpp:229] Iteration 37260, loss = 0.0343406
I1026 01:02:54.005266 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.013424 (* 1 = 0.013424 loss)
I1026 01:02:54.005270 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0209166 (* 1 = 0.0209166 loss)
I1026 01:02:54.005275 17176 sgd_solver.cpp:106] Iteration 37260, lr = 0.001
I1026 01:02:54.564839 17176 solver.cpp:229] Iteration 37280, loss = 0.0597202
I1026 01:02:54.564872 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0232291 (* 1 = 0.0232291 loss)
I1026 01:02:54.564877 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.036491 (* 1 = 0.036491 loss)
I1026 01:02:54.564880 17176 sgd_solver.cpp:106] Iteration 37280, lr = 0.001
I1026 01:02:55.131235 17176 solver.cpp:229] Iteration 37300, loss = 0.00957786
I1026 01:02:55.131268 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0026458 (* 1 = 0.0026458 loss)
I1026 01:02:55.131273 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00693206 (* 1 = 0.00693206 loss)
I1026 01:02:55.131275 17176 sgd_solver.cpp:106] Iteration 37300, lr = 0.001
I1026 01:02:55.683318 17176 solver.cpp:229] Iteration 37320, loss = 0.0184405
I1026 01:02:55.683351 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0158945 (* 1 = 0.0158945 loss)
I1026 01:02:55.683356 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00254602 (* 1 = 0.00254602 loss)
I1026 01:02:55.683358 17176 sgd_solver.cpp:106] Iteration 37320, lr = 0.001
I1026 01:02:56.246508 17176 solver.cpp:229] Iteration 37340, loss = 0.0175829
I1026 01:02:56.246541 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.012052 (* 1 = 0.012052 loss)
I1026 01:02:56.246546 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00553087 (* 1 = 0.00553087 loss)
I1026 01:02:56.246548 17176 sgd_solver.cpp:106] Iteration 37340, lr = 0.001
I1026 01:02:56.806807 17176 solver.cpp:229] Iteration 37360, loss = 0.0447775
I1026 01:02:56.806838 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.031535 (* 1 = 0.031535 loss)
I1026 01:02:56.806843 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0132425 (* 1 = 0.0132425 loss)
I1026 01:02:56.806848 17176 sgd_solver.cpp:106] Iteration 37360, lr = 0.001
I1026 01:02:57.374069 17176 solver.cpp:229] Iteration 37380, loss = 0.0565516
I1026 01:02:57.374100 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0354885 (* 1 = 0.0354885 loss)
I1026 01:02:57.374105 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0210631 (* 1 = 0.0210631 loss)
I1026 01:02:57.374110 17176 sgd_solver.cpp:106] Iteration 37380, lr = 0.001
I1026 01:02:57.924044 17176 solver.cpp:229] Iteration 37400, loss = 0.0416984
I1026 01:02:57.924077 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0183647 (* 1 = 0.0183647 loss)
I1026 01:02:57.924082 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0233337 (* 1 = 0.0233337 loss)
I1026 01:02:57.924085 17176 sgd_solver.cpp:106] Iteration 37400, lr = 0.001
I1026 01:02:58.480216 17176 solver.cpp:229] Iteration 37420, loss = 0.0341555
I1026 01:02:58.480249 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0181148 (* 1 = 0.0181148 loss)
I1026 01:02:58.480255 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0160408 (* 1 = 0.0160408 loss)
I1026 01:02:58.480259 17176 sgd_solver.cpp:106] Iteration 37420, lr = 0.001
I1026 01:02:59.047992 17176 solver.cpp:229] Iteration 37440, loss = 0.0478198
I1026 01:02:59.048025 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0350463 (* 1 = 0.0350463 loss)
I1026 01:02:59.048032 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0127734 (* 1 = 0.0127734 loss)
I1026 01:02:59.048035 17176 sgd_solver.cpp:106] Iteration 37440, lr = 0.001
I1026 01:02:59.605933 17176 solver.cpp:229] Iteration 37460, loss = 0.0612813
I1026 01:02:59.605967 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0466126 (* 1 = 0.0466126 loss)
I1026 01:02:59.605973 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0146687 (* 1 = 0.0146687 loss)
I1026 01:02:59.605978 17176 sgd_solver.cpp:106] Iteration 37460, lr = 0.001
I1026 01:03:00.163846 17176 solver.cpp:229] Iteration 37480, loss = 0.0307364
I1026 01:03:00.163878 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0152167 (* 1 = 0.0152167 loss)
I1026 01:03:00.163884 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0155197 (* 1 = 0.0155197 loss)
I1026 01:03:00.163888 17176 sgd_solver.cpp:106] Iteration 37480, lr = 0.001
I1026 01:03:00.723932 17176 solver.cpp:229] Iteration 37500, loss = 0.214122
I1026 01:03:00.723961 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0779384 (* 1 = 0.0779384 loss)
I1026 01:03:00.723966 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.136183 (* 1 = 0.136183 loss)
I1026 01:03:00.723971 17176 sgd_solver.cpp:106] Iteration 37500, lr = 0.001
I1026 01:03:01.294822 17176 solver.cpp:229] Iteration 37520, loss = 0.0168235
I1026 01:03:01.294855 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0151949 (* 1 = 0.0151949 loss)
I1026 01:03:01.294862 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00162868 (* 1 = 0.00162868 loss)
I1026 01:03:01.294867 17176 sgd_solver.cpp:106] Iteration 37520, lr = 0.001
I1026 01:03:01.859897 17176 solver.cpp:229] Iteration 37540, loss = 0.0458225
I1026 01:03:01.859941 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0145833 (* 1 = 0.0145833 loss)
I1026 01:03:01.859946 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0312392 (* 1 = 0.0312392 loss)
I1026 01:03:01.859951 17176 sgd_solver.cpp:106] Iteration 37540, lr = 0.001
I1026 01:03:02.410812 17176 solver.cpp:229] Iteration 37560, loss = 0.0237437
I1026 01:03:02.410846 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0146857 (* 1 = 0.0146857 loss)
I1026 01:03:02.410851 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00905801 (* 1 = 0.00905801 loss)
I1026 01:03:02.410856 17176 sgd_solver.cpp:106] Iteration 37560, lr = 0.001
I1026 01:03:02.976801 17176 solver.cpp:229] Iteration 37580, loss = 0.054327
I1026 01:03:02.976835 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0387219 (* 1 = 0.0387219 loss)
I1026 01:03:02.976840 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0156051 (* 1 = 0.0156051 loss)
I1026 01:03:02.976845 17176 sgd_solver.cpp:106] Iteration 37580, lr = 0.001
I1026 01:03:03.528090 17176 solver.cpp:229] Iteration 37600, loss = 0.059187
I1026 01:03:03.528122 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0336434 (* 1 = 0.0336434 loss)
I1026 01:03:03.528127 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0255436 (* 1 = 0.0255436 loss)
I1026 01:03:03.528132 17176 sgd_solver.cpp:106] Iteration 37600, lr = 0.001
I1026 01:03:04.091790 17176 solver.cpp:229] Iteration 37620, loss = 0.840504
I1026 01:03:04.091825 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.375554 (* 1 = 0.375554 loss)
I1026 01:03:04.091830 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.464951 (* 1 = 0.464951 loss)
I1026 01:03:04.091835 17176 sgd_solver.cpp:106] Iteration 37620, lr = 0.001
I1026 01:03:04.665035 17176 solver.cpp:229] Iteration 37640, loss = 0.0891298
I1026 01:03:04.665069 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0705311 (* 1 = 0.0705311 loss)
I1026 01:03:04.665074 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0185987 (* 1 = 0.0185987 loss)
I1026 01:03:04.665078 17176 sgd_solver.cpp:106] Iteration 37640, lr = 0.001
I1026 01:03:05.219120 17176 solver.cpp:229] Iteration 37660, loss = 0.0630368
I1026 01:03:05.219152 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.048447 (* 1 = 0.048447 loss)
I1026 01:03:05.219158 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0145898 (* 1 = 0.0145898 loss)
I1026 01:03:05.219163 17176 sgd_solver.cpp:106] Iteration 37660, lr = 0.001
I1026 01:03:05.785583 17176 solver.cpp:229] Iteration 37680, loss = 0.0500034
I1026 01:03:05.785617 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0233307 (* 1 = 0.0233307 loss)
I1026 01:03:05.785622 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0266727 (* 1 = 0.0266727 loss)
I1026 01:03:05.785627 17176 sgd_solver.cpp:106] Iteration 37680, lr = 0.001
I1026 01:03:06.350867 17176 solver.cpp:229] Iteration 37700, loss = 0.0273827
I1026 01:03:06.350901 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0259361 (* 1 = 0.0259361 loss)
I1026 01:03:06.350906 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00144669 (* 1 = 0.00144669 loss)
I1026 01:03:06.350910 17176 sgd_solver.cpp:106] Iteration 37700, lr = 0.001
I1026 01:03:06.916983 17176 solver.cpp:229] Iteration 37720, loss = 0.0405092
I1026 01:03:06.917016 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.014393 (* 1 = 0.014393 loss)
I1026 01:03:06.917021 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0261162 (* 1 = 0.0261162 loss)
I1026 01:03:06.917026 17176 sgd_solver.cpp:106] Iteration 37720, lr = 0.001
I1026 01:03:07.480657 17176 solver.cpp:229] Iteration 37740, loss = 0.0845031
I1026 01:03:07.480690 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0645277 (* 1 = 0.0645277 loss)
I1026 01:03:07.480695 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0199754 (* 1 = 0.0199754 loss)
I1026 01:03:07.480700 17176 sgd_solver.cpp:106] Iteration 37740, lr = 0.001
I1026 01:03:08.046380 17176 solver.cpp:229] Iteration 37760, loss = 0.0972235
I1026 01:03:08.046422 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0594216 (* 1 = 0.0594216 loss)
I1026 01:03:08.046427 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0378018 (* 1 = 0.0378018 loss)
I1026 01:03:08.046432 17176 sgd_solver.cpp:106] Iteration 37760, lr = 0.001
I1026 01:03:08.616536 17176 solver.cpp:229] Iteration 37780, loss = 0.138653
I1026 01:03:08.616569 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.122367 (* 1 = 0.122367 loss)
I1026 01:03:08.616574 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0162852 (* 1 = 0.0162852 loss)
I1026 01:03:08.616577 17176 sgd_solver.cpp:106] Iteration 37780, lr = 0.001
I1026 01:03:09.165488 17176 solver.cpp:229] Iteration 37800, loss = 0.0183195
I1026 01:03:09.165520 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0157872 (* 1 = 0.0157872 loss)
I1026 01:03:09.165525 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00253231 (* 1 = 0.00253231 loss)
I1026 01:03:09.165529 17176 sgd_solver.cpp:106] Iteration 37800, lr = 0.001
I1026 01:03:09.715622 17176 solver.cpp:229] Iteration 37820, loss = 0.035553
I1026 01:03:09.715656 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0107826 (* 1 = 0.0107826 loss)
I1026 01:03:09.715661 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0247705 (* 1 = 0.0247705 loss)
I1026 01:03:09.715664 17176 sgd_solver.cpp:106] Iteration 37820, lr = 0.001
I1026 01:03:10.275972 17176 solver.cpp:229] Iteration 37840, loss = 0.0637735
I1026 01:03:10.276005 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0306795 (* 1 = 0.0306795 loss)
I1026 01:03:10.276010 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.033094 (* 1 = 0.033094 loss)
I1026 01:03:10.276013 17176 sgd_solver.cpp:106] Iteration 37840, lr = 0.001
I1026 01:03:10.846813 17176 solver.cpp:229] Iteration 37860, loss = 0.0371107
I1026 01:03:10.846863 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0351962 (* 1 = 0.0351962 loss)
I1026 01:03:10.846868 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00191446 (* 1 = 0.00191446 loss)
I1026 01:03:10.846871 17176 sgd_solver.cpp:106] Iteration 37860, lr = 0.001
I1026 01:03:11.420044 17176 solver.cpp:229] Iteration 37880, loss = 0.0704597
I1026 01:03:11.420086 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0161401 (* 1 = 0.0161401 loss)
I1026 01:03:11.420091 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0543196 (* 1 = 0.0543196 loss)
I1026 01:03:11.420095 17176 sgd_solver.cpp:106] Iteration 37880, lr = 0.001
I1026 01:03:11.986352 17176 solver.cpp:229] Iteration 37900, loss = 0.0431434
I1026 01:03:11.986387 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0323716 (* 1 = 0.0323716 loss)
I1026 01:03:11.986392 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0107718 (* 1 = 0.0107718 loss)
I1026 01:03:11.986397 17176 sgd_solver.cpp:106] Iteration 37900, lr = 0.001
I1026 01:03:12.549583 17176 solver.cpp:229] Iteration 37920, loss = 0.0200733
I1026 01:03:12.549617 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0149615 (* 1 = 0.0149615 loss)
I1026 01:03:12.549621 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00511176 (* 1 = 0.00511176 loss)
I1026 01:03:12.549626 17176 sgd_solver.cpp:106] Iteration 37920, lr = 0.001
I1026 01:03:13.100844 17176 solver.cpp:229] Iteration 37940, loss = 0.0586901
I1026 01:03:13.100878 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0455037 (* 1 = 0.0455037 loss)
I1026 01:03:13.100883 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0131863 (* 1 = 0.0131863 loss)
I1026 01:03:13.100886 17176 sgd_solver.cpp:106] Iteration 37940, lr = 0.001
I1026 01:03:13.662451 17176 solver.cpp:229] Iteration 37960, loss = 0.0511767
I1026 01:03:13.662495 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.023132 (* 1 = 0.023132 loss)
I1026 01:03:13.662502 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0280447 (* 1 = 0.0280447 loss)
I1026 01:03:13.662505 17176 sgd_solver.cpp:106] Iteration 37960, lr = 0.001
I1026 01:03:14.234783 17176 solver.cpp:229] Iteration 37980, loss = 0.112573
I1026 01:03:14.234827 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0483851 (* 1 = 0.0483851 loss)
I1026 01:03:14.234833 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0641879 (* 1 = 0.0641879 loss)
I1026 01:03:14.234838 17176 sgd_solver.cpp:106] Iteration 37980, lr = 0.001
I1026 01:03:14.795748 17176 solver.cpp:229] Iteration 38000, loss = 0.0452971
I1026 01:03:14.795790 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0156714 (* 1 = 0.0156714 loss)
I1026 01:03:14.795795 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0296257 (* 1 = 0.0296257 loss)
I1026 01:03:14.795800 17176 sgd_solver.cpp:106] Iteration 38000, lr = 0.001
I1026 01:03:15.360203 17176 solver.cpp:229] Iteration 38020, loss = 0.196454
I1026 01:03:15.360239 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0698513 (* 1 = 0.0698513 loss)
I1026 01:03:15.360244 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.126602 (* 1 = 0.126602 loss)
I1026 01:03:15.360249 17176 sgd_solver.cpp:106] Iteration 38020, lr = 0.001
I1026 01:03:15.926046 17176 solver.cpp:229] Iteration 38040, loss = 0.028797
I1026 01:03:15.926090 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0236215 (* 1 = 0.0236215 loss)
I1026 01:03:15.926095 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00517553 (* 1 = 0.00517553 loss)
I1026 01:03:15.926100 17176 sgd_solver.cpp:106] Iteration 38040, lr = 0.001
I1026 01:03:16.500569 17176 solver.cpp:229] Iteration 38060, loss = 0.0795499
I1026 01:03:16.500613 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0493031 (* 1 = 0.0493031 loss)
I1026 01:03:16.500618 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0302468 (* 1 = 0.0302468 loss)
I1026 01:03:16.500623 17176 sgd_solver.cpp:106] Iteration 38060, lr = 0.001
I1026 01:03:17.069183 17176 solver.cpp:229] Iteration 38080, loss = 0.0492275
I1026 01:03:17.069216 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0224031 (* 1 = 0.0224031 loss)
I1026 01:03:17.069221 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0268245 (* 1 = 0.0268245 loss)
I1026 01:03:17.069224 17176 sgd_solver.cpp:106] Iteration 38080, lr = 0.001
I1026 01:03:17.630391 17176 solver.cpp:229] Iteration 38100, loss = 0.0156881
I1026 01:03:17.630424 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0138923 (* 1 = 0.0138923 loss)
I1026 01:03:17.630430 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00179585 (* 1 = 0.00179585 loss)
I1026 01:03:17.630434 17176 sgd_solver.cpp:106] Iteration 38100, lr = 0.001
I1026 01:03:18.192383 17176 solver.cpp:229] Iteration 38120, loss = 0.0783338
I1026 01:03:18.192416 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0626448 (* 1 = 0.0626448 loss)
I1026 01:03:18.192421 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.015689 (* 1 = 0.015689 loss)
I1026 01:03:18.192426 17176 sgd_solver.cpp:106] Iteration 38120, lr = 0.001
I1026 01:03:18.745682 17176 solver.cpp:229] Iteration 38140, loss = 0.0271849
I1026 01:03:18.745714 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0138528 (* 1 = 0.0138528 loss)
I1026 01:03:18.745719 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0133321 (* 1 = 0.0133321 loss)
I1026 01:03:18.745724 17176 sgd_solver.cpp:106] Iteration 38140, lr = 0.001
I1026 01:03:19.299770 17176 solver.cpp:229] Iteration 38160, loss = 0.042761
I1026 01:03:19.299805 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0159426 (* 1 = 0.0159426 loss)
I1026 01:03:19.299810 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0268184 (* 1 = 0.0268184 loss)
I1026 01:03:19.299814 17176 sgd_solver.cpp:106] Iteration 38160, lr = 0.001
I1026 01:03:19.872288 17176 solver.cpp:229] Iteration 38180, loss = 0.0252338
I1026 01:03:19.872320 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0122032 (* 1 = 0.0122032 loss)
I1026 01:03:19.872326 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0130306 (* 1 = 0.0130306 loss)
I1026 01:03:19.872331 17176 sgd_solver.cpp:106] Iteration 38180, lr = 0.001
I1026 01:03:20.441604 17176 solver.cpp:229] Iteration 38200, loss = 0.0622965
I1026 01:03:20.441637 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0302152 (* 1 = 0.0302152 loss)
I1026 01:03:20.441642 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0320813 (* 1 = 0.0320813 loss)
I1026 01:03:20.441646 17176 sgd_solver.cpp:106] Iteration 38200, lr = 0.001
I1026 01:03:20.994595 17176 solver.cpp:229] Iteration 38220, loss = 0.0986014
I1026 01:03:20.994627 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0953279 (* 1 = 0.0953279 loss)
I1026 01:03:20.994633 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00327349 (* 1 = 0.00327349 loss)
I1026 01:03:20.994637 17176 sgd_solver.cpp:106] Iteration 38220, lr = 0.001
I1026 01:03:21.564611 17176 solver.cpp:229] Iteration 38240, loss = 0.111169
I1026 01:03:21.564645 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0673583 (* 1 = 0.0673583 loss)
I1026 01:03:21.564651 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0438103 (* 1 = 0.0438103 loss)
I1026 01:03:21.564654 17176 sgd_solver.cpp:106] Iteration 38240, lr = 0.001
I1026 01:03:22.123903 17176 solver.cpp:229] Iteration 38260, loss = 0.0184053
I1026 01:03:22.123945 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0161107 (* 1 = 0.0161107 loss)
I1026 01:03:22.123951 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0022946 (* 1 = 0.0022946 loss)
I1026 01:03:22.123955 17176 sgd_solver.cpp:106] Iteration 38260, lr = 0.001
I1026 01:03:22.678565 17176 solver.cpp:229] Iteration 38280, loss = 0.0988948
I1026 01:03:22.678597 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0919398 (* 1 = 0.0919398 loss)
I1026 01:03:22.678602 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00695496 (* 1 = 0.00695496 loss)
I1026 01:03:22.678607 17176 sgd_solver.cpp:106] Iteration 38280, lr = 0.001
I1026 01:03:23.250778 17176 solver.cpp:229] Iteration 38300, loss = 0.0501321
I1026 01:03:23.250813 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0202558 (* 1 = 0.0202558 loss)
I1026 01:03:23.250818 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0298764 (* 1 = 0.0298764 loss)
I1026 01:03:23.250823 17176 sgd_solver.cpp:106] Iteration 38300, lr = 0.001
I1026 01:03:23.825510 17176 solver.cpp:229] Iteration 38320, loss = 0.0622022
I1026 01:03:23.825542 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0506187 (* 1 = 0.0506187 loss)
I1026 01:03:23.825547 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0115835 (* 1 = 0.0115835 loss)
I1026 01:03:23.825551 17176 sgd_solver.cpp:106] Iteration 38320, lr = 0.001
I1026 01:03:24.389111 17176 solver.cpp:229] Iteration 38340, loss = 0.046642
I1026 01:03:24.389153 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0337332 (* 1 = 0.0337332 loss)
I1026 01:03:24.389158 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0129088 (* 1 = 0.0129088 loss)
I1026 01:03:24.389171 17176 sgd_solver.cpp:106] Iteration 38340, lr = 0.001
I1026 01:03:24.949431 17176 solver.cpp:229] Iteration 38360, loss = 0.0165542
I1026 01:03:24.949465 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0114663 (* 1 = 0.0114663 loss)
I1026 01:03:24.949470 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00508792 (* 1 = 0.00508792 loss)
I1026 01:03:24.949475 17176 sgd_solver.cpp:106] Iteration 38360, lr = 0.001
I1026 01:03:25.513298 17176 solver.cpp:229] Iteration 38380, loss = 0.0585773
I1026 01:03:25.513330 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0341627 (* 1 = 0.0341627 loss)
I1026 01:03:25.513336 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0244146 (* 1 = 0.0244146 loss)
I1026 01:03:25.513341 17176 sgd_solver.cpp:106] Iteration 38380, lr = 0.001
I1026 01:03:26.072157 17176 solver.cpp:229] Iteration 38400, loss = 0.060239
I1026 01:03:26.072201 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0408293 (* 1 = 0.0408293 loss)
I1026 01:03:26.072206 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0194097 (* 1 = 0.0194097 loss)
I1026 01:03:26.072211 17176 sgd_solver.cpp:106] Iteration 38400, lr = 0.001
I1026 01:03:26.642024 17176 solver.cpp:229] Iteration 38420, loss = 0.0766991
I1026 01:03:26.642053 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.03085 (* 1 = 0.03085 loss)
I1026 01:03:26.642060 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0458491 (* 1 = 0.0458491 loss)
I1026 01:03:26.642062 17176 sgd_solver.cpp:106] Iteration 38420, lr = 0.001
I1026 01:03:27.209595 17176 solver.cpp:229] Iteration 38440, loss = 0.247906
I1026 01:03:27.209627 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0466534 (* 1 = 0.0466534 loss)
I1026 01:03:27.209632 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.201253 (* 1 = 0.201253 loss)
I1026 01:03:27.209637 17176 sgd_solver.cpp:106] Iteration 38440, lr = 0.001
I1026 01:03:27.779603 17176 solver.cpp:229] Iteration 38460, loss = 0.0285007
I1026 01:03:27.779635 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.009265 (* 1 = 0.009265 loss)
I1026 01:03:27.779641 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0192357 (* 1 = 0.0192357 loss)
I1026 01:03:27.779655 17176 sgd_solver.cpp:106] Iteration 38460, lr = 0.001
I1026 01:03:28.345684 17176 solver.cpp:229] Iteration 38480, loss = 0.190935
I1026 01:03:28.345716 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0519187 (* 1 = 0.0519187 loss)
I1026 01:03:28.345722 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.139016 (* 1 = 0.139016 loss)
I1026 01:03:28.345737 17176 sgd_solver.cpp:106] Iteration 38480, lr = 0.001
I1026 01:03:28.917481 17176 solver.cpp:229] Iteration 38500, loss = 0.124392
I1026 01:03:28.917515 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0630419 (* 1 = 0.0630419 loss)
I1026 01:03:28.917520 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0613504 (* 1 = 0.0613504 loss)
I1026 01:03:28.917524 17176 sgd_solver.cpp:106] Iteration 38500, lr = 0.001
I1026 01:03:29.493758 17176 solver.cpp:229] Iteration 38520, loss = 0.0774206
I1026 01:03:29.493791 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0355582 (* 1 = 0.0355582 loss)
I1026 01:03:29.493796 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0418625 (* 1 = 0.0418625 loss)
I1026 01:03:29.493801 17176 sgd_solver.cpp:106] Iteration 38520, lr = 0.001
I1026 01:03:30.043401 17176 solver.cpp:229] Iteration 38540, loss = 0.0733146
I1026 01:03:30.043448 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0553952 (* 1 = 0.0553952 loss)
I1026 01:03:30.043457 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0179194 (* 1 = 0.0179194 loss)
I1026 01:03:30.043470 17176 sgd_solver.cpp:106] Iteration 38540, lr = 0.001
I1026 01:03:30.603543 17176 solver.cpp:229] Iteration 38560, loss = 0.031201
I1026 01:03:30.603575 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0226158 (* 1 = 0.0226158 loss)
I1026 01:03:30.603580 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00858518 (* 1 = 0.00858518 loss)
I1026 01:03:30.603585 17176 sgd_solver.cpp:106] Iteration 38560, lr = 0.001
I1026 01:03:31.170395 17176 solver.cpp:229] Iteration 38580, loss = 0.150125
I1026 01:03:31.170426 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0903733 (* 1 = 0.0903733 loss)
I1026 01:03:31.170431 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.059752 (* 1 = 0.059752 loss)
I1026 01:03:31.170435 17176 sgd_solver.cpp:106] Iteration 38580, lr = 0.001
I1026 01:03:31.728363 17176 solver.cpp:229] Iteration 38600, loss = 0.0296579
I1026 01:03:31.728399 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0237763 (* 1 = 0.0237763 loss)
I1026 01:03:31.728404 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00588156 (* 1 = 0.00588156 loss)
I1026 01:03:31.728409 17176 sgd_solver.cpp:106] Iteration 38600, lr = 0.001
I1026 01:03:32.292491 17176 solver.cpp:229] Iteration 38620, loss = 0.0227582
I1026 01:03:32.292526 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0184911 (* 1 = 0.0184911 loss)
I1026 01:03:32.292531 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00426713 (* 1 = 0.00426713 loss)
I1026 01:03:32.292546 17176 sgd_solver.cpp:106] Iteration 38620, lr = 0.001
I1026 01:03:32.864387 17176 solver.cpp:229] Iteration 38640, loss = 0.0585193
I1026 01:03:32.864420 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0360726 (* 1 = 0.0360726 loss)
I1026 01:03:32.864426 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0224466 (* 1 = 0.0224466 loss)
I1026 01:03:32.864430 17176 sgd_solver.cpp:106] Iteration 38640, lr = 0.001
I1026 01:03:33.417444 17176 solver.cpp:229] Iteration 38660, loss = 0.149931
I1026 01:03:33.417477 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0733459 (* 1 = 0.0733459 loss)
I1026 01:03:33.417484 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0765847 (* 1 = 0.0765847 loss)
I1026 01:03:33.417487 17176 sgd_solver.cpp:106] Iteration 38660, lr = 0.001
I1026 01:03:33.974515 17176 solver.cpp:229] Iteration 38680, loss = 0.0914602
I1026 01:03:33.974550 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0370666 (* 1 = 0.0370666 loss)
I1026 01:03:33.974555 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0543935 (* 1 = 0.0543935 loss)
I1026 01:03:33.974570 17176 sgd_solver.cpp:106] Iteration 38680, lr = 0.001
I1026 01:03:34.534893 17176 solver.cpp:229] Iteration 38700, loss = 0.161224
I1026 01:03:34.534926 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.142502 (* 1 = 0.142502 loss)
I1026 01:03:34.534932 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0187212 (* 1 = 0.0187212 loss)
I1026 01:03:34.534947 17176 sgd_solver.cpp:106] Iteration 38700, lr = 0.001
I1026 01:03:35.082901 17176 solver.cpp:229] Iteration 38720, loss = 0.0765917
I1026 01:03:35.082932 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0708597 (* 1 = 0.0708597 loss)
I1026 01:03:35.082938 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.005732 (* 1 = 0.005732 loss)
I1026 01:03:35.082942 17176 sgd_solver.cpp:106] Iteration 38720, lr = 0.001
I1026 01:03:35.649050 17176 solver.cpp:229] Iteration 38740, loss = 0.064833
I1026 01:03:35.649083 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0405052 (* 1 = 0.0405052 loss)
I1026 01:03:35.649088 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0243278 (* 1 = 0.0243278 loss)
I1026 01:03:35.649093 17176 sgd_solver.cpp:106] Iteration 38740, lr = 0.001
I1026 01:03:36.220806 17176 solver.cpp:229] Iteration 38760, loss = 0.0620124
I1026 01:03:36.220839 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0249159 (* 1 = 0.0249159 loss)
I1026 01:03:36.220844 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0370964 (* 1 = 0.0370964 loss)
I1026 01:03:36.220849 17176 sgd_solver.cpp:106] Iteration 38760, lr = 0.001
I1026 01:03:36.792481 17176 solver.cpp:229] Iteration 38780, loss = 0.0634534
I1026 01:03:36.792524 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0217942 (* 1 = 0.0217942 loss)
I1026 01:03:36.792529 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0416591 (* 1 = 0.0416591 loss)
I1026 01:03:36.792533 17176 sgd_solver.cpp:106] Iteration 38780, lr = 0.001
I1026 01:03:37.360215 17176 solver.cpp:229] Iteration 38800, loss = 0.0536056
I1026 01:03:37.360247 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0124916 (* 1 = 0.0124916 loss)
I1026 01:03:37.360254 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.041114 (* 1 = 0.041114 loss)
I1026 01:03:37.360257 17176 sgd_solver.cpp:106] Iteration 38800, lr = 0.001
I1026 01:03:37.911419 17176 solver.cpp:229] Iteration 38820, loss = 0.0362597
I1026 01:03:37.911456 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0142561 (* 1 = 0.0142561 loss)
I1026 01:03:37.911461 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0220036 (* 1 = 0.0220036 loss)
I1026 01:03:37.911465 17176 sgd_solver.cpp:106] Iteration 38820, lr = 0.001
I1026 01:03:38.483232 17176 solver.cpp:229] Iteration 38840, loss = 0.204199
I1026 01:03:38.483266 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0485399 (* 1 = 0.0485399 loss)
I1026 01:03:38.483271 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.155659 (* 1 = 0.155659 loss)
I1026 01:03:38.483275 17176 sgd_solver.cpp:106] Iteration 38840, lr = 0.001
I1026 01:03:39.037995 17176 solver.cpp:229] Iteration 38860, loss = 0.0513645
I1026 01:03:39.038031 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.020373 (* 1 = 0.020373 loss)
I1026 01:03:39.038038 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0309916 (* 1 = 0.0309916 loss)
I1026 01:03:39.038041 17176 sgd_solver.cpp:106] Iteration 38860, lr = 0.001
I1026 01:03:39.599675 17176 solver.cpp:229] Iteration 38880, loss = 0.0285494
I1026 01:03:39.599709 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0269757 (* 1 = 0.0269757 loss)
I1026 01:03:39.599714 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0015737 (* 1 = 0.0015737 loss)
I1026 01:03:39.599719 17176 sgd_solver.cpp:106] Iteration 38880, lr = 0.001
I1026 01:03:40.171667 17176 solver.cpp:229] Iteration 38900, loss = 0.117606
I1026 01:03:40.171700 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0516822 (* 1 = 0.0516822 loss)
I1026 01:03:40.171706 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0659233 (* 1 = 0.0659233 loss)
I1026 01:03:40.171710 17176 sgd_solver.cpp:106] Iteration 38900, lr = 0.001
I1026 01:03:40.728410 17176 solver.cpp:229] Iteration 38920, loss = 0.0443536
I1026 01:03:40.728442 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0265572 (* 1 = 0.0265572 loss)
I1026 01:03:40.728446 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0177965 (* 1 = 0.0177965 loss)
I1026 01:03:40.728451 17176 sgd_solver.cpp:106] Iteration 38920, lr = 0.001
I1026 01:03:41.290169 17176 solver.cpp:229] Iteration 38940, loss = 0.0416112
I1026 01:03:41.290202 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0351137 (* 1 = 0.0351137 loss)
I1026 01:03:41.290207 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00649753 (* 1 = 0.00649753 loss)
I1026 01:03:41.290211 17176 sgd_solver.cpp:106] Iteration 38940, lr = 0.001
I1026 01:03:41.845299 17176 solver.cpp:229] Iteration 38960, loss = 0.0356311
I1026 01:03:41.845330 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0275631 (* 1 = 0.0275631 loss)
I1026 01:03:41.845335 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00806801 (* 1 = 0.00806801 loss)
I1026 01:03:41.845340 17176 sgd_solver.cpp:106] Iteration 38960, lr = 0.001
I1026 01:03:42.398903 17176 solver.cpp:229] Iteration 38980, loss = 0.0199145
I1026 01:03:42.398947 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0157498 (* 1 = 0.0157498 loss)
I1026 01:03:42.398952 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00416474 (* 1 = 0.00416474 loss)
I1026 01:03:42.398955 17176 sgd_solver.cpp:106] Iteration 38980, lr = 0.001
I1026 01:03:42.961294 17176 solver.cpp:229] Iteration 39000, loss = 0.134213
I1026 01:03:42.961328 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0731519 (* 1 = 0.0731519 loss)
I1026 01:03:42.961331 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0610615 (* 1 = 0.0610615 loss)
I1026 01:03:42.961336 17176 sgd_solver.cpp:106] Iteration 39000, lr = 0.001
I1026 01:03:43.515271 17176 solver.cpp:229] Iteration 39020, loss = 0.0250234
I1026 01:03:43.515305 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0167468 (* 1 = 0.0167468 loss)
I1026 01:03:43.515310 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00827664 (* 1 = 0.00827664 loss)
I1026 01:03:43.515313 17176 sgd_solver.cpp:106] Iteration 39020, lr = 0.001
I1026 01:03:44.067939 17176 solver.cpp:229] Iteration 39040, loss = 0.115563
I1026 01:03:44.067986 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0534604 (* 1 = 0.0534604 loss)
I1026 01:03:44.067991 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0621025 (* 1 = 0.0621025 loss)
I1026 01:03:44.067996 17176 sgd_solver.cpp:106] Iteration 39040, lr = 0.001
I1026 01:03:44.631413 17176 solver.cpp:229] Iteration 39060, loss = 0.0476588
I1026 01:03:44.631465 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0213002 (* 1 = 0.0213002 loss)
I1026 01:03:44.631470 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0263585 (* 1 = 0.0263585 loss)
I1026 01:03:44.631474 17176 sgd_solver.cpp:106] Iteration 39060, lr = 0.001
I1026 01:03:45.187250 17176 solver.cpp:229] Iteration 39080, loss = 0.0604493
I1026 01:03:45.187283 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0390097 (* 1 = 0.0390097 loss)
I1026 01:03:45.187288 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0214396 (* 1 = 0.0214396 loss)
I1026 01:03:45.187291 17176 sgd_solver.cpp:106] Iteration 39080, lr = 0.001
I1026 01:03:45.751893 17176 solver.cpp:229] Iteration 39100, loss = 0.0412949
I1026 01:03:45.751936 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0143691 (* 1 = 0.0143691 loss)
I1026 01:03:45.751941 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0269258 (* 1 = 0.0269258 loss)
I1026 01:03:45.751945 17176 sgd_solver.cpp:106] Iteration 39100, lr = 0.001
I1026 01:03:46.311236 17176 solver.cpp:229] Iteration 39120, loss = 0.0476229
I1026 01:03:46.311269 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0170798 (* 1 = 0.0170798 loss)
I1026 01:03:46.311275 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0305431 (* 1 = 0.0305431 loss)
I1026 01:03:46.311280 17176 sgd_solver.cpp:106] Iteration 39120, lr = 0.001
I1026 01:03:46.873903 17176 solver.cpp:229] Iteration 39140, loss = 0.0588208
I1026 01:03:46.873936 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0258631 (* 1 = 0.0258631 loss)
I1026 01:03:46.873940 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0329577 (* 1 = 0.0329577 loss)
I1026 01:03:46.873944 17176 sgd_solver.cpp:106] Iteration 39140, lr = 0.001
I1026 01:03:47.431103 17176 solver.cpp:229] Iteration 39160, loss = 0.0586902
I1026 01:03:47.431136 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0115111 (* 1 = 0.0115111 loss)
I1026 01:03:47.431140 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0471791 (* 1 = 0.0471791 loss)
I1026 01:03:47.431145 17176 sgd_solver.cpp:106] Iteration 39160, lr = 0.001
I1026 01:03:47.972034 17176 solver.cpp:229] Iteration 39180, loss = 0.0520375
I1026 01:03:47.972069 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.00769362 (* 1 = 0.00769362 loss)
I1026 01:03:47.972074 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0443439 (* 1 = 0.0443439 loss)
I1026 01:03:47.972079 17176 sgd_solver.cpp:106] Iteration 39180, lr = 0.001
I1026 01:03:48.539825 17176 solver.cpp:229] Iteration 39200, loss = 0.0227467
I1026 01:03:48.539855 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.020476 (* 1 = 0.020476 loss)
I1026 01:03:48.539877 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0022707 (* 1 = 0.0022707 loss)
I1026 01:03:48.539881 17176 sgd_solver.cpp:106] Iteration 39200, lr = 0.001
I1026 01:03:49.114423 17176 solver.cpp:229] Iteration 39220, loss = 0.0201583
I1026 01:03:49.114456 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0129192 (* 1 = 0.0129192 loss)
I1026 01:03:49.114462 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00723917 (* 1 = 0.00723917 loss)
I1026 01:03:49.114466 17176 sgd_solver.cpp:106] Iteration 39220, lr = 0.001
I1026 01:03:49.679975 17176 solver.cpp:229] Iteration 39240, loss = 0.0646156
I1026 01:03:49.680017 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0338405 (* 1 = 0.0338405 loss)
I1026 01:03:49.680023 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0307751 (* 1 = 0.0307751 loss)
I1026 01:03:49.680027 17176 sgd_solver.cpp:106] Iteration 39240, lr = 0.001
I1026 01:03:50.251453 17176 solver.cpp:229] Iteration 39260, loss = 0.0601537
I1026 01:03:50.251485 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0429488 (* 1 = 0.0429488 loss)
I1026 01:03:50.251490 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0172049 (* 1 = 0.0172049 loss)
I1026 01:03:50.251495 17176 sgd_solver.cpp:106] Iteration 39260, lr = 0.001
I1026 01:03:50.821472 17176 solver.cpp:229] Iteration 39280, loss = 0.310307
I1026 01:03:50.821508 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0710932 (* 1 = 0.0710932 loss)
I1026 01:03:50.821514 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.239214 (* 1 = 0.239214 loss)
I1026 01:03:50.821518 17176 sgd_solver.cpp:106] Iteration 39280, lr = 0.001
I1026 01:03:51.383500 17176 solver.cpp:229] Iteration 39300, loss = 0.195547
I1026 01:03:51.383534 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.127083 (* 1 = 0.127083 loss)
I1026 01:03:51.383540 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0684635 (* 1 = 0.0684635 loss)
I1026 01:03:51.383544 17176 sgd_solver.cpp:106] Iteration 39300, lr = 0.001
I1026 01:03:51.954838 17176 solver.cpp:229] Iteration 39320, loss = 0.0716224
I1026 01:03:51.954871 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0548271 (* 1 = 0.0548271 loss)
I1026 01:03:51.954877 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0167953 (* 1 = 0.0167953 loss)
I1026 01:03:51.954881 17176 sgd_solver.cpp:106] Iteration 39320, lr = 0.001
I1026 01:03:52.507164 17176 solver.cpp:229] Iteration 39340, loss = 0.110643
I1026 01:03:52.507196 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0444014 (* 1 = 0.0444014 loss)
I1026 01:03:52.507203 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0662411 (* 1 = 0.0662411 loss)
I1026 01:03:52.507208 17176 sgd_solver.cpp:106] Iteration 39340, lr = 0.001
I1026 01:03:53.070838 17176 solver.cpp:229] Iteration 39360, loss = 0.0506871
I1026 01:03:53.070888 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0248632 (* 1 = 0.0248632 loss)
I1026 01:03:53.070894 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0258239 (* 1 = 0.0258239 loss)
I1026 01:03:53.070899 17176 sgd_solver.cpp:106] Iteration 39360, lr = 0.001
I1026 01:03:53.626157 17176 solver.cpp:229] Iteration 39380, loss = 0.0693243
I1026 01:03:53.626200 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0289041 (* 1 = 0.0289041 loss)
I1026 01:03:53.626205 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0404202 (* 1 = 0.0404202 loss)
I1026 01:03:53.626210 17176 sgd_solver.cpp:106] Iteration 39380, lr = 0.001
I1026 01:03:54.187062 17176 solver.cpp:229] Iteration 39400, loss = 0.0216692
I1026 01:03:54.187094 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0168259 (* 1 = 0.0168259 loss)
I1026 01:03:54.187099 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00484329 (* 1 = 0.00484329 loss)
I1026 01:03:54.187103 17176 sgd_solver.cpp:106] Iteration 39400, lr = 0.001
I1026 01:03:54.754055 17176 solver.cpp:229] Iteration 39420, loss = 0.19737
I1026 01:03:54.754087 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0461204 (* 1 = 0.0461204 loss)
I1026 01:03:54.754093 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.15125 (* 1 = 0.15125 loss)
I1026 01:03:54.754098 17176 sgd_solver.cpp:106] Iteration 39420, lr = 0.001
I1026 01:03:55.326706 17176 solver.cpp:229] Iteration 39440, loss = 0.014525
I1026 01:03:55.326740 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0130904 (* 1 = 0.0130904 loss)
I1026 01:03:55.326745 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0014346 (* 1 = 0.0014346 loss)
I1026 01:03:55.326751 17176 sgd_solver.cpp:106] Iteration 39440, lr = 0.001
I1026 01:03:55.892508 17176 solver.cpp:229] Iteration 39460, loss = 0.050141
I1026 01:03:55.892541 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0163728 (* 1 = 0.0163728 loss)
I1026 01:03:55.892546 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0337682 (* 1 = 0.0337682 loss)
I1026 01:03:55.892551 17176 sgd_solver.cpp:106] Iteration 39460, lr = 0.001
I1026 01:03:56.464648 17176 solver.cpp:229] Iteration 39480, loss = 0.0617783
I1026 01:03:56.464680 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.044002 (* 1 = 0.044002 loss)
I1026 01:03:56.464686 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0177763 (* 1 = 0.0177763 loss)
I1026 01:03:56.464690 17176 sgd_solver.cpp:106] Iteration 39480, lr = 0.001
I1026 01:03:57.028877 17176 solver.cpp:229] Iteration 39500, loss = 0.0615633
I1026 01:03:57.028908 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0100289 (* 1 = 0.0100289 loss)
I1026 01:03:57.028913 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0515344 (* 1 = 0.0515344 loss)
I1026 01:03:57.028918 17176 sgd_solver.cpp:106] Iteration 39500, lr = 0.001
I1026 01:03:57.605386 17176 solver.cpp:229] Iteration 39520, loss = 0.237831
I1026 01:03:57.605419 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.190773 (* 1 = 0.190773 loss)
I1026 01:03:57.605427 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0470582 (* 1 = 0.0470582 loss)
I1026 01:03:57.605432 17176 sgd_solver.cpp:106] Iteration 39520, lr = 0.001
I1026 01:03:58.172125 17176 solver.cpp:229] Iteration 39540, loss = 0.0819155
I1026 01:03:58.172158 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0404314 (* 1 = 0.0404314 loss)
I1026 01:03:58.172164 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0414842 (* 1 = 0.0414842 loss)
I1026 01:03:58.172168 17176 sgd_solver.cpp:106] Iteration 39540, lr = 0.001
I1026 01:03:58.731603 17176 solver.cpp:229] Iteration 39560, loss = 0.0422556
I1026 01:03:58.731637 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0344284 (* 1 = 0.0344284 loss)
I1026 01:03:58.731642 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00782724 (* 1 = 0.00782724 loss)
I1026 01:03:58.731647 17176 sgd_solver.cpp:106] Iteration 39560, lr = 0.001
I1026 01:03:59.297039 17176 solver.cpp:229] Iteration 39580, loss = 0.0492947
I1026 01:03:59.297071 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0329278 (* 1 = 0.0329278 loss)
I1026 01:03:59.297076 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0163669 (* 1 = 0.0163669 loss)
I1026 01:03:59.297080 17176 sgd_solver.cpp:106] Iteration 39580, lr = 0.001
I1026 01:03:59.862157 17176 solver.cpp:229] Iteration 39600, loss = 0.0293265
I1026 01:03:59.862190 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0254962 (* 1 = 0.0254962 loss)
I1026 01:03:59.862195 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.00383032 (* 1 = 0.00383032 loss)
I1026 01:03:59.862200 17176 sgd_solver.cpp:106] Iteration 39600, lr = 0.001
I1026 01:04:00.444221 17176 solver.cpp:229] Iteration 39620, loss = 0.06299
I1026 01:04:00.444254 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0181124 (* 1 = 0.0181124 loss)
I1026 01:04:00.444260 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0448777 (* 1 = 0.0448777 loss)
I1026 01:04:00.444265 17176 sgd_solver.cpp:106] Iteration 39620, lr = 0.001
I1026 01:04:01.016302 17176 solver.cpp:229] Iteration 39640, loss = 0.0457082
I1026 01:04:01.016335 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0313212 (* 1 = 0.0313212 loss)
I1026 01:04:01.016341 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0143869 (* 1 = 0.0143869 loss)
I1026 01:04:01.016356 17176 sgd_solver.cpp:106] Iteration 39640, lr = 0.001
I1026 01:04:01.581249 17176 solver.cpp:229] Iteration 39660, loss = 0.11582
I1026 01:04:01.581284 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0592765 (* 1 = 0.0592765 loss)
I1026 01:04:01.581288 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0565431 (* 1 = 0.0565431 loss)
I1026 01:04:01.581303 17176 sgd_solver.cpp:106] Iteration 39660, lr = 0.001
I1026 01:04:02.136065 17176 solver.cpp:229] Iteration 39680, loss = 0.0631332
I1026 01:04:02.136098 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0464562 (* 1 = 0.0464562 loss)
I1026 01:04:02.136103 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.016677 (* 1 = 0.016677 loss)
I1026 01:04:02.136108 17176 sgd_solver.cpp:106] Iteration 39680, lr = 0.001
I1026 01:04:02.698189 17176 solver.cpp:229] Iteration 39700, loss = 0.0716312
I1026 01:04:02.698231 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0370354 (* 1 = 0.0370354 loss)
I1026 01:04:02.698237 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0345959 (* 1 = 0.0345959 loss)
I1026 01:04:02.698241 17176 sgd_solver.cpp:106] Iteration 39700, lr = 0.001
I1026 01:04:03.264026 17176 solver.cpp:229] Iteration 39720, loss = 0.0436684
I1026 01:04:03.264058 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0219608 (* 1 = 0.0219608 loss)
I1026 01:04:03.264065 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0217076 (* 1 = 0.0217076 loss)
I1026 01:04:03.264068 17176 sgd_solver.cpp:106] Iteration 39720, lr = 0.001
I1026 01:04:03.830875 17176 solver.cpp:229] Iteration 39740, loss = 0.0854757
I1026 01:04:03.830909 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0103472 (* 1 = 0.0103472 loss)
I1026 01:04:03.830914 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0751285 (* 1 = 0.0751285 loss)
I1026 01:04:03.830919 17176 sgd_solver.cpp:106] Iteration 39740, lr = 0.001
I1026 01:04:04.399531 17176 solver.cpp:229] Iteration 39760, loss = 0.0595302
I1026 01:04:04.399564 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0480341 (* 1 = 0.0480341 loss)
I1026 01:04:04.399569 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0114961 (* 1 = 0.0114961 loss)
I1026 01:04:04.399574 17176 sgd_solver.cpp:106] Iteration 39760, lr = 0.001
I1026 01:04:04.957222 17176 solver.cpp:229] Iteration 39780, loss = 0.0683183
I1026 01:04:04.957255 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0390523 (* 1 = 0.0390523 loss)
I1026 01:04:04.957260 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.029266 (* 1 = 0.029266 loss)
I1026 01:04:04.957265 17176 sgd_solver.cpp:106] Iteration 39780, lr = 0.001
I1026 01:04:05.514452 17176 solver.cpp:229] Iteration 39800, loss = 0.0431552
I1026 01:04:05.514487 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0124733 (* 1 = 0.0124733 loss)
I1026 01:04:05.514492 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0306818 (* 1 = 0.0306818 loss)
I1026 01:04:05.514497 17176 sgd_solver.cpp:106] Iteration 39800, lr = 0.001
I1026 01:04:06.075145 17176 solver.cpp:229] Iteration 39820, loss = 0.04539
I1026 01:04:06.075178 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0159876 (* 1 = 0.0159876 loss)
I1026 01:04:06.075184 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0294025 (* 1 = 0.0294025 loss)
I1026 01:04:06.075188 17176 sgd_solver.cpp:106] Iteration 39820, lr = 0.001
I1026 01:04:06.634855 17176 solver.cpp:229] Iteration 39840, loss = 0.0488757
I1026 01:04:06.634886 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0260974 (* 1 = 0.0260974 loss)
I1026 01:04:06.634891 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0227783 (* 1 = 0.0227783 loss)
I1026 01:04:06.634894 17176 sgd_solver.cpp:106] Iteration 39840, lr = 0.001
I1026 01:04:07.211411 17176 solver.cpp:229] Iteration 39860, loss = 0.0900607
I1026 01:04:07.211468 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0517686 (* 1 = 0.0517686 loss)
I1026 01:04:07.211474 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0382921 (* 1 = 0.0382921 loss)
I1026 01:04:07.211490 17176 sgd_solver.cpp:106] Iteration 39860, lr = 0.001
I1026 01:04:07.775993 17176 solver.cpp:229] Iteration 39880, loss = 0.106607
I1026 01:04:07.776036 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0469967 (* 1 = 0.0469967 loss)
I1026 01:04:07.776041 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0596102 (* 1 = 0.0596102 loss)
I1026 01:04:07.776046 17176 sgd_solver.cpp:106] Iteration 39880, lr = 0.001
I1026 01:04:08.342492 17176 solver.cpp:229] Iteration 39900, loss = 0.0335862
I1026 01:04:08.342526 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0207718 (* 1 = 0.0207718 loss)
I1026 01:04:08.342532 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0128144 (* 1 = 0.0128144 loss)
I1026 01:04:08.342536 17176 sgd_solver.cpp:106] Iteration 39900, lr = 0.001
I1026 01:04:08.912420 17176 solver.cpp:229] Iteration 39920, loss = 0.0891613
I1026 01:04:08.912451 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.020221 (* 1 = 0.020221 loss)
I1026 01:04:08.912456 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0689403 (* 1 = 0.0689403 loss)
I1026 01:04:08.912461 17176 sgd_solver.cpp:106] Iteration 39920, lr = 0.001
I1026 01:04:09.477361 17176 solver.cpp:229] Iteration 39940, loss = 0.0494107
I1026 01:04:09.477396 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0202496 (* 1 = 0.0202496 loss)
I1026 01:04:09.477401 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0291611 (* 1 = 0.0291611 loss)
I1026 01:04:09.477406 17176 sgd_solver.cpp:106] Iteration 39940, lr = 0.001
I1026 01:04:10.054070 17176 solver.cpp:229] Iteration 39960, loss = 0.0369757
I1026 01:04:10.054103 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0268614 (* 1 = 0.0268614 loss)
I1026 01:04:10.054108 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.0101143 (* 1 = 0.0101143 loss)
I1026 01:04:10.054112 17176 sgd_solver.cpp:106] Iteration 39960, lr = 0.001
I1026 01:04:10.603029 17176 solver.cpp:229] Iteration 39980, loss = 0.0504481
I1026 01:04:10.603062 17176 solver.cpp:245]     Train net output #0: rpn_cls_loss = 0.0243801 (* 1 = 0.0243801 loss)
I1026 01:04:10.603070 17176 solver.cpp:245]     Train net output #1: rpn_loss_bbox = 0.026068 (* 1 = 0.026068 loss)
I1026 01:04:10.603073 17176 sgd_solver.cpp:106] Iteration 39980, lr = 0.001
er
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
Wrote snapshot to: /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_rpn_stage2_iter_20000.caffemodel
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
Wrote snapshot to: /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_rpn_stage2_iter_30000.caffemodel
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
speed: 0.028s / iter
Wrote snapshot to: /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_rpn_stage2_iter_40000.caffemodel
done solving
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 2 RPN, generate proposals
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1026 01:04:11.836563 17240 net.cpp:49] Initializing net from parameters: 
name: "ZF"
input: "data"
input: "im_info"
state {
  phase: TEST
}
input_shape {
  dim: 1
  dim: 3
  dim: 224
  dim: 224
}
input_shape {
  dim: 1
  dim: 3
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 96
    pad: 3
    kernel_size: 7
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
    engine: CAFFE
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    stride: 2
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
    engine: CAFFE
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "rpn_conv1"
  type: "Convolution"
  bottom: "conv5"
  top: "rpn_conv1"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "rpn_relu1"
  type: "ReLU"
  bottom: "rpn_conv1"
  top: "rpn_conv1"
}
layer {
  name: "rpn_cls_score"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_cls_score"
  convolution_param {
    num_output: 18
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "rpn_bbox_pred"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_bbox_pred"
  convolution_param {
    num_output: 36
    pad: 0
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "rpn_cls_score_reshape"
  type: "Reshape"
  bottom: "rpn_cls_score"
  top: "rpn_cls_score_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 2
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "rpn_cls_prob"
  type: "Softmax"
  bottom: "rpn_cls_score_reshape"
  top: "rpn_cls_prob"
}
layer {
  name: "rpn_cls_prob_reshape"
  type: "Reshape"
  bottom: "rpn_cls_prob"
  top: "rpn_cls_prob_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 18
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "proposal"
  type: "Python"
  bottom: "rpn_cls_prob_reshape"
  bottom: "rpn_bbox_pred"
  bottom: "im_info"
  top: "rois"
  top: "scores"
  python_param {
    module: "rpn.proposal_layer"
    layer: "ProposalLayer"
    param_str: "\'feat_stride\': 16"
  }
}
I1026 01:04:11.836659 17240 net.cpp:413] Input 0 -> data
I1026 01:04:11.842630 17240 net.cpp:413] Input 1 -> im_info
I1026 01:04:11.842669 17240 layer_factory.hpp:77] Creating layer conv1
I1026 01:04:11.842696 17240 net.cpp:106] Creating Layer conv1
I1026 01:04:11.842700 17240 net.cpp:454] conv1 <- data
I1026 01:04:11.842705 17240 net.cpp:411] conv1 -> conv1
I1026 01:04:11.945828 17240 net.cpp:150] Setting up conv1
I1026 01:04:11.945854 17240 net.cpp:157] Top shape: 1 96 112 112 (1204224)
I1026 01:04:11.945857 17240 net.cpp:165] Memory required for data: 4816896
I1026 01:04:11.945868 17240 layer_factory.hpp:77] Creating layer relu1
I1026 01:04:11.945888 17240 net.cpp:106] Creating Layer relu1
I1026 01:04:11.945891 17240 net.cpp:454] relu1 <- conv1
I1026 01:04:11.945906 17240 net.cpp:397] relu1 -> conv1 (in-place)
I1026 01:04:11.946113 17240 net.cpp:150] Setting up relu1
I1026 01:04:11.946120 17240 net.cpp:157] Top shape: 1 96 112 112 (1204224)
I1026 01:04:11.946133 17240 net.cpp:165] Memory required for data: 9633792
I1026 01:04:11.946135 17240 layer_factory.hpp:77] Creating layer norm1
I1026 01:04:11.946142 17240 net.cpp:106] Creating Layer norm1
I1026 01:04:11.946143 17240 net.cpp:454] norm1 <- conv1
I1026 01:04:11.946157 17240 net.cpp:411] norm1 -> norm1
I1026 01:04:11.946257 17240 net.cpp:150] Setting up norm1
I1026 01:04:11.946262 17240 net.cpp:157] Top shape: 1 96 112 112 (1204224)
I1026 01:04:11.946264 17240 net.cpp:165] Memory required for data: 14450688
I1026 01:04:11.946265 17240 layer_factory.hpp:77] Creating layer pool1
I1026 01:04:11.946269 17240 net.cpp:106] Creating Layer pool1
I1026 01:04:11.946270 17240 net.cpp:454] pool1 <- norm1
I1026 01:04:11.946274 17240 net.cpp:411] pool1 -> pool1
I1026 01:04:11.946313 17240 net.cpp:150] Setting up pool1
I1026 01:04:11.946317 17240 net.cpp:157] Top shape: 1 96 57 57 (311904)
I1026 01:04:11.946318 17240 net.cpp:165] Memory required for data: 15698304
I1026 01:04:11.946329 17240 layer_factory.hpp:77] Creating layer conv2
I1026 01:04:11.946336 17240 net.cpp:106] Creating Layer conv2
I1026 01:04:11.946337 17240 net.cpp:454] conv2 <- pool1
I1026 01:04:11.946351 17240 net.cpp:411] conv2 -> conv2
I1026 01:04:11.948004 17240 net.cpp:150] Setting up conv2
I1026 01:04:11.948014 17240 net.cpp:157] Top shape: 1 256 29 29 (215296)
I1026 01:04:11.948015 17240 net.cpp:165] Memory required for data: 16559488
I1026 01:04:11.948020 17240 layer_factory.hpp:77] Creating layer relu2
I1026 01:04:11.948024 17240 net.cpp:106] Creating Layer relu2
I1026 01:04:11.948026 17240 net.cpp:454] relu2 <- conv2
I1026 01:04:11.948029 17240 net.cpp:397] relu2 -> conv2 (in-place)
I1026 01:04:11.948289 17240 net.cpp:150] Setting up relu2
I1026 01:04:11.948307 17240 net.cpp:157] Top shape: 1 256 29 29 (215296)
I1026 01:04:11.948307 17240 net.cpp:165] Memory required for data: 17420672
I1026 01:04:11.948309 17240 layer_factory.hpp:77] Creating layer norm2
I1026 01:04:11.948314 17240 net.cpp:106] Creating Layer norm2
I1026 01:04:11.948326 17240 net.cpp:454] norm2 <- conv2
I1026 01:04:11.948330 17240 net.cpp:411] norm2 -> norm2
I1026 01:04:11.948422 17240 net.cpp:150] Setting up norm2
I1026 01:04:11.948427 17240 net.cpp:157] Top shape: 1 256 29 29 (215296)
I1026 01:04:11.948428 17240 net.cpp:165] Memory required for data: 18281856
I1026 01:04:11.948441 17240 layer_factory.hpp:77] Creating layer pool2
I1026 01:04:11.948444 17240 net.cpp:106] Creating Layer pool2
I1026 01:04:11.948446 17240 net.cpp:454] pool2 <- norm2
I1026 01:04:11.948449 17240 net.cpp:411] pool2 -> pool2
I1026 01:04:11.948489 17240 net.cpp:150] Setting up pool2
I1026 01:04:11.948493 17240 net.cpp:157] Top shape: 1 256 15 15 (57600)
I1026 01:04:11.948494 17240 net.cpp:165] Memory required for data: 18512256
I1026 01:04:11.948505 17240 layer_factory.hpp:77] Creating layer conv3
I1026 01:04:11.948511 17240 net.cpp:106] Creating Layer conv3
I1026 01:04:11.948513 17240 net.cpp:454] conv3 <- pool2
I1026 01:04:11.948528 17240 net.cpp:411] conv3 -> conv3
I1026 01:04:11.950263 17240 net.cpp:150] Setting up conv3
I1026 01:04:11.950274 17240 net.cpp:157] Top shape: 1 384 15 15 (86400)
I1026 01:04:11.950276 17240 net.cpp:165] Memory required for data: 18857856
I1026 01:04:11.950283 17240 layer_factory.hpp:77] Creating layer relu3
I1026 01:04:11.950289 17240 net.cpp:106] Creating Layer relu3
I1026 01:04:11.950290 17240 net.cpp:454] relu3 <- conv3
I1026 01:04:11.950304 17240 net.cpp:397] relu3 -> conv3 (in-place)
I1026 01:04:11.950440 17240 net.cpp:150] Setting up relu3
I1026 01:04:11.950446 17240 net.cpp:157] Top shape: 1 384 15 15 (86400)
I1026 01:04:11.950448 17240 net.cpp:165] Memory required for data: 19203456
I1026 01:04:11.950460 17240 layer_factory.hpp:77] Creating layer conv4
I1026 01:04:11.950465 17240 net.cpp:106] Creating Layer conv4
I1026 01:04:11.950467 17240 net.cpp:454] conv4 <- conv3
I1026 01:04:11.950481 17240 net.cpp:411] conv4 -> conv4
I1026 01:04:11.952824 17240 net.cpp:150] Setting up conv4
I1026 01:04:11.952844 17240 net.cpp:157] Top shape: 1 384 15 15 (86400)
I1026 01:04:11.952847 17240 net.cpp:165] Memory required for data: 19549056
I1026 01:04:11.952852 17240 layer_factory.hpp:77] Creating layer relu4
I1026 01:04:11.952858 17240 net.cpp:106] Creating Layer relu4
I1026 01:04:11.952860 17240 net.cpp:454] relu4 <- conv4
I1026 01:04:11.952875 17240 net.cpp:397] relu4 -> conv4 (in-place)
I1026 01:04:11.953101 17240 net.cpp:150] Setting up relu4
I1026 01:04:11.953109 17240 net.cpp:157] Top shape: 1 384 15 15 (86400)
I1026 01:04:11.953110 17240 net.cpp:165] Memory required for data: 19894656
I1026 01:04:11.953122 17240 layer_factory.hpp:77] Creating layer conv5
I1026 01:04:11.953141 17240 net.cpp:106] Creating Layer conv5
I1026 01:04:11.953143 17240 net.cpp:454] conv5 <- conv4
I1026 01:04:11.953147 17240 net.cpp:411] conv5 -> conv5
I1026 01:04:11.954840 17240 net.cpp:150] Setting up conv5
I1026 01:04:11.954851 17240 net.cpp:157] Top shape: 1 256 15 15 (57600)
I1026 01:04:11.954864 17240 net.cpp:165] Memory required for data: 20125056
I1026 01:04:11.954870 17240 layer_factory.hpp:77] Creating layer relu5
I1026 01:04:11.954875 17240 net.cpp:106] Creating Layer relu5
I1026 01:04:11.954877 17240 net.cpp:454] relu5 <- conv5
I1026 01:04:11.954881 17240 net.cpp:397] relu5 -> conv5 (in-place)
I1026 01:04:11.955096 17240 net.cpp:150] Setting up relu5
I1026 01:04:11.955113 17240 net.cpp:157] Top shape: 1 256 15 15 (57600)
I1026 01:04:11.955116 17240 net.cpp:165] Memory required for data: 20355456
I1026 01:04:11.955127 17240 layer_factory.hpp:77] Creating layer rpn_conv1
I1026 01:04:11.955132 17240 net.cpp:106] Creating Layer rpn_conv1
I1026 01:04:11.955134 17240 net.cpp:454] rpn_conv1 <- conv5
I1026 01:04:11.955138 17240 net.cpp:411] rpn_conv1 -> rpn_conv1
I1026 01:04:11.956594 17240 net.cpp:150] Setting up rpn_conv1
I1026 01:04:11.956604 17240 net.cpp:157] Top shape: 1 256 15 15 (57600)
I1026 01:04:11.956619 17240 net.cpp:165] Memory required for data: 20585856
I1026 01:04:11.956624 17240 layer_factory.hpp:77] Creating layer rpn_relu1
I1026 01:04:11.956637 17240 net.cpp:106] Creating Layer rpn_relu1
I1026 01:04:11.956640 17240 net.cpp:454] rpn_relu1 <- rpn_conv1
I1026 01:04:11.956642 17240 net.cpp:397] rpn_relu1 -> rpn_conv1 (in-place)
I1026 01:04:11.956778 17240 net.cpp:150] Setting up rpn_relu1
I1026 01:04:11.956784 17240 net.cpp:157] Top shape: 1 256 15 15 (57600)
I1026 01:04:11.956785 17240 net.cpp:165] Memory required for data: 20816256
I1026 01:04:11.956797 17240 layer_factory.hpp:77] Creating layer rpn_conv1_rpn_relu1_0_split
I1026 01:04:11.956804 17240 net.cpp:106] Creating Layer rpn_conv1_rpn_relu1_0_split
I1026 01:04:11.956805 17240 net.cpp:454] rpn_conv1_rpn_relu1_0_split <- rpn_conv1
I1026 01:04:11.956810 17240 net.cpp:411] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_0
I1026 01:04:11.956815 17240 net.cpp:411] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_1
I1026 01:04:11.956850 17240 net.cpp:150] Setting up rpn_conv1_rpn_relu1_0_split
I1026 01:04:11.956854 17240 net.cpp:157] Top shape: 1 256 15 15 (57600)
I1026 01:04:11.956866 17240 net.cpp:157] Top shape: 1 256 15 15 (57600)
I1026 01:04:11.956868 17240 net.cpp:165] Memory required for data: 21277056
I1026 01:04:11.956871 17240 layer_factory.hpp:77] Creating layer rpn_cls_score
I1026 01:04:11.956874 17240 net.cpp:106] Creating Layer rpn_cls_score
I1026 01:04:11.956876 17240 net.cpp:454] rpn_cls_score <- rpn_conv1_rpn_relu1_0_split_0
I1026 01:04:11.956881 17240 net.cpp:411] rpn_cls_score -> rpn_cls_score
I1026 01:04:11.957761 17240 net.cpp:150] Setting up rpn_cls_score
I1026 01:04:11.957769 17240 net.cpp:157] Top shape: 1 18 15 15 (4050)
I1026 01:04:11.957772 17240 net.cpp:165] Memory required for data: 21293256
I1026 01:04:11.957775 17240 layer_factory.hpp:77] Creating layer rpn_bbox_pred
I1026 01:04:11.957780 17240 net.cpp:106] Creating Layer rpn_bbox_pred
I1026 01:04:11.957782 17240 net.cpp:454] rpn_bbox_pred <- rpn_conv1_rpn_relu1_0_split_1
I1026 01:04:11.957797 17240 net.cpp:411] rpn_bbox_pred -> rpn_bbox_pred
I1026 01:04:11.958509 17240 net.cpp:150] Setting up rpn_bbox_pred
I1026 01:04:11.958518 17240 net.cpp:157] Top shape: 1 36 15 15 (8100)
I1026 01:04:11.958519 17240 net.cpp:165] Memory required for data: 21325656
I1026 01:04:11.958523 17240 layer_factory.hpp:77] Creating layer rpn_cls_score_reshape
I1026 01:04:11.958544 17240 net.cpp:106] Creating Layer rpn_cls_score_reshape
I1026 01:04:11.958545 17240 net.cpp:454] rpn_cls_score_reshape <- rpn_cls_score
I1026 01:04:11.958550 17240 net.cpp:411] rpn_cls_score_reshape -> rpn_cls_score_reshape
I1026 01:04:11.958582 17240 net.cpp:150] Setting up rpn_cls_score_reshape
I1026 01:04:11.958586 17240 net.cpp:157] Top shape: 1 2 135 15 (4050)
I1026 01:04:11.958587 17240 net.cpp:165] Memory required for data: 21341856
I1026 01:04:11.958600 17240 layer_factory.hpp:77] Creating layer rpn_cls_prob
I1026 01:04:11.958603 17240 net.cpp:106] Creating Layer rpn_cls_prob
I1026 01:04:11.958606 17240 net.cpp:454] rpn_cls_prob <- rpn_cls_score_reshape
I1026 01:04:11.958608 17240 net.cpp:411] rpn_cls_prob -> rpn_cls_prob
I1026 01:04:11.958781 17240 net.cpp:150] Setting up rpn_cls_prob
I1026 01:04:11.958787 17240 net.cpp:157] Top shape: 1 2 135 15 (4050)
I1026 01:04:11.958788 17240 net.cpp:165] Memory required for data: 21358056
I1026 01:04:11.958801 17240 layer_factory.hpp:77] Creating layer rpn_cls_prob_reshape
I1026 01:04:11.958804 17240 net.cpp:106] Creating Layer rpn_cls_prob_reshape
I1026 01:04:11.958806 17240 net.cpp:454] rpn_cls_prob_reshape <- rpn_cls_prob
I1026 01:04:11.958811 17240 net.cpp:411] rpn_cls_prob_reshape -> rpn_cls_prob_reshape
I1026 01:04:11.958837 17240 net.cpp:150] Setting up rpn_cls_prob_reshape
I1026 01:04:11.958840 17240 net.cpp:157] Top shape: 1 18 15 15 (4050)
I1026 01:04:11.958842 17240 net.cpp:165] Memory required for data: 21374256
I1026 01:04:11.958843 17240 layer_factory.hpp:77] Creating layer proposal
I1026 01:04:11.959779 17240 net.cpp:106] Creating Layer proposal
I1026 01:04:11.959787 17240 net.cpp:454] proposal <- rpn_cls_prob_reshape
I1026 01:04:11.959800 17240 net.cpp:454] proposal <- rpn_bbox_pred
I1026 01:04:11.959802 17240 net.cpp:454] proposal <- im_info
I1026 01:04:11.959806 17240 net.cpp:411] proposal -> rois
I1026 01:04:11.959820 17240 net.cpp:411] proposal -> scores
I1026 01:04:11.960595 17240 net.cpp:150] Setting up proposal
I1026 01:04:11.960604 17240 net.cpp:157] Top shape: 1 5 (5)
I1026 01:04:11.960608 17240 net.cpp:157] Top shape: 1 1 1 1 (1)
I1026 01:04:11.960608 17240 net.cpp:165] Memory required for data: 21374280
I1026 01:04:11.960611 17240 net.cpp:228] proposal does not need backward computation.
I1026 01:04:11.960614 17240 net.cpp:228] rpn_cls_prob_reshape does not need backward computation.
I1026 01:04:11.960616 17240 net.cpp:228] rpn_cls_prob does not need backward computation.
I1026 01:04:11.960618 17240 net.cpp:228] rpn_cls_score_reshape does not need backward computation.
I1026 01:04:11.960630 17240 net.cpp:228] rpn_bbox_pred does not need backward computation.
I1026 01:04:11.960633 17240 net.cpp:228] rpn_cls_score does not need backward computation.
I1026 01:04:11.960634 17240 net.cpp:228] rpn_conv1_rpn_relu1_0_split does not need backward computation.
I1026 01:04:11.960636 17240 net.cpp:228] rpn_relu1 does not need backward computation.
I1026 01:04:11.960638 17240 net.cpp:228] rpn_conv1 does not need backward computation.
I1026 01:04:11.960639 17240 net.cpp:228] relu5 does not need backward computation.
I1026 01:04:11.960641 17240 net.cpp:228] conv5 does not need backward computation.
I1026 01:04:11.960644 17240 net.cpp:228] relu4 does not need backward computation.
I1026 01:04:11.960645 17240 net.cpp:228] conv4 does not need backward computation.
I1026 01:04:11.960647 17240 net.cpp:228] relu3 does not need backward computation.
I1026 01:04:11.960649 17240 net.cpp:228] conv3 does not need backward computation.
I1026 01:04:11.960650 17240 net.cpp:228] pool2 does not need backward computation.
I1026 01:04:11.960652 17240 net.cpp:228] norm2 does not need backward computation.
I1026 01:04:11.960654 17240 net.cpp:228] relu2 does not need backward computation.
I1026 01:04:11.960656 17240 net.cpp:228] conv2 does not need backward computation.
I1026 01:04:11.960657 17240 net.cpp:228] pool1 does not need backward computation.
I1026 01:04:11.960659 17240 net.cpp:228] norm1 does not need backward computation.
I1026 01:04:11.960661 17240 net.cpp:228] relu1 does not need backward computation.
I1026 01:04:11.960664 17240 net.cpp:228] conv1 does not need backward computation.
I1026 01:04:11.960664 17240 net.cpp:270] This network produces output rois
I1026 01:04:11.960666 17240 net.cpp:270] This network produces output scores
I1026 01:04:11.960677 17240 net.cpp:283] Network initialization done.
I1026 01:04:12.031862 17240 net.cpp:816] Ignoring source layer input-data
I1026 01:04:12.031883 17240 net.cpp:816] Ignoring source layer data_input-data_0_split
I1026 01:04:12.034519 17240 net.cpp:816] Ignoring source layer rpn_cls_score_rpn_cls_score_0_split
I1026 01:04:12.034541 17240 net.cpp:816] Ignoring source layer rpn-data
I1026 01:04:12.034543 17240 net.cpp:816] Ignoring source layer rpn_loss_cls
I1026 01:04:12.034544 17240 net.cpp:816] Ignoring source layer rpn_loss_bbox
I1026 01:04:12.034546 17240 net.cpp:816] Ignoring source layer dummy_roi_pool_conv5
I1026 01:04:12.034548 17240 net.cpp:816] Ignoring source layer fc6
I1026 01:04:12.034550 17240 net.cpp:816] Ignoring source layer relu6
I1026 01:04:12.034551 17240 net.cpp:816] Ignoring source layer fc7
I1026 01:04:12.034554 17240 net.cpp:816] Ignoring source layer silence_fc7
RPN model: /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_rpn_stage2_iter_40000.caffemodel
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 2000,
          'RPN_PRE_NMS_TOP_N': -1,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': True,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': False,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'selective_search',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage2',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for proposal generation
Output will be saved to `/home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train`
im_proposals: 1/712 0.055s
im_proposals: 2/712 0.056s
im_proposals: 3/712 0.053s
im_proposals: 4/712 0.053s
im_proposals: 5/712 0.051s
im_proposals: 6/712 0.050s
im_proposals: 7/712 0.049s
im_proposals: 8/712 0.048s
im_proposals: 9/712 0.048s
im_proposals: 10/712 0.049s
im_proposals: 11/712 0.048s
im_proposals: 12/712 0.048s
im_proposals: 13/712 0.048s
im_proposals: 14/712 0.048s
im_proposals: 15/712 0.049s
im_proposals: 16/712 0.049s
im_proposals: 17/712 0.049s
im_proposals: 18/712 0.049s
im_proposals: 19/712 0.048s
im_proposals: 20/712 0.048s
im_proposals: 21/712 0.048s
im_proposals: 22/712 0.048s
im_proposals: 23/712 0.048s
im_proposals: 24/712 0.048s
im_proposals: 25/712 0.048s
im_proposals: 26/712 0.047s
im_proposals: 27/712 0.047s
im_proposals: 28/712 0.047s
im_proposals: 29/712 0.047s
im_proposals: 30/712 0.047s
im_proposals: 31/712 0.047s
im_proposals: 32/712 0.046s
im_proposals: 33/712 0.046s
im_proposals: 34/712 0.046s
im_proposals: 35/712 0.046s
im_proposals: 36/712 0.046s
im_proposals: 37/712 0.046s
im_proposals: 38/712 0.046s
im_proposals: 39/712 0.046s
im_proposals: 40/712 0.046s
im_proposals: 41/712 0.046s
im_proposals: 42/712 0.046s
im_proposals: 43/712 0.046s
im_proposals: 44/712 0.046s
im_proposals: 45/712 0.046s
im_proposals: 46/712 0.046s
im_proposals: 47/712 0.046s
im_proposals: 48/712 0.046s
im_proposals: 49/712 0.046s
im_proposals: 50/712 0.046s
im_proposals: 51/712 0.046s
im_proposals: 52/712 0.046s
im_proposals: 53/712 0.046s
im_proposals: 54/712 0.046s
im_proposals: 55/712 0.046s
im_proposals: 56/712 0.046s
im_proposals: 57/712 0.046s
im_proposals: 58/712 0.046s
im_proposals: 59/712 0.046s
im_proposals: 60/712 0.046s
im_proposals: 61/712 0.046s
im_proposals: 62/712 0.046s
im_proposals: 63/712 0.046s
im_proposals: 64/712 0.046s
im_proposals: 65/712 0.046s
im_proposals: 66/712 0.046s
im_proposals: 67/712 0.046s
im_proposals: 68/712 0.046s
im_proposals: 69/712 0.046s
im_proposals: 70/712 0.046s
im_proposals: 71/712 0.046s
im_proposals: 72/712 0.046s
im_proposals: 73/712 0.046s
im_proposals: 74/712 0.046s
im_proposals: 75/712 0.046s
im_proposals: 76/712 0.046s
im_proposals: 77/712 0.046s
im_proposals: 78/712 0.046s
im_proposals: 79/712 0.046s
im_proposals: 80/712 0.046s
im_proposals: 81/712 0.046s
im_proposals: 82/712 0.046s
im_proposals: 83/712 0.046s
im_proposals: 84/712 0.046s
im_proposals: 85/712 0.046s
im_proposals: 86/712 0.046s
im_proposals: 87/712 0.046s
im_proposals: 88/712 0.046s
im_proposals: 89/712 0.046s
im_proposals: 90/712 0.046s
im_proposals: 91/712 0.046s
im_proposals: 92/712 0.046s
im_proposals: 93/712 0.046s
im_proposals: 94/712 0.046s
im_proposals: 95/712 0.046s
im_proposals: 96/712 0.046s
im_proposals: 97/712 0.046s
im_proposals: 98/712 0.046s
im_proposals: 99/712 0.046s
im_proposals: 100/712 0.046s
im_proposals: 101/712 0.046s
im_proposals: 102/712 0.046s
im_proposals: 103/712 0.046s
im_proposals: 104/712 0.046s
im_proposals: 105/712 0.045s
im_proposals: 106/712 0.045s
im_proposals: 107/712 0.046s
im_proposals: 108/712 0.045s
im_proposals: 109/712 0.045s
im_proposals: 110/712 0.045s
im_proposals: 111/712 0.045s
im_proposals: 112/712 0.045s
im_proposals: 113/712 0.045s
im_proposals: 114/712 0.045s
im_proposals: 115/712 0.045s
im_proposals: 116/712 0.045s
im_proposals: 117/712 0.045s
im_proposals: 118/712 0.045s
im_proposals: 119/712 0.045s
im_proposals: 120/712 0.045s
im_proposals: 121/712 0.045s
im_proposals: 122/712 0.045s
im_proposals: 123/712 0.045s
im_proposals: 124/712 0.045s
im_proposals: 125/712 0.045s
im_proposals: 126/712 0.045s
im_proposals: 127/712 0.045s
im_proposals: 128/712 0.045s
im_proposals: 129/712 0.045s
im_proposals: 130/712 0.045s
im_proposals: 131/712 0.045s
im_proposals: 132/712 0.045s
im_proposals: 133/712 0.045s
im_proposals: 134/712 0.045s
im_proposals: 135/712 0.045s
im_proposals: 136/712 0.045s
im_proposals: 137/712 0.045s
im_proposals: 138/712 0.045s
im_proposals: 139/712 0.045s
im_proposals: 140/712 0.045s
im_proposals: 141/712 0.045s
im_proposals: 142/712 0.045s
im_proposals: 143/712 0.045s
im_proposals: 144/712 0.045s
im_proposals: 145/712 0.045s
im_proposals: 146/712 0.045s
im_proposals: 147/712 0.045s
im_proposals: 148/712 0.045s
im_proposals: 149/712 0.045s
im_proposals: 150/712 0.045s
im_proposals: 151/712 0.045s
im_proposals: 152/712 0.045s
im_proposals: 153/712 0.045s
im_proposals: 154/712 0.045s
im_proposals: 155/712 0.045s
im_proposals: 156/712 0.045s
im_proposals: 157/712 0.045s
im_proposals: 158/712 0.045s
im_proposals: 159/712 0.045s
im_proposals: 160/712 0.045s
im_proposals: 161/712 0.045s
im_proposals: 162/712 0.045s
im_proposals: 163/712 0.045s
im_proposals: 164/712 0.045s
im_proposals: 165/712 0.045s
im_proposals: 166/712 0.045s
im_proposals: 167/712 0.045s
im_proposals: 168/712 0.045s
im_proposals: 169/712 0.045s
im_proposals: 170/712 0.045s
im_proposals: 171/712 0.045s
im_proposals: 172/712 0.045s
im_proposals: 173/712 0.045s
im_proposals: 174/712 0.045s
im_proposals: 175/712 0.045s
im_proposals: 176/712 0.045s
im_proposals: 177/712 0.045s
im_proposals: 178/712 0.045s
im_proposals: 179/712 0.045s
im_proposals: 180/712 0.045s
im_proposals: 181/712 0.045s
im_proposals: 182/712 0.045s
im_proposals: 183/712 0.045s
im_proposals: 184/712 0.045s
im_proposals: 185/712 0.045s
im_proposals: 186/712 0.045s
im_proposals: 187/712 0.045s
im_proposals: 188/712 0.045s
im_proposals: 189/712 0.045s
im_proposals: 190/712 0.045s
im_proposals: 191/712 0.045s
im_proposals: 192/712 0.045s
im_proposals: 193/712 0.045s
im_proposals: 194/712 0.045s
im_proposals: 195/712 0.045s
im_proposals: 196/712 0.045s
im_proposals: 197/712 0.045s
im_proposals: 198/712 0.045s
im_proposals: 199/712 0.045s
im_proposals: 200/712 0.045s
im_proposals: 201/712 0.045s
im_proposals: 202/712 0.045s
im_proposals: 203/712 0.045s
im_proposals: 204/712 0.045s
im_proposals: 205/712 0.045s
im_proposals: 206/712 0.045s
im_proposals: 207/712 0.045s
im_proposals: 208/712 0.045s
im_proposals: 209/712 0.045s
im_proposals: 210/712 0.045s
im_proposals: 211/712 0.045s
im_proposals: 212/712 0.045s
im_proposals: 213/712 0.045s
im_proposals: 214/712 0.045s
im_proposals: 215/712 0.045s
im_proposals: 216/712 0.045s
im_proposals: 217/712 0.045s
im_proposals: 218/712 0.045s
im_proposals: 219/712 0.045s
im_proposals: 220/712 0.045s
im_proposals: 221/712 0.045s
im_proposals: 222/712 0.045s
im_proposals: 223/712 0.045s
im_proposals: 224/712 0.045s
im_proposals: 225/712 0.045s
im_proposals: 226/712 0.045s
im_proposals: 227/712 0.045s
im_proposals: 228/712 0.045s
im_proposals: 229/712 0.045s
im_proposals: 230/712 0.045s
im_proposals: 231/712 0.045s
im_proposals: 232/712 0.045s
im_proposals: 233/712 0.045s
im_proposals: 234/712 0.045s
im_proposals: 235/712 0.045s
im_proposals: 236/712 0.045s
im_proposals: 237/712 0.045s
im_proposals: 238/712 0.045s
im_proposals: 239/712 0.045s
im_proposals: 240/712 0.045s
im_proposals: 241/712 0.045s
im_proposals: 242/712 0.045s
im_proposals: 243/712 0.045s
im_proposals: 244/712 0.045s
im_proposals: 245/712 0.045s
im_proposals: 246/712 0.045s
im_proposals: 247/712 0.045s
im_proposals: 248/712 0.045s
im_proposals: 249/712 0.045s
im_proposals: 250/712 0.045s
im_proposals: 251/712 0.045s
im_proposals: 252/712 0.045s
im_proposals: 253/712 0.045s
im_proposals: 254/712 0.045s
im_proposals: 255/712 0.045s
im_proposals: 256/712 0.045s
im_proposals: 257/712 0.045s
im_proposals: 258/712 0.045s
im_proposals: 259/712 0.045s
im_proposals: 260/712 0.045s
im_proposals: 261/712 0.045s
im_proposals: 262/712 0.045s
im_proposals: 263/712 0.045s
im_proposals: 264/712 0.045s
im_proposals: 265/712 0.045s
im_proposals: 266/712 0.045s
im_proposals: 267/712 0.045s
im_proposals: 268/712 0.045s
im_proposals: 269/712 0.045s
im_proposals: 270/712 0.045s
im_proposals: 271/712 0.045s
im_proposals: 272/712 0.045s
im_proposals: 273/712 0.045s
im_proposals: 274/712 0.045s
im_proposals: 275/712 0.045s
im_proposals: 276/712 0.045s
im_proposals: 277/712 0.045s
im_proposals: 278/712 0.045s
im_proposals: 279/712 0.045s
im_proposals: 280/712 0.045s
im_proposals: 281/712 0.045s
im_proposals: 282/712 0.045s
im_proposals: 283/712 0.045s
im_proposals: 284/712 0.045s
im_proposals: 285/712 0.045s
im_proposals: 286/712 0.045s
im_proposals: 287/712 0.045s
im_proposals: 288/712 0.045s
im_proposals: 289/712 0.045s
im_proposals: 290/712 0.045s
im_proposals: 291/712 0.045s
im_proposals: 292/712 0.045s
im_proposals: 293/712 0.045s
im_proposals: 294/712 0.045s
im_proposals: 295/712 0.045s
im_proposals: 296/712 0.045s
im_proposals: 297/712 0.045s
im_proposals: 298/712 0.045s
im_proposals: 299/712 0.045s
im_proposals: 300/712 0.045s
im_proposals: 301/712 0.045s
im_proposals: 302/712 0.045s
im_proposals: 303/712 0.045s
im_proposals: 304/712 0.045s
im_proposals: 305/712 0.045s
im_proposals: 306/712 0.045s
im_proposals: 307/712 0.045s
im_proposals: 308/712 0.045s
im_proposals: 309/712 0.045s
im_proposals: 310/712 0.045s
im_proposals: 311/712 0.045s
im_proposals: 312/712 0.045s
im_proposals: 313/712 0.045s
im_proposals: 314/712 0.045s
im_proposals: 315/712 0.045s
im_proposals: 316/712 0.045s
im_proposals: 317/712 0.045s
im_proposals: 318/712 0.045s
im_proposals: 319/712 0.045s
im_proposals: 320/712 0.045s
im_proposals: 321/712 0.045s
im_proposals: 322/712 0.045s
im_proposals: 323/712 0.045s
im_proposals: 324/712 0.045s
im_proposals: 325/712 0.045s
im_proposals: 326/712 0.045s
im_proposals: 327/712 0.045s
im_proposals: 328/712 0.045s
im_proposals: 329/712 0.045s
im_proposals: 330/712 0.045s
im_proposals: 331/712 0.045s
im_proposals: 332/712 0.045s
im_proposals: 333/712 0.045s
im_proposals: 334/712 0.045s
im_proposals: 335/712 0.045s
im_proposals: 336/712 0.045s
im_proposals: 337/712 0.045s
im_proposals: 338/712 0.045s
im_proposals: 339/712 0.045s
im_proposals: 340/712 0.045s
im_proposals: 341/712 0.045s
im_proposals: 342/712 0.045s
im_proposals: 343/712 0.045s
im_proposals: 344/712 0.045s
im_proposals: 345/712 0.045s
im_proposals: 346/712 0.045s
im_proposals: 347/712 0.045s
im_proposals: 348/712 0.045s
im_proposals: 349/712 0.045s
im_proposals: 350/712 0.045s
im_proposals: 351/712 0.045s
im_proposals: 352/712 0.045s
im_proposals: 353/712 0.045s
im_proposals: 354/712 0.045s
im_proposals: 355/712 0.045s
im_proposals: 356/712 0.045s
im_proposals: 357/712 0.045s
im_proposals: 358/712 0.045s
im_proposals: 359/712 0.045s
im_proposals: 360/712 0.045s
im_proposals: 361/712 0.045s
im_proposals: 362/712 0.045s
im_proposals: 363/712 0.045s
im_proposals: 364/712 0.045s
im_proposals: 365/712 0.045s
im_proposals: 366/712 0.045s
im_proposals: 367/712 0.045s
im_proposals: 368/712 0.045s
im_proposals: 369/712 0.045s
im_proposals: 370/712 0.045s
im_proposals: 371/712 0.045s
im_proposals: 372/712 0.045s
im_proposals: 373/712 0.045s
im_proposals: 374/712 0.045s
im_proposals: 375/712 0.045s
im_proposals: 376/712 0.045s
im_proposals: 377/712 0.045s
im_proposals: 378/712 0.045s
im_proposals: 379/712 0.045s
im_proposals: 380/712 0.045s
im_proposals: 381/712 0.045s
im_proposals: 382/712 0.045s
im_proposals: 383/712 0.045s
im_proposals: 384/712 0.045s
im_proposals: 385/712 0.045s
im_proposals: 386/712 0.045s
im_proposals: 387/712 0.045s
im_proposals: 388/712 0.045s
im_proposals: 389/712 0.045s
im_proposals: 390/712 0.045s
im_proposals: 391/712 0.045s
im_proposals: 392/712 0.045s
im_proposals: 393/712 0.045s
im_proposals: 394/712 0.045s
im_proposals: 395/712 0.045s
im_proposals: 396/712 0.045s
im_proposals: 397/712 0.045s
im_proposals: 398/712 0.045s
im_proposals: 399/712 0.045s
im_proposals: 400/712 0.045s
im_proposals: 401/712 0.045s
im_proposals: 402/712 0.045s
im_proposals: 403/712 0.045s
im_proposals: 404/712 0.045s
im_proposals: 405/712 0.045s
im_proposals: 406/712 0.045s
im_proposals: 407/712 0.045s
im_proposals: 408/712 0.045s
im_proposals: 409/712 0.045s
im_proposals: 410/712 0.045s
im_proposals: 411/712 0.045s
im_proposals: 412/712 0.045s
im_proposals: 413/712 0.045s
im_proposals: 414/712 0.045s
im_proposals: 415/712 0.045s
im_proposals: 416/712 0.045s
im_proposals: 417/712 0.045s
im_proposals: 418/712 0.045s
im_proposals: 419/712 0.045s
im_proposals: 420/712 0.045s
im_proposals: 421/712 0.045s
im_proposals: 422/712 0.045s
im_proposals: 423/712 0.045s
im_proposals: 424/712 0.045s
im_proposals: 425/712 0.045s
im_proposals: 426/712 0.045s
im_proposals: 427/712 0.045s
im_proposals: 428/712 0.045s
im_proposals: 429/712 0.045s
im_proposals: 430/712 0.045s
im_proposals: 431/712 0.045s
im_proposals: 432/712 0.045s
im_proposals: 433/712 0.045s
im_proposals: 434/712 0.045s
im_proposals: 435/712 0.045s
im_proposals: 436/712 0.045s
im_proposals: 437/712 0.045s
im_proposals: 438/712 0.045s
im_proposals: 439/712 0.045s
im_proposals: 440/712 0.045s
im_proposals: 441/712 0.045s
im_proposals: 442/712 0.045s
im_proposals: 443/712 0.045s
im_proposals: 444/712 0.045s
im_proposals: 445/712 0.045s
im_proposals: 446/712 0.045s
im_proposals: 447/712 0.045s
im_proposals: 448/712 0.045s
im_proposals: 449/712 0.045s
im_proposals: 450/712 0.045s
im_proposals: 451/712 0.045s
im_proposals: 452/712 0.045s
im_proposals: 453/712 0.045s
im_proposals: 454/712 0.045s
im_proposals: 455/712 0.045s
im_proposals: 456/712 0.045s
im_proposals: 457/712 0.045s
im_proposals: 458/712 0.045s
im_proposals: 459/712 0.045s
im_proposals: 460/712 0.045s
im_proposals: 461/712 0.045s
im_proposals: 462/712 0.045s
im_proposals: 463/712 0.045s
im_proposals: 464/712 0.045s
im_proposals: 465/712 0.045s
im_proposals: 466/712 0.045s
im_proposals: 467/712 0.045s
im_proposals: 468/712 0.045s
im_proposals: 469/712 0.045s
im_proposals: 470/712 0.045s
im_proposals: 471/712 0.045s
im_proposals: 472/712 0.045s
im_proposals: 473/712 0.045s
im_proposals: 474/712 0.045s
im_proposals: 475/712 0.045s
im_proposals: 476/712 0.045s
im_proposals: 477/712 0.045s
im_proposals: 478/712 0.045s
im_proposals: 479/712 0.045s
im_proposals: 480/712 0.045s
im_proposals: 481/712 0.045s
im_proposals: 482/712 0.045s
im_proposals: 483/712 0.045s
im_proposals: 484/712 0.045s
im_proposals: 485/712 0.045s
im_proposals: 486/712 0.045s
im_proposals: 487/712 0.045s
im_proposals: 488/712 0.045s
im_proposals: 489/712 0.045s
im_proposals: 490/712 0.045s
im_proposals: 491/712 0.045s
im_proposals: 492/712 0.045s
im_proposals: 493/712 0.045s
im_proposals: 494/712 0.045s
im_proposals: 495/712 0.045s
im_proposals: 496/712 0.045s
im_proposals: 497/712 0.045s
im_proposals: 498/712 0.045s
im_proposals: 499/712 0.045s
im_proposals: 500/712 0.045s
im_proposals: 501/712 0.045s
im_proposals: 502/712 0.045s
im_proposals: 503/712 0.045s
im_proposals: 504/712 0.045s
im_proposals: 505/712 0.045s
im_proposals: 506/712 0.045s
im_proposals: 507/712 0.045s
im_proposals: 508/712 0.045s
im_proposals: 509/712 0.045s
im_proposals: 510/712 0.045s
im_proposals: 511/712 0.045s
im_proposals: 512/712 0.045s
im_proposals: 513/712 0.045s
im_proposals: 514/712 0.045s
im_proposals: 515/712 0.045s
im_proposals: 516/712 0.045s
im_proposals: 517/712 0.045s
im_proposals: 518/712 0.045s
im_proposals: 519/712 0.045s
im_proposals: 520/712 0.045s
im_proposals: 521/712 0.045s
im_proposals: 522/712 0.045s
im_proposals: 523/712 0.045s
im_proposals: 524/712 0.045s
im_proposals: 525/712 0.045s
im_proposals: 526/712 0.045s
im_proposals: 527/712 0.045s
im_proposals: 528/712 0.045s
im_proposals: 529/712 0.045s
im_proposals: 530/712 0.045s
im_proposals: 531/712 0.045s
im_proposals: 532/712 0.045s
im_proposals: 533/712 0.045s
im_proposals: 534/712 0.045s
im_proposals: 535/712 0.045s
im_proposals: 536/712 0.045s
im_proposals: 537/712 0.045s
im_proposals: 538/712 0.045s
im_proposals: 539/712 0.045s
im_proposals: 540/712 0.045s
im_proposals: 541/712 0.045s
im_proposals: 542/712 0.045s
im_proposals: 543/712 0.045s
im_proposals: 544/712 0.045s
im_proposals: 545/712 0.045s
im_proposals: 546/712 0.045s
im_proposals: 547/712 0.045s
im_proposals: 548/712 0.045s
im_proposals: 549/712 0.045s
im_proposals: 550/712 0.045s
im_proposals: 551/712 0.045s
im_proposals: 552/712 0.045s
im_proposals: 553/712 0.045s
im_proposals: 554/712 0.045s
im_proposals: 555/712 0.045s
im_proposals: 556/712 0.045s
im_proposals: 557/712 0.045s
im_proposals: 558/712 0.045s
im_proposals: 559/712 0.045s
im_proposals: 560/712 0.045s
im_proposals: 561/712 0.045s
im_proposals: 562/712 0.045s
im_proposals: 563/712 0.045s
im_proposals: 564/712 0.045s
im_proposals: 565/712 0.045s
im_proposals: 566/712 0.045s
im_proposals: 567/712 0.045s
im_proposals: 568/712 0.045s
im_proposals: 569/712 0.045s
im_proposals: 570/712 0.045s
im_proposals: 571/712 0.045s
im_proposals: 572/712 0.045s
im_proposals: 573/712 0.045s
im_proposals: 574/712 0.045s
im_proposals: 575/712 0.045s
im_proposals: 576/712 0.045s
im_proposals: 577/712 0.045s
im_proposals: 578/712 0.045s
im_proposals: 579/712 0.045s
im_proposals: 580/712 0.045s
im_proposals: 581/712 0.045s
im_proposals: 582/712 0.045s
im_proposals: 583/712 0.045s
im_proposals: 584/712 0.045s
im_proposals: 585/712 0.045s
im_proposals: 586/712 0.045s
im_proposals: 587/712 0.045s
im_proposals: 588/712 0.045s
im_proposals: 589/712 0.045s
im_proposals: 590/712 0.045s
im_proposals: 591/712 0.045s
im_proposals: 592/712 0.045s
im_proposals: 593/712 0.045s
im_proposals: 594/712 0.045s
im_proposals: 595/712 0.045s
im_proposals: 596/712 0.045s
im_proposals: 597/712 0.045s
im_proposals: 598/712 0.045s
im_proposals: 599/712 0.045s
im_proposals: 600/712 0.045s
im_proposals: 601/712 0.045s
im_proposals: 602/712 0.045s
im_proposals: 603/712 0.045s
im_proposals: 604/712 0.045s
im_proposals: 605/712 0.045s
im_proposals: 606/712 0.045s
im_proposals: 607/712 0.045s
im_proposals: 608/712 0.045s
im_proposals: 609/712 0.045s
im_proposals: 610/712 0.045s
im_proposals: 611/712 0.045s
im_proposals: 612/712 0.045s
im_proposals: 613/712 0.045s
im_proposals: 614/712 0.045s
im_proposals: 615/712 0.045s
im_proposals: 616/712 0.045s
im_proposals: 617/712 0.045s
im_proposals: 618/712 0.045s
im_proposals: 619/712 0.045s
im_proposals: 620/712 0.045s
im_proposals: 621/712 0.045s
im_proposals: 622/712 0.045s
im_proposals: 623/712 0.045s
im_proposals: 624/712 0.045s
im_proposals: 625/712 0.045s
im_proposals: 626/712 0.045s
im_proposals: 627/712 0.045s
im_proposals: 628/712 0.045s
im_proposals: 629/712 0.045s
im_proposals: 630/712 0.045s
im_proposals: 631/712 0.045s
im_proposals: 632/712 0.045s
im_proposals: 633/712 0.045s
im_proposals: 634/712 0.045s
im_proposals: 635/712 0.045s
im_proposals: 636/712 0.045s
im_proposals: 637/712 0.045s
im_proposals: 638/712 0.045s
im_proposals: 639/712 0.045s
im_proposals: 640/712 0.045s
im_proposals: 641/712 0.045s
im_proposals: 642/712 0.045s
im_proposals: 643/712 0.045s
im_proposals: 644/712 0.045s
im_proposals: 645/712 0.045s
im_proposals: 646/712 0.045s
im_proposals: 647/712 0.045s
im_proposals: 648/712 0.045s
im_proposals: 649/712 0.045s
im_proposals: 650/712 0.045s
im_proposals: 651/712 0.045s
im_proposals: 652/712 0.045s
im_proposals: 653/712 0.045s
im_proposals: 654/712 0.045s
im_proposals: 655/712 0.045s
im_proposals: 656/712 0.045s
im_proposals: 657/712 0.045s
im_proposals: 658/712 0.045s
im_proposals: 659/712 0.045s
im_proposals: 660/712 0.045s
im_proposals: 661/712 0.045s
im_proposals: 662/712 0.045s
im_proposals: 663/712 0.045s
im_proposals: 664/712 0.045s
im_proposals: 665/712 0.045s
im_proposals: 666/712 0.045s
im_proposals: 667/712 0.045s
im_proposals: 668/712 0.045s
im_proposals: 669/712 0.045s
im_proposals: 670/712 0.045s
im_proposals: 671/712 0.045s
im_proposals: 672/712 0.045s
im_proposals: 673/712 0.045s
im_proposals: 674/712 0.045s
im_proposals: 675/712 0.045s
im_proposals: 676/712 0.045s
im_proposals: 677/712 0.045s
im_proposals: 678/712 0.045s
im_proposals: 679/712 0.045s
im_proposals: 680/712 0.045s
im_proposals: 681/712 0.045s
im_proposals: 682/712 0.045s
im_proposals: 683/712 0.045s
im_proposals: 684/712 0.045s
im_proposals: 685/712 0.045s
im_proposals: 686/712 0.045s
im_proposals: 687/712 0.045s
im_proposals: 688/712 0.045s
im_proposals: 689/712 0.045s
im_proposals: 690/712 0.045s
im_proposals: 691/712 0.045s
im_proposals: 692/712 0.045s
im_proposals: 693/712 0.045s
im_proposals: 694/712 0.045s
im_proposals: 695/712 0.045s
im_proposals: 696/712 0.045s
im_proposals: 697/712 0.045s
im_proposals: 698/712 0.045s
im_proposals: 699/712 0.045s
im_proposals: 700/712 0.045s
im_proposals: 701/712 0.045s
im_proposals: 702/712 0.045s
im_proposals: 703/712 0.045s
im_proposals: 704/712 0.045s
im_proposals: 705/712 0.045s
im_proposals: 706/712 0.045s
im_proposals: 707/712 0.045s
im_proposals: 708/712 0.045s
im_proposals: 709/712 0.045s
im_proposals: 710/712 0.045s
im_proposals: 711/712 0.045s
im_proposals: 712/712 0.045s
Wrote RPN proposals to /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_rpn_stage2_iter_40000_proposals.pkl
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Stage 2 Fast R-CNN, init from stage 2 RPN R-CNN model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Init model: /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_rpn_stage2_iter_40000.caffemodel
RPN proposals: /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_rpn_stage2_iter_40000_proposals.pkl
Using config:
{'DATA_DIR': '/home/cgangee/code/cg-py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_alt_opt',
 'GPU_ID': 0,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/cgangee/code/cg-py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 3,
 'ROOT_DIR': '/home/cgangee/code/cg-py-faster-rcnn',
 'TEST': {'BBOX_REG': True,
          'HAS_RPN': True,
          'MAX_SIZE': 1000,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 128,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': False,
           'BBOX_REG': True,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': False,
           'IMS_PER_BATCH': 2,
           'MAX_SIZE': 1000,
           'PROPOSAL_METHOD': 'rpn',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': 'stage2',
           'SNAPSHOT_ITERS': 10000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': True}
Loaded dataset `tattoo_train` for training
Set proposal method: rpn
Appending horizontally-flipped training examples...

loading /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_rpn_stage2_iter_40000_proposals.pkl
375
[[ 160.          191.          227.          248.        ]
 [   0.           20.80112457  195.62976074  499.375     ]
 [   0.           84.97673035  154.97717285  499.375     ]
 ..., 
 [ 156.30316162   59.24116898  270.25860596  116.50565338]
 [ 235.28125     328.51275635  329.52139282  499.375     ]
 [ 261.21340942  302.16671753  356.30908203  347.28076172]]
500
[[ 208.          164.          227.          183.        ]
 [ 212.77215576  167.34562683  229.24777222  184.81297302]
 [ 179.61387634   26.15167046  386.6116333   331.44665527]
 ..., 
 [ 167.82814026  106.07542419  211.71583557  144.52801514]
 [ 250.5978241    40.64997101  326.20962524  231.43968201]
 [ 259.95385742  187.25186157  293.79165649  233.82852173]]
333
[[  26.          294.          176.          425.        ]
 [ 147.          378.          265.          435.        ]
 [ 252.          309.          290.          352.        ]
 ..., 
 [ 203.95999146  145.60446167  331.25189209  276.06661987]
 [ 108.38999939  424.56890869  239.01280212  459.36437988]
 [ 165.80833435  273.33203125  236.19993591  305.03045654]]
333
[[  19.          224.           82.          356.        ]
 [ 250.          235.          293.          291.        ]
 [ 225.          299.          264.          339.        ]
 ..., 
 [ 167.79837036   30.7264843   332.44500732   95.96189117]
 [  60.79713821  232.04634094  138.24736023  286.96737671]
 [ 172.14454651  466.3236084   203.8936615   496.74731445]]
375
[[  95.           60.          259.          452.        ]
 [   0.          117.26963043  184.20869446  499.375     ]
 [  97.13577271   34.22660828  318.18731689  499.375     ]
 ..., 
 [ 339.66421509  103.64379883  374.375       159.09373474]
 [ 146.89886475    0.          235.13845825   27.10714531]
 [  65.26700592  109.11199188  165.9417572   146.60102844]]
500
[[ 138.          225.          343.          342.        ]
 [ 131.            0.          333.          193.        ]
 [ 122.03054047    0.          358.18212891  334.27023315]
 ..., 
 [ 448.95294189   72.23790741  490.68847656  121.99775696]
 [   0.          101.01346588  218.61700439  210.72015381]
 [ 198.31660461   48.44820786  257.22140503  117.16934967]]
375
[[ 160.          122.          261.          231.        ]
 [ 131.71257019    0.          322.4883728   411.15789795]
 [ 156.4241333     0.          355.71981812  330.23928833]
 ..., 
 [   0.            0.          123.33985901  459.3734436 ]
 [ 339.77597046    0.          374.375       174.41564941]
 [  16.32442856  268.76016235   46.92016602  305.7175293 ]]
375
[[  36.           83.          307.          453.        ]
 [ 147.66540527    0.          323.39450073  499.375     ]
 [  53.12707901    0.          279.02114868  428.12335205]
 ..., 
 [  11.63057899  377.13772583   29.99637032  404.0798645 ]
 [  33.63241196  168.5512085   129.70965576  277.66705322]
 [  61.32197189  259.56420898  133.34851074  376.94012451]]
333
[[   0.           17.          177.          476.        ]
 [ 140.          198.          189.          400.        ]
 [ 159.           49.          301.          219.        ]
 ..., 
 [ 131.522995    193.84648132  149.8817749   209.45094299]
 [  51.28636932  305.23007202  138.05751038  409.5380249 ]
 [  48.70411301  237.97294617   82.77351379  285.20083618]]
333
[[  10.           78.          312.          465.        ]
 [   0.           42.30773926  179.86564636  499.5       ]
 [ 158.41215515    0.          332.44500732  414.76052856]
 ..., 
 [ 212.50941467  290.57553101  259.08374023  338.82516479]
 [ 104.70920563   95.63923645  261.2562561   149.57385254]
 [  20.0185585    50.78259277   67.39154053  111.83737183]]
376
[[ 100.           88.          285.          425.        ]
 [  30.86315155    0.          333.16610718  343.3430481 ]
 [  73.78852844  192.44833374  280.14245605  499.45330811]
 ..., 
 [ 248.26383972    0.          375.37332153   47.48931503]
 [  18.11642838  127.94481659  125.66049957  179.13929749]
 [ 272.62588501  254.57067871  298.63198853  288.42132568]]
333
[[  49.            0.          285.          499.        ]
 [ 265.            0.          332.          136.        ]
 [  65.26660919   96.62680817  322.3414917   499.5       ]
 ..., 
 [   1.34919643  397.63021851   78.52983856  442.36450195]
 [   0.            0.          107.07550812  294.38647461]
 [ 255.54374695  119.68921661  285.32406616  146.02320862]]
333
[[  24.           32.          317.          471.        ]
 [  22.33657265    0.          214.96862793  499.5       ]
 [  40.18487549    0.          194.94064331  432.49813843]
 ..., 
 [  26.1917038   276.21096802  143.4289856   385.2227478 ]
 [ 184.55317688  246.20161438  258.31668091  314.97473145]
 [  84.73706818  275.58895874  116.78030396  314.31567383]]
376
[[  5.70000000e+01   1.45000000e+02   3.09000000e+02   4.17000000e+02]
 [  1.22476936e+02   0.00000000e+00   3.30372192e+02   4.99453308e+02]
 [  5.70596161e+01   1.65776703e+02   3.67001129e+02   3.94573639e+02]
 ..., 
 [  1.42806046e+02   1.65672180e+02   1.87511734e+02   2.01461945e+02]
 [  2.99970180e-01   2.78933624e+02   1.29108337e+02   4.08572296e+02]
 [  8.07773304e+00   2.71084198e+02   1.25897385e+02   3.39987030e+02]]
316
[[  50.           28.          277.          470.        ]
 [  20.70120811   48.76642609  232.30177307  499.27999878]
 [ 149.55108643    0.          315.47332764  467.75463867]
 ..., 
 [ 222.42469788  432.48477173  257.28915405  468.76171875]
 [ 132.32804871  198.75883484  169.82640076  258.7538147 ]
 [  17.49209404  118.66493988   33.97878265  136.38354492]]
500
[[ 254.           88.          306.          248.        ]
 [ 263.75366211    0.          499.66000366  331.44665527]
 [   0.           88.989151     92.66338348  331.44665527]
 ..., 
 [ 259.12002563  246.29708862  279.34591675  279.56484985]
 [ 103.23013306   28.8503685   123.25569153   47.53559494]
 [ 291.14318848  245.82933044  365.39160156  291.94073486]]
333
[[ 167.          378.          268.          453.        ]
 [ 270.          211.          287.          245.        ]
 [ 104.          209.          273.          293.        ]
 ..., 
 [   0.          126.8746109    55.53823471  277.5213623 ]
 [ 160.21722412  210.66893005  227.35572815  263.37792969]
 [ 275.67559814  113.94748688  296.95401001  134.54203796]]
500
[[ 142.          153.          253.          276.        ]
 [ 256.          212.          265.          227.        ]
 [ 250.          202.          302.          287.        ]
 ..., 
 [ 347.88198853   97.86712646  467.18566895  160.84739685]
 [  50.80598831  260.26019287  282.07284546  352.04574585]
 [ 444.70339966   44.83148956  499.375       200.45980835]]
500
[[ 192.          136.          366.          281.        ]
 [ 166.5271759     0.          260.93972778  395.33999634]
 [ 171.29336548   21.57099152  331.43444824  395.33999634]
 ..., 
 [  43.76364899  360.19076538   69.52391052  385.02868652]
 [ 122.52702332  308.0105896   141.3341217   332.83758545]
 [ 476.54382324    1.31517613  499.61999512  191.87501526]]
500
[[ 133.            0.          231.           64.        ]
 [ 278.            0.          314.           76.        ]
 [ 304.           37.          370.           67.        ]
 ..., 
 [ 398.14758301   54.29626083  437.72003174   88.12928772]
 [ 380.1539917   308.90814209  402.40475464  342.54000854]
 [  73.02361298   33.60335159  361.10568237  252.02990723]]
500
[[ 184.           98.          307.          228.        ]
 [ 160.99118042   73.48429871  290.51202393  332.44500732]
 [ 203.40278625   67.47811127  336.2779541   332.44500732]
 ..., 
 [ 479.42816162   66.33771515  499.5          95.58586884]
 [ 454.37918091  280.15466309  470.10794067  296.87039185]
 [ 433.43280029   79.20359039  453.26077271  104.17599487]]
500
[[ 203.           74.          277.          148.        ]
 [ 210.27201843   67.54096222  276.88156128  141.34277344]
 [ 202.54246521   76.73399353  283.9850769   146.06817627]
 ..., 
 [ 231.48168945  247.16685486  249.57707214  263.31546021]
 [ 187.36636353  154.23332214  224.16108704  185.4415741 ]
 [ 403.03451538   31.92061424  431.24761963   64.61891174]]
375
[[ 197.          274.          324.          467.        ]
 [ 154.80661011  170.22850037  322.90298462  499.375     ]
 [ 127.59444427  135.12736511  374.375       499.375     ]
 ..., 
 [  60.20829773  188.81759644   81.27230835  211.45732117]
 [ 158.14382935    0.          250.77108765  229.9017334 ]
 [ 179.96702576  153.07339478  200.0055542   168.5458374 ]]
500
[[ 240.          147.          285.          308.        ]
 [ 238.61355591  107.572052    273.30026245  332.44500732]
 [ 241.14079285  168.84269714  278.86532593  332.44500732]
 ..., 
 [  64.62691498  162.28797913   98.48921204  195.14100647]
 [  58.93320847  112.15802765  148.98403931  332.44500732]
 [  95.54754639  105.11860657  203.90483093  158.05252075]]
375
[[   0.          234.          155.          332.        ]
 [ 141.          122.          211.          210.        ]
 [  99.          332.          182.          497.        ]
 ..., 
 [ 164.0942688   369.76965332  263.30502319  499.375     ]
 [ 137.97593689  127.06896973  274.90582275  166.06491089]
 [ 194.50332642  211.42405701  273.70397949  436.93881226]]
500
[[ 280.          210.          339.          273.        ]
 [ 309.89379883  219.94799805  343.44497681  336.31100464]
 [ 282.44989014  217.86309814  335.44192505  265.97540283]
 ..., 
 [ 299.02911377  251.14042664  445.45742798  374.375     ]
 [ 420.96807861  350.39892578  447.72171021  374.375     ]
 [ 255.65306091  343.34442139  284.13009644  374.375     ]]
500
[[  49.           71.           79.          116.        ]
 [  67.          113.           94.          153.        ]
 [  49.          206.           72.          229.        ]
 ..., 
 [ 163.77877808  253.28283691  254.63496399  303.75500488]
 [  45.88896179  104.56359863  248.77824402  184.44099426]
 [   0.          307.63647461   16.7748394   332.44500732]]
500
[[  99.           28.          430.          344.        ]
 [  10.99441528    0.          264.8812561   374.375     ]
 [ 230.55236816    0.          439.17205811  326.27227783]
 ..., 
 [ 362.85681152  106.26506042  401.12350464  159.20050049]
 [ 374.93560791    0.          499.375        44.20199203]
 [  74.7504425   181.3374939   242.73072815  250.95671082]]
413
[[  76.          302.          250.          471.        ]
 [   0.            0.          336.90197754  499.04165649]
 [ 124.11029816  195.49610901  312.29486084  499.04165649]
 ..., 
 [ 229.68774414  135.21492004  254.05950928  152.10990906]
 [ 101.18466949  133.49401855  388.8734436   246.8662262 ]
 [ 328.45965576   73.18903351  379.40100098  124.86038971]]
500
[[  53.          156.          147.          264.        ]
 [ 141.          179.          194.          263.        ]
 [ 193.          180.          248.          269.        ]
 ..., 
 [ 121.07205963   64.87793732  150.29664612   93.91986084]
 [ 274.3270874   110.47425842  290.61627197  132.19963074]
 [ 190.12103271  236.92964172  223.69555664  271.72903442]]
500
[[  17.           25.          458.          207.        ]
 [ 280.20724487    0.          465.00692749  226.5       ]
 [ 248.46229553    0.          499.5         152.7283783 ]
 ..., 
 [  61.18304825  180.60510254   97.07363892  198.71688843]
 [  86.32672882  180.63911438  112.72200775  203.04136658]
 [ 342.76119995  112.21595764  417.61624146  185.50398254]]
500
[[  80.           92.          371.          377.        ]
 [  41.17178726    0.          348.37103271  404.32501221]
 [  65.61005402   31.13752747  284.14239502  404.32501221]
 ..., 
 [ 331.71710205  366.49319458  499.50003052  404.32501221]
 [ 338.00305176  328.2286377   364.30737305  362.12860107]
 [ 128.86804199    6.24012327  166.06330872   26.16324997]]
375
[[  97.           70.          292.          498.        ]
 [   0.            0.          317.35742188  499.375     ]
 [ 113.75235748    0.          344.36773682  499.375     ]
 ..., 
 [ 133.67900085   75.11989594  154.70626831   98.89461517]
 [  84.83521271  109.09835815  108.67861938  164.9850769 ]
 [ 270.62466431   75.32993317  295.8901062   103.76850128]]
320
[[ 156.          148.          202.          183.        ]
 [ 163.05580139  152.93318176  198.51712036  187.82112122]
 [ 134.31326294    0.          319.6000061   206.3828125 ]
 ..., 
 [   0.87435609  193.01785278   67.58380127  233.13522339]
 [ 170.57705688  197.48880005  210.77018738  221.03579712]
 [ 135.63000488   70.97009277  205.50708008  103.4304657 ]]
500
[[  99.           21.          362.          345.        ]
 [ 104.78231049    0.          494.54656982  374.375     ]
 [ 148.64942932    0.          388.21517944  374.375     ]
 ..., 
 [   0.          315.34796143  129.59909058  374.375     ]
 [ 119.40680695  116.62198639  163.22711182  154.765625  ]
 [ 476.56506348  135.07533264  499.375       172.90916443]]
500
[[ 374.          122.          431.          203.        ]
 [  50.          128.          110.          200.        ]
 [ 326.5402832     0.          499.11001587  380.36502075]
 ..., 
 [  21.31557846  166.7905426    44.58300781  212.47442627]
 [  55.26986313  153.54328918   72.28096008  170.85263062]
 [  24.39820671    0.          116.91690063  257.9281311 ]]
375
[[  51.          164.          316.          480.        ]
 [ 273.          330.          297.          371.        ]
 [ 278.          376.          304.          451.        ]
 ..., 
 [ 329.99145508  316.31552124  360.97091675  369.25253296]
 [ 239.40013123  267.24539185  279.48672485  320.81713867]
 [ 242.36312866  461.54110718  374.375       499.375     ]]
480
[[ 180.           78.          407.          115.        ]
 [ 176.          135.          438.          319.        ]
 [ 260.44744873  146.66917419  441.50137329  319.4666748 ]
 ..., 
 [ 194.54788208   10.65454769  230.2933197    41.03611755]
 [  75.46767426  226.93338013  112.33997345  266.8013916 ]
 [ 124.35028839  140.12075806  293.34466553  253.63285828]]
406
[[   0.           41.          405.          499.        ]
 [ 232.7396698    27.32510376  403.44506836  499.37997437]
 [   0.            0.          248.28013611  499.37997437]
 ..., 
 [   0.          389.94332886   75.66784668  442.46279907]
 [ 258.9276123   367.30200195  361.71432495  450.36141968]
 [ 353.65274048  330.03161621  398.04031372  379.23928833]]
500
[[ 216.           59.          403.          267.        ]
 [ 153.51428223    0.          384.51803589  374.375     ]
 [ 134.029953     55.31515121  428.29345703  374.375     ]
 ..., 
 [  51.06315613  273.60632324   70.97907257  297.5413208 ]
 [  70.5503006   164.8218689    89.8792572   183.20532227]
 [ 141.31391907   65.18260956  171.43128967   90.80807495]]
337
[[  88.          193.          158.          353.        ]
 [ 166.          193.          234.          351.        ]
 [ 164.91035461  218.61499023  226.98100281  345.71762085]
 ..., 
 [ 314.99621582  344.02139282  335.68518066  375.53161621]
 [ 170.60765076  244.28959656  189.34796143  262.2333374 ]
 [ 273.95474243  248.08218384  336.43832397  296.11019897]]
500
[[ 159.           46.          337.          349.        ]
 [ 116.55500031    0.          393.1678772   374.375     ]
 [ 185.0007019     0.          340.84945679  374.375     ]
 ..., 
 [ 108.06328583   88.41583252  130.02131653  128.72071838]
 [  20.19523621   66.28830719   46.35017776   97.72511292]
 [  30.58508301  311.69415283  115.62221527  356.04718018]]
500
[[ 155.            7.          406.          331.        ]
 [  96.40190125   19.23479462  345.09741211  321.19238281]
 [ 141.08734131   50.66233063  332.59262085  332.44500732]
 ..., 
 [ 216.37826538   65.670784    322.92050171  107.09862518]
 [ 252.81286621   71.90570068  339.90856934  138.38832092]
 [ 117.24806976  184.62382507  144.03007507  225.12557983]]
375
[[   0.            0.          373.          499.        ]
 [  67.21025085   80.94280243  342.86437988  404.69848633]
 [   0.          120.05065918  236.52404785  485.79623413]
 ..., 
 [ 261.79840088  242.29090881  369.59564209  336.50341797]
 [ 207.49687195  266.29550171  311.41009521  321.90460205]
 [ 162.57173157  102.09661102  200.83123779  158.29322815]]
500
[[ 221.          117.          350.          283.        ]
 [ 188.46238708   14.34593868  379.80526733  345.42333984]
 [ 239.48548889  118.24695587  395.13049316  198.40274048]
 ..., 
 [  31.23223305  322.7472229   140.57972717  345.42333984]
 [ 186.14126587   27.82867241  222.97955322   63.44010162]
 [ 387.69070435   37.08437347  480.10998535   74.55525208]]
500
[[   9.          205.          117.          256.        ]
 [ 123.          180.          259.          232.        ]
 [ 264.          170.          300.          208.        ]
 ..., 
 [ 154.02493286  297.06787109  180.12023926  325.30059814]
 [  34.64326477  213.96026611  127.55094147  374.375     ]
 [ 134.87913513  215.45759583  157.3059845   232.95999146]]
500
[[ 250.           62.          370.          193.        ]
 [ 212.38122559    0.          437.11257935  374.375     ]
 [ 176.73706055    0.          401.06646729  348.25686646]
 ..., 
 [ 231.49295044  218.40101624  254.52052307  242.9670105 ]
 [  65.90359497  189.08969116   86.41304016  209.33779907]
 [  53.77872467  233.12490845  146.48973083  269.80020142]]
500
[[  36.           61.          424.          288.        ]
 [ 123.          235.          249.          299.        ]
 [  26.55885315    0.          348.2958374   332.44500732]
 ..., 
 [ 276.8855896   192.76249695  293.10525513  226.58752441]
 [ 382.74017334   74.78170776  404.30667114   92.65325928]
 [ 218.81037903   44.11699677  344.97702026  282.41854858]]
294
[[  55.          186.          198.          335.        ]
 [  52.10679626   24.44046021  226.82429504  499.5       ]
 [  60.94680023  114.7618866   210.08203125  434.31787109]
 ..., 
 [ 130.99833679  457.98608398  168.32194519  499.5       ]
 [ 128.34259033  436.22003174  159.66915894  463.55053711]
 [   0.          460.24777222   41.44493866  499.5       ]]
500
[[  27.          122.           69.          168.        ]
 [ 181.           24.          232.          114.        ]
 [ 230.           52.          259.          114.        ]
 ..., 
 [ 405.93490601  152.62646484  451.73983765  209.37176514]
 [ 175.09507751  228.10404968  204.32267761  274.57907104]
 [ 477.16506958  261.22427368  497.54711914  317.50732422]]
333
[[   1.           40.          324.          463.        ]
 [   0.            0.          195.83843994  398.1227417 ]
 [   9.64817238    0.          231.14079285  499.5       ]
 ..., 
 [  25.35601997   51.65852737  111.71257019  175.32850647]
 [ 283.7147522   146.13922119  307.9178772   196.15052795]
 [ 170.0304718   428.06060791  265.14077759  477.52487183]]
333
[[ 142.          171.          161.          187.        ]
 [ 107.          150.          164.          231.        ]
 [  93.          177.          103.          191.        ]
 ..., 
 [ 182.80073547  306.77893066  276.25723267  343.93222046]
 [   0.            0.           59.87466049   43.57833862]
 [ 201.75172424  454.86227417  219.39405823  476.41357422]]
500
[[ 179.          175.          321.          246.        ]
 [ 163.          215.          175.          234.        ]
 [ 151.          192.          167.          213.        ]
 ..., 
 [ 113.27773285  293.02908325  131.13244629  311.70101929]
 [ 335.65765381  301.50540161  499.66000366  331.44665527]
 [  39.22734833  169.01864624   56.38092041  188.45225525]]
375
[[  98.           42.          246.          209.        ]
 [  59.          103.           91.          265.        ]
 [ 276.           79.          330.          225.        ]
 ..., 
 [  41.21460724  374.31341553   69.16951752  406.49679565]
 [  16.07163239  162.02354431   39.68500137  198.24913025]
 [ 169.15391541  394.63809204  319.48925781  499.375     ]]
375
[[ 125.          297.          225.          430.        ]
 [  86.           50.          130.          118.        ]
 [ 268.           24.          307.          156.        ]
 ..., 
 [ 245.05651855  453.55395508  323.50006104  494.05667114]
 [ 189.66436768  270.08920288  209.00869751  291.11779785]
 [ 269.82479858  273.7616272   374.375       400.55889893]]
375
[[  72.          140.          233.          280.        ]
 [   8.77891541    0.          248.07162476  444.10131836]
 [ 201.36491394    0.          374.375       317.00106812]
 ..., 
 [ 277.41213989  384.99768066  374.375       422.34768677]
 [ 245.82359314   43.15447235  313.12686157   99.18164825]
 [ 167.00389099  125.41858673  273.33880615  184.54124451]]
500
[[ 217.          123.          278.          193.        ]
 [ 208.6566925   125.83441162  275.94125366  197.39698792]
 [ 215.19863892  112.99847412  276.62942505  184.32836914]
 ..., 
 [  15.74931526   80.98109436  150.24485779  263.90982056]
 [ 117.12450409  145.99465942  137.03222656  164.931427  ]
 [ 233.71438599  111.6815033   252.14086914  129.54962158]]
375
[[  45.           99.          195.          398.        ]
 [ 194.          178.          335.          412.        ]
 [ 128.41616821  107.3273468   374.375       499.375     ]
 ..., 
 [  33.74396896  402.34967041   73.07905579  434.70516968]
 [  21.5286541   198.65997314  111.01050568  449.52056885]
 [ 151.92529297   33.09253693  187.53691101   80.37036896]]
500
[[ 146.           34.          300.          303.        ]
 [  99.17922974    0.          329.9107666   374.375     ]
 [ 128.68769836    0.          382.52658081  335.55221558]
 ..., 
 [ 245.89904785  192.74925232  273.45904541  231.32006836]
 [ 419.16705322  176.7792511   442.03189087  199.5038147 ]
 [ 419.69955444  246.41137695  444.24545288  263.9034729 ]]
373
[[  40.           40.          318.          465.        ]
 [   0.           93.08841705  230.39137268  499.19833374]
 [  87.60678864    0.          321.25668335  409.24465942]
 ..., 
 [  49.23313522  466.08764648  120.08896637  497.96832275]
 [  57.05898285  405.24227905   91.86995697  466.06964111]
 [ 252.25093079   59.78387833  320.10845947  140.59840393]]
353
[[   6.            2.          335.          485.        ]
 [   0.            0.          217.2122345   499.49499512]
 [  63.0596199    58.65996552  261.9069519   314.5982666 ]
 ..., 
 [ 192.88600159   10.59013081  338.58804321   49.7793541 ]
 [ 231.80645752   28.45625114  345.5425415    96.48886108]
 [ 284.92178345  179.43969727  305.70730591  228.37478638]]
333
[[ 156.          379.          230.          497.        ]
 [ 113.          262.          165.          328.        ]
 [ 115.          151.          188.          238.        ]
 ..., 
 [ 294.46282959  166.57420349  332.44500732  219.33436584]
 [ 240.60458374  262.05438232  273.58068848  294.63397217]
 [ 148.89370728  386.80255127  174.04031372  410.83462524]]
271
[[  13.           52.          241.          493.        ]
 [  21.72247314    0.          267.81399536  499.5       ]
 [  27.35234833  148.28678894  257.48010254  499.5       ]
 ..., 
 [ 210.95071411  129.3000946   231.81497192  150.41993713]
 [   0.          401.05844116  140.51750183  492.1569519 ]
 [ 151.30412292  454.26568604  185.99098206  478.0524292 ]]
375
[[  70.            0.          301.          461.        ]
 [  25.          246.          101.          359.        ]
 [  42.59835052    0.          279.15426636  386.77542114]
 ..., 
 [  16.74980164  252.25112915   77.10202789  331.23892212]
 [  59.72750473  305.10089111  156.76313782  499.375     ]
 [  67.18718719   51.14980698   97.75447083   89.66847229]]
500
[[ 207.          106.          331.          242.        ]
 [ 132.05130005    0.          337.66928101  321.91702271]
 [ 205.76843262   92.90618896  360.44760132  374.375     ]
 ..., 
 [  53.6343956   266.31192017   80.9630127   285.20315552]
 [ 259.14935303    0.          326.34106445  127.48672485]
 [ 434.46105957  205.73382568  460.22842407  224.34832764]]
500
[[  62.           14.          333.          338.        ]
 [ 102.07330322    0.          353.10043335  339.43331909]
 [  60.82860947    0.          272.08459473  339.43331909]
 ..., 
 [  71.81700134   94.35500336  198.43522644  158.98582458]
 [ 314.65274048   84.15512848  345.06561279  115.12039185]
 [ 321.03039551  247.34944153  342.85818481  269.17047119]]
333
[[  83.           88.          211.          401.        ]
 [ 115.90319824  174.35131836  188.50271606  471.32373047]
 [ 104.08297729  153.65234375  222.04605103  463.6829834 ]
 ..., 
 [ 155.85664368   55.77131271  175.69932556   79.43885803]
 [ 214.38719177  362.00338745  299.12188721  395.76922607]
 [ 203.14944458  196.45404053  226.55648804  234.63877869]]
316
[[ 114.          232.          201.          255.        ]
 [ 125.          199.          200.          241.        ]
 [ 116.66288757  204.66444397  193.45541382  242.29968262]
 ..., 
 [  34.34396744  165.11894226   60.75276947  216.0135498 ]
 [  18.51545906   32.96113205   42.21839142   82.37619781]
 [ 257.43728638  317.04605103  285.70748901  343.72860718]]
375
[[  68.           82.          277.          441.        ]
 [  43.64767075    0.          294.42422485  448.40621948]
 [  13.84627342    0.          274.81228638  319.36056519]
 ..., 
 [ 239.41644287  317.46908569  294.88980103  375.89941406]
 [ 301.54364014  122.99937439  317.61434937  148.92268372]
 [   0.          395.67486572   19.28056526  474.3822937 ]]
333
[[  33.           53.           92.          394.        ]
 [ 138.           49.          275.          448.        ]
 [ 264.          451.          280.          472.        ]
 ..., 
 [  82.31010437  135.28392029  236.27853394  327.56542969]
 [ 227.42086792  193.26513672  320.76101685  262.54174805]
 [   2.04216528   34.16786194   46.75421906   75.03131866]]
500
[[ 178.          116.          299.          289.        ]
 [ 192.           79.          226.          116.        ]
 [ 229.           69.          270.          111.        ]
 ..., 
 [ 275.45278931   50.17593384  362.16107178   96.94117737]
 [ 398.71038818  173.8237915   495.94152832  230.7540741 ]
 [  34.20895004  109.89772034   59.83105087  137.9788208 ]]
500
[[ 176.          189.          242.          241.        ]
 [ 175.22810364  177.32244873  242.02528381  218.61700439]
 [ 184.1255188   193.19680786  240.96865845  234.34667969]
 ..., 
 [ 379.23907471  334.17431641  401.51913452  366.30770874]
 [  76.1381073    94.42883301  159.30218506  143.61582947]
 [ 393.05578613  265.2557373   432.94332886  302.38656616]]
500
[[ 204.          142.          302.          237.        ]
 [ 176.48744202   13.24032688  320.43249512  332.44500732]
 [ 221.40760803    0.          386.49423218  332.44500732]
 ..., 
 [  88.91960144  239.65744019  107.23277283  260.3053894 ]
 [   0.           25.87471008   22.22149849   54.28353119]
 [ 268.86495972   94.70435333  347.20532227  136.51220703]]
500
[[ 381.          100.          448.          240.        ]
 [ 336.78408813    0.          499.375       374.375     ]
 [ 277.09295654    0.          499.375       348.56506348]
 ..., 
 [ 116.26927948    0.          252.2951355   118.12134552]
 [ 372.30636597  124.49082947  482.01208496  276.13659668]
 [  70.34583282   89.95713806  118.02613068  296.12249756]]
334
[[  28.            5.          297.          484.        ]
 [  63.74470139    0.          285.25985718  324.74682617]
 [  25.74337006    0.          323.73590088  279.42211914]
 ..., 
 [ 114.31893921   29.6445446   150.75772095   89.63140869]
 [ 213.65452576  261.64709473  262.68508911  324.45306396]
 [ 135.74052429    0.49145138  189.40423584   22.58898354]]
375
[[  87.          220.          195.          411.        ]
 [   0.            0.          212.3039856   352.25268555]
 [   0.            0.          184.48130798  270.14941406]
 ..., 
 [ 109.21192169  159.07112122  140.93623352  198.53713989]
 [ 170.19015503  162.86772156  282.22280884  211.00457764]
 [ 177.66677856   67.47374725  266.80279541  137.30053711]]
250
[[  31.           13.          222.          476.        ]
 [   0.           82.18203735  179.2020874   499.5       ]
 [   0.            0.          177.57632446  392.5229187 ]
 ..., 
 [  82.39717865   37.32661438  249.5         165.51229858]
 [  51.63326263  418.26052856   86.01659393  455.93551636]
 [   0.          215.52160645   23.49474907  262.98953247]]
333
[[   5.          194.          194.          496.        ]
 [   0.           91.22990417  235.54522705  499.5       ]
 [   0.           11.72420597  265.88345337  462.15325928]
 ..., 
 [ 205.45249939  223.59921265  302.00131226  380.18740845]
 [ 128.52848816  166.86763     159.66383362  196.18560791]
 [ 258.16305542  378.16589355  285.04376221  419.28482056]]
305
[[  16.            4.          301.          489.        ]
 [  71.80580902   15.60630417  208.90435791  276.56045532]
 [  65.55962372    0.          152.86346436  231.68745422]
 ..., 
 [ 257.63040161  283.65057373  287.31072998  307.17807007]
 [  69.51325226    0.          189.50505066  357.9793396 ]
 [   0.          420.015625     28.51233101  479.63012695]]
143
[[  10.           10.          136.          480.        ]
 [   0.            0.          142.5         477.81658936]
 [   0.           48.6934967   142.5         360.45794678]
 ..., 
 [  18.98973083  440.26098633  114.28129578  499.5       ]
 [   0.          387.2833252    55.81190109  415.18212891]
 [  33.8482666   482.65435791  142.23570251  499.5       ]]
500
[[ 208.          154.          288.          211.        ]
 [ 142.          133.          200.          189.        ]
 [ 231.81599426  162.96096802  435.86630249  332.44500732]
 ..., 
 [ 362.75302124  142.54859924  395.17092896  164.83589172]
 [ 284.86779785  284.03692627  364.04690552  332.44500732]
 [ 173.53140259  143.26454163  195.20289612  170.99983215]]
217
[[   5.            1.          215.          446.        ]
 [  53.43773651    0.          209.78909302  261.15896606]
 [  38.01927567    0.          216.50799561  186.18547058]
 ..., 
 [ 140.34996033   98.08083344  159.73934937  126.55822754]
 [  56.35990143  289.59729004  102.37892151  332.86865234]
 [   2.30920625  299.61828613   42.01663208  355.93624878]]
333
[[  22.           71.          265.          376.        ]
 [  14.0132246    22.70662689  295.16339111  353.58828735]
 [  22.26569939    0.          319.95132446  499.5       ]
 ..., 
 [ 260.83105469  170.15827942  287.53723145  206.85792542]
 [ 132.7366333   306.05303955  187.32012939  380.33306885]
 [  41.36808395  136.33062744  144.80743408  189.28556824]]
182
[[   4.            0.          175.          486.        ]
 [   0.            0.          181.5         499.5       ]
 [   0.          204.0062561   181.5         422.61477661]
 ..., 
 [ 134.00643921   55.5241394   181.5         179.56491089]
 [ 152.77418518  291.19848633  169.38691711  499.5       ]
 [   0.          387.13928223   57.59863281  452.54101562]]
375
[[  54.           24.          278.          464.        ]
 [ 117.91572571   37.3307991   318.10910034  489.35958862]
 [   0.            0.          224.68688965  499.375     ]
 ..., 
 [ 225.15513611  452.30749512  244.8865509   473.53683472]
 [   1.42652869  149.09182739   30.44870758  184.45922852]
 [   0.          149.89390564   20.13856125  207.38665771]]
600
[[ 187.          162.          289.          249.        ]
 [ 198.70487976  135.41763306  306.96395874  237.26634216]
 [ 237.83299255  206.61991882  271.5632019   268.37826538]
 ..., 
 [   0.          154.46311951   84.88716888  213.32159424]
 [ 469.86941528  216.80091858  552.28369141  312.3885498 ]
 [ 131.39146423  278.35986328  159.94761658  308.51586914]]
338
[[ 176.            0.          300.          225.        ]
 [ 130.71058655    0.          337.43667603  411.47006226]
 [ 167.89154053    0.          337.43667603  296.73956299]
 ..., 
 [ 225.1471405   307.32861328  270.82336426  366.11795044]
 [ 149.25299072    9.90903759  178.78855896   57.8024826 ]
 [ 194.66622925   44.60330582  238.125        88.53921509]]
333
[[  49.           35.          163.          174.        ]
 [  58.74717331    0.          209.95323181  248.58499146]
 [  48.73989868   46.15013885  172.91253662  160.69136047]
 ..., 
 [ 308.61410522  165.0721283   324.65237427  191.46903992]
 [ 114.43144226   20.3546772   185.97840881   50.33203125]
 [ 153.0512085    91.35093689  308.80685425  148.37364197]]
291
[[  26.            0.          258.          499.        ]
 [  11.4039917     0.          281.47351074  499.5       ]
 [  27.81962585  181.97634888  261.94354248  499.5       ]
 ..., 
 [ 232.75234985  194.54856873  290.5         309.76654053]
 [ 228.61853027  232.16564941  265.34960938  274.6529541 ]
 [  95.33808136   71.39240265  150.72454834  132.81349182]]
375
[[ 136.          347.          210.          423.        ]
 [ 191.          227.          226.          249.        ]
 [ 222.          218.          232.          232.        ]
 ..., 
 [ 297.27645874   36.72052765  324.1418457    77.84490204]
 [ 222.5459137   284.72589111  317.66400146  345.49383545]
 [ 100.259758    223.48803711  261.12182617  396.49450684]]
333
[[  14.          337.           96.          427.        ]
 [ 119.40853119   68.35944366  332.44500732  499.5       ]
 [ 154.40429688  161.80873108  332.44500732  499.5       ]
 ..., 
 [   0.           97.65869141   42.69789124  360.61349487]
 [   0.          461.72521973  150.78546143  499.5       ]
 [ 219.56488037   89.1907959   332.44500732  141.36978149]]
297
[[ 155.          215.          191.          272.        ]
 [ 155.61479187  222.68061829  196.77432251  294.42739868]
 [ 150.8366394   239.39027405  187.43769836  295.74728394]
 ..., 
 [  31.21132469   82.58148956   53.39916229  106.46891022]
 [ 194.86254883   91.94187927  267.02957153  291.2046814 ]
 [ 171.97268677  375.13220215  188.12802124  392.68783569]]
275
[[  36.           42.          227.          465.        ]
 [   0.            7.44061279  201.03018188  499.5       ]
 [  38.02885437  153.09675598  243.06666565  499.5       ]
 ..., 
 [  56.69621277  209.25967407  113.4316864   279.5246582 ]
 [  21.00893211  234.92678833   43.20323181  259.42059326]
 [  65.84694672  284.94299316  115.87580109  368.72772217]]
360
[[ 129.           76.          255.          218.        ]
 [ 106.94031525   38.28319168  249.73693848  269.54998779]
 [ 138.34587097   68.48026276  253.56988525  269.54998779]
 ..., 
 [ 209.30943298   41.20726395  350.53192139  128.96549988]
 [ 239.07344055   92.82707214  331.21551514  188.19677734]
 [  49.34631348   53.41253281  133.07800293   96.24073792]]
500
[[ 148.           47.          378.          254.        ]
 [ 111.72885132    0.          398.27493286  331.44665527]
 [ 130.4584198     0.          382.73867798  229.70584106]
 ..., 
 [   0.           82.01303864  111.30901337  185.27919006]
 [   0.          288.32873535  217.26280212  331.44665527]
 [  63.13526154  298.53335571  136.89746094  331.44665527]]
500
[[ 253.          133.          375.          273.        ]
 [ 224.69042969   49.73495483  499.375       374.375     ]
 [ 248.66983032  205.60725403  499.375       374.375     ]
 ..., 
 [ 151.50146484   32.1871109   199.05914307   86.58963013]
 [ 268.60906982  154.19813538  290.72714233  175.75282288]
 [ 451.3192749   154.75717163  499.375       295.58355713]]
375
[[ 213.           64.          319.          167.        ]
 [ 199.63465881    0.          374.375       327.30917358]
 [ 219.75624084   72.99943542  319.11212158  171.87361145]
 ..., 
 [   0.          204.12593079  170.20913696  305.99468994]
 [ 319.55172729   67.13414764  374.375       113.0821991 ]
 [ 145.37615967  169.66410828  178.38793945  200.08155823]]
333
[[  46.          113.          319.          396.        ]
 [  84.08664703    4.99652433  332.44500732  499.5       ]
 [  26.6594696     0.          242.48442078  499.5       ]
 ..., 
 [  64.12284088  109.27818298   87.5630722   129.39065552]
 [ 251.16773987  351.56573486  330.79705811  402.80291748]
 [  40.8094635   357.04159546  271.18728638  452.35720825]]
352
[[ 119.          138.          248.          287.        ]
 [  90.78250885    5.65920448  252.90751648  287.52001953]
 [  61.43603134    0.          299.28826904  287.52001953]
 ..., 
 [   0.          268.72775269  187.83943176  287.52001953]
 [ 216.39331055    0.          314.42749023   53.10846329]
 [  38.8002739   194.22926331  133.93858337  281.23303223]]
500
[[ 170.          130.          181.          145.        ]
 [ 182.          141.          194.          159.        ]
 [ 196.          136.          207.          151.        ]
 ..., 
 [ 461.96514893  135.8301239   499.61334229  285.15374756]
 [ 126.55440521  214.3936615   148.3298645   239.5184021 ]
 [ 400.54925537   45.1595459   488.23062134   98.33441925]]
500
[[ 359.          112.          375.          135.        ]
 [ 376.          125.          393.          145.        ]
 [ 365.37792969  110.39453125  397.45196533  136.89379883]
 ..., 
 [ 282.15734863  206.10864258  311.4526062   229.2305603 ]
 [  46.05003357  334.69482422   71.32327271  353.69955444]
 [  98.82291412  164.21897888  170.35157776  187.50193787]]
375
[[ 215.          243.          233.          264.        ]
 [ 151.6427002    72.1295929   374.375       499.375     ]
 [ 180.40640259  174.83538818  374.375       499.375     ]
 ..., 
 [  51.32529068  342.39727783  124.27489471  364.86843872]
 [  46.86779785  188.94418335  173.02081299  224.45619202]
 [ 183.18183899   43.95937729  205.30253601   63.872612  ]]
332
[[ 108.           20.          290.          498.        ]
 [  58.08739471    0.          275.18795776  415.54812622]
 [ 124.98169708    0.          331.44665527  315.43554688]
 ..., 
 [ 305.79901123    0.          331.44665527   30.71980858]
 [ 202.50071716  297.15658569  291.68652344  348.23986816]
 [  56.4608078   180.01531982  117.45219421  474.39834595]]
378
[[ 126.           76.          268.          183.        ]
 [ 240.18945312   45.89116287  375.78604126  282.52835083]
 [ 169.17245483   88.11582184  377.33334351  282.52835083]
 ..., 
 [  25.81053352  197.13253784   47.38632584  223.092453  ]
 [   0.           29.50750351   19.67742538   66.82288361]
 [ 262.73605347   93.12265015  352.9324646   141.46412659]]
232
[[  14.            3.          229.          494.        ]
 [   4.98733521    0.          230.65103149  292.73834229]
 [   0.            0.          231.5         499.5       ]
 ..., 
 [  24.45822906  268.51300049   59.35909271  324.55438232]
 [   0.           59.37506104  138.18536377  162.80401611]
 [  20.75924873  264.49594116   54.40135956  313.10653687]]
500
[[ 197.           37.          406.          224.        ]
 [ 278.          195.          291.          240.        ]
 [ 222.18489075    0.          463.93041992  332.44500732]
 ..., 
 [ 104.55599976  212.16627502  128.1754303   232.26893616]
 [ 246.1153717     0.          337.04797363  111.67153168]
 [ 214.96809387  260.59713745  232.69438171  277.07376099]]
334
[[ 201.          252.          210.          275.        ]
 [ 187.67533875    0.          333.44332886  380.7149353 ]
 [  31.51934624    0.          268.21105957  263.34637451]
 ..., 
 [ 200.48901367  321.4520874   333.44332886  458.49822998]
 [ 262.73806763  325.80331421  280.76651001  350.62432861]
 [ 213.11572266  281.53009033  238.84416199  310.3237915 ]]
500
[[ 313.          283.          400.          343.        ]
 [ 148.          160.          373.          303.        ]
 [ 156.45201111  166.19662476  361.46484375  309.16430664]
 ..., 
 [   4.86036777  152.73545837  133.10583496  206.54470825]
 [ 184.1028595   255.34156799  207.45819092  274.81994629]
 [ 363.42776489  115.11540985  430.1270752   173.80648804]]
500
[[ 212.          166.          264.          223.        ]
 [  41.47970963    0.          372.97955322  323.07901001]
 [ 221.85502625  171.16851807  265.91442871  224.54864502]
 ..., 
 [ 289.74908447  333.26333618  314.48291016  348.83129883]
 [ 429.5289917   259.43734741  453.40249634  278.51138306]
 [ 236.45623779  305.7197876   257.50140381  322.80645752]]
375
[[ 125.          112.          258.          356.        ]
 [  61.93864822    0.          362.23434448  499.375     ]
 [  60.01070786  221.02496338  195.05271912  499.375     ]
 ..., 
 [  95.86410522  416.66137695  118.23442078  439.84176636]
 [ 228.31240845  375.084198    250.98648071  396.54980469]
 [ 101.02238464   91.60436249  120.40209961  114.35681915]]
500
[[  75.           89.          105.          123.        ]
 [ 109.          174.          256.          218.        ]
 [ 330.          221.          430.          279.        ]
 ..., 
 [ 448.06027222   90.09317017  477.894104    132.09391785]
 [  73.97264099   81.72603607  182.55412292  224.15873718]
 [ 298.9894104   328.06369019  393.7706604   369.65710449]]
339
[[   8.            9.          119.          168.        ]
 [  29.           93.          189.          393.        ]
 [  26.56121635  167.79034424  203.28581238  424.31716919]
 ..., 
 [  83.86173248   74.60496521  136.50030518   97.97018433]
 [ 115.0165329    21.25898743  159.85308838   43.01442719]
 [ 129.45858765   26.72265244  170.58058167   56.46722412]]
339
[[  30.          119.           67.          249.        ]
 [  45.          104.          300.          457.        ]
 [   0.            0.          253.81910706  469.28848267]
 ..., 
 [ 171.95404053   45.93344498  243.55534363  135.96531677]
 [ 227.31716919   54.60555649  300.81469727  127.61613464]
 [  85.75305939  227.58766174  175.77430725  337.50921631]]
500
[[ 107.           46.          400.          355.        ]
 [  73.40314484    0.          427.32443237  374.375     ]
 [  64.23930359    0.          292.83950806  374.375     ]
 ..., 
 [ 120.82814789   14.9973774   250.88514709  200.86204529]
 [   0.          283.28115845  105.34310913  374.375     ]
 [  71.96525574  327.59707642   94.57288361  346.66912842]]
500
[[ 127.           48.          414.          336.        ]
 [ 174.27320862    0.          410.5524292   374.375     ]
 [ 121.63528442    0.          361.42047119  374.375     ]
 ..., 
 [  13.15997314  255.96520996   30.59298325  272.9654541 ]
 [ 388.52819824  159.68890381  433.11453247  209.83035278]
 [ 160.06454468   41.55065536  488.540802    116.95119476]]
500
[[ 145.           10.          403.          321.        ]
 [ 142.66218567   34.71176147  399.0586853   374.375     ]
 [ 138.3815918     0.          475.29211426  374.375     ]
 ..., 
 [ 344.18554688  148.26901245  396.91671753  208.48822021]
 [ 170.28450012   67.59661865  319.74301147  199.47166443]
 [ 336.91589355  184.73588562  409.3057251   250.25453186]]
500
[[ 175.           51.          363.          259.        ]
 [ 140.26423645    0.          300.04605103  373.99127197]
 [ 116.05461884    0.          405.10458374  374.375     ]
 ..., 
 [ 300.63726807  102.54974365  366.57150269  210.33015442]
 [ 435.86981201  253.53303528  468.99230957  290.00152588]
 [ 130.74049377  240.65466309  151.73272705  261.29803467]]
356
[[   5.           20.          338.          479.        ]
 [  68.62064362  112.39862823  316.06256104  490.60244751]
 [   0.            0.          247.76589966  499.58666992]
 ..., 
 [ 320.55163574    0.          355.40667725  139.85823059]
 [ 127.21922302  400.07669067  252.75442505  459.80999756]
 [ 220.04272461  206.11654663  332.40603638  287.39874268]]
375
[[ 103.          162.          256.          355.        ]
 [  28.04533005    0.          355.34765625  499.375     ]
 [  58.6309433     0.          271.09576416  499.375     ]
 ..., 
 [  47.9948616   149.24443054   67.92287445  166.31661987]
 [ 261.38684082  228.53112793  296.94458008  280.72702026]
 [ 141.73245239  480.10165405  216.53062439  499.375     ]]
500
[[   0.           24.          169.          204.        ]
 [  56.          129.          157.          250.        ]
 [ 155.          126.          337.          281.        ]
 ..., 
 [ 361.91448975  103.09669495  381.65911865  125.91598511]
 [ 473.11309814  279.05661011  491.89077759  300.07949829]
 [ 180.69506836  219.56954956  371.12774658  268.28265381]]
500
[[   0.          323.          226.          374.        ]
 [  28.           50.          451.          309.        ]
 [ 148.6734314     0.          405.9737854   374.375     ]
 ..., 
 [   0.          328.89959717   37.20273972  364.09667969]
 [ 402.78302002  337.14477539  427.30450439  366.23129272]
 [  87.13445282  114.79196167  170.04199219  167.67985535]]
448
[[ 113.           84.          298.          225.        ]
 [ 111.55530548    0.          251.47341919  299.5       ]
 [ 119.65244293   48.57955933  228.56469727  299.5       ]
 ..., 
 [   0.          243.31246948   18.03341103  264.1416626 ]
 [ 196.91651917   21.54434204  342.01507568  226.83454895]
 [ 108.79937744   75.46318054  179.44418335  102.23912048]]
500
[[  27.           74.          499.          311.        ]
 [  71.            1.          343.           31.        ]
 [ 413.            0.          498.           45.        ]
 ..., 
 [  56.07390213  167.83580017   92.42040253  196.71160889]
 [ 444.30532837  160.11068726  477.51464844  189.24676514]
 [ 129.03657532    0.          229.41625977  103.97712708]]
487
[[  20.            0.          484.          479.        ]
 [ 166.59838867    0.          486.18832397  499.17498779]
 [   4.30168724    1.44246197  289.90319824  499.17498779]
 ..., 
 [ 143.45204163   69.91353607  276.56137085  154.77568054]
 [  83.39724731   27.05353165  182.77090454  192.30326843]
 [ 147.59013367  465.18942261  213.17393494  490.7522583 ]]
375
[[  38.           65.          331.          416.        ]
 [   0.          116.88233948  261.99020386  499.375     ]
 [   0.          195.10150146  215.97215271  499.375     ]
 ..., 
 [ 282.48995972   70.39337158  301.23425293   93.62625122]
 [  84.83135986  188.27043152  164.81401062  261.11953735]
 [ 277.20678711    7.2630105   310.78759766   29.81642723]]
500
[[ 182.           75.          329.          197.        ]
 [  63.85858536  126.48912048  319.13137817  374.375     ]
 [ 111.94011688    0.          336.27896118  374.375     ]
 ..., 
 [ 270.42144775  319.36846924  345.29421997  374.375     ]
 [ 334.23834229  134.87712097  363.53500366  195.86280823]
 [ 267.46920776   38.71132278  371.18484497   86.3212738 ]]
500
[[ 154.          117.          400.          274.        ]
 [ 160.41511536    7.24687576  445.95129395  374.375     ]
 [ 205.88745117   60.92985153  406.52172852  374.375     ]
 ..., 
 [  93.33660889  285.04394531  132.55596924  332.5151062 ]
 [ 445.61489868  269.81796265  497.95620728  327.21203613]
 [ 474.60543823  301.57345581  491.64172363  328.76681519]]
500
[[  89.          124.          414.          162.        ]
 [  13.92208099  123.0819397   459.52166748  155.9430542 ]
 [  70.6238327   118.87445068  420.38314819  155.2494812 ]
 ..., 
 [ 346.05673218   40.20273209  472.2975769    92.2592926 ]
 [ 226.40219116    0.          258.61502075   23.55369949]
 [ 191.8868103   225.81304932  219.82061768  251.38650513]]
500
[[ 105.           20.          340.          302.        ]
 [ 187.          356.          272.          372.        ]
 [  86.99790192    0.          337.18811035  374.375     ]
 ..., 
 [ 331.51855469  164.71801758  362.83041382  201.98046875]
 [ 410.78005981  118.75051117  452.91378784  146.04904175]
 [  33.94002914  280.04129028   52.91824341  297.59973145]]
398
[[   5.            6.          375.          493.        ]
 [  12.           98.           70.          235.        ]
 [  42.41947556    0.          314.97094727  499.48999023]
 ..., 
 [ 175.84892273   57.98999405  267.20098877  153.38259888]
 [  66.05640411  244.12213135  100.54759216  287.4675293 ]
 [ 337.49880981   47.07274628  366.29281616   75.07360077]]
310
[[  74.          219.          200.          295.        ]
 [ 146.          111.          192.          144.        ]
 [ 135.          168.          196.          205.        ]
 ..., 
 [ 261.61672974  113.95253754  290.19290161  143.17056274]
 [  29.12895584  307.13928223  140.25117493  354.38989258]
 [  98.62063599  119.14848328  145.3120575   169.16073608]]
334
[[   0.          251.           69.          388.        ]
 [  18.           91.          331.          485.        ]
 [   1.34768438    0.          273.55245972  499.32998657]
 ..., 
 [ 209.10754395  174.73104858  317.54605103  260.71990967]
 [ 257.93292236  359.56326294  333.44332886  455.14285278]
 [ 297.26574707  117.58616638  333.44332886  350.53799438]]
352
[[  30.          390.           49.          442.        ]
 [ 239.          287.          304.          340.        ]
 [  64.          206.          272.          474.        ]
 ..., 
 [ 102.15251923  195.83503723  134.79179382  212.26745605]
 [  88.24982452  420.26193237  232.3497467   485.8397522 ]
 [ 242.33724976  390.0809021   262.18685913  409.89987183]]
334
[[  38.          294.           95.          486.        ]
 [  61.          190.          288.          461.        ]
 [   0.            0.          253.19714355  377.48400879]
 ..., 
 [ 231.27108765  160.53596497  333.44332886  292.30728149]
 [ 197.96313477  214.00292969  223.00708008  252.52627563]
 [  95.68502808    0.          143.6348114    21.07562065]]
375
[[  53.           64.          361.          391.        ]
 [   0.            0.          283.5133667   499.375     ]
 [ 134.42781067    9.36489105  357.58529663  499.375     ]
 ..., 
 [ 151.81742859  274.16732788  265.09979248  399.91943359]
 [   0.           37.92191315   94.0187912    85.73499298]
 [  18.61942673  370.09060669   36.63579559  405.3664856 ]]
439
[[   7.            0.          220.          478.        ]
 [ 214.            0.          421.          473.        ]
 [ 196.74565125    0.          438.26834106  498.99667358]
 ..., 
 [ 350.74087524  293.5227356   438.26834106  320.8359375 ]
 [ 230.37762451  346.42276001  251.18621826  363.98199463]
 [   0.          291.54751587   87.33887482  355.42727661]]
286
[[  59.            3.          267.          493.        ]
 [   1.03282166    0.          251.25508118  328.64007568]
 [ 139.98391724   32.11514282  285.5         499.5       ]
 ..., 
 [ 177.34477234  456.93261719  242.76356506  499.5       ]
 [ 225.98161316  201.83694458  285.5         277.43289185]
 [  11.20998383   85.94403839   29.54227066  114.98543549]]
500
[[  56.          238.           85.          283.        ]
 [ 146.          175.          212.          221.        ]
 [ 220.          215.          238.          264.        ]
 ..., 
 [ 468.77008057  149.05422974  499.59002686  225.23513794]
 [ 104.76712799   51.97253036  144.53302002   77.41979218]
 [   0.          149.22650146  117.54071045  205.64204407]]
333
[[  72.          120.          249.          380.        ]
 [ 124.46479797  122.11608887  264.21295166  418.46569824]
 [  84.41053009   26.21211433  262.65853882  399.91687012]
 ..., 
 [  59.45798874  248.50553894   76.84022522  266.69262695]
 [ 122.26430511  455.06130981  195.79905701  495.78070068]
 [  82.90378571  154.8296814   123.3245163   217.8603363 ]]
500
[[ 142.          125.          323.          238.        ]
 [ 106.31652069   45.57607651  288.64682007  374.375     ]
 [ 130.08169556  124.91105652  346.56188965  228.11750793]
 ..., 
 [ 193.45243835  312.46017456  211.5997467   331.17141724]
 [ 174.68232727  218.80133057  277.26690674  273.97225952]
 [ 441.18502808  144.51013184  499.375       194.38467407]]
375
[[  52.           65.          334.          498.        ]
 [ 124.5020752     0.          374.375       499.375     ]
 [ 183.04983521   93.76367188  374.375       499.375     ]
 ..., 
 [ 101.61540222  217.18447876  147.59391785  284.18670654]
 [ 171.06513977  180.68106079  316.63198853  248.77832031]
 [ 120.4914093   315.71453857  145.61880493  337.41262817]]
500
[[  24.          318.          360.          374.        ]
 [ 313.          126.          498.          370.        ]
 [  45.            0.          381.          239.        ]
 ..., 
 [ 134.5897522   239.96791077  153.48413086  258.47320557]
 [ 385.57394409  102.95162201  418.57092285  143.31367493]
 [ 224.47373962   85.78900909  251.09938049  124.23606873]]
375
[[ 128.           85.          374.          337.        ]
 [ 256.          304.          374.          494.        ]
 [ 182.12413025    0.          374.375       499.375     ]
 ..., 
 [ 110.89086151  386.34457397  128.88949585  407.52749634]
 [  66.85194397   45.63962936  107.07397461   93.41067505]
 [ 183.67547607  303.34518433  204.40237427  321.86880493]]
375
[[  12.            7.          297.          498.        ]
 [  69.28102112    0.          264.17199707  499.375     ]
 [  98.69166565    0.          359.54260254  473.67602539]
 ..., 
 [ 334.13235474  223.57429504  371.72473145  263.16421509]
 [ 348.25375366  387.94232178  374.375       464.4100647 ]
 [ 275.12335205  116.28360748  371.24099731  234.31292725]]
333
[[  74.          130.          241.          359.        ]
 [   0.            0.          332.44500732  499.5       ]
 [   0.           59.01676559  282.38400269  438.5413208 ]
 ..., 
 [ 155.09753418  135.67721558  198.68159485  171.45729065]
 [ 297.48452759  168.52563477  332.44500732  213.55599976]
 [ 194.47174072   39.95009232  225.88746643   88.64865875]]
474
[[  95.            8.          366.          232.        ]
 [  68.47717285    0.          291.93991089  278.7119751 ]
 [  93.6044693     0.          318.38415527  225.94662476]
 ..., 
 [  38.7635498   220.40475464   81.43744659  272.98300171]
 [ 255.04222107   43.40000153  334.98471069  106.47605133]
 [  57.69589233  239.20336914   85.45948029  272.98922729]]
398
[[  99.          164.          280.          438.        ]
 [ 101.22650909   66.75424957  237.74388123  415.32122803]
 [  21.61988258   59.53227234  286.59048462  395.40454102]
 ..., 
 [ 226.55484009   36.75823593  397.33666992  149.11546326]
 [  96.55530548   48.02760696  260.93960571  263.61087036]
 [ 206.20843506  247.92233276  359.25704956  397.59277344]]
345
[[ 116.          244.          163.          299.        ]
 [ 199.          300.          226.          333.        ]
 [ 201.65768433  298.95767212  232.46080017  341.34933472]
 ..., 
 [  66.38680267  113.91139221  194.595047    266.10317993]
 [ 202.69732666  211.42547607  252.11276245  258.25457764]
 [ 253.07937622  195.29145813  292.80276489  234.05844116]]
335
[[  57.           51.          265.          485.        ]
 [  75.10637665    0.          267.6897583   399.06533813]
 [ 104.1916275   178.9704895   289.71551514  499.70831299]
 ..., 
 [ 246.92735291  354.15231323  283.39080811  396.86871338]
 [  96.79142761  310.52734375  201.64822388  405.40139771]
 [ 117.05490112   44.2891655   186.53999329   84.72692871]]
375
[[ 103.          140.          305.          436.        ]
 [ 164.81086731  134.32608032  305.23440552  499.375     ]
 [ 144.52467346   28.0080986   334.89746094  499.375     ]
 ..., 
 [ 196.09727478  353.378479    236.25944519  409.25808716]
 [ 171.46774292  103.55092621  192.09902954  119.30142975]
 [  18.29938507  289.86029053   42.18496323  318.28445435]]
500
[[ 113.          185.          135.          214.        ]
 [ 359.          194.          374.          220.        ]
 [ 137.          146.          349.          236.        ]
 ..., 
 [ 191.18727112  139.24952698  284.35211182  176.02711487]
 [ 182.53117371  266.34707642  367.36703491  332.44500732]
 [ 129.46725464  118.42584991  337.26940918  293.21182251]]
375
[[ 112.           36.          301.          482.        ]
 [ 283.           66.          296.           87.        ]
 [  89.6271286    84.98638153  288.4961853   499.375     ]
 ..., 
 [ 259.33084106  175.14292908  343.75213623  231.25933838]
 [ 158.20433044  287.01983643  244.22221375  364.12365723]
 [ 125.43992615   11.65628624  175.02807617   37.2167511 ]]
300
[[  61.           73.          262.          339.        ]
 [  54.81460571   49.24562073  292.41674805  399.5       ]
 [  12.54244232   80.5403595   266.07681274  399.5       ]
 ..., 
 [  75.19961548   48.21111298  142.83654785   84.35149384]
 [ 248.27389526  150.04145813  273.03994751  181.21055603]
 [ 266.87179565  348.0038147   294.48458862  385.89389038]]
500
[[ 302.          123.          337.          150.        ]
 [ 303.1373291   123.71865082  336.64361572  154.44273376]
 [ 285.52029419   88.10691071  475.75006104  332.44500732]
 ..., 
 [  92.19800568  210.456604    132.92817688  239.00318909]
 [ 420.50793457  304.95379639  488.04888916  329.54406738]
 [ 314.81341553   91.30780029  336.33706665  115.31812286]]
500
[[  55.          112.          287.          321.        ]
 [  25.04672432    0.          321.7868042   325.45666504]
 [  23.28081703   50.50147247  272.80722046  318.41171265]
 ..., 
 [ 182.1075592    77.38316345  199.81858826  103.62927246]
 [   0.          200.58389282   94.75956726  300.46459961]
 [ 161.01472473  255.15106201  184.6309967   276.30773926]]
500
[[  85.           78.          448.          223.        ]
 [ 188.24568176  133.18174744  393.63937378  279.69564819]
 [ 148.55461121  135.82928467  368.84643555  263.94015503]
 ..., 
 [ 121.79811096  180.43261719  193.77227783  240.86322021]
 [ 180.98878479  343.0874939   202.24382019  364.22299194]
 [ 401.50534058  107.17964935  499.375       174.69497681]]
289
[[  76.            4.          194.          491.        ]
 [  74.87737274    0.          210.67266846  414.94598389]
 [  37.45185089   56.01211548  265.77783203  499.5       ]
 ..., 
 [  39.03142548  294.85906982   61.85048676  321.14135742]
 [   0.          318.2741394    56.63593674  366.0632019 ]
 [  26.38891983  421.30462646   53.26053238  446.9276123 ]]
500
[[  24.           49.          466.          261.        ]
 [ 108.           77.          181.          135.        ]
 [ 158.           47.          235.           94.        ]
 ..., 
 [ 114.57878113  149.9511261   173.92816162  202.18203735]
 [ 344.92788696  109.42933655  362.40591431  145.34515381]
 [ 172.9866333   182.995224    314.26306152  325.81799316]]
375
[[  80.          278.          159.          387.        ]
 [  32.08816528    0.          374.375       340.19897461]
 [  64.63466644  220.90263367  208.96585083  452.60284424]
 ..., 
 [ 317.84472656  382.26016235  345.01574707  407.62860107]
 [ 165.88304138    3.93346071  302.84112549   66.19628906]
 [ 216.55230713  158.41040039  374.375       233.32044983]]
375
[[ 106.          192.          129.          221.        ]
 [ 125.           89.          276.          305.        ]
 [ 175.          161.          189.          198.        ]
 ..., 
 [ 322.840271    271.4944458   344.6697998   290.9944458 ]
 [  38.69902039  448.10366821   57.39038086  471.56689453]
 [  97.75487518  201.56881714  131.66062927  231.73028564]]
500
[[ 121.            1.          321.          362.        ]
 [  17.15547562    0.          366.98623657  374.375     ]
 [  60.44318008   74.06856537  374.99453735  374.375     ]
 ..., 
 [ 129.61976624  195.27505493  149.36737061  215.73846436]
 [   0.          216.40368652   94.78729248  274.23596191]
 [ 397.73776245   70.63349152  425.61154175   98.63587952]]
375
[[ 175.           72.          207.           95.        ]
 [ 136.           93.          221.          465.        ]
 [ 134.47105408  186.67152405  247.66894531  499.375     ]
 ..., 
 [ 111.93748474  285.86605835  159.62023926  339.890625  ]
 [ 161.48002625   98.14450073  182.21942139  118.94135284]
 [ 116.36384583  326.58920288  139.12438965  388.85742188]]
375
[[ 141.           38.          246.          377.        ]
 [  96.29958344   28.58417511  299.86984253  499.375     ]
 [  77.25727844    0.          309.17788696  404.34405518]
 ..., 
 [  62.39611435  301.51358032   89.46598816  326.82409668]
 [ 285.39608765  239.06599426  370.7053833   284.91802979]
 [ 246.43962097  365.89889526  284.79049683  402.00622559]]
500
[[ 202.           64.          238.          104.        ]
 [ 136.60183716   24.74867249  352.87316895  277.5       ]
 [ 211.64961243   72.17192078  243.03593445  114.21084595]
 ..., 
 [ 169.7331543   241.73048401  247.42489624  257.49456787]
 [  83.36193085    0.          228.15463257   73.4715271 ]
 [   0.          237.95852661  149.31500244  277.5       ]]
500
[[ 145.          211.          225.          290.        ]
 [ 143.99612427  206.93519592  231.99601746  296.43310547]
 [ 137.5846405    86.03501892  265.29260254  409.31665039]
 ..., 
 [ 206.71554565  177.77801514  268.9024353   220.24958801]
 [ 203.44848633   82.53044128  220.17704773  109.76689148]
 [ 339.62460327  129.97853088  378.25268555  167.54353333]]
375
[[ 225.           79.          372.          269.        ]
 [   0.          386.          105.          432.        ]
 [  76.          394.          225.          498.        ]
 ..., 
 [  47.96113586  359.11361694   79.05358887  380.63452148]
 [ 124.47502136   94.96762085  188.58203125  168.46914673]
 [ 237.63848877  138.39952087  317.19732666  384.04418945]]
375
[[ 176.           90.          262.          266.        ]
 [ 185.23989868  171.37586975  209.71987915  228.79187012]
 [ 170.88710022   47.34552765  316.13314819  207.09381104]
 ..., 
 [ 283.83166504  118.26927948  324.83370972  167.07965088]
 [ 274.15118408  282.13323975  355.30975342  369.97268677]
 [  50.12150574  169.23986816   80.26210785  194.09536743]]
500
[[  22.           29.          206.          343.        ]
 [  56.          132.           75.          160.        ]
 [  78.          157.          101.          185.        ]
 ..., 
 [ 333.17175293  276.97128296  408.16589355  318.9541626 ]
 [ 189.57632446  133.50492859  213.67425537  186.24235535]
 [ 444.56542969  141.14027405  488.82125854  224.80982971]]
500
[[  33.            0.          497.          331.        ]
 [ 225.97810364    0.          448.29043579  331.44665527]
 [  41.04906464    0.          301.83758545  331.44665527]
 ..., 
 [ 227.63578796  104.08029938  333.19250488  173.52607727]
 [ 365.31842041  208.47732544  407.08459473  261.49621582]
 [ 275.7538147   232.33630371  322.39431763  288.90368652]]
333
[[  36.            0.          208.          499.        ]
 [  12.45896053    0.          265.4487915   499.5       ]
 [  49.0471344   102.83693695  210.56324768  499.5       ]
 ..., 
 [ 254.62141418  154.4201355   281.887146    192.63261414]
 [ 226.54272461   40.11854172  257.0736084    67.72377777]
 [   0.           59.66003036   55.51602554  107.11952209]]
500
[[ 284.           44.          409.          250.        ]
 [ 282.32183838    0.          449.69107056  374.375     ]
 [ 337.43408203    0.          449.05975342  374.375     ]
 ..., 
 [  65.20385742   36.54271698   95.2790451    60.0858078 ]
 [ 465.85913086  219.60229492  491.69708252  251.51673889]
 [  86.33081055  230.79994202  204.74119568  287.99957275]]
500
[[ 262.           49.          354.          144.        ]
 [ 125.          211.          160.          263.        ]
 [ 159.          207.          242.          283.        ]
 ..., 
 [ 184.17254639  131.71038818  203.95320129  153.22113037]
 [ 252.74406433   49.74597549  329.77523804  105.01546478]
 [ 323.02145386  207.51554871  353.40942383  232.81121826]]
500
[[   0.           40.           88.          235.        ]
 [ 108.           60.          176.          117.        ]
 [ 303.           73.          372.          134.        ]
 ..., 
 [ 269.68261719  137.22467041  301.19342041  161.54769897]
 [ 119.07689667   95.97322845  158.5491333   120.47699738]
 [ 191.87728882    0.          363.83462524   66.35997009]]
466
[[  52.           28.          392.          447.        ]
 [  42.59763336    0.          465.22332764  499.39666748]
 [ 223.44958496    0.          465.22332764  499.39666748]
 ..., 
 [ 371.6144104   293.6222229   406.5397644   331.05780029]
 [ 229.7224884   312.60345459  381.21380615  414.55477905]
 [   0.          225.87446594  108.68506622  292.75247192]]
500
[[ 253.           70.          433.          283.        ]
 [ 101.          123.          340.          302.        ]
 [ 237.13389587    0.          462.12875366  376.37167358]
 ..., 
 [ 250.14987183  311.68118286  302.76599121  352.26672363]
 [ 150.44287109  316.68283081  169.59579468  336.05523682]
 [ 183.11260986    7.12075281  239.90341187   74.99017334]]
500
[[ 357.          219.          417.          313.        ]
 [ 268.          137.          309.          165.        ]
 [ 358.48928833  220.47158813  431.2102356   300.72235107]
 ..., 
 [  91.82872009  147.40966797  131.86529541  199.43728638]
 [   0.           47.79106522  127.28791809  164.33683777]
 [  33.89894867  272.86813354   57.14279556  304.95056152]]
375
[[  54.            0.          240.          498.        ]
 [   0.            0.          232.09191895  317.5519104 ]
 [  52.88049698  106.06084442  283.90548706  499.375     ]
 ..., 
 [  57.0568924   178.11442566   97.42465973  386.10861206]
 [  12.96675587  308.82473755   48.52005005  348.98510742]
 [ 139.2071991    33.04674149  196.55395508   78.03935242]]
333
[[  58.           47.          266.          403.        ]
 [  45.          318.           63.          399.        ]
 [ 115.40514374    0.          332.44500732  481.78851318]
 ..., 
 [  83.14769745  430.66448975  332.44500732  481.97232056]
 [  45.78919983  140.88230896   61.57197571  167.81407166]
 [ 274.3996582   223.77334595  311.59375     264.44885254]]
333
[[ 221.          113.          281.          203.        ]
 [ 198.          253.          276.          472.        ]
 [  51.          188.           91.          215.        ]
 ..., 
 [  51.97829056    0.          125.37949371  269.18148804]
 [ 293.54632568  168.42156982  309.84381104  212.72421265]
 [ 180.48646545  321.63708496  201.92855835  361.90948486]]
500
[[ 120.           29.          375.          312.        ]
 [  69.76138306    0.          411.45477295  365.39001465]
 [ 136.0112915    98.9833374   411.18887329  365.39001465]
 ..., 
 [ 158.90074158  315.93106079  187.43313599  338.67321777]
 [ 359.86975098  334.7645874   384.98876953  352.96942139]
 [ 428.27056885  117.78560638  455.63882446  158.56440735]]
500
[[ 151.           72.          370.          255.        ]
 [ 103.27991486    0.          336.12423706  374.375     ]
 [  64.49684143   68.44623566  313.34896851  374.375     ]
 ..., 
 [ 286.06442261  169.96913147  382.17028809  226.60102844]
 [  56.67344666   52.10700989  190.0348053   113.34190369]
 [ 322.54736328   68.00614166  345.99737549   96.36328888]]
334
[[ 149.           88.          168.          153.        ]
 [ 218.           89.          251.          121.        ]
 [ 168.          188.          204.          223.        ]
 ..., 
 [ 147.4282074    67.32215118  333.44332886  215.05027771]
 [  62.5630188    23.17823982  141.21832275  176.48640442]
 [ 233.21711731   60.46317291  311.14865112   95.84696198]]
334
[[  46.            1.          316.          498.        ]
 [  59.90499878   82.19979858  298.45178223  499.32998657]
 [  27.09357643    0.          269.5819397   450.60012817]
 ..., 
 [ 244.17720032  388.74746704  299.00802612  447.39920044]
 [  33.43210602  321.60888672   66.3141861   430.83856201]
 [  10.46434498  135.55363464   78.98311615  346.4475708 ]]
500
[[ 119.           39.          372.          321.        ]
 [  54.23645782    0.          350.98312378  374.375     ]
 [ 155.95953369   99.12071228  411.09564209  374.375     ]
 ..., 
 [  37.23145294  112.49897003  126.39500427  165.34432983]
 [ 313.79187012  132.24551392  352.78128052  222.4546051 ]
 [ 282.8387146   241.36061096  499.375       344.04888916]]
375
[[  80.           10.          371.          470.        ]
 [  43.90949249    0.          372.25985718  499.375     ]
 [ 165.00434875    0.          374.375       499.375     ]
 ..., 
 [ 264.40838623  449.24017334  300.91766357  499.375     ]
 [  63.47618484   93.1661377   203.90365601  272.35824585]
 [   0.          258.67358398  136.64779663  455.9982605 ]]
375
[[ 102.           29.          287.          467.        ]
 [   0.           11.7376709   192.92773438  499.375     ]
 [  10.92728615    0.          257.14953613  499.375     ]
 ..., 
 [ 156.00097656  185.25602722  243.51669312  235.19178772]
 [ 265.63964844   89.68495178  293.36676025  132.4899292 ]
 [ 303.97247314  104.14292908  327.49014282  170.38002014]]
375
[[  99.           93.          270.          470.        ]
 [ 112.70065308  207.21174622  164.74136353  343.05517578]
 [   5.20563126   90.02948761  374.375       423.72158813]
 ..., 
 [  10.62967777  297.48187256  137.95397949  421.55682373]
 [ 223.94456482  322.88821411  312.88671875  381.83529663]
 [  20.64582253    0.          105.11846161  142.50201416]]
500
[[ 261.          138.          316.          163.        ]
 [ 296.99926758    0.          499.375       374.375     ]
 [   0.            0.          203.92398071  374.375     ]
 ..., 
 [ 170.96025085  141.62654114  217.58500671  180.58581543]
 [ 229.0316925    47.66107941  254.15060425   80.20736694]
 [  96.07460022   22.19230652  284.97280884  250.91142273]]
500
[[ 152.          122.          399.          211.        ]
 [ 301.          196.          373.          219.        ]
 [ 232.1502533     0.          427.99588013  332.44500732]
 ..., 
 [  19.71498299   46.88183975  168.63282776  256.5291748 ]
 [  99.87376404  250.76083374  264.14678955  332.44500732]
 [ 291.15777588  210.50178528  390.07519531  257.12762451]]
453
[[   0.            0.          420.          499.        ]
 [  22.99197197    0.          336.19631958  499.05499268]
 [   0.            0.          337.51812744  285.91275024]
 ..., 
 [ 155.60705566  251.21208191  192.78479004  296.0098877 ]
 [   0.           58.76856232   17.95014381  112.10430908]
 [   0.          353.63702393  112.95453644  437.02331543]]
375
[[  93.           96.          268.          357.        ]
 [ 208.           75.          245.          106.        ]
 [  78.            0.          235.           81.        ]
 ..., 
 [  54.69438553  423.65042114   73.23410797  440.86477661]
 [ 238.12823486  457.17001343  261.5977478   473.81582642]
 [   0.          427.07565308   75.37615967  479.45632935]]
500
[[  16.           24.          228.          139.        ]
 [  31.          162.          209.          252.        ]
 [ 274.           22.          469.          162.        ]
 ..., 
 [ 107.34687042  154.54832458  342.61978149  241.98690796]
 [ 426.27456665   38.45239258  460.94821167   72.3732605 ]
 [  15.01081657  213.74255371   58.37201691  273.65872192]]
316
[[  11.           48.          288.          492.        ]
 [  87.84270477  248.8021698   189.78111267  499.27999878]
 [  49.15909576   10.69167995  313.87866211  499.27999878]
 ..., 
 [  31.62546349  225.42167664   47.36185455  241.1136322 ]
 [ 155.58309937  315.34451294  248.19210815  401.32342529]
 [ 237.92805481  366.60128784  294.59811401  432.38171387]]
333
[[ 104.          184.          172.          249.        ]
 [  66.51098633    0.          238.47967529  293.83624268]
 [  72.12405396    0.          287.99282837  331.73345947]
 ..., 
 [ 106.68145752  123.0319519   213.9161377   168.54290771]
 [  80.19796753  223.74609375  108.64820099  255.19238281]
 [ 104.2612381   156.59552002  142.56903076  197.98579407]]
254
[[  14.           11.          240.          494.        ]
 [   0.            0.          250.15234375  499.5       ]
 [   0.           72.32424927  200.48199463  499.5       ]
 ..., 
 [ 154.45768738  175.27349854  194.75456238  240.34558105]
 [  28.24838829  469.20288086   89.67816925  485.51715088]
 [  35.0025177   188.65858459   89.24002075  278.12460327]]
500
[[  27.          202.           46.          223.        ]
 [   2.          211.           85.          324.        ]
 [ 277.          331.          349.          366.        ]
 ..., 
 [ 356.30828857  211.26908875  423.64691162  352.5340271 ]
 [ 341.03207397  118.08956146  368.69244385  145.47624207]
 [ 381.39175415   58.9382515   397.26721191   81.71768188]]
500
[[ 369.          240.          420.          287.        ]
 [ 389.          350.          413.          372.        ]
 [  56.77347183    0.          394.16412354  328.55102539]
 ..., 
 [ 188.01828003   92.3888092   290.64398193  143.42260742]
 [ 183.43305969  330.85272217  211.4861145   359.84542847]
 [   5.06211042   69.18431854   26.32530022  109.78185272]]
500
[[  39.           68.          470.          276.        ]
 [ 265.15609741    0.          499.375       374.375     ]
 [ 106.36844635   28.89379501  329.66558838  374.375     ]
 ..., 
 [ 266.36035156   80.57407379  340.16525269  120.80867767]
 [ 160.43444824  113.14530945  236.54335022  197.0531311 ]
 [ 300.07546997  179.86471558  408.46511841  283.32107544]]
500
[[  66.           61.          475.          202.        ]
 [ 165.426651    111.43029785  412.23876953  244.87176514]
 [  15.98197937    0.          356.80004883  357.23117065]
 ..., 
 [  93.27701569  156.61483765  213.80194092  214.07997131]
 [ 332.11355591   87.20024872  352.26416016  104.98473358]
 [ 308.33682251  102.47993469  435.36938477  234.44491577]]
500
[[ 176.           34.          387.          368.        ]
 [ 141.96600342    0.          349.03509521  374.375     ]
 [  93.62895966    0.          391.37512207  374.375     ]
 ..., 
 [  59.99380875  214.82211304  154.60430908  270.38494873]
 [ 176.75254822  198.1729126   398.99258423  356.99020386]
 [  40.26005173  145.07113647  151.32421875  374.375     ]]
500
[[  47.          134.          466.          348.        ]
 [ 142.68534851   43.6813736   405.46142578  374.375     ]
 [ 143.14756775   80.56054688  341.76223755  374.375     ]
 ..., 
 [ 159.10049438  104.1595459   177.12936401  122.47594452]
 [ 335.32000732   79.17087555  359.17712402  100.65597534]
 [   0.          128.25622559   37.12214279  178.03466797]]
500
[[ 126.           26.          413.          312.        ]
 [ 201.          110.          211.          125.        ]
 [ 142.39431763    0.          398.55517578  374.375     ]
 ..., 
 [ 281.73492432  321.9095459   379.19317627  345.08947754]
 [ 139.42221069  100.82671356  167.47816467  120.67828369]
 [  39.17929459  194.84526062   70.07354736  255.76509094]]
375
[[   0.           94.          284.          389.        ]
 [   7.          185.           36.          219.        ]
 [ 135.           98.          167.          122.        ]
 ..., 
 [ 218.62696838  137.45600891  254.77134705  178.22241211]
 [   0.           46.86811066  111.18247986  178.35902405]
 [  57.10521317  272.48352051  155.53076172  403.87402344]]
500
[[ 102.            6.          364.          359.        ]
 [ 115.30549622    0.          298.1300354   278.2364502 ]
 [ 119.40999603    0.          340.82000732  374.375     ]
 ..., 
 [ 329.44244385  268.78759766  398.8494873   342.74389648]
 [ 133.07575989  315.14593506  159.20513916  337.80303955]
 [ 291.48019409   93.30932617  353.568573    226.61843872]]
500
[[  73.          132.          326.          259.        ]
 [  33.08052826    0.          274.24157715  332.44500732]
 [ 123.03761292    0.          395.23519897  332.44500732]
 ..., 
 [  22.28592491   78.10295105   53.68356323  113.52365112]
 [ 230.99511719  103.43969727  303.1489563   140.8926239 ]
 [ 464.56692505  176.70504761  489.44692993  194.73194885]]
375
[[ 117.           19.          290.          361.        ]
 [ 122.53414154   19.87043381  283.02511597  401.84744263]
 [ 103.4161377     0.          337.99121094  372.35720825]
 ..., 
 [ 218.2354126   462.43869019  238.30091858  480.32345581]
 [   0.            0.           38.44939041   31.0287571 ]
 [ 255.96757507   19.4891243   320.39022827   43.93418121]]
375
[[  58.            0.          269.          395.        ]
 [  33.          292.           52.          363.        ]
 [ 100.          438.          141.          484.        ]
 ..., 
 [   0.          359.62219238   69.47319031  413.815979  ]
 [   0.          378.41436768   71.36397552  433.88076782]
 [   0.          482.19204712   76.41162109  499.375     ]]
480
[[  68.           23.          404.          296.        ]
 [  44.          298.           80.          358.        ]
 [  72.          319.          121.          357.        ]
 ..., 
 [ 177.0453949    73.20557404  217.41674805  118.6836319 ]
 [ 279.81027222   89.38570404  319.02642822  133.53919983]
 [ 204.07266235   65.17338562  332.57540894  187.4604187 ]]
375
[[  8.80000000e+01   0.00000000e+00   2.96000000e+02   4.11000000e+02]
 [  8.20000000e+01   2.71000000e+02   9.50000000e+01   3.02000000e+02]
 [  1.95000000e+02   4.30000000e+02   2.20000000e+02   4.49000000e+02]
 ..., 
 [  1.88636780e-02   3.68123779e+02   1.01357994e+02   4.21645966e+02]
 [  2.60903748e+02   2.94163483e+02   3.48695496e+02   3.33456848e+02]
 [  2.13585754e+02   1.90281372e+02   2.67213684e+02   2.67914612e+02]]
480
[[  84.           32.          348.          302.        ]
 [ 223.98564148    0.          370.55154419  349.58291626]
 [  58.64198303    0.          304.50842285  359.3999939 ]
 ..., 
 [ 172.59191895  129.00709534  318.49398804  200.83857727]
 [  25.4432106   170.07774353   45.24185562  185.8089447 ]
 [   0.            8.28330708  161.41087341  100.51554871]]
375
[[ 134.           48.          264.          425.        ]
 [  94.7742157    13.14907074  303.70755005  499.375     ]
 [  32.54070282   56.41328812  374.375       454.99008179]
 ..., 
 [  70.90142059   82.67410278   98.59188843  110.55200195]
 [ 353.99603271  160.35009766  374.375       221.40509033]
 [   0.           94.73343658   16.95266533  131.05303955]]
500
[[ 179.          165.          275.          248.        ]
 [ 269.           95.          350.          161.        ]
 [ 186.21336365  162.51205444  275.27478027  247.42706299]
 ..., 
 [ 231.32342529  274.66516113  261.11425781  305.27355957]
 [ 127.234375    332.37692261  147.36643982  353.12078857]
 [ 168.25865173  298.77478027  191.81565857  320.05673218]]
500
[[ 154.           72.          364.          275.        ]
 [  71.6336441     0.          385.86032104  374.375     ]
 [ 135.27815247    0.          295.95495605  353.68240356]
 ..., 
 [ 275.47988892  107.18141174  334.80532837  155.23361206]
 [ 109.53551483   33.15050888  131.89955139   53.86116028]
 [ 262.01367188  199.92889404  399.22833252  321.64291382]]
500
[[ 127.          133.          348.          200.        ]
 [ 133.          192.          177.          235.        ]
 [ 177.          201.          224.          249.        ]
 ..., 
 [ 269.14929199  105.42811584  354.48135376  146.6860199 ]
 [ 188.08358765   58.91626358  492.65151978  127.36095428]
 [  37.61922836   74.55063629   69.59889221  141.12156677]]
375
[[ 178.           58.          337.          450.        ]
 [ 171.53709412    0.          374.375       430.64044189]
 [ 234.05570984  151.53549194  374.375       481.98150635]
 ..., 
 [ 120.42434692  136.74575806  202.          194.51777649]
 [  37.06877899  132.80058289   64.36608124  162.95631409]
 [  47.37977982  219.25646973   75.26406097  258.73510742]]
500
[[  63.           79.          475.          250.        ]
 [  71.23514557    0.          285.5597229   332.44500732]
 [   0.           26.62306213  325.26266479  332.44500732]
 ..., 
 [   0.          144.60418701  230.61256409  261.5017395 ]
 [  62.06567764    5.17841864   94.08720398   39.42961502]
 [ 407.00643921  182.54017639  434.39321899  214.91262817]]
500
[[ 356.          181.          381.          208.        ]
 [ 368.          207.          386.          227.        ]
 [ 384.          225.          396.          242.        ]
 ..., 
 [ 194.38356018   81.4744339   280.13131714  323.87512207]
 [ 389.52758789  281.07208252  499.5         332.44500732]
 [  97.67936707  111.93573761  179.08888245  211.05236816]]
375
[[  53.           28.          338.          496.        ]
 [ 129.75914001    0.          297.92999268  440.73535156]
 [ 102.60554504  175.49636841  363.48269653  499.375     ]
 ..., 
 [ 111.12193298  419.47637939  145.29818726  459.98968506]
 [   0.           16.09014511  128.80949402  144.77760315]
 [ 269.90612793  270.14318848  318.2121582   415.93817139]]
256
[[  25.           17.          242.          485.        ]
 [  19.49498749  210.2729187   236.02600098  499.5       ]
 [  37.54299927  271.08483887  211.23352051  499.5       ]
 ..., 
 [   5.10113335  232.41792297   49.11353302  309.74157715]
 [ 123.805336    146.61218262  169.65422058  216.17242432]
 [  16.88708305  255.75920105   45.34818268  302.06716919]]
500
[[  97.           59.          331.          224.        ]
 [ 194.          355.          211.          374.        ]
 [ 209.          359.          222.          380.        ]
 ..., 
 [ 315.89825439  411.33856201  361.66903687  426.84277344]
 [ 303.08175659  295.72964478  413.22622681  350.91680908]
 [ 288.58709717  260.75097656  414.1105957   326.17153931]]
375
[[  59.           60.          239.          461.        ]
 [ 214.          384.          234.          422.        ]
 [ 182.          289.          231.          370.        ]
 ..., 
 [ 193.37210083  459.19818115  222.05969238  490.84698486]
 [ 234.21603394  203.15974426  267.40621948  258.52111816]
 [ 205.20120239  132.6124115   374.375       241.91416931]]
500
[[  82.           38.          301.          330.        ]
 [ 157.94903564    0.          370.60540771  333.44332886]
 [ 209.46179199    0.          373.77114868  285.18301392]
 ..., 
 [ 277.06698608   64.17150879  357.58932495  244.61141968]
 [ 328.22286987  263.9145813   415.70318604  305.38699341]
 [ 251.12594604  221.52325439  297.84750366  289.05212402]]
500
[[ 170.          172.          280.          269.        ]
 [ 198.          266.          233.          318.        ]
 [ 105.          231.          132.          372.        ]
 ..., 
 [ 388.08166504  230.22943115  417.97598267  269.22521973]
 [ 297.2383728   127.94267273  398.83398438  208.06578064]
 [  55.71463013  262.11322021  140.21186829  327.17288208]]
375
[[ 130.          149.          289.          429.        ]
 [ 116.33987427   10.95905304  363.2371521   499.375     ]
 [  69.62374878   51.91465378  373.29943848  426.29638672]
 ..., 
 [ 144.08906555   64.5904541   174.01820374  108.56002808]
 [ 226.75688171   81.91123962  270.20346069  122.44908905]
 [ 329.93817139  289.38009644  352.71304321  311.12133789]]
500
[[  72.            0.          402.          373.        ]
 [ 100.22348022    0.          343.64776611  374.375     ]
 [ 151.91455078    0.          378.79367065  374.375     ]
 ..., 
 [ 202.27702332   40.0813942   252.62606812   66.77236176]
 [ 457.28192139  299.16122437  485.74307251  336.59277344]
 [  35.75174332  272.33807373  410.40029907  374.375     ]]
500
[[ 189.          242.          281.          296.        ]
 [ 200.          311.          345.          344.        ]
 [ 244.          131.          439.          322.        ]
 ..., 
 [ 117.07962799  112.12287903  154.99424744  168.03674316]
 [ 129.00674438  244.87501526  159.40940857  267.31375122]
 [ 142.54000854  280.79162598  177.1638031   316.52255249]]
500
[[  62.           73.          358.          292.        ]
 [ 131.80473328    0.          389.68478394  406.32168579]
 [  44.4135704     0.          328.94223022  406.32168579]
 ..., 
 [ 359.52420044  311.01257324  395.32424927  357.50704956]
 [ 188.8434906     0.          457.2538147    38.12386322]
 [ 312.62640381  324.06262207  338.69772339  346.26422119]]
500
[[ 166.          115.          203.          138.        ]
 [ 158.91383362    0.          352.91061401  374.375     ]
 [ 161.28187561  101.7013092   201.57792664  128.26014709]
 ..., 
 [ 227.65733337  340.32492065  327.12698364  374.375     ]
 [  53.61693954  340.16390991   75.99971008  366.793396  ]
 [ 195.29924011    0.          303.74649048  234.73312378]]
281
[[  71.           14.           92.           56.        ]
 [  73.           60.          120.          121.        ]
 [  97.          126.          139.          184.        ]
 ..., 
 [  65.85708618  116.31420898  153.59074402  134.26557922]
 [  83.99319458   21.58234024  193.22790527  133.53030396]
 [  95.89044952  119.80106354  126.95882416  142.03669739]]
500
[[ 379.          187.          396.          208.        ]
 [ 384.          201.          414.          234.        ]
 [  86.          147.          116.          191.        ]
 ..., 
 [ 252.50082397  202.0667572   272.62329102  233.58470154]
 [ 164.30708313  244.61701965  206.86087036  316.12539673]
 [ 209.43449402  111.60714722  362.71582031  272.3644104 ]]
375
[[ 195.           20.          374.          472.        ]
 [   0.           23.          181.          486.        ]
 [   0.            0.          263.58209229  499.375     ]
 ..., 
 [  86.48765564  388.67626953  134.60165405  459.93695068]
 [ 211.78266907    0.          261.81915283   18.0080452 ]
 [ 131.8913269   402.89135742  175.49905396  447.34240723]]
500
[[  34.            0.          499.          373.        ]
 [ 142.11123657    0.          376.69247437  374.375     ]
 [  16.17670059   11.15772247  241.99098206  374.375     ]
 ..., 
 [ 275.68054199  233.39491272  368.48739624  304.5093689 ]
 [ 340.74972534  248.6934967   435.45547485  332.36831665]
 [ 393.46395874  165.27381897  499.375       250.74023438]]
269
[[   8.          123.          259.          498.        ]
 [   8.16028595   31.25927734  221.14712524  499.5       ]
 [  71.12532806  141.06954956  268.5         499.5       ]
 ..., 
 [  80.09893799  439.33502197   96.53669739  462.74560547]
 [ 142.06381226   10.08886909  231.37976074   66.59289551]
 [   0.           58.48823547   46.07353973  100.16085815]]
375
[[  38.           67.          330.          431.        ]
 [  24.12517548    0.          372.4083252   499.375     ]
 [   0.            0.          257.15509033  302.33294678]
 ..., 
 [ 194.02636719  147.91900635  285.75665283  242.37510681]
 [ 128.59857178  235.06011963  267.57241821  300.6640625 ]
 [  65.70754242  154.90153503  152.82385254  240.30436707]]
500
[[   0.          184.           77.          363.        ]
 [   0.            0.          158.1552124   374.375     ]
 [  30.34324646    0.          342.93521118  374.375     ]
 ..., 
 [ 218.69790649  213.41523743  287.1418457   281.66894531]
 [ 401.37695312  273.52523804  476.29623413  307.77288818]
 [ 321.09069824  216.15988159  357.53225708  254.1083374 ]]
333
[[   8.          293.          321.          428.        ]
 [ 161.          411.          171.          422.        ]
 [ 107.          418.          117.          432.        ]
 ..., 
 [ 265.34100342  136.68025208  287.78164673  162.05557251]
 [   0.          387.43441772   84.76734161  434.38061523]
 [  73.27754974  410.43301392  158.85423279  461.54360962]]
375
[[ 104.          188.          301.          414.        ]
 [ 176.          430.          259.          460.        ]
 [ 257.          428.          295.          452.        ]
 ..., 
 [ 354.24343872  193.35754395  374.05444336  243.1552887 ]
 [  37.73330688  123.59876251  272.27362061  233.99934387]
 [  63.79403687  365.50305176   81.9969635   403.04690552]]
375
[[   5.           77.          174.          376.        ]
 [   0.            0.          374.375       456.58529663]
 [  35.52344131    0.          265.52841187  119.40345001]
 ..., 
 [ 212.53746033   85.66628265  246.11155701  128.1980896 ]
 [ 199.8515625   138.01879883  260.66601562  205.22720337]
 [ 147.12715149  116.20118713  182.6033783   168.15058899]]
375
[[   0.            0.          374.          498.        ]
 [   0.            0.          281.86932373  499.375     ]
 [  97.53010559    0.          275.0105896   465.99258423]
 ..., 
 [ 203.84341431  224.96604919  315.65914917  369.30651855]
 [ 146.94084167  145.42701721  221.45179749  250.79180908]
 [ 147.27827454   65.72803497  246.40307617  176.57595825]]
375
[[ 124.           96.          278.          469.        ]
 [ 159.          206.          198.          256.        ]
 [ 156.24464417  199.11834717  215.01554871  260.16958618]
 ..., 
 [  76.02219391  237.57444763  347.6505127   347.5269165 ]
 [ 210.20892334   81.2789917   233.13433838   99.6105957 ]
 [ 310.92840576  236.3069458   327.32611084  252.12199402]]
334
[[ 117.          115.          233.          350.        ]
 [  87.46639252    0.          264.2114563   453.00180054]
 [ 125.21401978   40.43382645  257.76992798  404.09579468]
 ..., 
 [ 206.00440979   34.28154373  255.92784119  143.49443054]
 [ 265.94680786   92.00901031  302.49295044  122.84261322]
 [ 156.66958618  102.25852966  177.10966492  122.02765656]]
500
[[  37.          190.           91.          274.        ]
 [ 321.          171.          392.          255.        ]
 [ 321.25921631  165.79530334  391.2802124   237.78204346]
 ..., 
 [ 398.83398438  131.44700623  429.25003052  153.96255493]
 [ 437.61358643  231.39819336  460.45263672  253.56430054]
 [  91.26224518  322.74108887  244.34364319  374.375     ]]
500
[[ 173.          125.          259.          211.        ]
 [ 332.78866577    0.          499.375       374.375     ]
 [ 312.1072998     0.          499.375       276.72888184]
 ..., 
 [ 151.90966797    0.          273.91082764   85.81184387]
 [ 275.96817017  143.89297485  306.2348938   181.82228088]
 [ 250.14006042  254.55058289  351.70324707  374.375     ]]
500
[[ 230.          127.          323.          287.        ]
 [ 105.57517242    5.04762554  468.56896973  426.28833008]
 [  44.07312012  182.99087524  400.90820312  426.28833008]
 ..., 
 [ 250.70291138  100.19078827  337.49810791  300.47079468]
 [  20.65060425  323.35061646  142.24511719  388.25473022]
 [ 412.04052734   58.16749954  448.84515381  100.50988007]]
500
[[ 274.           75.          293.          115.        ]
 [ 221.          109.          428.          276.        ]
 [ 273.64025879   84.8090744   293.93206787  128.1232605 ]
 ..., 
 [ 150.07556152   73.72424316  171.27116394   90.75076294]
 [ 195.0030365   319.42425537  227.60369873  350.47976685]
 [  33.94403076    7.72913933  157.83166504  318.76541138]]
498
[[  23.           68.          144.          323.        ]
 [ 122.          101.          348.          357.        ]
 [ 200.           38.          264.           78.        ]
 ..., 
 [ 336.97512817    0.          453.13372803  229.93048096]
 [   0.            0.           43.85766983   44.3805809 ]
 [   0.          366.13806152   17.81602287  424.65969849]]
332
[[  56.          258.          149.          321.        ]
 [  64.47657013  273.76757812  115.33028412  309.91757202]
 [  64.27586365  260.96737671  124.33691406  320.17202759]
 ..., 
 [  87.9839325   264.51742554  211.26095581  374.84109497]
 [  85.14260101  169.64103699  179.95018005  283.62121582]
 [ 119.49836731  234.50878906  239.46234131  279.6121521 ]]
332
[[ 217.          294.          275.          313.        ]
 [ 227.61701965  290.62628174  279.90109253  313.98272705]
 [   0.            5.97499704  181.90847778  226.38414001]
 ..., 
 [ 231.32325745  304.78900146  269.79415894  345.40292358]
 [  11.38913345  179.10861206   48.37865448  227.72816467]
 [ 204.92832947  219.84799194  324.0111084   341.9828186 ]]
500
[[ 146.          192.          163.          253.        ]
 [ 169.          209.          200.          312.        ]
 [ 211.          213.          257.          330.        ]
 ..., 
 [  54.54105377   68.65059662   81.29516602   92.06578064]
 [ 249.4362793   148.47290039  275.86755371  172.57528687]
 [ 461.90863037    0.          496.65222168   29.54117203]]
375
[[  78.          256.          247.          494.        ]
 [  17.95976639   59.36340332  236.59423828  499.375     ]
 [  27.24412918  174.49607849  261.7137146   499.375     ]
 ..., 
 [ 220.07250977  174.31642151  264.03897095  214.56182861]
 [ 220.40026855   79.31484222  267.73529053  132.95736694]
 [ 204.63111877   53.36634445  258.98196411   81.72409821]]
300
[[  40.           27.          279.          471.        ]
 [  75.01674652    0.          299.5         499.5       ]
 [  82.64516449  129.42134094  299.5         470.09106445]
 ..., 
 [ 207.80181885  359.65701294  279.52386475  436.92050171]
 [ 164.78895569  394.97747803  250.67176819  499.5       ]
 [ 175.70620728  138.72492981  211.9156189   194.2318573 ]]
334
[[  71.          133.          100.          178.        ]
 [ 219.          131.          239.          161.        ]
 [ 243.          131.          314.          288.        ]
 ..., 
 [   0.          182.30018616  152.27139282  271.35327148]
 [ 131.24183655  460.03198242  160.23066711  479.85623169]
 [ 151.01524353  283.87133789  200.42701721  457.56588745]]
375
[[ 137.          189.          226.          349.        ]
 [ 133.82810974  190.91490173  236.90077209  341.82302856]
 [ 123.75497437  211.07673645  294.56829834  327.97439575]
 ..., 
 [ 351.13009644  356.54440308  374.375       387.150177  ]
 [ 117.44365692   39.56188965  257.16412354  183.50146484]
 [   0.           20.74900055  141.86564636  165.12965393]]
500
[[ 356.          140.          372.          159.        ]
 [ 349.          155.          371.          164.        ]
 [  97.56690216   84.85494995  347.26461792  442.26165771]
 ..., 
 [ 191.04321289  342.07162476  247.80578613  397.353302  ]
 [  17.20411301  126.85823822   42.14192963  215.82394409]
 [ 419.95654297  297.80819702  444.17913818  319.21896362]]
333
[[  22.            0.          258.          468.        ]
 [  45.16387939    0.          314.66680908  478.61575317]
 [  31.31098557   31.76976967  121.38043976  475.04055786]
 ..., 
 [  19.73008919  318.32559204   86.92453766  419.12734985]
 [ 132.37713623  217.80767822  201.2298584   299.30819702]
 [   0.            0.           39.94642639  101.55383301]]
442
[[ 131.           42.          319.          197.        ]
 [  86.70217133   21.38262939  265.2722168   229.05862427]
 [ 113.53804016    0.          324.55990601  160.45976257]
 ..., 
 [   8.33740997  184.78486633   32.97389984  210.29852295]
 [ 165.12799072   69.78382874  248.84475708  127.37361145]
 [ 116.35847473  248.87765503  143.54481506  270.54833984]]
500
[[   8.            5.          492.          343.        ]
 [ 253.60452271    0.          470.46270752  347.42001343]
 [ 219.10353088    0.          499.38000488  264.67956543]
 ..., 
 [ 240.29049683  286.76452637  283.99093628  343.4725647 ]
 [ 308.71130371   72.45414734  359.90939331  137.15055847]
 [ 390.11343384   51.03480148  483.41726685  103.25354004]]
367
[[ 199.            1.          349.          359.        ]
 [  75.           81.          127.          154.        ]
 [ 207.61528015    2.28786993  366.38830566  467.39208984]
 ..., 
 [ 285.68453979  473.04067993  308.09793091  498.3260498 ]
 [ 277.75296021  377.99404907  302.42800903  413.10528564]
 [  31.91709328  345.2729187   123.48453522  379.95953369]]
375
[[  37.            1.          373.          497.        ]
 [  77.82514191    0.          338.78790283  461.65161133]
 [   0.            0.          282.11199951  499.375     ]
 ..., 
 [ 268.48922729  180.08016968  356.03625488  487.53057861]
 [ 222.76156616  414.12664795  249.29141235  454.37133789]
 [ 238.7791748   259.34753418  284.62850952  312.52392578]]
303
[[  77.          230.          259.          487.        ]
 [  40.78293228   12.25398636  256.06414795  499.44500732]
 [  86.95281982  104.67311859  260.87954712  499.44500732]
 ..., 
 [  92.22817993  269.41418457  113.80688477  288.38348389]
 [ 120.43188477  186.53089905  252.95703125  294.28256226]
 [ 163.31654358  424.84347534  199.51344299  464.61456299]]
305
[[  31.           30.          284.          440.        ]
 [ 115.          432.          134.          455.        ]
 [   0.            5.37020159  246.15292358  456.4833374 ]
 ..., 
 [ 107.3635025   190.61082458  169.29678345  342.86459351]
 [ 284.3039856   325.68731689  304.4916687   389.20675659]
 [ 232.30702209   60.17167664  304.4916687   169.13015747]]
500
[[  17.            6.          459.          305.        ]
 [ 181.90957642    0.          379.76464844  332.44500732]
 [ 111.71000671    0.          346.57611084  332.44500732]
 ..., 
 [ 155.38331604  206.22851562  206.71194458  282.39349365]
 [  86.09622955  104.06542206  109.78127289  124.1832428 ]
 [ 382.97216797  314.32147217  453.07904053  332.44500732]]
400
[[ 173.          135.          306.          266.        ]
 [ 148.1056366    10.53579044  322.3427124   266.55499268]
 [ 167.86843872   88.37270355  333.5526123   266.55499268]
 ..., 
 [ 352.67129517  204.53088379  399.61001587  240.97557068]
 [ 309.84048462    8.74324226  385.9039917    50.85576248]
 [  91.33358002   74.36146545  160.77340698  212.81832886]]
375
[[  78.            2.          296.          485.        ]
 [  89.44123077  120.60308075  290.99316406  499.375     ]
 [ 106.16265869    0.          374.375       461.81723022]
 ..., 
 [ 307.25994873  411.01016235  333.56939697  436.60662842]
 [ 196.30592346  300.89193726  241.60502625  357.50396729]
 [ 293.0645752   278.85491943  311.27529907  307.71835327]]
500
[[  78.           35.          422.          202.        ]
 [ 130.          232.          202.          250.        ]
 [ 318.          238.          355.          250.        ]
 ..., 
 [  49.31288147  219.30984497  131.65568542  252.5       ]
 [ 170.58964539   74.32070923  288.1600647   140.02377319]
 [ 322.00482178   75.0083313   391.66192627  118.87762451]]
411
[[ 187.          221.          242.          283.        ]
 [ 181.11413574  212.4730072   243.34738159  275.70889282]
 [ 192.01194763  239.87428284  241.66513062  303.98876953]
 ..., 
 [ 326.51135254  121.1348114   383.5718689   166.45744324]
 [  92.85418701  330.61206055  353.70843506  499.36499023]
 [ 198.26150513  325.98840332  237.80645752  362.88201904]]
333
[[ 130.           44.          330.          293.        ]
 [   0.           13.82926941  199.5717926   499.5       ]
 [ 146.28897095   40.53139496  332.44500732  332.00756836]
 ..., 
 [  99.79549408  255.12098694  158.90005493  283.63894653]
 [ 140.68336487   99.94676208  250.79606628  149.84768677]
 [ 242.71376038  308.88894653  266.91342163  325.74645996]]
415
[[  18.            0.          219.          497.        ]
 [  56.          334.           83.          404.        ]
 [ 245.            5.          409.          477.        ]
 ..., 
 [  76.39035797  273.09213257  163.14395142  372.49658203]
 [ 175.27099609   29.78770065  235.2910614    60.09630966]
 [ 296.65426636  224.72314453  359.93460083  302.2227478 ]]
500
[[ 365.          223.          404.          356.        ]
 [  24.          244.          170.          356.        ]
 [ 164.          266.          186.          289.        ]
 ..., 
 [ 272.19107056  160.86962891  389.13024902  217.46073914]
 [ 149.87773132  168.59716797  172.2469635   193.17445374]
 [ 271.4074707    38.09516525  394.290802    100.55745697]]
500
[[  78.           64.          121.           89.        ]
 [ 177.95635986    0.          418.25274658  280.5       ]
 [  75.63004303   54.06567764  121.46868134   83.79419708]
 ..., 
 [  24.49073029   52.06664276   53.29257202   81.09986115]
 [ 300.16622925  147.42579651  352.40151978  173.30271912]
 [ 249.62350464   54.91065598  275.08450317   73.77462006]]
375
[[  57.            8.          321.          495.        ]
 [   0.           72.67238617  263.80865479  499.375     ]
 [   0.           86.3453064   374.375       446.44592285]
 ..., 
 [ 313.64120483   67.57099152  361.04840088  180.03097534]
 [ 328.47570801  289.26223755  344.88464355  308.39230347]
 [   1.27911568  449.51696777  309.76699829  499.375     ]]
333
[[ 158.          131.          259.          303.        ]
 [ 160.58947754   42.08181381  297.79144287  437.24902344]
 [ 110.25761414    0.          330.4357605   454.89736938]
 ..., 
 [ 299.70904541  201.47271729  323.35852051  239.76132202]
 [ 169.21368408  392.09597778  205.93521118  411.12188721]
 [ 293.9704895   208.20394897  317.9524231   243.07287598]]
334
[[   0.          116.           17.          142.        ]
 [ 121.          104.          172.          153.        ]
 [ 147.          190.          260.          323.        ]
 ..., 
 [ 230.40087891  372.04553223  310.77706909  418.6645813 ]
 [ 283.93917847  399.31518555  314.18270874  461.34234619]
 [ 215.58328247  105.3835144   244.68171692  126.20095825]]
375
[[  37.            0.          308.          494.        ]
 [   0.            0.          294.9677124   375.40136719]
 [ 108.91042328    0.          374.375       446.89544678]
 ..., 
 [ 280.4642334   234.97790527  350.1605835   304.62390137]
 [ 206.13305664  342.91714478  250.97212219  414.95394897]
 [ 240.51394653  399.35757446  332.59671021  433.98486328]]
375
[[  41.           51.          342.          463.        ]
 [ 245.            0.          314.           73.        ]
 [ 112.8695755     0.          374.375       499.375     ]
 ..., 
 [ 184.35743713  105.9344635   207.39347839  124.77157593]
 [ 267.49310303  292.16299438  360.18255615  457.14660645]
 [ 185.59606934  453.61178589  214.64888     482.42749023]]
262
[[  43.           12.          223.          487.        ]
 [ 158.74406433    0.          256.11767578  355.55856323]
 [ 127.74590302    0.          261.5         311.61129761]
 ..., 
 [  13.45183563  139.32424927  141.88845825  227.41424561]
 [ 127.33946228  309.69351196  192.95875549  329.54623413]
 [ 150.72346497  202.45632935  189.99604797  262.11853027]]
500
[[ 141.           82.          332.          255.        ]
 [ 142.16423035    0.          398.55932617  374.375     ]
 [  67.64495087    0.          271.85394287  374.375     ]
 ..., 
 [   0.            6.54084206   26.32226753   74.58889771]
 [ 131.67417908   46.03256989  152.3487854    77.64155579]
 [ 149.75907898  307.33340454  232.16642761  353.26571655]]
375
[[  97.          148.          263.          366.        ]
 [  93.26491547  211.45335388  374.375       499.375     ]
 [   0.            3.88725281  276.33029175  499.375     ]
 ..., 
 [   0.          386.18789673   82.51046753  430.80810547]
 [ 343.16830444  135.97192383  359.43423462  155.098526  ]
 [ 318.78707886  241.00611877  343.9546814   262.90515137]]
331
[[ 100.           10.          231.          491.        ]
 [  72.12637329  130.54605103  294.99069214  499.2583313 ]
 [ 117.12874603  174.93939209  258.98693848  499.2583313 ]
 ..., 
 [ 278.02935791  252.99490356  298.48815918  271.17877197]
 [ 123.5499115    82.07391357  170.92062378  167.64569092]
 [ 174.99560547    4.31895971  236.29556274   72.19760895]]
375
[[  83.          205.          155.          298.        ]
 [ 179.          189.          213.          243.        ]
 [ 322.          264.          367.          372.        ]
 ..., 
 [   0.          109.87038422   79.34774017  157.16184998]
 [   0.           17.60726929   67.01171875   80.9130249 ]
 [ 242.29234314  472.23040771  299.17581177  499.375     ]]
375
[[  86.          356.          158.          407.        ]
 [ 182.          377.          236.          431.        ]
 [ 245.          364.          286.          408.        ]
 ..., 
 [ 106.27524567  199.19490051  123.72956085  217.07362366]
 [   0.           97.31678009  135.93530273  286.2640686 ]
 [   0.72247386   45.84080124   30.63024521   67.07563019]]
246
[[  31.            6.          218.          476.        ]
 [   0.          288.03485107  169.72756958  499.5       ]
 [  40.41970825   33.98550415  179.32223511  383.74053955]
 ..., 
 [ 160.13444519  391.48733521  245.5         499.5       ]
 [  14.99834061  421.46661377   34.8533287   442.65734863]
 [ 175.01979065  196.55758667  245.5         239.19119263]]
375
[[  66.            1.          338.          430.        ]
 [ 147.91397095    0.          374.375       404.11297607]
 [  95.96714783    0.          374.375       499.375     ]
 ..., 
 [ 105.84953308  163.61122131  209.22541809  230.25390625]
 [ 179.93040466  261.96389771  204.88821411  282.41311646]
 [  41.28894806  212.25627136   59.85113907  232.0730896 ]]
392
[[  60.           84.          283.          382.        ]
 [ 194.          103.          228.          142.        ]
 [ 284.          297.          302.          350.        ]
 ..., 
 [  38.70513153  328.39962769   66.02839661  353.67880249]
 [ 249.12281799   31.71638489  391.34667969  141.38876343]
 [ 277.27908325   55.28673935  316.93026733   94.16706848]]
500
[[ 208.           76.          335.          241.        ]
 [ 178.15527344   12.49820995  387.38162231  333.44332886]
 [ 115.0560379     0.          413.27005005  333.44332886]
 ..., 
 [ 430.39144897    0.          474.08114624   24.08320236]
 [ 340.22451782  237.17948914  377.86608887  256.72860718]
 [   0.          288.49484253   87.7565918   333.44332886]]
333
[[   0.          287.           79.          445.        ]
 [  83.          224.          233.          352.        ]
 [ 165.          127.          203.          163.        ]
 ..., 
 [  54.28930283  221.87992859  131.3984375   499.5       ]
 [  94.1647644    17.44036102  177.03636169  380.20162964]
 [ 214.2243042   392.89129639  231.33926392  412.49514771]]
333
[[ 141.          298.          184.          342.        ]
 [ 144.54714966  314.00982666  182.88343811  368.17211914]
 [ 151.05352783  295.89630127  191.74209595  334.37814331]
 ..., 
 [ 137.10491943  169.89427185  332.44500732  277.79324341]
 [ 108.56687927  355.3265686   308.32736206  441.8777771 ]
 [ 245.7233429    82.08786774  328.21020508  118.60829926]]
333
[[ 182.          356.          241.          422.        ]
 [  33.          246.          231.          353.        ]
 [ 193.          140.          236.          245.        ]
 ..., 
 [ 175.26045227  144.95214844  243.37728882  196.7366333 ]
 [   0.          314.1890564    54.12233353  364.83639526]
 [ 255.21389771   32.31143951  311.38470459   76.36971283]]
333
[[ 105.           71.          164.          172.        ]
 [ 119.          468.          173.          498.        ]
 [  18.          327.          104.          341.        ]
 ..., 
 [ 161.70184326  309.98577881  305.54998779  451.24188232]
 [ 204.78596497  147.61759949  297.98565674  404.06033325]
 [ 277.70620728  299.68603516  314.28839111  363.8269043 ]]
333
[[ 140.          234.          211.          300.        ]
 [ 253.          235.          263.          264.        ]
 [ 240.          266.          265.          302.        ]
 ..., 
 [ 260.4128418   154.22442627  285.20117188  180.65913391]
 [ 222.69340515    7.43725586  249.1967926    35.39760208]
 [ 300.85925293  348.57052612  332.44500732  499.5       ]]
333
[[ 107.          189.          265.          274.        ]
 [ 288.          427.          317.          459.        ]
 [ 283.98187256  434.58459473  313.11871338  467.92623901]
 ..., 
 [ 229.0025177     0.          299.72232056  133.33537292]
 [ 257.49331665   95.57607269  275.86291504  131.23698425]
 [ 101.20661926  451.48669434  122.46312714  469.75909424]]
333
[[  77.          237.           93.          258.        ]
 [  75.          275.           85.          291.        ]
 [  98.          268.          121.          292.        ]
 ..., 
 [  19.53542328   71.3576889    87.08076477  161.43934631]
 [ 191.55793762  249.0358429   219.93688965  281.21743774]
 [ 213.7323761   317.81057739  244.98446655  366.39419556]]
333
[[  14.           51.          246.          437.        ]
 [   0.            0.          266.09786987  499.5       ]
 [   0.           18.8500824   234.12156677  371.35238647]
 ..., 
 [ 130.34989929   44.63561249  170.60877991  103.59123993]
 [ 139.30484009  464.86230469  230.52554321  499.5       ]
 [   0.          437.93130493  106.79476929  475.75418091]]
333
[[  23.          134.          166.          449.        ]
 [ 155.          327.          187.          346.        ]
 [ 124.          114.          187.          178.        ]
 ..., 
 [ 129.6708374   234.78689575  146.88955688  301.96405029]
 [  35.50384521  126.50182343   84.81660461  156.46157837]
 [ 242.84651184   60.27804184  309.20587158  252.24510193]]
500
[[ 164.          214.          187.          242.        ]
 [ 175.          250.          197.          286.        ]
 [ 141.          240.          156.          258.        ]
 ..., 
 [ 441.29452515   84.11763     470.58566284  131.15428162]
 [ 103.6620636   180.78053284  204.74554443  238.23403931]
 [ 466.90090942   54.65460205  499.5          96.4280777 ]]
500
[[  89.          119.          333.          249.        ]
 [  70.          244.           80.          278.        ]
 [ 220.          110.          235.          134.        ]
 ..., 
 [ 260.78027344  143.24635315  370.70706177  200.80062866]
 [ 323.58297729  213.80528259  381.33380127  253.80227661]
 [ 149.28717041  113.8033905   236.60136414  155.31022644]]
333
[[  36.          182.           95.          394.        ]
 [  89.          295.          144.          414.        ]
 [ 101.          144.          301.          440.        ]
 ..., 
 [ 237.19822693   11.97624302  296.93899536   64.25899506]
 [  41.03226852  265.259552    125.40395355  314.97216797]
 [ 118.84250641  192.99371338  146.55778503  245.16351318]]
500
[[ 145.           19.          346.          328.        ]
 [ 116.89949799    0.          292.18988037  334.44165039]
 [  69.18979645    0.90775168  257.47625732  334.44165039]
 ..., 
 [ 415.86328125   41.1743927   493.06219482  108.38277435]
 [  29.77953529  120.77758789   52.01409531  138.15756226]
 [ 416.32855225  166.09536743  499.70831299  307.13150024]]
333
[[   5.          168.          315.          279.        ]
 [   0.            0.          242.23205566  469.00564575]
 [  46.72916794    0.          312.23358154  455.73034668]
 ..., 
 [  21.09797668  159.93251038  103.72969055  207.35346985]
 [   0.            0.           37.69489288   46.93934631]
 [   0.          401.44165039   80.96292877  446.7835083 ]]
333
[[  80.          157.          231.          407.        ]
 [  79.08106995    0.          252.34889221  499.5       ]
 [  48.54963684   57.92895508  235.63188171  436.31414795]
 ..., 
 [   0.          167.21560669   62.21064377  215.70875549]
 [  36.07328033  140.03205872   99.52841187  466.65826416]
 [ 121.99021149  311.14828491  192.49710083  396.59356689]]
500
[[  75.          148.          234.          208.        ]
 [ 221.35163879    0.          448.29858398  349.41665649]
 [  58.33430862  154.95671082  165.90478516  203.98696899]
 ..., 
 [ 289.57757568  126.78663635  317.28356934  148.55096436]
 [ 162.24876404   25.24903297  282.56628418  132.13822937]
 [ 475.85818481   13.63936901  499.33334351   81.67105103]]
222
[[   5.            4.          216.          492.        ]
 [   0.            0.          218.79428101  499.5       ]
 [  75.72896576    0.          221.5         374.58328247]
 ..., 
 [  50.93426895  386.61395264  148.45310974  499.5       ]
 [  79.03959656  277.59298706  123.77616882  332.81155396]
 [ 144.28489685   80.12510681  197.13551331  123.86643982]]
333
[[  80.            0.          226.          496.        ]
 [  69.64160919    0.          234.57582092  422.90005493]
 [  38.77626419    0.          182.80799866  499.5       ]
 ..., 
 [  60.75709534  395.66384888  147.02865601  436.00494385]
 [ 220.34828186   53.8980484   240.41468811   88.62075806]
 [ 190.64588928  151.52966309  279.43997192  386.41046143]]
402
[[ 129.          243.          296.          315.        ]
 [   0.          225.10812378  160.118927    499.15002441]
 [   0.          297.79309082  175.28070068  499.15002441]
 ..., 
 [  71.14266968   98.1839447    92.50514221  119.34568787]
 [ 372.07824707  268.65829468  394.86029053  287.94741821]
 [ 118.28834534  223.60928345  167.71879578  344.51586914]]
375
[[   0.            0.           36.          203.        ]
 [   2.            0.          270.          495.        ]
 [ 238.          136.          362.          497.        ]
 ..., 
 [   6.65486336    9.42378235   49.45007706   78.81999207]
 [ 279.49316406  388.48678589  305.55841064  428.63088989]
 [ 166.30041504  165.05995178  282.49191284  221.67512512]]
338
[[ 123.          241.          163.          303.        ]
 [ 143.          301.          175.          321.        ]
 [ 173.          274.          190.          294.        ]
 ..., 
 [ 215.76191711  421.61990356  231.36732483  444.54092407]
 [  14.43470478   84.52444458   38.75333786  111.91527557]
 [  48.69537735  260.0050354   137.99121094  312.22894287]]
336
[[ 160.           99.          305.          395.        ]
 [ 142.73239136    0.          335.44000244  470.8475647 ]
 [ 121.43623352   10.72861099  293.14849854  445.71166992]
 ..., 
 [ 238.31832886  436.08370972  261.5791626   460.94662476]
 [  87.06885529  465.4206543   209.88723755  499.52001953]
 [ 294.41723633  105.21533203  312.49768066  122.66990662]]
353
[[  98.            1.          277.          497.        ]
 [  29.31843758    0.          330.30142212  384.24017334]
 [  56.85991669    0.          339.25418091  499.49499512]
 ..., 
 [ 205.97175598  334.64453125  261.46881104  401.68423462]
 [  56.49246979   96.24829865  193.6242981   259.91101074]
 [  47.9693222   316.88659668   66.19342804  334.14129639]]
253
[[  40.          106.          190.          271.        ]
 [ 112.01608276    0.          252.57833862  302.7543335 ]
 [  39.04425812    9.132658    240.68647766  336.48999023]
 ..., 
 [ 128.35760498  119.93751526  182.69702148  220.0892334 ]
 [ 153.13215637  164.55445862  199.08224487  241.06349182]
 [ 223.96788025  259.43621826  239.78691101  291.75546265]]
500
[[ 120.           16.          365.          331.        ]
 [ 150.2117157     0.          364.2305603   374.375     ]
 [ 104.64643097   16.97635651  299.66201782  374.375     ]
 ..., 
 [ 257.40292358  128.89657593  350.70333862  307.19415283]
 [  50.58459854  336.08929443  159.94812012  362.56924438]
 [ 268.21282959   16.33600807  383.20782471   48.24586487]]
345
[[ 118.           33.          199.          477.        ]
 [ 113.           12.          120.           19.        ]
 [  97.           32.          101.           39.        ]
 ..., 
 [   0.          310.23352051   59.61352921  361.47015381]
 [ 136.48506165  269.05453491  159.04620361  340.91888428]
 [ 106.78388214  353.82434082  122.59055328  373.64050293]]
500
[[  69.           40.           76.           66.        ]
 [  47.           59.           57.           81.        ]
 [  51.          115.           62.          141.        ]
 ..., 
 [ 358.30407715   42.12024689  457.26373291  171.44511414]
 [ 209.17883301   24.92095566  397.16357422  170.48947144]
 [ 402.29675293  135.85562134  499.67501831  294.9447937 ]]
500
[[ 138.           36.          373.          352.        ]
 [  58.95191193    0.          365.63284302  374.375     ]
 [ 145.09306335    0.          411.96228027  374.375     ]
 ..., 
 [ 253.74041748  228.21749878  327.70574951  330.36224365]
 [   0.           18.29785347   47.60041046   34.0071373 ]
 [ 307.29147339   85.0089798   377.02850342  169.0216217 ]]
500
[[  92.           10.          332.          279.        ]
 [  66.14163208   49.19984818  321.2980957   332.44500732]
 [ 114.85314178    0.          348.4720459   332.44500732]
 ..., 
 [ 172.94764709  171.57420349  208.75578308  215.39982605]
 [ 325.26986694   86.71669769  345.72085571  109.41853333]
 [ 461.07904053   89.37909698  495.65063477  117.62047577]]
333
[[ 102.           47.          258.          498.        ]
 [ 105.          296.          121.          317.        ]
 [  14.12429142   35.42963791  241.70825195  490.74127197]
 ..., 
 [ 108.79839325  179.00984192  131.53248596  215.84420776]
 [ 137.57284546  415.98556519  156.39205933  490.16168213]
 [  71.56828308  177.87124634   98.04529572  218.87103271]]
353
[[  36.           28.          150.          465.        ]
 [ 198.           28.          299.          454.        ]
 [  40.65099716  132.27345276  159.83190918  499.49499512]
 ..., 
 [  27.56300354   47.42104721   56.71196747   91.35930634]
 [ 272.6390686   326.13241577  306.83526611  396.28485107]
 [  35.69573593  319.76593018  268.78744507  450.81774902]]
333
[[  69.          264.          108.          293.        ]
 [ 140.47612       0.          331.58404541  340.25460815]
 [   0.          199.61904907  155.53727722  499.5       ]
 ..., 
 [ 181.44915771  378.11355591  228.40652466  404.52716064]
 [  74.46471405  483.45492554  170.23446655  499.5       ]
 [ 247.81271362  479.17993164  290.98345947  499.5       ]]
500
[[ 216.          198.          351.          372.        ]
 [ 258.29858398  288.28704834  329.03744507  349.12414551]
 [ 151.50479126   84.69524384  377.76913452  374.375     ]
 ..., 
 [ 308.02731323  285.19488525  405.77407837  374.375     ]
 [ 108.37433624  274.93865967  389.89416504  361.32009888]
 [ 126.81069183  302.73236084  149.48651123  329.93179321]]
333
[[  72.          219.          166.          350.        ]
 [  19.17310905   94.9444809   247.16867065  499.5       ]
 [  66.66061401   99.41864014  202.69403076  499.5       ]
 ..., 
 [ 158.38275146  221.55065918  190.28773499  311.70776367]
 [   0.          195.00880432   23.38874626  398.75976562]
 [  55.40477753   54.39187241  266.03857422  133.89108276]]
375
[[ 149.          136.          332.          222.        ]
 [ 142.35253906    0.          374.375       485.55075073]
 [ 147.47225952  139.14105225  332.32937622  242.01228333]
 ..., 
 [ 299.89379883  104.40067291  322.35708618  123.88738251]
 [ 150.99691772  400.49819946  181.48277283  432.8772583 ]
 [ 260.89978027  204.5307312   316.33453369  242.3469696 ]]
333
[[  93.          190.          215.          497.        ]
 [ 144.          481.          164.          498.        ]
 [ 200.          448.          211.          474.        ]
 ..., 
 [   0.          320.82861328  163.25215149  464.42272949]
 [ 272.04348755  453.94104004  315.95700073  481.07452393]
 [ 164.4563446   330.84411621  203.53598022  386.1914978 ]]
500
[[ 134.          205.          218.          316.        ]
 [ 133.40852356  203.27090454  213.11003113  302.57980347]
 [ 278.52017212    0.          499.375       374.375     ]
 ..., 
 [   0.          198.15522766   69.97322083  246.96600342]
 [ 226.24154663  165.56903076  369.09384155  284.49484253]
 [ 350.3727417   149.6051178   456.07113647  205.84013367]]
375
[[ 122.          111.          275.          417.        ]
 [  58.75318909    0.          215.76913452  499.375     ]
 [  38.68281174   47.14134216  239.36737061  463.90386963]
 ..., 
 [ 169.37187195  408.46725464  211.86587524  464.87310791]
 [  99.70188141  253.58488464  154.5345459   336.01153564]
 [ 283.30780029  410.81896973  342.10998535  470.39828491]]
500
[[   0.          106.           64.          177.        ]
 [  64.          128.          262.          248.        ]
 [ 272.           85.          383.          371.        ]
 ..., 
 [ 166.62413025   57.56497955  212.85951233   94.26826477]
 [ 114.5470047    19.5508709   308.96411133  147.73623657]
 [ 445.16500854  356.66357422  467.68667603  374.375     ]]
333
[[  88.          128.          249.          424.        ]
 [  93.31765747    0.          286.90872192  499.5       ]
 [  30.89064217    0.          219.53868103  499.5       ]
 ..., 
 [ 130.98098755  394.1126709   195.18612671  432.75183105]
 [  94.93169403   91.85453033  177.59855652  131.09312439]
 [ 196.10586548   75.70959473  224.47496033   95.72480774]]
500
[[ 148.          178.          198.          230.        ]
 [  77.85683441  105.20965576  245.14997864  333.44332886]
 [  99.26596069   55.165905    297.09832764  333.44332886]
 ..., 
 [ 371.63265991  125.59939575  401.98803711  161.4309082 ]
 [ 226.77078247   34.83387375  417.45983887  127.14746094]
 [ 451.92810059   48.64175415  499.32998657   89.95614624]]
500
[[ 303.          246.          338.          285.        ]
 [ 194.          251.          244.          332.        ]
 [ 309.59188843  253.05410767  341.85870361  290.27099609]
 ..., 
 [ 174.98031616  198.04794312  273.59976196  237.12397766]
 [  46.90124893  306.93978882   70.42563629  331.91589355]
 [ 317.73873901  337.48867798  339.33734131  368.8732605 ]]
500
[[  23.            3.          193.          396.        ]
 [  46.          363.           55.          396.        ]
 [ 198.            5.          363.          393.        ]
 ..., 
 [ 395.39221191    0.          499.48999023   24.91871262]
 [  24.75967026   24.42593575   44.05779648   54.95000839]
 [  39.85932541   66.59990692  103.88843536  207.0819397 ]]
500
[[  64.          253.          370.          374.        ]
 [ 259.          324.          324.          350.        ]
 [ 293.          321.          322.          334.        ]
 ..., 
 [ 284.56921387  201.02030945  307.66955566  236.80435181]
 [  82.23374939  182.9322052   131.1960144   235.75061035]
 [ 165.35240173  212.76739502  203.49899292  245.34315491]]
439
[[   0.          403.          295.          498.        ]
 [ 143.          207.          406.          414.        ]
 [ 156.           59.          362.          244.        ]
 ..., 
 [   0.            0.           70.99180603   24.09710503]
 [ 132.8470459   181.49868774  160.16552734  202.98719788]
 [  39.3878212     0.          138.95619202  183.99662781]]
500
[[ 232.          136.          341.          230.        ]
 [  26.63333893    0.          270.18179321  369.39855957]
 [  40.36006927    0.          364.80404663  374.375     ]
 ..., 
 [ 223.65214539  191.94625854  348.65719604  245.86425781]
 [ 313.69503784   74.58969879  499.375       199.27833557]
 [ 439.60171509  150.93492126  499.375       198.43908691]]
500
[[ 170.          109.          301.          143.        ]
 [ 231.          136.          256.          189.        ]
 [ 262.          134.          268.          139.        ]
 ..., 
 [ 260.82937622  133.46937561  499.375       228.12353516]
 [  61.88949966  288.35150146  188.58270264  374.375     ]
 [ 435.28356934  179.51808167  460.50689697  216.36775208]]
375
[[  42.           78.          345.          451.        ]
 [   0.           42.25143433  288.06057739  499.375     ]
 [ 129.2326355     0.          374.375       499.375     ]
 ..., 
 [  91.38253784   96.27643585  110.49077606  131.53697205]
 [  37.08605194  179.28652954  174.14535522  332.07983398]
 [   3.65048885  262.35769653  119.9754715   322.65753174]]
400
[[  83.           72.          345.          438.        ]
 [  19.00470924    0.          261.24996948  499.33334351]
 [  80.05706787    0.          399.33334351  499.33334351]
 ..., 
 [ 366.59616089  385.4178772   386.68331909  408.30258179]
 [   0.           69.77007294  110.43603516  108.001091  ]
 [ 261.23233032   91.57666016  284.0921936   109.57073975]]
500
[[ 177.          205.          408.          332.        ]
 [ 415.          128.          498.          332.        ]
 [ 436.          198.          449.          207.        ]
 ..., 
 [   0.           39.94765091   61.16650009   87.68202972]
 [   0.            0.           37.88882446   93.58829498]
 [ 350.52362061  138.06948853  444.95248413  266.72833252]]
500
[[ 138.          145.          150.          178.        ]
 [ 143.13275146    0.          394.64135742  332.44500732]
 [   0.            0.          499.5         322.84738159]
 ..., 
 [  40.34949875   93.52068329  228.84928894  278.09655762]
 [ 441.840271    237.70950317  485.84277344  258.02020264]
 [ 135.08885193   62.52449036  178.11387634  105.63095856]]
500
[[  28.           76.          243.          317.        ]
 [ 358.          130.          443.          301.        ]
 [  20.51073647    0.          236.83981323  371.38000488]
 ..., 
 [ 320.50265503  122.02947235  344.51025391  184.50125122]
 [ 291.05514526  181.00585938  422.00704956  229.56884766]
 [ 292.14831543  219.71995544  321.31567383  261.86898804]]
500
[[   0.            0.           81.          188.        ]
 [  77.            0.          287.          228.        ]
 [ 298.           86.          445.          373.        ]
 ..., 
 [ 283.81973267  143.23957825  428.13635254  242.46958923]
 [ 300.34683228    0.          341.99099731   24.69894409]
 [ 198.72846985  307.16897583  243.74681091  325.43426514]]
447
[[  52.           51.          374.          442.        ]
 [  45.89918518    0.          347.60488892  499.14996338]
 [   8.49315453   88.7095871   359.10336304  473.45385742]
 ..., 
 [ 254.30978394  274.90811157  384.18313599  370.62033081]
 [ 162.55099487  300.79666138  301.91101074  444.19213867]
 [ 310.78717041    0.          418.35757446  242.40760803]]
500
[[  33.          109.          178.          233.        ]
 [ 209.          155.          267.          213.        ]
 [ 312.          106.          455.          233.        ]
 ..., 
 [   0.          227.63735962   44.13728714  271.21374512]
 [ 441.04492188  242.65971375  499.5         272.73687744]
 [ 326.16467285  143.74769592  375.12329102  174.56278992]]
500
[[  54.           66.          259.          317.        ]
 [  44.82759094  155.56428528  359.19403076  317.24197388]
 [   0.          153.25128174  364.62432861  353.90396118]
 ..., 
 [   5.24308443   99.13050842  136.86799622  204.75787354]
 [  96.28437805  190.78147888  130.22412109  245.63999939]
 [ 103.63547516  325.66223145  131.45500183  354.35327148]]
333
[[  57.          136.          179.          264.        ]
 [   0.           85.           83.          263.        ]
 [   0.            0.          162.40054321  439.02166748]
 ..., 
 [  74.12549591  426.20944214   96.99935913  458.92422485]
 [ 144.05352783    0.          208.13485718  133.82466125]
 [ 244.01826477   73.51417542  270.27258301  107.48487854]]
500
[[  72.           76.          240.          222.        ]
 [ 237.           65.          408.          208.        ]
 [  88.          234.          248.          398.        ]
 ..., 
 [   0.           54.67423248   24.02252197   94.38569641]
 [ 103.16824341  115.2270813   168.15887451  186.05990601]
 [ 301.89035034  178.58178711  390.20556641  234.62652588]]
375
[[  52.           52.          358.          469.        ]
 [  18.78573418    0.          243.48284912  499.375     ]
 [ 101.18969727   78.47732544  374.375       411.95330811]
 ..., 
 [ 159.32962036  283.77990723  196.48095703  343.62600708]
 [ 324.88934326  149.15003967  362.89263916  248.16902161]
 [ 208.65319824  228.63180542  258.94216919  287.59658813]]
500
[[ 170.           88.          235.          188.        ]
 [ 108.41466522    0.          293.11187744  332.44500732]
 [  56.69649887    0.          300.59848022  290.35021973]
 ..., 
 [ 407.56695557   91.21970367  483.52734375  140.49394226]
 [  91.87126923   81.39736176  117.5452652   107.88528442]
 [ 287.40023804  308.11022949  313.08401489  332.44500732]]
375
[[ 122.          253.          308.          405.        ]
 [ 222.          301.          243.          319.        ]
 [ 134.43606567   86.56879425  356.44604492  499.375     ]
 ..., 
 [ 267.23348999  292.65463257  289.67440796  321.74072266]
 [ 332.70431519  237.2545929   351.79449463  254.25918579]
 [  39.90129471  429.65628052  141.87875366  475.67129517]]
375
[[  67.           50.          310.          494.        ]
 [  68.26831818    0.          350.34689331  482.40081787]
 [ 148.77587891  137.26667786  298.32531738  499.375     ]
 ..., 
 [ 299.56530762  216.61437988  327.47625732  300.24597168]
 [  15.50777817  157.24893188   47.49469376  199.42308044]
 [ 118.41973877   49.37096024  199.24581909  132.93917847]]
500
[[ 288.          216.          351.          301.        ]
 [ 239.          200.          424.          225.        ]
 [   0.            0.          232.01060486  385.35665894]
 ..., 
 [ 281.44140625  125.01324463  421.72232056  172.54714966]
 [ 212.40098572  266.83724976  300.30163574  385.35665894]
 [ 247.85783386  249.04719543  339.20410156  302.71115112]]
307
[[  42.           23.          262.          492.        ]
 [  55.36457825    0.          305.12347412  354.11422729]
 [  99.49317169    0.          269.95220947  425.8086853 ]
 ..., 
 [  81.3334198   150.02058411  137.43270874  218.57867432]
 [ 248.99430847   42.41653824  279.39694214   65.0335083 ]
 [ 153.28733826  256.40237427  201.96456909  401.45001221]]
500
[[ 306.          137.          363.          200.        ]
 [ 148.69192505    0.          445.59103394  351.41333008]
 [  88.70321655    0.          499.25332642  226.98417664]
 ..., 
 [ 381.1043396   248.332901    402.75579834  277.13916016]
 [ 247.80621338   56.36778259  375.83990479  124.63484955]
 [ 394.54187012   79.88251495  442.99066162  131.1756897 ]]
403
[[  87.            0.          294.          293.        ]
 [  53.19541168    0.          305.26141357  444.58584595]
 [  87.0355835     0.          363.03359985  358.56640625]
 ..., 
 [   5.38231134   74.82303619   28.79680252  106.88651276]
 [   0.          103.66915131   76.34603119  160.61206055]
 [ 223.41772461  106.77688599  274.04867554  208.26585388]]
500
[[  87.          107.          303.          253.        ]
 [ 267.          110.          295.          134.        ]
 [ 298.          118.          332.          144.        ]
 ..., 
 [  20.15997124  175.1966095    38.60624695  198.71635437]
 [ 463.76278687  157.28526306  493.2644043   201.14474487]
 [   0.          193.00209045   63.49472809  333.44332886]]
500
[[ 178.          158.          305.          248.        ]
 [ 123.62065887    0.          372.09555054  362.9954834 ]
 [ 106.36604309    0.          344.37149048  195.31184387]
 ..., 
 [ 360.03799438   66.37179565  459.28070068  336.0065918 ]
 [  58.20251465  339.52178955  348.5802002   374.375     ]
 [  99.65516663  166.82069397  125.50054169  188.62028503]]
500
[[ 427.          266.          447.          290.        ]
 [   5.           67.           86.          267.        ]
 [   0.            0.          156.45889282  341.46414185]
 ..., 
 [ 313.98501587  271.76815796  331.54589844  288.96713257]
 [ 162.21307373  219.6055603   210.04598999  310.51345825]
 [ 376.26419067  303.26473999  406.9826355   351.88412476]]
362
[[ 101.          107.          197.          204.        ]
 [  30.30814743    0.          361.39666748  428.11697388]
 [  80.37373352   92.35310364  228.46684265  223.86943054]
 ..., 
 [ 230.57746887  276.07699585  261.162323    309.64663696]
 [ 105.45769501  459.47442627  123.63227081  476.47647095]
 [  53.87189102  453.58605957   73.02880859  471.0866394 ]]
500
[[   1.          143.          396.          294.        ]
 [   0.            1.          395.          144.        ]
 [ 404.            5.          498.          291.        ]
 ..., 
 [ 446.42385864   34.39154053  499.5         142.72232056]
 [ 189.0098114    62.46141052  292.65032959  103.39653015]
 [ 154.58418274  214.06800842  259.89874268  284.2545166 ]]
375
[[  30.          229.          342.          367.        ]
 [ 102.31019592   92.05895996  350.71105957  499.375     ]
 [  52.26557541   97.25749969  311.98687744  499.375     ]
 ..., 
 [ 280.24456787  123.77941895  298.75259399  141.95716858]
 [   0.          294.90829468   27.09869576  337.69332886]
 [ 264.4442749    71.14217377  286.65911865   91.74188232]]
500
[[  14.           50.          480.          321.        ]
 [ 206.64733887    0.          499.375       374.375     ]
 [ 250.37690735    0.          451.76071167  374.375     ]
 ..., 
 [ 116.41083527  250.72015381  232.37098694  301.56011963]
 [ 432.49099731  162.61557007  491.1723938   211.73379517]
 [ 344.84991455  341.63793945  434.12597656  374.375     ]]
500
[[  28.           74.          308.          333.        ]
 [  72.2821579     0.          298.26132202  333.44332886]
 [ 137.82617188    0.          382.60775757  333.44332886]
 ..., 
 [   0.           74.07213593   89.30027008  243.83830261]
 [ 180.27723694  179.19839478  289.66516113  299.39138794]
 [ 181.92729187    5.62061882  291.8626709    48.77406693]]
500
[[ 278.          160.          314.          180.        ]
 [ 235.65003967    0.          433.94195557  332.44500732]
 [ 287.11993408  155.43339539  319.60836792  186.70228577]
 ..., 
 [  93.61529541  289.50726318  166.68559265  331.67593384]
 [ 101.55580902   42.97164917  147.86730957   79.19282532]
 [  39.42337799  112.53634644  340.04501343  272.48501587]]
291
[[  14.           33.          263.          461.        ]
 [  86.66487122   51.72718811  290.5         499.5       ]
 [  99.14456177   13.9316864   262.59454346  442.21490479]
 ..., 
 [ 119.72952271  281.46060181  203.38189697  344.46908569]
 [   0.          120.82078552  105.74303436  162.31272888]
 [  25.48783112  143.63414001  199.76757812  258.26226807]]
500
[[  66.          152.          242.          429.        ]
 [   0.            0.          230.92298889  499.16665649]
 [   0.            0.          354.00268555  499.16665649]
 ..., 
 [ 156.08551025  376.35482788  181.67123413  399.91418457]
 [ 215.21595764  313.99734497  244.75538635  380.25924683]
 [ 133.98097229   60.82766724  167.63568115   92.3585434 ]]
500
[[ 132.          160.          336.          374.        ]
 [ 397.          254.          419.          290.        ]
 [ 392.65823364  280.09295654  422.01690674  312.69287109]
 ..., 
 [   0.           36.90612411   71.29075623   85.49214935]
 [ 101.64852142  162.42073059  121.78464508  189.67826843]
 [  89.4525528    77.35612488  108.82909393   94.7322998 ]]
375
[[ 123.          200.          151.          256.        ]
 [ 159.          210.          176.          239.        ]
 [ 184.          212.          214.          241.        ]
 ..., 
 [ 100.79357147  269.06210327  130.9463501   340.54266357]
 [ 150.04240417  285.21429443  311.99371338  467.78341675]
 [ 280.15151978  345.57363892  303.42297363  375.25076294]]
500
[[ 338.           91.          425.          273.        ]
 [ 371.78164673  224.61729431  417.32104492  269.17697144]
 [ 331.23779297   88.93819427  427.12533569  245.57531738]
 ..., 
 [   0.          264.3611145    64.74085999  310.05700684]
 [ 395.14767456   75.58132935  419.96859741  102.0880127 ]
 [ 437.94561768   95.52855682  454.95040894  115.10231018]]
333
[[  64.          138.          174.          297.        ]
 [  46.2862587     0.          213.98556519  461.3187561 ]
 [  70.79896545    0.          187.47064209  419.70422363]
 ..., 
 [ 217.11715698  236.790802    237.09448242  273.89581299]
 [  11.25738907  409.3979187    97.17712402  499.5       ]
 [ 259.37106323  296.73666382  275.37658691  315.48971558]]
500
[[  41.          128.          332.          332.        ]
 [  68.48210144   34.72488022  353.66952515  332.44500732]
 [   0.           55.98225403  210.84095764  332.44500732]
 ..., 
 [ 216.44845581  134.22938538  272.49124146  190.6555481 ]
 [ 318.3963623   207.25816345  417.51217651  261.32254028]
 [ 153.81352234  187.62440491  183.64616394  213.89356995]]
333
[[ 156.          394.          173.          417.        ]
 [ 148.13819885  395.46810913  164.93206787  419.38195801]
 [ 153.08790588  404.10232544  172.49285889  428.48736572]
 ..., 
 [ 170.15126038  261.07894897  255.24383545  300.55697632]
 [ 183.61203003  198.92803955  200.85839844  218.24368286]
 [ 292.57672119  209.7376709   332.44500732  246.69554138]]
333
[[  30.          206.          278.          324.        ]
 [   8.6602726   195.52088928  256.97763062  302.06188965]
 [  62.90512085   39.11408615  232.67781067  499.5       ]
 ..., 
 [  72.13871765   50.48267365  184.56513977   85.20347595]
 [ 125.23284912  296.38171387  234.17121887  325.02087402]
 [ 192.28372192    9.84192562  292.27151489   53.47312546]]
500
[[  0.00000000e+00   2.00000000e+00   4.99000000e+02   4.83000000e+02]
 [  1.85641083e+02   0.00000000e+00   4.99550018e+02   4.84191681e+02]
 [  4.98143578e+01   2.45228335e-01   4.79013214e+02   4.13587158e+02]
 ..., 
 [  2.54698517e+02   8.72995071e+01   4.23368805e+02   1.70031891e+02]
 [  3.53193970e+02   3.15948853e+02   4.46127197e+02   4.55739563e+02]
 [  3.96291809e+01   1.45624191e+02   1.09925026e+02   2.06165329e+02]]
375
[[  51.          287.          337.          440.        ]
 [   0.          284.           48.          439.        ]
 [  88.69779205  165.0141449   374.375       499.375     ]
 ..., 
 [ 239.92332458  234.86299133  255.57551575  253.67433167]
 [ 246.29013062  258.08279419  263.59805298  280.14584351]
 [ 267.01168823  293.43981934  306.67617798  349.3571167 ]]
500
[[  40.          164.          374.          364.        ]
 [ 124.69684601   67.67445374  318.87094116  402.32830811]
 [  39.22815704  164.96388245  366.9319458   402.32830811]
 ..., 
 [ 356.81216431  332.17633057  410.70907593  357.2979126 ]
 [ 343.10302734  104.30199432  495.27166748  277.33288574]
 [ 388.67251587  151.52638245  499.04830933  205.21359253]]
500
[[  28.           64.          199.          310.        ]
 [ 280.           35.          454.          310.        ]
 [ 272.94192505   45.33731461  456.38308716  374.375     ]
 ..., 
 [ 391.42227173   24.2525425   434.47235107   79.75635529]
 [   4.33801031  288.38543701   29.29613495  310.94299316]
 [  84.21566772  273.8984375   140.28399658  349.81420898]]
500
[[   2.           22.          136.          397.        ]
 [ 171.           12.          320.          408.        ]
 [ 342.           25.          492.          409.        ]
 ..., 
 [ 319.26055908   59.44543457  443.33633423  225.66514587]
 [  13.674613    108.09988403  125.14176941  259.61489868]
 [ 275.77487183   70.29462433  328.53997803  136.18115234]]
393
[[ 101.          409.          144.          433.        ]
 [ 154.          414.          297.          440.        ]
 [  65.           69.          318.          419.        ]
 ..., 
 [ 215.61219788  141.21913147  245.72512817  173.35929871]
 [ 339.23602295  294.79284668  355.41009521  311.85235596]
 [  45.94297028  479.24780273  349.15856934  499.11001587]]
375
[[ 115.          147.          263.          320.        ]
 [  73.08909607    0.          315.53561401  499.375     ]
 [ 136.92552185  158.1229248   260.0824585   284.13922119]
 ..., 
 [ 195.05622864   55.95060349  321.25030518  108.54047394]
 [ 172.81008911   64.90670013  374.375       259.32406616]
 [ 225.90480042  355.70196533  244.55470276  371.78375244]]
375
[[  96.           71.          317.          439.        ]
 [  92.69896698    0.          362.43179321  499.375     ]
 [  45.11194229    0.          292.84719849  461.4503479 ]
 ..., 
 [ 156.43986511  388.66912842  192.85536194  435.11123657]
 [ 327.72006226  255.64468384  358.48803711  285.75500488]
 [ 286.31268311  438.91723633  301.89810181  458.76815796]]
333
[[ 173.          174.          287.          329.        ]
 [ 265.          135.          332.          399.        ]
 [ 102.          146.          181.          384.        ]
 ..., 
 [ 180.23001099  170.56227112  318.628479    262.87069702]
 [  98.95088959  348.84500122  153.53509521  396.71557617]
 [  11.61074734    0.           77.54219818  150.12547302]]
500
[[  50.           21.          410.          294.        ]
 [  81.77106476    0.          332.10244751  321.42227173]
 [  49.08096695    0.          292.36209106  218.63331604]
 ..., 
 [ 307.54840088  242.04534912  406.09130859  289.49438477]
 [  18.57480621   34.88405991   40.63757324   61.16778564]
 [ 371.4420166   152.94711304  395.11819458  182.2109375 ]]
375
[[  86.           58.          293.          410.        ]
 [   0.          119.90772247  350.62033081  456.25616455]
 [  58.73167801    1.55830383  284.32174683  499.375     ]
 ..., 
 [ 212.51765442  163.10237122  236.51153564  195.98765564]
 [ 277.9480896     1.63665056  374.375        52.73417282]
 [ 121.59104919  430.28448486  207.01750183  469.26864624]]
335
[[  69.            0.          221.          425.        ]
 [  44.78342438   54.89177322  224.5657196   436.13186646]
 [  72.39266968    0.          272.79452515  406.78057861]
 ..., 
 [  15.74450779  177.49284363  117.10613251  231.47761536]
 [  17.09458733  172.24343872   94.18655396  208.03559875]
 [  37.38644028  224.05627441  133.74981689  283.71356201]]
394
[[  79.          420.          138.          484.        ]
 [ 220.          334.          272.          481.        ]
 [ 230.          491.          244.          499.        ]
 ..., 
 [   0.          440.23022461   22.96047211  465.80032349]
 [   5.82991314  378.84747314   31.43137169  405.1633606 ]
 [ 242.38284302  372.237854    267.63766479  399.26516724]]
500
[[ 283.           85.          456.          278.        ]
 [ 301.77947998    3.3343811   483.65765381  280.5       ]
 [ 266.3923645    23.83674622  428.90206909  280.5       ]
 ..., 
 [  48.66867828    4.52444458  139.42300415  236.44361877]
 [ 335.11495972   61.3850708   360.53598022   77.69688416]
 [  21.59006691   32.39923859   54.05297852  106.07846832]]
500
[[ 300.          310.          465.          362.        ]
 [ 300.70129395  296.56881714  429.74185181  367.75540161]
 [ 307.49539185  310.91967773  445.49847412  366.59179688]
 ..., 
 [   0.          256.57531738  123.09104156  411.98291016]
 [   0.           83.24198151  123.09104156  238.64956665]
 [   0.          123.24198151  123.09104156  278.64956665]]
500
[[   0.            0.          186.          181.        ]
 [ 186.            0.          263.          107.        ]
 [ 131.          104.          257.          291.        ]
 ..., 
 [ 221.90220642  392.56732178  243.5448761   411.62246704]
 [   0.          269.1940918    31.99582672  320.9916687 ]
 [ 333.37838745  197.21878052  364.22583008  227.84393311]]
500
[[ 145.          262.          274.          307.        ]
 [ 212.          294.          292.          347.        ]
 [ 305.          431.          379.          485.        ]
 ..., 
 [  56.72047424  216.61909485  102.32102203  266.19058228]
 [ 140.128479     70.96884918  168.44326782   97.18690491]
 [  89.12815857   60.89805603  221.39723206  277.34411621]]
500
[[  83.          102.          199.          449.        ]
 [  96.11404419    1.30546629  221.46459961  495.17330933]
 [  32.28718948    0.          265.2723999   464.51373291]
 ..., 
 [ 422.77975464  296.36514282  499.30664062  344.49353027]
 [  61.85097885   53.17305756  163.55776978  177.14860535]
 [ 398.42608643   69.75576019  423.43774414   92.33352661]]
389
[[  68.           68.          330.          469.        ]
 [   0.            0.          269.8125      499.2166748 ]
 [ 136.82078552  143.77384949  388.35165405  499.2166748 ]
 ..., 
 [ 232.17662048   49.63860321  257.15509033   66.36972046]
 [  81.28987885  248.88406372  329.23202515  433.91500854]
 [  73.70150757  203.04483032  155.11412048  272.65853882]]
333
[[ 113.          389.          234.          499.        ]
 [ 145.          165.          239.          321.        ]
 [  65.          129.          131.          209.        ]
 ..., 
 [  65.07752991   43.93193436   87.65465546   68.24738312]
 [  49.6694603   301.93069458  148.47218323  499.5       ]
 [ 132.53051758   90.381073    253.87312317  225.13040161]]
375
[[  30.          122.          326.          466.        ]
 [ 276.          235.          294.          265.        ]
 [ 274.          167.          291.          186.        ]
 ..., 
 [  91.9354248    44.6334877   113.75152588   61.92926025]
 [  14.25556278  424.05947876   52.4192009   463.56350708]
 [ 129.70339966   61.29857635  155.85998535   87.01061249]]
500
[[  92.           98.          402.          203.        ]
 [ 127.80954742  110.66217804  324.17602539  211.01205444]
 [  32.61487579    0.          422.50775146  333.44332886]
 ..., 
 [  93.47695923  107.07006836  109.65005493  123.69832611]
 [ 463.78735352  209.51976013  493.67004395  242.43345642]
 [  53.14741135  297.90740967   97.47644806  333.35494995]]
500
[[ 227.          215.          309.          313.        ]
 [ 314.          214.          389.          290.        ]
 [ 389.          184.          461.          245.        ]
 ..., 
 [ 286.09991455  114.2660141   310.912323    134.20845032]
 [ 404.31967163  154.49996948  423.38470459  173.80966187]
 [  38.01390076  216.67001343   64.84117126  253.56507874]]
500
[[ 232.          119.          361.          272.        ]
 [ 226.17544556    0.          418.9017334   374.375     ]
 [ 257.25860596  119.61133575  316.2522583   189.99571228]
 ..., 
 [ 304.33050537  147.31271362  392.74032593  195.85371399]
 [  94.31235504  133.42341614  140.07189941  184.67382812]
 [  65.09246063  317.22488403   87.22172546  344.22476196]]
375
[[  86.          122.          287.          385.        ]
 [ 304.          170.          328.          210.        ]
 [ 297.          268.          333.          307.        ]
 ..., 
 [  16.79536819  226.67199707   56.77736282  292.91235352]
 [ 313.4664917   416.58309937  357.00531006  463.18258667]
 [  54.45100403  150.0761261   153.06784058  191.92216492]]
500
[[ 314.          139.          421.          231.        ]
 [ 317.29672241  158.69534302  426.87548828  224.43127441]
 [ 275.75341797    0.          486.35824585  332.44500732]
 ..., 
 [ 380.39639282   17.54674149  432.234375     39.03637695]
 [ 293.23995972  201.92799377  315.45318604  220.15878296]
 [  74.57029724  167.99815369  198.30548096  290.60696411]]
500
[[ 331.           29.          398.          133.        ]
 [  18.           41.          156.           90.        ]
 [  38.28604507   45.24200058  151.60498047   91.14614868]
 ..., 
 [ 314.04275513   72.44561768  367.89138794  118.56185913]
 [ 136.34202576   37.80160522  207.07215881   80.80673218]
 [   0.          152.65341187  105.70645142  264.22506714]]
375
[[ 171.          218.          201.          248.        ]
 [ 175.          249.          195.          282.        ]
 [ 164.50926208  222.16925049  194.61569214  259.27410889]
 ..., 
 [ 133.71459961  384.93054199  155.72123718  416.81307983]
 [ 247.94638062  178.31513977  265.38494873  197.14369202]
 [ 127.57437897  137.45941162  144.53033447  160.22349548]]
375
[[  82.          123.          374.          499.        ]
 [ 203.80526733   78.51087189  374.375       499.375     ]
 [   0.            0.          261.23052979  447.99591064]
 ..., 
 [ 107.25875854  197.08763123  165.43815613  240.4058075 ]
 [  46.60021591  451.84475708   78.92731476  495.13827515]
 [ 118.92629242  451.07373047  204.80665588  489.5440979 ]]
304
[[  46.          122.          303.          496.        ]
 [  99.89338684  234.84300232  267.79098511  499.57333374]
 [  12.03205013  125.26322174  255.37492371  499.57333374]
 ..., 
 [  56.32791901  457.37637329   95.81060791  492.39624023]
 [ 153.27787781  206.31471252  177.68226624  232.13645935]
 [ 197.30307007  345.03942871  274.88916016  390.70361328]]
300
[[  39.          148.          111.          205.        ]
 [   0.          127.           43.          171.        ]
 [ 177.           64.          204.          106.        ]
 ..., 
 [ 249.74049377   75.26678467  299.625       179.79112244]
 [ 208.21206665   47.3296051   276.277771     78.79285431]
 [ 236.67866516   90.13977814  299.625       170.83609009]]
500
[[  27.           19.          479.          394.        ]
 [   5.88708067    0.          310.44482422  398.33499146]
 [ 263.42016602    0.          499.41500854  398.33499146]
 ..., 
 [ 392.48040771  229.24169922  422.6887207   255.38442993]
 [ 175.24552917  357.67941284  435.92626953  398.33499146]
 [ 334.17556763   21.5440712   401.60943604   69.82926941]]
333
[[   7.           67.          328.          413.        ]
 [ 169.95025635    0.          332.44500732  430.86706543]
 [  98.09762573   54.77752304  332.44500732  413.21731567]
 ..., 
 [ 176.82514954  212.83981323  220.37574768  308.48513794]
 [ 233.60961914   64.33065033  303.66494751  126.5217514 ]
 [ 128.67933655  135.52581787  149.22441101  151.08236694]]
334
[[  46.           25.          237.          477.        ]
 [  19.          141.          120.          498.        ]
 [ 108.84131622    0.          300.15335083  499.32998657]
 ..., 
 [ 220.57411194  470.79608154  253.98234558  499.32998657]
 [ 284.93945312  125.86486816  305.95220947  148.02351379]
 [  21.16742516   35.52730179  119.99246979  417.85095215]]
333
[[  99.           25.          210.          475.        ]
 [ 207.            0.          249.          118.        ]
 [  67.3832016    50.22123337  247.80804443  499.5       ]
 ..., 
 [  97.9361496   297.06826782  119.09012604  345.73822021]
 [ 290.67190552  174.47271729  317.9263916   213.62742615]
 [  80.32282257  234.1822052   324.26248169  383.46679688]]
400
[[  89.           10.          303.          486.        ]
 [ 125.83673096    9.01812744  352.33724976  499.33334351]
 [  95.22379303    0.          306.54953003  499.33334351]
 ..., 
 [   9.51683807  224.95483398   41.70483017  252.25952148]
 [   9.51757431  214.29045105   41.70514297  241.59484863]
 [   9.51704025  203.62632751   41.70578003  230.93133545]]
333
[[  29.           23.          313.          493.        ]
 [ 126.7884903   262.5847168   257.50213623  499.5       ]
 [   4.088552     23.15143394  284.84051514  499.5       ]
 ..., 
 [ 152.2146759   459.90249634  230.00396729  495.36557007]
 [  84.48221588  458.37698364  146.90126038  490.26205444]
 [ 171.53979492  316.45062256  187.152771    343.84994507]]
375
[[  80.            5.          272.          494.        ]
 [  85.7355423     0.          309.80014038  499.375     ]
 [ 117.50814056   50.57640076  251.15423584  499.375     ]
 ..., 
 [  82.44716644  143.28898621  103.83209229  169.92762756]
 [  63.11684418  301.91668701  302.52758789  414.08740234]
 [ 138.13658142    0.          301.01028442  106.00196838]]
334
[[  82.           19.          242.          476.        ]
 [  77.25546265   71.30082703  254.09335327  499.32998657]
 [  26.93314171    0.          233.17547607  335.10964966]
 ..., 
 [ 315.86203003  132.92626953  333.44332886  187.42051697]
 [  65.84003448  119.58006287  178.54171753  214.33149719]
 [ 315.896698     79.49342346  333.44332886  134.02124023]]
334
[[  40.          407.           82.          499.        ]
 [  19.           29.          310.          484.        ]
 [ 117.24636841    0.          325.94592285  374.54852295]
 ..., 
 [  45.508358    201.21156311   75.25778198  223.54978943]
 [ 224.24795532  362.95352173  295.3699646   455.12411499]
 [ 171.14477539  414.88262939  188.80422974  441.65011597]]
500
[[ 208.          195.          278.          276.        ]
 [ 209.68746948  204.59410095  274.77084351  262.70776367]
 [ 108.46839905  125.67724609  353.5692749   333.44332886]
 ..., 
 [ 267.02310181  312.0697937   305.28701782  333.44332886]
 [   8.12401104  290.92626953   35.99652863  316.68515015]
 [ 391.63208008  201.48538208  422.73446655  240.38124084]]
500
[[  17.           40.          219.          225.        ]
 [ 283.           53.          435.          159.        ]
 [  50.15644073    0.          263.83578491  251.5       ]
 ..., 
 [ 349.24456787  111.05889893  368.29876709  139.92408752]
 [ 216.36791992  189.23674011  237.01370239  208.03117371]
 [ 277.46899414   52.28959656  299.27746582   82.24499512]]
500
[[ 202.          212.          289.          304.        ]
 [   0.           29.32794571  268.22766113  374.375     ]
 [   0.          155.43746948  185.89279175  374.375     ]
 ..., 
 [   0.           64.46243286  123.00659943  107.61777496]
 [  60.78415298  207.81578064  216.13378906  374.375     ]
 [  65.48583984  306.15408325  135.89949036  374.375     ]]
375
[[  14.            0.          367.          458.        ]
 [ 166.            0.          209.           61.        ]
 [ 232.            3.          273.           40.        ]
 ..., 
 [  74.47553253  456.02514648  121.0546875   499.375     ]
 [ 341.03549194  344.3086853   357.04983521  365.769104  ]
 [ 205.49560547  404.83932495  346.97851562  467.65264893]]
500
[[ 404.          313.          465.          374.        ]
 [ 439.          328.          471.          353.        ]
 [ 442.          312.          461.          323.        ]
 ..., 
 [ 345.5145874   151.90426636  389.0958252   181.06019592]
 [ 254.39982605  239.58564758  370.51269531  282.3215332 ]
 [ 381.34832764  274.44284058  422.25332642  323.64883423]]
336
[[ 260.          277.          308.          383.        ]
 [  90.          323.          103.          332.        ]
 [  86.          305.          130.          364.        ]
 ..., 
 [ 193.82539368  257.77636719  212.85527039  283.47146606]
 [ 303.19506836  282.53286743  335.44000244  364.73364258]
 [ 280.03848267   79.86361694  301.61566162  100.59617615]]
375
[[ 126.          151.          214.          339.        ]
 [ 179.           98.          290.          401.        ]
 [   0.           96.           28.          127.        ]
 ..., 
 [  49.72407913  356.50164795   67.02471161  379.54632568]
 [ 177.93630981  430.68722534  219.55712891  458.91842651]
 [ 164.7882843   178.03373718  196.67472839  251.29724121]]
472
[[  54.           35.          162.          105.        ]
 [ 199.          180.          229.          206.        ]
 [ 222.          133.          327.          166.        ]
 ..., 
 [  94.33782196  188.26690674  189.2462616   229.15426636]
 [ 354.69161987  240.14048767  388.50512695  271.71881104]
 [ 382.50170898  302.68392944  397.93099976  322.25411987]]
500
[[ 232.          140.          376.          200.        ]
 [ 174.          137.          229.          210.        ]
 [ 235.76463318  133.15422058  414.13458252  195.02520752]
 ..., 
 [   0.          151.35597229   16.04360962  209.12826538]
 [  67.01979828  211.40785217  155.30438232  258.33197021]
 [ 153.14878845   36.25847626  307.64761353  203.91879272]]
342
[[  87.           84.          252.          344.        ]
 [ 115.16106415    0.          292.36569214  389.70800781]
 [ 159.01547241    0.          295.63882446  313.62155151]
 ..., 
 [  65.01157379  138.82752991  180.30032349  186.46128845]
 [  79.50473785  121.83407593  105.1234436   158.22000122]
 [ 113.99975586  114.19094849  135.02755737  139.34754944]]
500
[[  56.           92.          377.          245.        ]
 [  72.12669373    3.05095673  326.11541748  364.14227295]
 [  86.26148224  102.19947815  370.89916992  263.07644653]
 ..., 
 [   0.            0.           36.57779312  191.54718018]
 [ 257.66372681  160.01921082  310.42584229  213.73536682]
 [ 315.68252563  302.9229126   341.03643799  320.08792114]]
377
[[ 193.          188.          274.          284.        ]
 [ 205.          184.          252.          247.        ]
 [ 178.          183.          209.          258.        ]
 ..., 
 [   0.          310.99609375   25.87146568  344.59066772]
 [ 252.82389832  136.49804688  289.30535889  176.47654724]
 [ 326.93029785    0.          369.63119507   17.06738281]]
333
[[   0.          290.          332.          499.        ]
 [ 111.21931458  114.23950195  314.89501953  499.5       ]
 [  56.73596191  201.40742493  332.44500732  499.5       ]
 ..., 
 [  28.84727097  463.90618896  133.57054138  488.19116211]
 [  21.28705406  354.15313721   40.38986588  382.05740356]
 [  44.54865265  419.53857422  105.63715363  482.17666626]]
333
[[  31.          103.          220.          215.        ]
 [  41.5882988   106.35301208  225.20835876  210.35806274]
 [   0.           49.49960327  163.74169922  481.59375   ]
 ..., 
 [ 249.19458008   75.22444916  281.51864624   98.13198853]
 [ 238.50630188  465.57785034  262.70043945  496.23944092]
 [ 157.31144714  168.39994812  298.29953003  261.12628174]]
364
[[   9.           14.          345.          466.        ]
 [ 151.9291687     0.          324.87548828  468.21798706]
 [  95.6591568     0.          315.02111816  411.32199097]
 ..., 
 [ 321.90817261  324.01953125  363.39334106  375.23754883]
 [  87.59962463   69.62760162  116.05386353  104.89463043]
 [   0.           21.21079445   20.14554596   67.33812714]]
500
[[ 203.          244.          323.          455.        ]
 [ 330.          186.          344.          212.        ]
 [ 244.          185.          283.          251.        ]
 ..., 
 [ 441.6010437   421.74453735  459.52841187  437.52297974]
 [ 411.06134033   37.6775589   499.32000732  100.81686401]
 [ 185.49752808  106.21066284  208.35115051  131.32241821]]
375
[[  55.           16.          322.          485.        ]
 [   4.77010727    0.          257.80621338  499.375     ]
 [  16.30451202    0.          190.96018982  419.84527588]
 ..., 
 [  77.16835022  149.3195343   120.27698517  195.37783813]
 [  28.62286377  301.75756836   65.49544525  355.7149353 ]
 [  20.98421478   27.90336609   45.82979584   55.01068878]]
500
[[ 163.            5.          337.          374.        ]
 [ 202.98141479    0.          367.50137329  374.375     ]
 [ 177.98690796    0.          415.28012085  374.375     ]
 ..., 
 [ 328.57406616  178.19021606  351.18728638  208.70214844]
 [  95.50349426  263.35049438  112.3288269   282.40005493]
 [ 322.46243286   87.12158966  354.05276489  147.17158508]]
500
[[ 244.          160.          348.          271.        ]
 [ 208.99757385    0.          424.70065308  374.375     ]
 [ 189.01454163    0.          400.87103271  264.00393677]
 ..., 
 [ 438.6633606   329.45178223  459.02331543  350.34283447]
 [ 266.81295776  174.04039001  406.91519165  253.12002563]
 [ 133.66149902  184.56665039  162.13731384  222.56568909]]
500
[[ 338.          221.          365.          269.        ]
 [ 234.51898193    0.          499.375       374.375     ]
 [ 331.86599731  213.18101501  358.80905151  264.19073486]
 ..., 
 [ 381.94061279   93.94372559  499.375       249.76678467]
 [ 271.91107178  234.87257385  380.26983643  300.81915283]
 [ 460.51150513  291.13278198  496.50860596  349.5123291 ]]
500
[[ 224.          153.          357.          333.        ]
 [ 227.15403748    0.          423.89971924  373.08807373]
 [ 275.69363403  153.37744141  380.56484985  303.60083008]
 ..., 
 [ 286.63342285  112.63654327  333.75213623  171.6690979 ]
 [  34.11319733   18.31189156  363.04968262  197.96229553]
 [ 326.71844482  348.98974609  348.67111206  369.89407349]]
375
[[ 120.           36.          326.          335.        ]
 [ 223.05952454    0.          328.98651123  450.77557373]
 [ 186.19042969  135.71798706  260.78274536  228.44046021]
 ..., 
 [  74.85710907  216.40449524   92.59017181  238.45367432]
 [  85.73196411  231.97798157  171.85220337  294.7756958 ]
 [ 120.27590942  175.1375885   136.33282471  191.69261169]]
375
[[ 132.          134.          321.          465.        ]
 [  55.63337326    0.          344.46334839  486.41342163]
 [  81.66060638   48.04933548  291.50747681  499.375     ]
 ..., 
 [ 279.61166382   81.01239014  309.63821411  109.81780243]
 [ 316.25506592  254.70835876  336.86062622  274.98245239]
 [ 318.62472534  439.24502563  360.45050049  488.77642822]]
500
[[ 138.           98.          316.          278.        ]
 [ 220.           66.          235.           76.        ]
 [ 137.62658691   74.27936554  316.55053711  239.61105347]
 ..., 
 [  74.38865662   97.13905334  191.49324036  216.17243958]
 [   0.          157.8618927    62.49134064  195.73823547]
 [ 120.49435425  120.80866241  184.54107666  158.67160034]]
450
[[  22.           34.          377.          272.        ]
 [  96.75097656   42.64867401  312.0894165   337.43667603]
 [ 243.09927368    0.          449.54000854  337.43667603]
 ..., 
 [ 178.10154724   12.87185478  240.23696899   45.0210762 ]
 [ 172.56680298   44.99040604  349.80227661  133.61500549]
 [ 278.71884155  122.46968079  386.50274658  163.17033386]]
500
[[  35.           76.          376.          273.        ]
 [  31.36048317    0.          254.36256409  374.375     ]
 [   0.            0.          330.94259644  374.375     ]
 ..., 
 [ 320.57675171  224.64102173  399.12188721  252.13945007]
 [ 419.97091675  112.13868713  499.375       156.59616089]
 [  54.63961411  243.18057251  189.3522644   295.34896851]]
416
[[  81.           31.          326.          454.        ]
 [   0.            0.          271.38015747  499.19998169]
 [ 124.918396      0.          409.37753296  499.19998169]
 ..., 
 [ 197.03907776  365.06542969  335.66711426  456.59625244]
 [ 345.30245972  284.99047852  369.29177856  305.64196777]
 [ 393.63644409  421.57788086  415.30667114  488.98654175]]
282
[[  59.           81.          224.          401.        ]
 [  55.04846191   59.11325073  242.51889038  473.44290161]
 [  43.80780792  143.52935791  209.83001709  430.66876221]
 ..., 
 [  71.73202515  372.48046875  103.63839722  404.86212158]
 [  18.55447388  257.83172607  202.43252563  422.51873779]
 [ 119.12888336  225.27783203  140.78282166  244.06103516]]
450
[[  34.           11.          377.          305.        ]
 [ 185.92045593   16.2281189   449.54000854  337.43667603]
 [  43.82761765    1.18102646  289.33609009  337.43667603]
 ..., 
 [ 309.42483521   98.27501678  325.94674683  124.01726532]
 [ 211.24493408   79.92740631  226.86759949  103.72796631]
 [  54.44481277   14.2740097    98.27275848   34.97080612]]
450
[[  44.           54.          377.          276.        ]
 [  69.92222595    0.          349.91067505  337.43667603]
 [ 171.34280396    0.          427.20114136  337.43667603]
 ..., 
 [  69.66003418   82.94784546  142.30232239  124.83544159]
 [  38.32872772  222.28442383   64.0714798   242.00845337]
 [   0.          207.39279175   21.44093895  244.94117737]]
500
[[ 179.           29.          377.          359.        ]
 [ 200.40382385  119.50036621  499.33334351  399.33334351]
 [ 345.99118042  109.4525528   499.33334351  399.33334351]
 ..., 
 [ 193.164505    249.48210144  261.18737793  299.04626465]
 [ 121.5766983   334.99572754  278.06912231  399.33334351]
 [  92.96878815  238.11299133  121.53210449  273.5222168 ]]
375
[[  6.00000000e+01   1.51000000e+02   1.49000000e+02   3.67000000e+02]
 [  0.00000000e+00   1.12299919e+01   2.46412994e+02   3.99820160e+02]
 [  1.77681065e+01   3.43322754e-01   2.16132431e+02   4.78668335e+02]
 ..., 
 [  2.68621521e+02   2.46696198e+02   3.42882080e+02   4.99375000e+02]
 [  2.94285889e+02   3.15236328e+02   3.13117798e+02   3.32706024e+02]
 [  1.31121597e+02   2.38582001e+02   1.54745407e+02   2.85014404e+02]]
375
[[  44.            0.          322.          480.        ]
 [  56.83143616    0.          374.375       483.43917847]
 [   0.            0.          345.6104126   299.04327393]
 ..., 
 [ 310.82589722  194.63163757  331.09417725  213.91023254]
 [ 312.56430054   69.7873764   332.52279663  103.38269043]
 [ 316.31088257  375.17718506  335.96694946  396.29046631]]
500
[[ 247.          134.          274.          258.        ]
 [ 248.86331177  136.07504272  273.09368896  211.05810547]
 [ 234.39645386  135.54545593  268.806427    221.06082153]
 ..., 
 [ 461.70474243  191.6027832   483.8321228   210.73524475]
 [   0.          163.6104126    49.74960327  244.79907227]
 [  47.41289139  287.81884766  120.36701202  327.86489868]]
333
[[  31.          234.           85.          327.        ]
 [  58.2711525   232.957901     90.23362732  265.590271  ]
 [  39.73130417  237.34336853   88.07562256  283.36779785]
 ..., 
 [ 131.03796387  335.67956543  188.95608521  372.35598755]
 [ 166.47525024  467.95901489  248.41644287  499.5       ]
 [  90.20774841  217.56083679  127.69307709  258.11599731]]
298
[[  34.          159.          108.          252.        ]
 [ 127.          212.          203.          287.        ]
 [   0.           55.40917969  212.2019043   356.99081421]
 ..., 
 [   0.           98.15949249   19.29232025  118.71839142]
 [ 111.13273621   35.05569839  227.84114075   71.16995239]
 [ 202.23168945  240.56616211  219.75360107  259.57354736]]
321
[[  32.          284.          126.          371.        ]
 [  11.3049612    27.0136795   222.8657074   422.17184448]
 [   0.57687849   77.04483032  176.70817566  499.69000244]
 ..., 
 [   9.16788197   50.88496017   36.49036026   78.38703918]
 [  37.2631073   186.65202332   53.56505585  203.74110413]
 [  11.37585545  233.5133667    38.59502411  260.15982056]]
333
[[  34.          334.          135.          466.        ]
 [  56.          256.           75.          325.        ]
 [  21.          257.           50.          283.        ]
 ..., 
 [  44.82167053  325.63482666  128.18713379  368.58898926]
 [ 289.87774658   86.34877014  306.88757324  103.2273407 ]
 [ 194.53115845   14.79144096  287.69290161  224.72415161]]
294
[[  25.           20.          272.          498.        ]
 [   0.            0.          256.39511108  389.93841553]
 [   0.            0.          242.46508789  186.40365601]
 ..., 
 [ 228.59254456  165.09176636  293.5         210.58099365]
 [  85.35888672  316.55761719  110.27926636  352.75811768]
 [ 239.60852051   53.39558792  293.5         137.39369202]]
340
[[  64.           13.          267.          480.        ]
 [   0.            0.          293.14886475  499.2333374 ]
 [  35.47888947    0.          270.74136353  430.79934692]
 ..., 
 [ 229.18411255  334.88851929  261.89962769  392.17337036]
 [ 244.10247803  381.30392456  282.94076538  434.14511108]
 [ 246.52218628    0.          328.35745239  150.28062439]]
500
[[ 163.          192.          377.          238.        ]
 [ 113.24459076  192.76132202  389.32034302  233.2517395 ]
 [ 231.38824463  200.3132782   437.45108032  240.19281006]
 ..., 
 [ 402.17959595  272.11166382  426.36715698  295.6545105 ]
 [ 130.60830688  228.59335327  266.10549927  273.18215942]
 [  64.02664185  357.66207886  407.01223755  374.375     ]]
405
[[  15.            1.          377.          496.        ]
 [ 121.73361969   55.20337677  404.32501221  499.50003052]
 [ 125.66394043  216.41879272  391.09692383  499.50003052]
 ..., 
 [ 357.09609985   81.08276367  392.125       123.61157227]
 [   0.          375.3991394    24.93893623  409.8972168 ]
 [ 214.34184265  247.1239624   290.40536499  317.31826782]]
500
[[  93.          133.          365.          330.        ]
 [ 108.04232025   76.08174896  458.84713745  430.28164673]
 [   6.60635138    0.          261.18469238  430.28164673]
 ..., 
 [ 247.83042908  378.52639771  273.20809937  398.79003906]
 [ 386.01571655  276.04248047  414.49731445  309.83905029]
 [ 148.67553711  378.10830688  181.45236206  402.77682495]]
500
[[  31.           51.          377.          332.        ]
 [  54.67632675    0.          316.16751099  356.40499878]
 [ 126.22940826    0.          400.16452026  356.40499878]
 ..., 
 [ 256.08190918   96.05849457  324.78057861  154.02674866]
 [ 147.0687561     0.          167.46902466   29.87405014]
 [ 388.77688599  140.39352417  417.2208252   168.26325989]]
500
[[  88.          132.          284.          334.        ]
 [ 189.          266.          286.          373.        ]
 [ 250.           77.          283.          117.        ]
 ..., 
 [ 246.65881348  278.14749146  314.00375366  341.04110718]
 [ 245.31716919  271.22305298  283.14404297  321.37387085]
 [  72.22991943  258.46380615  125.68157196  350.77230835]]
299
[[  27.           17.          251.          496.        ]
 [ 116.13535309  197.45048523  288.50210571  499.5       ]
 [ 135.90475464  248.82731628  271.27709961  499.5       ]
 ..., 
 [  17.72346878  357.17855835  108.75410461  410.43673706]
 [  66.75191498  260.96469116  110.24459076  324.52853394]
 [ 192.75531006  325.52294922  298.5         409.51947021]]
344
[[  50.          204.          252.          445.        ]
 [  72.60739136  153.56272888  268.21026611  491.94284058]
 [  58.85913849   25.71400261  292.17855835  499.37332153]
 ..., 
 [ 126.71478271   53.76893997  242.34094238  272.71847534]
 [ 123.91420746  169.59010315  222.74369812  199.6870575 ]
 [ 293.4241333   160.19673157  313.92337036  181.22859192]]
361
[[  54.           64.          225.          489.        ]
 [  39.55951691    0.          272.82574463  487.93740845]
 [   0.            0.          330.18023682  499.3833313 ]
 ..., 
 [ 156.78157043  242.76860046  201.78791809  337.7003479 ]
 [ 248.98544312  165.24060059  360.39831543  416.11984253]
 [  93.30971527  358.10223389  192.39334106  464.29037476]]
500
[[  92.           30.          377.          313.        ]
 [  85.47253418    0.          351.22622681  374.375     ]
 [ 169.32128906    0.          423.76635742  374.375     ]
 ..., 
 [ 371.72979736  189.42811584  398.56451416  218.11384583]
 [ 479.87103271  213.80950928  497.13299561  237.67242432]
 [ 201.60536194  209.69308472  302.27911377  278.0329895 ]]
500
[[ 183.          301.          250.          346.        ]
 [ 179.          214.          281.          274.        ]
 [ 104.          255.          170.          301.        ]
 ..., 
 [ 162.48114014  121.10152435  226.76068115  158.52581787]
 [ 110.83451843   55.4015274   224.74093628  106.65176392]
 [  38.33647537  241.49902344   87.43839264  282.42657471]]
500
[[  77.          148.          356.          215.        ]
 [  86.26138306  142.85340881  324.9005127   219.27415466]
 [ 185.14476013  119.31160736  461.02084351  230.57685852]
 ..., 
 [ 417.71832275  208.30978394  437.86624146  225.51036072]
 [ 388.01260376   93.99021912  415.77963257  119.72460938]
 [ 375.71835327  299.02908325  400.48846436  322.05480957]]
375
[[ 148.          243.          246.          348.        ]
 [ 154.69673157  241.44160461  256.11737061  341.81689453]
 [ 118.6255188   203.29368591  288.42727661  375.43341064]
 ..., 
 [  29.86268044   96.60327911  114.46446228  138.25401306]
 [ 165.0276947   134.53691101  183.72348022  163.05209351]
 [ 226.08755493   62.30205917  243.76069641   80.26230621]]
500
[[  52.           33.          377.          405.        ]
 [   0.            0.          334.4694519   431.2800293 ]
 [  98.45823669    0.          339.52368164  431.2800293 ]
 ..., 
 [ 415.92340088  213.29495239  498.96002197  358.61819458]
 [   0.           32.12136078   79.21278381   91.912323  ]
 [ 387.45962524  271.59719849  436.5526123   349.33792114]]
466
[[  28.           19.          145.          443.        ]
 [ 169.           17.          308.          440.        ]
 [ 326.           27.          377.          437.        ]
 ..., 
 [ 344.85702515   72.52271271  397.46853638  190.20170593]
 [ 187.82307434  329.29159546  227.42710876  417.33233643]
 [ 192.79829407   56.43546295  330.73199463  181.48377991]]
500
[[  25.           74.          214.          417.        ]
 [ 259.           78.          377.          396.        ]
 [ 113.24717712    0.          499.55001831  484.19168091]
 ..., 
 [ 378.99209595  231.84877014  445.25927734  354.86914062]
 [ 386.55923462  253.9763031   444.10916138  351.9185791 ]
 [ 115.85031891   40.18029404  264.04556274  138.19340515]]
319
[[  92.          428.          147.          443.        ]
 [ 175.          390.          194.          416.        ]
 [  19.          174.          189.          257.        ]
 ..., 
 [ 225.42199707  101.35819244  241.39820862  118.31165314]
 [ 264.79299927  344.49719238  289.57626343  370.703125  ]
 [ 280.64880371  216.69152832  315.59927368  267.21929932]]
500
[[ 282.          131.          376.          266.        ]
 [  60.          136.          199.          274.        ]
 [ 236.03796387    0.          414.98556519  332.44500732]
 ..., 
 [  81.90596008  133.26919556  112.73796844  160.02952576]
 [  73.80947113  139.62150574   96.16932678  167.00210571]
 [ 170.99473572   22.50153351  272.06655884  256.3302002 ]]
500
[[  29.           19.          377.          368.        ]
 [  28.54489326    0.          322.7612915   374.375     ]
 [  84.17999268    0.          393.32250977  374.375     ]
 ..., 
 [ 375.57318115  151.60188293  424.83023071  198.72956848]
 [ 125.31990814  292.7835083   167.27615356  352.15072632]
 [  63.28413773   76.60850525  108.57361603  112.40306854]]
333
[[  22.          259.          163.          497.        ]
 [ 176.          313.          261.          498.        ]
 [  85.            0.          332.          244.        ]
 ..., 
 [  11.0684309   246.1149292    46.70709991  286.52297974]
 [ 272.70004272  325.51477051  292.79489136  353.42529297]
 [ 266.76342773  437.1953125   285.69241333  458.35360718]]
500
[[ 280.          139.          379.          264.        ]
 [ 175.           72.          233.          230.        ]
 [ 109.          132.          179.          262.        ]
 ..., 
 [ 448.42086792  197.82705688  472.35076904  232.00056458]
 [ 133.05700684   86.60607147  164.8578186   136.09144592]
 [ 156.85020447  322.69824219  322.82650757  374.375     ]]
332
[[ 104.          280.          201.          319.        ]
 [ 104.          185.          202.          302.        ]
 [  64.93002319    3.70946765  238.40397644  495.74398804]
 ..., 
 [ 221.32879639  189.42445374  305.82577515  241.97747803]
 [ 189.46318054  411.79370117  287.22418213  467.93814087]
 [ 293.17193604  232.87297058  311.03070068  257.76501465]]
375
[[  67.            9.          321.          497.        ]
 [ 109.79363251    0.          374.375       499.375     ]
 [  33.21628571    2.64007568  329.72937012  499.375     ]
 ..., 
 [ 144.12637329  164.0500946   198.72480774  241.82536316]
 [ 182.75788879  273.25823975  234.67515564  331.18139648]
 [ 290.13632202  395.26385498  309.90997314  462.16671753]]
432
[[ 165.          193.          268.          258.        ]
 [ 105.50704193  162.30401611  368.08761597  498.96002197]
 [ 109.37043762  271.56411743  321.57321167  498.96002197]
 ..., 
 [  70.92760468   88.66633606  351.09863281  224.31317139]
 [ 205.18315125  378.41213989  238.62286377  407.90557861]
 [ 236.29328918  338.32061768  374.87545776  403.5055542 ]]
500
[[ 187.          128.          291.          209.        ]
 [  35.31713867    0.          296.2144165   331.44665527]
 [  44.47045135    0.          312.34890747  195.86878967]
 ..., 
 [ 130.35142517  184.62390137  147.50126648  202.46853638]
 [ 302.31585693  239.24958801  330.12362671  282.59509277]
 [ 391.44482422  256.04095459  407.90197754  275.96377563]]
500
[[ 122.            6.          332.          353.        ]
 [ 116.39442444    0.          365.45092773  374.375     ]
 [ 100.2299118    60.94142914  290.25909424  374.375     ]
 ..., 
 [ 477.23019409  158.93836975  493.39083862  177.6804657 ]
 [ 316.56109619  136.78370667  334.64169312  155.80958557]
 [ 372.21273804  248.02973938  388.96759033  272.56304932]]
500
[[ 139.          149.          288.          211.        ]
 [  86.          253.          134.          301.        ]
 [ 144.66493225  152.37374878  374.90689087  227.04081726]
 ..., 
 [ 303.45611572  131.35055542  330.62802124  150.15820312]
 [   9.5956955   238.64247131   58.03857422  274.62313843]
 [ 224.86032104   79.4370575   305.137146    116.20677185]]
500
[[  96.          112.          323.          372.        ]
 [ 107.39974976    0.          397.26669312  374.375     ]
 [  16.99596405    0.          305.61209106  374.375     ]
 ..., 
 [ 118.34992218  110.23075104  163.74766541  175.52432251]
 [ 373.15054321  108.21170807  393.74740601  132.56608582]
 [ 398.02658081  340.33682251  414.48690796  357.80950928]]
500
[[ 107.          140.          241.          277.        ]
 [  86.31218719  135.48672485  295.11477661  231.76272583]
 [  67.6133728   129.41169739  273.3923645   245.73464966]
 ..., 
 [ 233.95967102   55.67285919  254.74919128   72.21621704]
 [  15.41980076    0.           64.67116547   98.12798309]
 [ 307.71014404  326.23571777  336.66525269  349.41665649]]
375
[[ 103.          180.          332.          279.        ]
 [  91.30577087  163.33479309  302.73208618  281.80462646]
 [ 120.27053833  154.40713501  333.64416504  284.86373901]
 ..., 
 [ 292.44390869   87.29582977  325.11795044  118.94229126]
 [ 197.53768921  201.01298523  237.89398193  218.63822937]
 [ 240.03405762   53.06459045  350.95092773  102.89315796]]
375
[[  81.          221.          331.          334.        ]
 [ 144.89118958   44.49077606  374.375       499.375     ]
 [ 220.67752075   76.62097931  374.375       499.375     ]
 ..., 
 [ 208.01567078  447.69735718  294.63684082  483.32809448]
 [ 322.14215088  429.43649292  340.74435425  448.59365845]
 [ 315.60928345  272.20825195  358.57107544  355.76416016]]
257
[[  29.           15.          230.          459.        ]
 [  30.40209961    0.          235.72592163  414.60180664]
 [   0.            6.32769775  199.94232178  499.5       ]
 ..., 
 [ 107.38908386  426.712677    141.78091431  462.96841431]
 [ 157.91960144  173.95101929  202.0428009   230.50204468]
 [ 233.93515015  203.55075073  251.93139648  231.92321777]]
375
[[ 237.          272.          292.          331.        ]
 [ 209.          312.          239.          338.        ]
 [ 138.          322.          202.          375.        ]
 ..., 
 [  62.24212265  350.34542847   97.52949524  375.7571106 ]
 [ 271.08129883  171.20707703  373.80752563  223.05648804]
 [ 225.41345215  183.90605164  345.53366089  223.74839783]]
375
[[  69.           31.          332.          446.        ]
 [  86.40336609    0.          374.375       499.375     ]
 [  52.79196548    0.          271.83093262  499.375     ]
 ..., 
 [ 268.64697266  283.0291748   348.3772583   499.375     ]
 [ 279.19369507  253.58981323  314.73657227  288.99893188]
 [ 167.99261475  294.96826172  328.96057129  369.47912598]]
500
[[  28.            0.          332.          301.        ]
 [ 158.62921143    0.          406.93173218  374.375     ]
 [   0.            0.          243.79443359  374.375     ]
 ..., 
 [ 348.67019653  339.68930054  481.51937866  374.375     ]
 [ 196.70236206  314.27145386  215.58668518  332.60681152]
 [   0.          203.82228088   87.93881989  298.04888916]]
333
[[  42.          117.          245.          388.        ]
 [  16.96133232   84.63767242  218.58680725  499.5       ]
 [  61.10725784  190.30802917  270.65548706  499.5       ]
 ..., 
 [ 161.30953979  341.15197754  266.0880127   397.76107788]
 [ 252.39666748  158.60771179  298.35961914  215.54223633]
 [  71.65869141    0.          144.2797699   273.99078369]]
334
[[   0.           17.          332.          497.        ]
 [  76.88803864   82.35071564  301.96026611  499.32998657]
 [  57.73927689   23.28145218  317.37789917  430.45144653]
 ..., 
 [ 224.38635254  482.89450073  276.78982544  499.32998657]
 [ 124.81484985   35.33765793  246.38005066   81.31826782]
 [ 167.12690735  464.67202759  215.71775818  499.32998657]]
500
[[ 245.          183.          315.          227.        ]
 [ 148.          230.          299.          333.        ]
 [ 250.63302612  178.00036621  310.81729126  226.98622131]
 ..., 
 [ 105.04195404  170.48364258  221.17674255  298.28900146]
 [ 315.42114258  301.37493896  333.50860596  327.78900146]
 [ 438.5296936    78.4682312   466.42443848  112.57707214]]
500
[[  82.            7.          332.          492.        ]
 [   1.          271.           78.          378.        ]
 [   0.          220.           66.          261.        ]
 ..., 
 [ 417.19555664  387.95907593  484.28500366  413.22244263]
 [ 194.99456787  338.24575806  382.43328857  499.16665649]
 [  52.9403801   481.46044922   79.47481537  499.16665649]]
333
[[  82.           78.          241.          435.        ]
 [  69.18009949   15.38573551  318.21356201  499.5       ]
 [  25.67052841   27.80419922  260.82339478  499.5       ]
 ..., 
 [ 138.24555969  274.45263672  210.37953186  350.2817688 ]
 [ 219.4954071   145.71003723  251.6991272   189.78196716]
 [ 162.84475708  157.33996582  210.39813232  250.31912231]]
500
[[  24.          264.          122.          324.        ]
 [  21.37836838    0.          268.23666382  419.29998779]
 [  21.88065529  263.16412354  121.52076721  324.53610229]
 ..., 
 [ 161.55574036  158.73486328  211.06437683  205.00608826]
 [ 355.53305054   52.98997498  375.93362427   70.97307587]
 [ 451.49938965   58.54639435  499.09997559  111.69046783]]
375
[[  93.           99.          267.          424.        ]
 [  75.390625     17.00080872  315.43896484  460.19216919]
 [  60.58481216   44.43462372  266.38058472  499.375     ]
 ..., 
 [ 154.08338928  260.51040649  248.6473999   323.84109497]
 [ 103.844841     31.84501076  126.25050354   53.53269196]
 [ 142.78488159   27.68983841  176.66078186   48.41682434]]
333
[[  78.          288.          153.          367.        ]
 [  30.           67.          256.          183.        ]
 [  87.67377472   71.21147156  299.92910767  165.33302307]
 ..., 
 [ 257.91394043  176.20129395  276.59765625  193.96389771]
 [  28.70752907   92.31822968   44.17024231  109.20367432]
 [ 151.2804718    46.48579788  220.76715088   78.56824493]]
460
[[ 247.          126.          332.          208.        ]
 [ 255.2099762   148.93841553  332.23907471  213.77474976]
 [ 267.87811279  145.88160706  327.35122681  204.29772949]
 ..., 
 [ 375.75592041  468.40744019  459.23330688  499.09997559]
 [ 326.13723755  354.27288818  440.74765015  499.09997559]
 [ 145.38598633  270.66534424  173.36387634  305.32821655]]
403
[[  72.           39.          332.          493.        ]
 [   0.           22.29243279  268.84823608  499.04830933]
 [ 119.17483521    0.          373.23913574  499.04830933]
 ..., 
 [ 227.66064453   74.80278015  245.05879211   96.2383728 ]
 [ 147.79283142  204.45837402  175.34394836  240.93966675]
 [ 361.50839233  133.49108887  379.49087524  182.8639679 ]]
375
[[ 135.           89.          257.          207.        ]
 [   0.          182.44288635  187.29576111  499.375     ]
 [  25.70566177    0.          308.56106567  473.38537598]
 ..., 
 [  36.40958786    0.          292.53927612  109.79201508]
 [   0.          174.57099915   19.17914581  210.76820374]
 [  34.11530304  343.34494019   75.74736786  368.83978271]]
500
[[ 254.          141.          332.          220.        ]
 [ 119.75994873   80.24506378  413.12646484  325.45666504]
 [ 257.49563599  130.83384705  335.79446411  226.1050415 ]
 ..., 
 [  75.11672974  223.86949158   96.11234283  239.48461914]
 [ 386.16558838  268.74938965  411.46002197  295.35137939]
 [ 289.91195679  279.72827148  365.38223267  317.22723389]]
334
[[ 100.          398.          186.          444.        ]
 [ 167.71263123    0.          333.44332886  499.32998657]
 [   0.            0.          204.77961731  493.52032471]
 ..., 
 [ 245.0892334   340.2973938   279.78051758  364.26522827]
 [  15.32056904  105.19674683   73.40008545  177.75309753]
 [ 231.23266602  254.99156189  253.38442993  275.94439697]]
331
[[   0.            1.          322.           99.        ]
 [   3.          112.          323.          225.        ]
 [  20.          246.          326.          342.        ]
 ..., 
 [  99.10184479  223.91320801  233.40974426  284.50567627]
 [ 269.777771    144.96633911  318.90390015  264.40209961]
 [   0.           82.65213776  129.78623962  213.03042603]]
333
[[  48.            0.          309.          496.        ]
 [ 108.47856903    0.          330.41970825  358.74142456]
 [  42.40431595    0.          332.44500732  405.33834839]
 ..., 
 [   8.13053036  200.69726562   37.54538345  245.81181335]
 [  75.72076416  437.83544922  162.57518005  492.73254395]
 [ 206.109375    408.82235718  232.45318604  434.33874512]]
333
[[  76.           45.          281.          439.        ]
 [  67.4319458     0.          248.57949829  346.3270874 ]
 [  61.23152542   14.29472256  244.32377625  459.14117432]
 ..., 
 [   2.74161243   82.40332031   21.71400642  116.07758331]
 [ 152.97944641  402.72232056  188.5556488   444.00756836]
 [ 263.4836731   355.3260498   294.27954102  395.83358765]]
333
[[  24.          144.          275.          436.        ]
 [ 131.06422424   57.79034042  299.24032593  453.7696228 ]
 [  78.31877136    0.          309.28729248  469.7064209 ]
 ..., 
 [ 252.87440491  243.48762512  285.71893311  292.61456299]
 [ 151.45372009  299.89382935  261.27557373  380.42004395]
 [ 230.1967926    66.97766113  265.78707886  105.41866302]]
333
[[  19.          162.          303.          454.        ]
 [  49.73606491   28.15480042  295.67111206  499.5       ]
 [  98.30212402  122.22306824  274.23886108  462.23406982]
 ..., 
 [  90.73905945  176.4667511   189.34953308  279.60696411]
 [ 146.05618286   89.69425201  167.24435425  108.09602356]
 [ 313.76977539  276.12557983  332.44500732  301.48620605]]
333
[[  34.          170.          259.          453.        ]
 [   0.            0.          232.42349243  499.5       ]
 [  25.77503967   27.2350235   153.60664368  499.5       ]
 ..., 
 [  56.50321198   36.06201172   92.41587067   68.49228668]
 [  74.53084564   61.2289505   140.52508545  110.96742249]
 [  72.45196533  326.52938843  104.8855896   372.17306519]]
333
[[  69.           47.          249.          366.        ]
 [ 102.39531708   43.15888977  255.05036926  420.7618103 ]
 [  63.06380463    0.          299.48937988  377.49667358]
 ..., 
 [ 304.25170898  110.4784317   328.71337891  134.29933167]
 [ 218.48406982  482.72393799  260.30050659  499.5       ]
 [ 168.51565552  252.90106201  195.22229004  285.74810791]]
333
[[  67.           67.          260.          435.        ]
 [  48.86009598  113.65351105  263.25756836  499.5       ]
 [ 119.09626007  152.36419678  297.22207642  499.5       ]
 ..., 
 [   0.           71.32309723   36.32888031  122.75300598]
 [ 234.57720947   26.56703949  252.9733429    47.49407578]
 [  13.89789486   49.18144989   53.00652695  115.02054596]]
333
[[  39.           50.          282.          442.        ]
 [  43.84505081    0.          245.68701172  473.39852905]
 [  45.00876617    0.          200.15893555  345.14651489]
 ..., 
 [  57.1078186   419.97003174   73.26727295  444.83175659]
 [ 235.80044556  365.77792358  282.34613037  417.82904053]
 [ 234.57673645  329.59420776  332.44500732  471.1885376 ]]
360
[[  25.           33.          331.          433.        ]
 [   0.            0.          224.81881714  464.40002441]
 [  18.27720451   39.16554642  323.51242065  464.40002441]
 ..., 
 [  29.19265938   70.6780777    69.81006622  150.98497009]
 [ 302.31317139  440.60577393  349.73086548  464.40002441]
 [ 126.00392151  243.48933411  237.72297668  323.53164673]]
341
[[  38.            2.          303.          438.        ]
 [ 116.84644318  149.97302246  312.38259888  499.56500244]
 [ 167.8739624     0.          340.43167114  417.97134399]
 ..., 
 [ 230.32014465  315.76501465  260.90942383  378.53198242]
 [ 197.76489258  165.70071411  247.13743591  221.19508362]
 [ 206.87791443  205.45768738  271.81222534  332.44537354]]
333
[[   2.          372.           73.          464.        ]
 [ 168.89227295    0.          332.44500732  499.5       ]
 [   0.           44.81282806  122.603508    499.5       ]
 ..., 
 [ 131.10858154  332.40130615  226.66668701  369.11334229]
 [ 200.93000793  162.43078613  326.47821045  293.19766235]
 [   0.          284.87103271   50.1529007   341.67404175]]
500
[[  80.           92.          331.          330.        ]
 [ 152.06745911    0.          367.90142822  332.44500732]
 [   9.00398922   15.70779705  253.1633606   330.25515747]
 ..., 
 [  54.20198059  208.6918335   101.56108856  267.58172607]
 [ 205.16934204  148.57905579  402.54827881  197.55429077]
 [ 458.41308594  272.23352051  499.5         312.46365356]]
500
[[ 161.            3.          286.          330.        ]
 [   0.            0.          205.67378235  332.44500732]
 [   0.            0.          190.78048706  244.59889221]
 ..., 
 [ 129.2127533   224.55717468  162.70678711  280.40670776]
 [ 461.10455322  221.41033936  499.5         278.58612061]
 [ 474.02670288   57.4184761   499.5          79.31505585]]
333
[[  50.          345.          285.          448.        ]
 [  73.76810455  190.89706421  323.03234863  499.5       ]
 [  74.20978546    0.          248.3013916   387.00354004]
 ..., 
 [ 118.19931793   73.98088837  147.38887024   98.88269043]
 [ 105.98365021  343.07894897  194.92738342  378.50891113]
 [ 157.50460815  344.04382324  178.67532349  363.53076172]]
500
[[  57.          165.           73.          190.        ]
 [ 116.          233.          198.          280.        ]
 [ 247.          264.          290.          281.        ]
 ..., 
 [ 128.21705627    0.          458.21939087  127.68187714]
 [ 231.78443909  124.33072662  273.04022217  161.16088867]
 [   5.538239    286.73452759   21.40191269  311.4473877 ]]
243
[[  11.            7.          242.          496.        ]
 [  30.36714935    0.          242.5         399.84185791]
 [  66.18908691    0.          215.71313477  448.85644531]
 ..., 
 [ 174.97055054  157.31282043  217.29547119  216.21620178]
 [  37.78604126  243.59849548   93.48847961  314.68753052]
 [ 155.79797363   81.33612061  200.36883545  118.34190369]]
375
[[  51.          169.          158.          423.        ]
 [   6.3988018    42.42275238  222.28282166  499.375     ]
 [  28.32733154  126.57128906  212.36083984  499.375     ]
 ..., 
 [ 323.63861084   49.81252289  361.64398193  114.24279785]
 [ 148.97572327  201.86705017  217.32765198  266.66177368]
 [ 137.63204956  452.97961426  184.03131104  486.52706909]]
333
[[ 122.          161.          228.          255.        ]
 [ 112.          207.          329.          493.        ]
 [ 137.19160461  138.46948242  254.83937073  499.5       ]
 ..., 
 [ 270.2454834   428.98318481  301.51589966  468.93243408]
 [ 172.70869446   80.3375473   264.03170776  121.05669403]
 [ 296.83126831  426.12237549  326.7749939   463.41687012]]
375
[[  60.          236.          207.          412.        ]
 [ 202.          233.          229.          283.        ]
 [ 231.          203.          241.          237.        ]
 ..., 
 [ 163.10064697  141.54344177  187.52633667  164.64201355]
 [ 195.83503723  126.158638    223.17367554  151.30197144]
 [ 285.13320923  331.11935425  304.48007202  347.05981445]]
500
[[ 174.           72.          332.          215.        ]
 [ 217.66889954    0.          465.78198242  333.44332886]
 [ 241.14997864   50.28927994  443.27807617  333.44332886]
 ..., 
 [   7.70040798  274.51867676  113.31241608  324.75140381]
 [  11.65289783   25.31811905   56.15049744   56.61043549]
 [ 180.64579773  237.35327148  235.79986572  292.96557617]]
313
[[  79.          118.          230.          353.        ]
 [  92.31128693   46.33959579  251.80873108  417.88497925]
 [  71.64415741    0.          275.32254028  422.88687134]
 ..., 
 [  76.31413269  438.13348389  164.88194275  467.5397644 ]
 [ 102.26622772  155.89385986  136.49583435  194.95935059]
 [  85.90518951  136.05792236  263.31158447  295.95498657]]
500
[[ 182.           46.          314.          331.        ]
 [  93.02861786    0.          407.10150146  374.375     ]
 [ 169.36694336   11.57745361  368.26647949  374.375     ]
 ..., 
 [   0.          201.21951294   22.74699593  235.02467346]
 [ 352.47039795  346.78860474  372.90008545  370.12936401]
 [   0.          220.26298523   22.54603767  254.81768799]]
333
[[ 118.          158.          206.          401.        ]
 [  40.25597      37.94121552  313.86929321  374.43960571]
 [ 166.18229675    0.          332.44500732  242.1043396 ]
 ..., 
 [  58.36484909  294.40551758  152.86938477  349.17382812]
 [  42.78974152  171.82980347  100.43084717  229.35461426]
 [  45.81141663  370.2434082    86.31671143  448.73077393]]
500
[[ 111.          114.          331.          286.        ]
 [ 258.94561768    0.          499.29000854  373.37667847]
 [ 339.70736694    0.          499.29000854  373.37667847]
 ..., 
 [ 406.22402954  232.27880859  431.73959351  255.47955322]
 [ 389.9781189   251.31716919  412.8704834   278.46435547]
 [   0.          327.31524658   76.93049622  373.37667847]]
364
[[  79.           44.          208.          349.        ]
 [   7.41029215    0.          229.16011047  499.28665161]
 [   0.96618998    0.          276.75106812  415.85339355]
 ..., 
 [ 233.07878113  262.26950073  294.25189209  364.80722046]
 [  71.0584259   246.64767456  311.56539917  320.36773682]
 [  29.69685936    0.          147.33825684  112.97215271]]
345
[[   6.           39.          307.          362.        ]
 [ 174.          336.          246.          469.        ]
 [ 122.52030182    0.          344.42501831  452.25646973]
 ..., 
 [ 199.75317383  383.84442139  300.82376099  431.97012329]
 [  34.62712097  291.27505493  153.31446838  366.34460449]
 [ 245.79342651  412.84506226  281.54669189  454.08587646]]
500
[[  71.            6.          446.          320.        ]
 [ 193.50532532    0.          393.57623291  332.44500732]
 [ 131.65055847    0.          328.86395264  319.75238037]
 ..., 
 [ 248.31375122  302.50592041  277.78497314  324.0350647 ]
 [ 332.5802002   274.99588013  431.15759277  304.55548096]
 [ 344.2225647   237.02275085  435.5536499   293.22927856]]
375
[[  95.           80.          228.          467.        ]
 [  79.0815506     0.          250.79759216  499.375     ]
 [   0.           88.12606812  286.46017456  452.44372559]
 ..., 
 [  46.81503296  230.67834473   63.92995834  248.88322449]
 [   0.          367.52301025   85.8348999   427.01199341]
 [ 157.92559814  451.1529541   193.64128113  499.375     ]]
500
[[ 304.          142.          354.          209.        ]
 [ 368.          324.          400.          373.        ]
 [ 351.98989868    0.          499.375       374.375     ]
 ..., 
 [ 347.76174927  170.06365967  386.19979858  217.67799377]
 [ 286.84317017  167.91442871  381.78573608  323.97665405]
 [ 187.06503296  207.86997986  218.97676086  241.19509888]]
484
[[ 132.          299.          262.          446.        ]
 [ 155.84864807    0.          483.19332886  499.32666016]
 [   0.            0.          469.04888916  499.32666016]
 ..., 
 [ 439.24386597  197.99615479  478.24737549  218.84527588]
 [  55.34688187  127.88643646  199.95542908  185.13208008]
 [ 284.5118103   410.64602661  306.04986572  436.50817871]]
500
[[ 255.          321.          268.          333.        ]
 [ 276.          314.          288.          336.        ]
 [ 300.          323.          311.          335.        ]
 ..., 
 [  61.57539749  103.30395508   95.1325531   136.55451965]
 [ 206.46731567  253.16133118  292.31060791  286.86489868]
 [ 225.70132446  345.42721558  492.18457031  374.375     ]]
500
[[   0.           76.          498.          373.        ]
 [ 224.31207275   40.82798004  499.375       374.375     ]
 [ 127.08726501    0.          366.20306396  374.375     ]
 ..., 
 [  75.70645905  104.87403107  111.87024689  151.91261292]
 [ 245.71803284  123.76584625  317.66235352  151.96447754]
 [ 405.69338989   36.18385315  499.375       178.26460266]]
500
[[ 419.          342.          484.          371.        ]
 [ 394.          234.          468.          341.        ]
 [ 409.94067383  336.08926392  477.27139282  384.42077637]
 ..., 
 [ 267.42312622  142.37522888  370.05688477  213.74829102]
 [ 270.77737427  342.88110352  358.83877563  381.71670532]
 [ 384.1736145   330.52151489  437.06155396  399.33334351]]
370
[[ 278.          151.          318.          195.        ]
 [ 287.          421.          322.          458.        ]
 [ 284.55859375  162.02937317  321.90936279  192.95507812]
 ..., 
 [ 148.20220947  215.8377533   165.95129395  231.77146912]
 [ 342.20388794  376.37640381  369.3833313   444.14776611]
 [ 254.90309143  140.69320679  369.3833313   306.95758057]]
500
[[ 170.          122.          194.          231.        ]
 [ 191.           92.          304.          270.        ]
 [ 164.81706238  163.28686523  196.06518555  250.62979126]
 ..., 
 [ 110.30993652  266.34313965  127.8650589   282.76226807]
 [ 153.70137024  241.01094055  239.35733032  281.49295044]
 [ 204.47608948    0.          361.20187378   75.84882355]]
375
[[ 181.          178.          283.          368.        ]
 [ 119.1246109    57.94000626  368.51864624  499.375     ]
 [ 180.45176697  202.14295959  284.14950562  396.30981445]
 ..., 
 [ 194.40301514  443.82748413  236.19226074  465.09310913]
 [ 139.42674255  346.31988525  187.20437622  397.11959839]
 [ 297.40216064  403.59661865  366.53231812  433.11999512]]
375
[[  55.           24.          304.          308.        ]
 [  57.          291.           57.          291.        ]
 [ 343.           76.          373.          187.        ]
 ..., 
 [  38.15513992  376.9128418    80.26355743  461.69036865]
 [  96.76726532  111.99487305  162.09832764  236.9788208 ]
 [  13.02303791   89.52882385  128.94288635  151.40719604]]
500
[[ 181.          199.          224.          233.        ]
 [ 221.          288.          252.          337.        ]
 [ 170.          142.          216.          181.        ]
 ..., 
 [  71.28870392  244.94021606  178.8122406   300.5824585 ]
 [ 321.52096558  235.83435059  389.61672974  286.95440674]
 [ 262.90542603  250.96954346  383.79318237  315.88400269]]
500
[[   5.           15.          154.          359.        ]
 [ 158.           15.          324.          360.        ]
 [ 320.           15.          466.          354.        ]
 ..., 
 [ 418.55404663    0.          453.59420776   35.36061859]
 [ 175.14878845  108.43502045  196.80209351  144.97518921]
 [ 229.6300354   317.67080688  261.66842651  345.25482178]]
500
[[   8.           76.           49.          171.        ]
 [  34.           16.          115.           70.        ]
 [  24.          177.          118.          344.        ]
 ..., 
 [ 364.42349243  215.91003418  434.09838867  337.59371948]
 [ 233.60752869   36.4330864   301.97122192  114.1332016 ]
 [ 309.2321167   243.71426392  362.46716309  315.62841797]]
500
[[ 142.           72.          337.          219.        ]
 [ 169.94813538    0.          410.53744507  318.46832275]
 [ 193.55831909    0.          357.58236694  318.46832275]
 ..., 
 [ 219.85514832  116.85604095  343.17327881  239.68765259]
 [  62.21913528  209.65194702   78.2868042   228.23318481]
 [ 174.21287537  289.59625244  196.32620239  313.86755371]]
351
[[  27.           42.          270.          427.        ]
 [   0.           62.16112137  227.35269165  494.81066895]
 [  63.8696785    20.95334816  315.03103638  499.58999634]
 ..., 
 [ 210.40209961  108.5250473   230.12667847  129.77378845]
 [ 149.92486572  464.01315308  174.79440308  485.95144653]
 [ 178.81806946  324.59777832  222.52816772  365.54290771]]
500
[[  25.            0.          147.           61.        ]
 [  28.            1.          492.          246.        ]
 [  96.82723999    0.          341.62701416  272.5       ]
 ..., 
 [   0.          232.34820557  152.82719421  272.5       ]
 [ 425.85693359  239.4896698   499.5         272.5       ]
 [ 324.02127075  131.35209656  499.5         212.9967804 ]]
500
[[ 123.           96.          403.          198.        ]
 [  58.92433929    0.          269.43139648  334.44165039]
 [  97.74134064    0.          369.50045776  334.44165039]
 ..., 
 [ 218.06852722  115.0190506   251.32814026  164.00610352]
 [   0.           39.94556046   95.09676361  156.18873596]
 [ 204.47903442   43.93156815  239.70463562   86.56532288]]
500
[[  17.          119.          491.          327.        ]
 [ 105.780159      0.          339.50866699  374.375     ]
 [  38.73661041    0.          413.48937988  374.375     ]
 ..., 
 [ 246.85949707  124.18109894  348.48562622  303.09521484]
 [ 288.51254272  156.77017212  322.99285889  179.4546051 ]
 [ 265.92617798  139.38865662  375.12496948  183.76861572]]
500
[[ 246.          146.          435.          234.        ]
 [  74.           99.          240.          216.        ]
 [ 177.          373.          177.          373.        ]
 ..., 
 [ 467.97521973  159.43017578  499.29000854  290.09841919]
 [ 227.99401855  273.37304688  244.14833069  290.85064697]
 [   0.          187.16140747   99.40935516  229.79000854]]
284
[[  44.           17.          255.          493.        ]
 [   0.            0.          251.29986572  499.5       ]
 [   0.           49.16331482  201.28123474  404.36053467]
 ..., 
 [ 102.76118469   82.03651428  153.22659302  137.91629028]
 [  51.74952698  391.98867798   94.1466217   439.72921753]
 [  43.57091141  374.88778687   59.50986862  391.42398071]]
500
[[  20.           26.          201.          233.        ]
 [ 262.            2.          438.          227.        ]
 [ 228.80711365    0.          499.5         137.14698792]
 ..., 
 [ 257.35284424   17.59678078  289.23956299   49.29856873]
 [ 247.13444519  107.95729828  344.13494873  168.37924194]
 [ 323.16296387   57.89029312  407.0602417   138.61392212]]
345
[[ 113.          150.          227.          467.        ]
 [  78.02899933   36.63738632  285.3442688   499.67501831]
 [ 106.84116364   97.70044708  268.74679565  499.67501831]
 ..., 
 [ 227.84883118   50.19485474  343.67108154   94.52765656]
 [ 187.14796448  191.85253906  284.23486328  228.03262329]
 [   0.          310.38317871   69.27419281  358.42642212]]
375
[[  14.            2.          357.          495.        ]
 [ 116.95973206   61.63894653  365.83187866  499.375     ]
 [  53.87375641   73.14670563  364.16378784  423.73928833]
 ..., 
 [  15.74284077   57.55970764  112.48239136  111.90956879]
 [   0.           38.18115234   93.60307312   87.58935547]
 [ 112.79193878  213.97505188  158.52005005  384.32159424]]
358
[[  74.           12.          269.          490.        ]
 [ 270.           11.          301.          257.        ]
 [ 259.          230.          285.          306.        ]
 ..., 
 [  81.91610718   85.1238327   173.25253296  210.77210999]
 [ 211.24493408  109.9263916   304.2640686   284.74609375]
 [ 231.63056946  297.38964844  312.8526001   343.07339478]]
500
[[ 157.            1.          266.          313.        ]
 [ 147.97566223    0.          279.59881592  332.44500732]
 [ 150.97789001    2.54876065  258.1491394   283.4487915 ]
 ..., 
 [   0.          159.61450195  333.01489258  254.66763306]
 [ 454.89147949  281.80258179  472.49874878  298.36044312]
 [  89.19685364    0.          159.34892273  229.21060181]]
375
[[  74.           48.          296.          459.        ]
 [  55.90527344   99.70315552  227.56271362  499.375     ]
 [  76.88405609    0.          288.77120972  461.22653198]
 ..., 
 [ 246.96740723  218.75627136  284.57855225  260.02270508]
 [   0.          319.75094604  293.75302124  423.00082397]
 [ 109.55659485  338.75875854  259.7227478   396.32345581]]
500
[[ 300.          154.          369.          279.        ]
 [ 271.56506348    0.          423.6628418   343.80450439]
 [  60.95956802   26.81602097  256.30535889  137.55828857]
 ..., 
 [ 309.59799194   81.43321228  340.91085815  106.73654938]
 [ 324.62435913  347.35760498  402.75460815  368.55102539]
 [ 244.43617249  241.74412537  264.77243042  272.25463867]]
500
[[ 102.          229.          200.          312.        ]
 [ 204.          182.          255.          241.        ]
 [  94.5704422    34.83640671  322.487854    374.375     ]
 ..., 
 [  86.5582962   173.48304749  103.09815216  188.88549805]
 [ 377.23236084  145.52622986  490.67938232  374.375     ]
 [ 433.34301758   52.73948288  478.65460205  135.18457031]]
500
[[ 153.           61.          256.          156.        ]
 [ 274.          114.          325.          156.        ]
 [ 296.           70.          377.          155.        ]
 ..., 
 [ 455.43310547  166.6934967   499.375       317.84689331]
 [ 206.74655151  155.97579956  453.54443359  276.89254761]
 [  13.62211227  122.9644928    41.57940292  149.23765564]]
500
[[ 110.          165.          227.          257.        ]
 [ 250.           87.          399.          175.        ]
 [ 260.           52.          326.           78.        ]
 ..., 
 [ 465.59597778  203.06793213  488.91357422  231.80438232]
 [ 147.69812012    0.          230.26246643  239.19859314]
 [ 349.2612915   246.37158203  477.09997559  332.44500732]]
500
[[ 178.          124.          294.          205.        ]
 [ 111.02152252  106.4175415   321.94760132  229.86845398]
 [ 148.92070007    0.          365.36288452  332.44500732]
 ..., 
 [   0.          188.51600647  147.8162384   280.6076355 ]
 [ 465.18051147  242.47627258  487.85903931  269.07498169]
 [ 270.16311646  161.97885132  289.56115723  212.61924744]]
500
[[ 127.          159.          287.          244.        ]
 [  41.07034683    0.          301.22113037  374.375     ]
 [  44.3145256   110.80712891  233.64871216  374.375     ]
 ..., 
 [ 358.22485352   88.69289398  408.24020386  131.48158264]
 [ 162.34622192  214.50656128  185.80882263  236.02401733]
 [ 195.73080444   64.04915619  213.05471802   82.17732239]]
333
[[ 111.           35.          235.          450.        ]
 [ 196.17349243  154.9122467   221.29507446  175.15513611]
 [ 153.10658264   44.06760788  265.91842651  478.11508179]
 ..., 
 [   0.          389.67401123   18.27485085  413.61566162]
 [ 227.88771057    0.          317.69689941   19.22731972]
 [ 251.72370911  444.93096924  332.44500732  499.5       ]]
192
[[  74.          130.          141.          188.        ]
 [  45.58718872   46.20207214  170.70195007  239.67999268]
 [  46.07958984    0.          191.67999268  239.67999268]
 ..., 
 [   0.          126.51373291   37.45381927  144.89764404]
 [  59.54042816   67.8240509   109.45906067  199.06452942]
 [ 162.93647766   28.45658112  191.67999268  161.8690033 ]]
333
[[ 105.          242.          231.          298.        ]
 [  62.02662659   82.11181641  258.06460571  499.5       ]
 [  97.95523071  223.32647705  195.2878418   287.44732666]
 ..., 
 [ 209.1431427   160.29838562  228.37953186  176.15132141]
 [ 283.31564331    1.84744596  332.44500732   47.69147491]
 [ 212.70046997  141.82264709  284.58798218  175.43516541]]
173
[[  63.          272.          169.          441.        ]
 [   7.            0.          153.          217.        ]
 [  87.06211853   75.87909698  172.5         305.96533203]
 ..., 
 [  13.85362244  251.63812256  172.5         316.53326416]
 [  41.86619186  306.47674561  121.70240784  352.54309082]
 [ 105.8505249    10.46755791  172.5          43.89954376]]
500
[[ 135.          235.          177.          285.        ]
 [ 147.90847778  243.82653809  176.9846344   296.99743652]
 [ 135.55888367  224.06845093  183.6595459   265.06930542]
 ..., 
 [  32.20624924   93.96951294  278.68911743  181.57217407]
 [ 126.79424286  211.9057312   152.71238708  232.06544495]
 [   0.          109.37983704  104.05563354  169.17121887]]
500
[[ 117.          236.          220.          330.        ]
 [ 289.          186.          396.          299.        ]
 [   0.            0.          389.70690918  499.16665649]
 ..., 
 [ 183.18574524  408.89483643  468.38122559  499.16665649]
 [ 260.68099976  181.70170593  333.86303711  312.37411499]
 [   0.          233.81356812   16.53439522  272.87432861]]
500
[[ 246.           19.          376.          312.        ]
 [ 229.06813049   31.05108643  433.8973999   333.44332886]
 [ 250.44152832    0.          482.35025024  333.44332886]
 ..., 
 [  45.99887466  125.8510437    67.58581543  146.90618896]
 [  23.34238815  263.77105713  206.31025696  333.44332886]
 [ 182.50363159   25.55786514  271.24575806   80.22433472]]
333
[[  37.          148.          303.          462.        ]
 [ 115.          314.          171.          359.        ]
 [  59.06448746   52.47226334  271.05227661  499.5       ]
 ..., 
 [ 229.1496582   452.12713623  255.58990479  476.29754639]
 [ 140.52508545   42.35017776  172.08529663   76.38370514]
 [  71.48187256  249.79708862  107.79801178  308.96728516]]
335
[[ 123.           56.          296.          424.        ]
 [  70.42971802    0.          260.69268799  379.93460083]
 [  37.68113327    0.          302.900177    312.93191528]
 ..., 
 [ 113.49452209  169.22839355  158.04364014  261.29705811]
 [ 111.203125     64.2653656   135.68078613  107.39904785]
 [ 236.46470642  448.4503479   278.46951294  479.49954224]]
500
[[  94.          197.          403.          323.        ]
 [ 112.98470306  191.63685608  422.16317749  326.23703003]
 [   6.81446409   18.83229065  204.36280823  335.44000244]
 ..., 
 [   0.          283.29943848   26.08816719  335.44000244]
 [ 399.37731934  236.19996643  470.30639648  333.7520752 ]
 [ 451.9491272   267.21389771  499.52001953  316.17901611]]
500
[[ 182.          128.          297.          244.        ]
 [ 195.86700439  111.61686707  303.58401489  221.14494324]
 [ 134.83045959    0.          366.8508606   374.375     ]
 ..., 
 [ 144.77940369  196.87310791  185.23509216  259.14071655]
 [ 349.57965088  190.85353088  456.95965576  374.375     ]
 [  38.00851822  224.93888855   57.295784    244.46403503]]
354
[[  80.           36.          332.          474.        ]
 [  38.15226364   87.79335785  330.65719604  499.13998413]
 [ 114.57854462    0.          353.41000366  490.04327393]
 ..., 
 [ 253.37310791  264.97424316  323.11050415  364.21688843]
 [ 293.11135864  314.25375366  351.85604858  377.44821167]
 [  76.12186432  358.57608032  146.47595215  453.54296875]]
333
[[ 189.          298.          332.          498.        ]
 [ 175.37756348  278.17297363  332.44500732  499.5       ]
 [ 137.88739014  163.38304138  300.23287964  499.5       ]
 ..., 
 [ 213.1414032    20.68370438  301.26831055   70.92882538]
 [ 242.47357178  186.9208374   330.78771973  419.56536865]
 [   0.          223.14909363  100.33110046  260.70135498]]
371
[[  74.           91.          274.          409.        ]
 [  25.73932838   11.12101746  156.63514709  499.61334229]
 [  30.09220695   21.581604    218.53207397  499.61334229]
 ..., 
 [  22.60997772  251.8699646    44.52951813  276.27688599]
 [ 122.93608093   25.50562286  207.64753723   74.24253845]
 [ 215.12391663  101.76490784  253.24441528  145.07272339]]
333
[[  62.            0.          201.          419.        ]
 [   0.82033902    0.          306.15109253  499.5       ]
 [  63.87413406   50.97826004  219.66436768  499.5       ]
 ..., 
 [ 101.47548676   77.16823578  175.59869385  148.0965271 ]
 [  85.94145203  470.70565796  108.20272827  497.98730469]
 [  85.55661774  285.87426758  168.78303528  406.22293091]]
500
[[ 140.          111.          354.          299.        ]
 [  78.15796661    0.          305.67919922  332.44500732]
 [ 135.00166321    0.          266.84915161  332.44500732]
 ..., 
 [ 118.94087219  253.71138     151.65097046  292.89447021]
 [ 194.1872406   247.3575592   220.67610168  271.00234985]
 [  75.98072815  311.24066162  119.41547394  332.44500732]]
333
[[ 199.          287.          304.          373.        ]
 [ 101.          285.          141.          327.        ]
 [   0.            0.          222.41584778  421.131073  ]
 ..., 
 [ 295.73776245  442.5788269   332.44500732  499.5       ]
 [ 180.23425293  140.57923889  204.21910095  159.25862122]
 [ 142.70248413  297.52133179  164.03479004  319.13311768]]
334
[[  79.          160.          310.          488.        ]
 [  96.74307251   19.71117783  333.44332886  492.76971436]
 [ 171.40003967   33.85655594  333.44332886  408.12466431]
 ..., 
 [ 237.40187073  272.57455444  333.44332886  414.53591919]
 [ 281.21966553  126.71291351  333.44332886  270.09677124]
 [  72.91002655  150.05567932   97.98995972  181.37133789]]
375
[[  95.           37.          213.          361.        ]
 [ 126.72736359    0.          255.80851746  358.27423096]
 [  78.72184753   38.72873306  199.6048584   499.375     ]
 ..., 
 [ 329.15786743  428.08621216  346.40563965  443.76797485]
 [  78.62786865  158.46296692  232.2277832   378.43054199]
 [  77.67015839  177.14846802  102.40776062  206.98493958]]
308
[[  48.           67.          162.          175.        ]
 [ 182.36366272    0.          307.61499023  230.61500549]
 [ 147.4002533     0.          269.64865112  230.61500549]
 ..., 
 [   3.48981166   97.53470612   76.4015274   126.14602661]
 [ 114.63240051   72.60639191  174.14364624  136.04547119]
 [  77.46116638  132.76649475  134.58599854  230.61500549]]
494
[[  11.           36.          408.          445.        ]
 [   0.            0.          360.31826782  498.93997192]
 [   0.          141.68920898  329.87844849  498.93997192]
 ..., 
 [   0.           58.92937469   75.26566315  136.05532837]
 [   0.          242.03097534  143.14796448  320.75619507]
 [  21.73492622  383.32095337  169.10046387  459.94403076]]
375
[[  27.          202.          208.          494.        ]
 [  68.            1.          271.          174.        ]
 [   0.           22.08831787  261.40621948  499.375     ]
 ..., 
 [   0.          432.99841309   85.57317352  481.55075073]
 [  57.82709503  201.73757935   79.79864502  227.38340759]
 [  87.63639069  446.43844604  120.05870819  485.22619629]]
500
[[  18.           25.          350.          365.        ]
 [ 380.          233.          486.          282.        ]
 [ 361.           28.          479.          198.        ]
 ..., 
 [  37.67028427   52.64648438   56.94868851   70.75279236]
 [ 391.41125488  216.65574646  408.35522461  243.67791748]
 [ 458.09085083  327.76785278  478.90536499  364.32699585]]
375
[[ 141.           39.          368.          441.        ]
 [ 130.65481567    0.          374.375       499.375     ]
 [  63.91869354    0.          348.16992188  484.19226074]
 ..., 
 [  68.17607117  210.75701904   96.1917038   243.65336609]
 [  52.65339661  246.2052002    90.04730225  306.45584106]
 [ 171.97892761  461.44134521  190.98977661  481.53219604]]
500
[[  73.          125.          342.          228.        ]
 [ 103.60798645    0.          296.58685303  265.5       ]
 [ 164.78085327    0.          359.9460144   265.5       ]
 ..., 
 [  38.81797791  187.23648071   63.38521576  216.28143311]
 [  54.7111969   197.20281982   89.50395203  232.74780273]
 [ 308.11303711  191.49520874  349.41204834  224.50552368]]
500
[[ 279.           23.          435.          214.        ]
 [ 152.          154.          215.          276.        ]
 [  72.          257.          197.          399.        ]
 ..., 
 [ 414.11938477  228.1652832   433.54867554  269.8086853 ]
 [ 315.97158813  111.0217514   427.91397095  221.55885315]
 [ 383.28549194  330.71224976  417.22964478  369.00601196]]
500
[[  26.           57.          446.          316.        ]
 [ 224.04957581    0.          457.55914307  363.39334106]
 [  36.78180313    0.          344.58920288  363.39334106]
 ..., 
 [ 199.10353088   92.10590363  298.34265137  148.48271179]
 [ 215.62199402   83.36915588  305.58346558  315.18411255]
 [  38.64727783  103.64655304   64.69595337  126.78076935]]
443
[[  48.            2.          383.          451.        ]
 [  77.59326172    0.          387.05532837  499.11331177]
 [  55.69138336  198.37088013  401.95925903  499.11331177]
 ..., 
 [  78.23104095  353.27401733  117.65585327  402.20977783]
 [ 321.43615723  120.32208252  344.29733276  164.18508911]
 [   0.           25.4771862   207.97593689  277.33230591]]
500
[[  69.          213.          276.          319.        ]
 [  58.56596375  204.01333618  242.92869568  331.44665527]
 [  72.84700775  203.22521973  290.75753784  314.23510742]
 ..., 
 [ 193.33599854  123.69088745  263.07299805  284.78070068]
 [ 261.46414185  118.02244568  286.80651855  137.44874573]
 [ 173.26654053  213.26818848  197.00126648  239.88877869]]
333
[[  84.           38.          211.          205.        ]
 [ 102.          207.          236.          381.        ]
 [  65.54182434   86.60454559  289.32583618  499.5       ]
 ..., 
 [ 122.29161072  113.85638428  240.567276    199.63270569]
 [ 220.38798523  269.45950317  253.72720337  320.73840332]
 [   2.9260211   261.60339355  286.79467773  338.17456055]]
500
[[  94.           56.          371.          354.        ]
 [  50.93046951    0.          336.11004639  365.4125061 ]
 [   0.           58.35587692  359.26159668  380.36502075]
 ..., 
 [ 418.49783325  268.22805786  444.24664307  308.40924072]
 [ 406.82537842  182.84396362  428.39840698  203.65962219]
 [  83.84134674  358.77008057  110.83080292  375.62045288]]
357
[[  82.          204.          286.          404.        ]
 [  68.13700867  220.77740479  304.65847778  401.73742676]
 [ 111.91140747    6.84587717  356.40499878  499.20498657]
 ..., 
 [ 209.21818542  218.3452301   306.83575439  342.27874756]
 [   8.56664562  399.78833008   36.90439224  427.20895386]
 [  81.18733978  229.24946594  185.17861938  277.02792358]]
375
[[ 114.          150.          250.          362.        ]
 [ 113.          127.          129.          157.        ]
 [  69.09547424  105.68668365  237.56896973  460.01318359]
 ..., 
 [ 312.02133179  105.7453537   338.828125    133.97489929]
 [  47.73765564  169.75802612  155.81506348  224.81834412]
 [ 202.9863739   445.49343872  225.0692749   473.16134644]]
375
[[ 159.            2.          284.          477.        ]
 [ 138.62846375    0.          294.45663452  499.375     ]
 [  42.70029068  124.3715744   310.17373657  439.2432251 ]
 ..., 
 [   0.           26.28889275  140.51716614   46.3571167 ]
 [  41.80229568   18.29379082  122.85138702  130.70370483]
 [  69.34503174  433.46072388  109.75617981  458.67953491]]
375
[[  65.            0.          274.          498.        ]
 [  82.01119232   40.99103928  280.49194336  405.7616272 ]
 [  45.93446732   15.69757462  298.57962036  363.14880371]
 ..., 
 [ 207.01116943    9.10584927  374.375       109.88263702]
 [ 246.10588074  115.16811371  300.87994385  348.32577515]
 [ 112.09445953  302.87265015  152.24406433  372.93551636]]
375
[[  36.           42.          336.          472.        ]
 [  36.33470535  168.67185974  306.77209473  499.375     ]
 [  74.46305084    0.          373.57788086  470.78717041]
 ..., 
 [  62.05441284  465.61587524  129.85179138  489.69494629]
 [ 131.86805725  222.50854492  245.93673706  293.33221436]
 [ 185.9256897   338.12542725  272.70742798  385.12774658]]
500
[[ 233.          316.          463.          450.        ]
 [ 182.04571533    0.          498.95999146  461.23001099]
 [   0.            0.          498.95999146  365.56625366]
 ..., 
 [   0.          302.94094849   68.69714355  380.17340088]
 [ 280.53866577  198.34030151  316.89590454  235.74960327]
 [ 399.15533447  197.43173218  476.98080444  461.23001099]]
500
[[  83.           79.          393.          322.        ]
 [ 116.4580307     0.          441.34628296  374.375     ]
 [ 129.42053223  115.35680389  348.41598511  372.60748291]
 ..., 
 [ 312.32388306   24.47729492  343.72927856   59.82396698]
 [ 101.75673676  207.09181213  219.237854    246.62413025]
 [ 312.77038574  215.22531128  366.41680908  272.80395508]]
500
[[ 144.          104.          203.          160.        ]
 [ 243.           62.          445.          238.        ]
 [ 189.            0.          222.           23.        ]
 ..., 
 [ 447.67755127  170.65618896  474.91525269  195.90589905]
 [ 212.83230591  328.67910767  237.65969849  346.70483398]
 [ 401.16653442  293.49310303  426.35159302  321.10626221]]
352
[[  34.          227.          251.          471.        ]
 [ 226.          283.          263.          307.        ]
 [   3.63076091   44.7957077   277.5395813   499.25332642]
 ..., 
 [ 335.32162476  435.42803955  351.41333008  497.37207031]
 [  65.26954651  306.19046021  170.13168335  409.68707275]
 [ 226.0917511   216.56390381  281.76925659  264.98291016]]
500
[[  54.          211.          193.          353.        ]
 [ 265.46389771    0.          444.05450439  348.31063843]
 [ 227.77658081   49.11682129  478.91824341  357.40332031]
 ..., 
 [   0.          271.58798218   72.50132751  330.47158813]
 [  78.871521    258.09179688  120.68248749  312.42541504]
 [ 108.20379639   47.03558731  127.80034637   67.60792542]]
332
[[  47.          252.          171.          483.        ]
 [ 161.          269.          242.          350.        ]
 [   0.           72.78533936  224.41928101  499.66000366]
 ..., 
 [   0.          395.57131958   61.42635727  447.15359497]
 [ 209.69429016   41.84889221  290.8961792    82.94841003]
 [ 131.08135986    0.          161.72846985   20.71611977]]
500
[[ 111.           12.          404.          333.        ]
 [ 122.53446198    0.          379.17797852  333.44332886]
 [ 209.34408569    0.          453.5562439   333.44332886]
 ..., 
 [  23.65319824   76.72020721  124.96678925  333.44332886]
 [ 430.59683228   49.42967224  446.28143311   81.08978271]
 [   0.          237.74113464   39.32176971  285.55963135]]
500
[[  41.           88.          473.          274.        ]
 [ 174.9677124     0.          413.10308838  332.44500732]
 [  71.46990204    0.          305.13900757  332.44500732]
 ..., 
 [ 123.6577301   282.50830078  158.16795349  328.03067017]
 [  82.07254028  268.94140625  169.59991455  317.37765503]
 [ 218.44241333   24.81375122  243.02528381   50.93599319]]
333
[[ 107.           78.          235.          394.        ]
 [  67.71963501   58.90928268  226.53123474  435.85617065]
 [  97.17918396  104.49985504  257.05438232  499.5       ]
 ..., 
 [ 238.64857483  215.75183105  332.44500732  327.81295776]
 [  94.1120224   456.57583618  130.97817993  489.89678955]
 [ 224.44754028  281.69555664  264.31225586  342.61248779]]
500
[[ 172.          143.          259.          330.        ]
 [ 280.          154.          362.          337.        ]
 [ 193.08937073    0.          472.40264893  369.3833313 ]
 ..., 
 [ 351.12054443   84.58113861  383.2074585   125.9822464 ]
 [ 383.78591919   44.57367325  420.40960693   90.54652405]
 [ 436.84155273  112.53436279  457.66506958  140.87585449]]
500
[[ 132.          178.          225.          292.        ]
 [ 131.5103302   100.11998749  289.68002319  374.375     ]
 [  85.96551514   52.52386093  272.58468628  374.375     ]
 ..., 
 [ 205.13002014  250.86199951  248.28108215  304.14852905]
 [ 378.26217651  292.52990723  478.90371704  343.77661133]
 [ 164.29650879  196.12710571  204.17280579  242.87591553]]
500
[[  10.           95.          478.          261.        ]
 [  81.82542419    0.          303.00683594  374.375     ]
 [  16.46644592   65.54409027  331.66491699  374.375     ]
 ..., 
 [ 454.45483398   30.49373055  491.55926514   80.24171448]
 [  92.93205261  191.57240295  196.64477539  239.21759033]
 [ 280.05786133  141.72038269  414.07611084  210.48617554]]
500
[[ 135.          268.          152.          290.        ]
 [ 156.          229.          184.          269.        ]
 [ 176.          174.          240.          249.        ]
 ..., 
 [ 432.12728882  179.95358276  449.34182739  197.92459106]
 [ 293.27822876  317.48666382  362.0925293   368.06536865]
 [  44.54500198    9.88429928  155.81085205  401.33001709]]
500
[[  41.           33.          404.          163.        ]
 [ 130.88687134    0.          362.21560669  233.5       ]
 [ 228.82217407    0.          457.21652222  233.5       ]
 ..., 
 [ 182.79135132  129.20849609  209.23254395  155.09738159]
 [ 221.47071838   31.81764221  243.78422546   55.77462006]
 [ 143.81944275  175.72151184  164.35475159  195.42195129]]
500
[[ 114.          115.          379.          221.        ]
 [ 153.74507141    0.          389.93121338  374.375     ]
 [ 108.20202637    0.          331.70645142  374.375     ]
 ..., 
 [   0.           13.88216972  238.66065979  103.24585724]
 [ 135.65287781  133.13670349  254.17404175  374.375     ]
 [ 370.08670044  294.06143188  398.90179443  327.38949585]]
500
[[ 181.          128.          328.          344.        ]
 [ 302.           95.          334.          121.        ]
 [ 100.47975159    0.          364.58206177  374.375     ]
 ..., 
 [  90.12715912  245.0519104   123.9862442   284.41104126]
 [ 204.78613281   60.82384109  299.43930054  285.31509399]
 [ 438.03424072    0.          499.375        64.40357208]]
500
[[  46.           26.          466.          365.        ]
 [ 166.86851501    0.          491.71517944  487.18667603]
 [  61.80385208   61.25307465  357.45910645  487.18667603]
 ..., 
 [ 394.14215088  122.12491608  468.06787109  225.27078247]
 [ 199.0098114   194.23773193  248.38095093  254.71104431]
 [ 425.36306763  327.38870239  461.75317383  349.98553467]]
500
[[ 143.          122.          398.          256.        ]
 [ 151.44920349  135.79141235  381.97988892  274.73486328]
 [ 118.49674225    0.          380.24295044  374.375     ]
 ..., 
 [ 212.81756592   17.78399849  329.41357422   90.02202606]
 [ 455.6114502    58.92539978  490.47076416  106.39222717]
 [ 126.36393738  169.6927948   345.91018677  326.9730835 ]]
500
[[  80.           74.          121.          103.        ]
 [ 164.          190.          383.          291.        ]
 [ 137.           49.          229.          209.        ]
 ..., 
 [  60.3093338   160.65713501   95.30703735  210.91772461]
 [  91.80370331  130.05323792  124.07064056  170.7820282 ]
 [ 100.46871185   41.29069901  218.32627869  151.53997803]]
489
[[ 249.           73.          415.          265.        ]
 [ 190.91282654    0.          488.18496704  476.98336792]
 [   0.            0.          208.15733337  498.77996826]
 ..., 
 [ 186.051651      0.          337.36819458   38.27044678]
 [ 238.34202576   71.74866486  414.98706055  104.36690521]
 [ 455.77752686  349.88018799  476.5072937   390.03808594]]
405
[[  79.           55.          309.          379.        ]
 [ 114.21077728  115.86447144  273.74731445  499.50003052]
 [   0.            0.          251.17375183  468.32537842]
 ..., 
 [  24.73990631  122.23573303   50.43916321  152.32052612]
 [ 347.76977539  314.01757812  373.33218384  343.8961792 ]
 [ 251.08680725  174.72340393  350.52975464  464.51980591]]
500
[[  57.          158.          335.          307.        ]
 [   0.            0.          228.3518219   374.375     ]
 [  88.34103394    0.          318.49569702  374.375     ]
 ..., 
 [  41.14883804  261.69506836  136.33654785  324.26947021]
 [ 328.7364502    28.62281799  486.71710205  192.07611084]
 [ 423.06866455  228.43170166  446.78469849  283.35354614]]
500
[[   8.           49.          353.          226.        ]
 [ 121.           22.          148.           44.        ]
 [ 139.16194153    0.          364.34677124  357.40332031]
 ..., 
 [   0.           83.63056946   98.34445953  125.91130066]
 [ 283.73712158  303.42703247  320.67169189  329.4019165 ]
 [ 438.38293457   55.32963943  499.40997314  207.61932373]]
333
[[ 141.          211.          219.          364.        ]
 [ 100.84489441   99.63134003  255.59968567  493.76635742]
 [  86.48107147   39.15985107  293.76397705  475.89328003]
 ..., 
 [ 208.95982361  391.5402832   236.38690186  425.89587402]
 [  21.44865417  160.23503113  262.01983643  265.92016602]
 [ 299.00479126   76.4238205   321.02139282  101.40693665]]
332
[[ 222.          148.          282.          313.        ]
 [  60.          173.          102.          298.        ]
 [ 223.03334045  163.48852539  273.97589111  330.91210938]
 ..., 
 [ 154.87521362  130.65716553  230.11662292  162.98565674]
 [ 182.31460571   59.67201996  202.75267029   78.85282135]
 [  24.72268105  162.69519043  124.0920105   208.25123596]]
500
[[  73.          213.          209.          276.        ]
 [   0.            0.          296.44390869  391.34667969]
 [ 249.19558716    0.          499.14666748  308.59844971]
 ..., 
 [ 201.88941956  160.61053467  299.79141235  391.34667969]
 [ 356.93920898  317.27893066  485.26617432  391.34667969]
 [ 437.1869812   201.85346985  459.10070801  222.9536438 ]]
334
[[ 216.          173.          333.          497.        ]
 [ 202.20922852  203.89009094  330.28921509  499.32998657]
 [   0.            0.          184.96907043  499.32998657]
 ..., 
 [ 154.86497498    0.          263.23831177  319.72717285]
 [ 165.49655151  386.39096069  200.74549866  416.75662231]
 [ 117.7612915   101.48723602  146.28326416  130.5201416 ]]
269
[[  74.           90.          250.          484.        ]
 [  79.30342102   62.51434326  237.65586853  421.359375  ]
 [ 115.78159332   32.93757629  229.01339722  431.08300781]
 ..., 
 [ 211.18896484  268.16616821  268.5         306.75424194]
 [ 102.38208008  346.87402344  143.18075562  379.55474854]
 [ 131.16567993   61.68765259  218.11453247  101.92951965]]
344
[[  70.          126.          259.          380.        ]
 [ 126.90409088  105.99979401  253.45420837  439.17333984]
 [ 157.15541077   82.01999664  330.69869995  439.17333984]
 ..., 
 [   8.41198444  110.30128479  147.88380432  256.81259155]
 [ 138.58776855  160.19229126  175.65718079  192.87490845]
 [ 183.19416809  112.95330811  232.92268372  173.87960815]]
333
[[   0.          105.          331.          494.        ]
 [  93.           22.          264.          120.        ]
 [   0.          155.50787354  328.33950806  455.10461426]
 ..., 
 [ 271.86444092  424.08154297  332.44500732  478.04608154]
 [ 228.45729065  212.03799438  265.43585205  282.59420776]
 [   0.          396.88253784   34.58054352  424.87402344]]
408
[[  31.           54.           63.           76.        ]
 [   0.          107.           20.          138.        ]
 [  51.           76.          326.          244.        ]
 ..., 
 [  17.76881599  181.26153564   38.32128143  200.30323792]
 [  51.43855286   73.38793945  115.07402039  140.12402344]
 [  93.18700409   48.88666153  124.63722229   82.84049225]]
448
[[   1.           18.          425.          495.        ]
 [  29.20476532    0.          358.24090576  499.51998901]
 [   0.            0.          229.83840942  499.51998901]
 ..., 
 [ 374.02011108  466.94430542  394.80328369  486.3447876 ]
 [ 130.00978088  440.65368652  166.76902771  473.14016724]
 [  70.92354584  102.23736572  217.62785339  191.7559967 ]]
450
[[ 157.           40.          180.           61.        ]
 [ 154.           72.          273.          340.        ]
 [ 157.62171936   43.93202591  287.86636353  229.10293579]
 ..., 
 [ 185.81735229    0.          295.64916992  217.43870544]
 [ 276.62054443  412.67004395  297.05221558  430.12017822]
 [ 230.15264893  318.37585449  359.9336853   388.37020874]]
375
[[  38.          221.          141.          477.        ]
 [  96.          394.          166.          497.        ]
 [  69.          282.          187.          379.        ]
 ..., 
 [ 114.16969299    0.          317.7623291   117.75580597]
 [ 182.43814087  266.45950317  206.48405457  295.83789062]
 [ 325.50192261  116.33916473  348.70474243  135.4233551 ]]
450
[[  49.           56.          395.          209.        ]
 [ 216.1915741     0.          374.84082031  254.69998169]
 [ 193.62007141   32.18198776  449.54998779  188.82324219]
 ..., 
 [   0.           20.95481682   56.68194962   66.92090607]
 [  96.35498047   88.76584625  142.69012451  129.07926941]
 [ 181.73612976   80.64273071  237.95852661  112.62673187]]
333
[[  78.           67.          252.          432.        ]
 [  37.01205444    0.          332.44500732  499.5       ]
 [ 101.54078674    0.          274.3850708   451.81903076]
 ..., 
 [  73.16652679   97.16864014   96.87563324  120.85508728]
 [  22.79630852  399.3197937    58.92127609  440.92419434]
 [  45.6866188   154.45025635   66.74868774  172.27554321]]
333
[[  23.           42.          288.          468.        ]
 [  70.60404205    0.          312.2303772   260.70553589]
 [  54.80746078    0.          290.71405029  337.93618774]
 ..., 
 [  13.25946522  186.01217651   39.18363571  215.44068909]
 [ 133.25479126  250.76683044  247.96925354  408.95043945]
 [  32.76993561   52.80304718   64.23633575   89.21989441]]
500
[[ 230.          146.          259.          166.        ]
 [ 229.00202942  150.42987061  262.03903198  174.27723694]
 [ 238.86669922  152.94818115  267.54406738  170.85658264]
 ..., 
 [ 378.89675903   58.36762238  404.82025146   84.52336121]
 [ 302.12347412  183.85038757  408.21749878  232.68112183]
 [  88.96258545  140.41390991  121.662323    165.77667236]]
455
[[ 148.          230.          327.          428.        ]
 [ 210.39038086    0.          408.17544556  498.98330688]
 [ 220.44995117    0.          340.96237183  498.98330688]
 ..., 
 [   0.          177.88337708  229.6884613   297.71121216]
 [ 130.97460938   79.33531952  175.45303345  129.50009155]
 [  41.6956749     0.           98.86455536   34.87843323]]
334
[[ 103.           97.          229.          316.        ]
 [  62.04933167    0.          277.17590332  411.38122559]
 [ 101.84111786   19.36229324  294.95291138  446.82394409]
 ..., 
 [  74.67824554  324.67834473  102.90737152  363.4095459 ]
 [ 131.97344971   85.49092865  219.70600891  230.09889221]
 [   6.70818329  346.00393677  304.62719727  421.24993896]]
150
[[  42.           30.          107.          142.        ]
 [  19.53445435    0.          116.63230896  179.75      ]
 [   0.            8.34550476  125.29737854  165.49861145]
 ..., 
 [ 128.77574158   81.48683929  146.1113739   161.29397583]
 [  95.87454987  102.03191376  124.76496124  123.03484344]
 [   0.           59.15364838   34.73988342  121.13124847]]
332
[[   0.            0.          252.          466.        ]
 [ 101.           98.          147.          120.        ]
 [   0.            0.          267.0604248   499.66000366]
 ..., 
 [ 159.98522949  162.25534058  190.65953064  211.88232422]
 [  97.19343567  206.11953735  189.05734253  265.87121582]
 [ 107.39901733  129.67306519  179.73286438  207.77590942]]
343
[[   0.            0.          338.          487.        ]
 [   1.7512002     0.          211.86955261  451.2086792 ]
 [  13.21463585    0.          268.722229    491.68768311]
 ..., 
 [ 185.19404602  174.50996399  256.38143921  499.63665771]
 [ 128.8163147     0.          197.95063782   83.02380371]
 [ 149.78434753  415.82922363  223.09832764  463.61132812]]
500
[[   0.            1.          492.          316.        ]
 [ 122.59454346    0.          345.29385376  328.45166016]
 [  58.64553833    0.          252.25566101  328.45166016]
 ..., 
 [   0.          178.06219482   18.05904198  215.76409912]
 [ 183.99911499  214.2052002   230.29992676  278.29412842]
 [ 296.93548584  121.30718994  425.70565796  200.39102173]]
357
[[  31.          162.          356.          499.        ]
 [  15.          321.           61.          354.        ]
 [   0.          409.           10.          463.        ]
 ..., 
 [ 125.0967865    95.79299927  226.08685303  143.09474182]
 [ 262.00192261  474.98968506  302.05899048  499.20498657]
 [  97.73016357   81.54753876  113.20082092   98.84204865]]
375
[[  82.          244.          162.          332.        ]
 [  81.          319.          135.          497.        ]
 [ 143.          307.          306.          453.        ]
 ..., 
 [ 237.4236908    55.2482872   261.8319397    75.88513184]
 [  86.41087341  178.01695251  107.23042297  196.85746765]
 [  32.99020386  147.442276     68.13153839  182.30860901]]
346
[[  34.          139.          305.          497.        ]
 [  44.83768082    0.          295.47000122  499.39334106]
 [   4.05712509  218.69577026  236.17150879  499.39334106]
 ..., 
 [ 187.69419861  104.54665375  306.45227051  244.33555603]
 [ 210.70542908  173.65614319  253.33398438  246.0128479 ]
 [ 108.3651123    99.16796875  159.5322113   136.74409485]]
333
[[  72.          109.          256.          338.        ]
 [ 215.          129.          258.          163.        ]
 [  42.47118378    0.          311.53189087  467.92611694]
 ..., 
 [ 212.92457581  255.82078552  324.03765869  297.53811646]
 [ 103.26811981  229.38092041  210.92205811  306.24752808]
 [ 230.1783905   234.01777649  332.44500732  369.33071899]]
342
[[  80.            3.          223.          495.        ]
 [  32.26552582    0.          224.06159973  390.88082886]
 [  25.5287571     0.          306.35952759  393.47598267]
 ..., 
 [ 115.97229767   90.09468842  200.98217773  202.25909424]
 [ 197.15126038  295.24932861  256.16235352  412.48931885]
 [ 270.03942871  426.90255737  310.8237915   480.77960205]]
284
[[ 180.          118.          189.          124.        ]
 [ 189.          133.          195.          148.        ]
 [ 198.          149.          202.          159.        ]
 ..., 
 [  55.34815216    6.48976135  160.79153442   49.60292053]
 [  59.08927917  335.14642334   77.01451111  368.03808594]
 [   0.          378.17678833   73.58766174  421.47573853]]
333
[[ 165.           15.          320.          228.        ]
 [ 157.99072266    0.          332.44500732  345.8633728 ]
 [  84.65410614    0.          316.79803467  360.34616089]
 ..., 
 [ 207.17050171  233.66304016  301.12277222  499.5       ]
 [   0.            0.           19.64014053   30.76146889]
 [  91.40712738  389.59344482  125.00382996  418.0012207 ]]
333
[[  87.          110.          229.          418.        ]
 [ 139.          444.          170.          468.        ]
 [  56.45598984  108.39650726  269.68395996  499.5       ]
 ..., 
 [ 222.35079956  293.63659668  257.31970215  347.97891235]
 [ 170.86135864  362.86172485  275.65002441  412.0078125 ]
 [   7.67632866  265.61978149  226.15057373  370.68670654]]
333
[[   0.            0.          330.          381.        ]
 [  26.          345.          279.          498.        ]
 [   9.75893307    1.36261916  204.66014099  417.72164917]
 ..., 
 [ 244.85643005  356.10235596  291.8710022   413.98968506]
 [   0.           78.09736633   71.03770447  142.11116028]
 [   0.          292.61828613   53.31358337  369.11935425]]
500
[[ 285.          142.          469.          324.        ]
 [ 270.09896851   37.8516655   460.0145874   332.44500732]
 [ 335.99002075   16.81258011  499.5         332.44500732]
 ..., 
 [ 408.17160034   24.2771759   480.61828613  257.42373657]
 [ 216.73217773  131.60466003  424.18746948  215.64997864]
 [ 136.11454773  289.09591675  161.96585083  322.57730103]]
375
[[  84.          138.          287.          345.        ]
 [  43.73332977   63.74320984  339.93154907  385.46514893]
 [  68.96396637    0.          296.92611694  499.375     ]
 ..., 
 [  44.35532379   84.26224518   68.33539581  105.83248901]
 [  38.53900909    0.          315.47381592   48.64240646]
 [   0.          326.26235962  282.42401123  422.81295776]]
333
[[ 132.          271.          259.          475.        ]
 [  84.64923096  109.36169434  328.30123901  499.5       ]
 [  70.05184174  191.39915466  271.62097168  499.5       ]
 ..., 
 [ 190.44882202  154.60621643  222.92915344  196.31987   ]
 [  92.33203888  311.68432617  108.91452789  330.79299927]
 [ 233.16998291  137.31176758  255.75405884  173.93348694]]
477
[[  93.           25.          354.          302.        ]
 [  28.          165.          114.          298.        ]
 [  73.          322.          241.          484.        ]
 ..., 
 [ 156.59617615  259.21386719  211.10493469  323.29385376]
 [ 439.12002563  184.61941528  461.63449097  209.59408569]
 [ 305.64779663   70.50273132  476.20501709  246.78262329]]
500
[[  97.          127.          206.          278.        ]
 [  83.           26.          161.          114.        ]
 [ 321.          116.          383.          181.        ]
 ..., 
 [ 214.21961975   95.72446442  416.22241211  360.2722168 ]
 [  88.64309692  228.08634949  471.24246216  376.59234619]
 [  27.33360672  245.25007629  267.98944092  366.58029175]]
220
[[  56.           23.          163.          197.        ]
 [  76.49355316    6.23119593  196.54444885  228.80000305]
 [   2.45219517   30.81431198  204.51417542  228.80000305]
 ..., 
 [ 187.1890564    13.07984924  219.6333313    30.76918793]
 [ 166.12254333   23.22403526  197.89590454   55.54483032]
 [ 166.8558197    82.162323    219.6333313   109.32206726]]
500
[[   7.           57.          425.          267.        ]
 [ 432.           89.          467.          218.        ]
 [  96.9527359     0.          376.26660156  374.375     ]
 ..., 
 [ 151.89414978    0.          208.74026489   65.00350952]
 [   0.          161.97874451   41.11702728  222.99996948]
 [ 349.53640747  116.81396484  391.54208374  190.02070618]]
500
[[  23.           23.          459.          257.        ]
 [ 205.57243347    0.          409.33306885  374.375     ]
 [ 125.52170563    0.          419.00787354  374.375     ]
 ..., 
 [ 387.56713867    0.          465.64337158  129.6991272 ]
 [   0.          142.05755615   58.83145142  196.32911682]
 [ 275.20022583  263.05700684  338.81945801  305.07000732]]
500
[[  64.            1.          454.          385.        ]
 [ 174.28738403    0.          461.57858276  386.35498047]
 [  89.63643646    0.          391.89025879  386.35498047]
 ..., 
 [  75.57270813  297.28747559   98.73706055  318.90264893]
 [ 419.15161133   23.88220024  499.22998047   84.41557312]
 [ 476.14471436  117.99482727  494.05123901  134.79162598]]
500
[[  13.          111.          196.          302.        ]
 [ 306.           92.          482.          299.        ]
 [   0.            0.          212.24214172  374.375     ]
 ..., 
 [ 253.86331177  153.68804932  275.42160034  178.76933289]
 [  82.79444885  276.12521362  138.61386108  307.92294312]
 [  35.2320137    85.03270721  132.14826965  128.67285156]]
333
[[  21.           47.          281.          467.        ]
 [  66.75856018    0.          311.99633789  477.96832275]
 [ 109.22097778    0.          299.38464355  345.59854126]
 ..., 
 [ 274.08859253  226.79858398  299.18087769  246.79896545]
 [ 112.72480774  135.88606262  228.54473877  180.71958923]
 [ 119.41474915  123.37513733  188.50587463  190.75720215]]
333
[[   1.          173.          255.          401.        ]
 [   0.          397.          201.          469.        ]
 [  64.          253.          135.          329.        ]
 ..., 
 [ 263.80117798  188.17982483  332.44500732  373.47210693]
 [ 135.49552917  291.85534668  175.89894104  341.7984314 ]
 [ 251.10527039  226.20901489  267.37573242  248.35861206]]
333
[[ 100.          232.          240.          407.        ]
 [ 141.          185.          180.          232.        ]
 [  63.28785324  128.08477783  295.2003479   499.5       ]
 ..., 
 [  86.73681641  228.53565979  112.91034698  269.73336792]
 [  31.37779999  121.71646118  179.33003235  334.58145142]
 [  44.13359451  252.98318481  115.66320038  296.16937256]]
500
[[  84.          131.          176.          156.        ]
 [  42.61693954   35.99236298  275.33703613  332.44500732]
 [  79.25147247  131.72514343  183.97203064  156.81413269]
 ..., 
 [ 169.18574524  128.33898926  349.82983398  332.44500732]
 [ 221.60614014  314.38223267  312.94421387  332.44500732]
 [ 307.19656372  174.83546448  396.7897644   219.22286987]]
500
[[  25.          236.           66.          310.        ]
 [  62.           47.          112.           86.        ]
 [   0.           47.           48.          168.        ]
 ..., 
 [  18.34377861  225.91601562   57.86145401  333.44332886]
 [   0.          250.41639709   16.73265076  279.75845337]
 [  69.17575836   54.50611496   97.67610931   73.03153229]]
500
[[  80.            5.          191.          211.        ]
 [ 290.           32.          403.          226.        ]
 [ 248.21731567    0.          453.11248779  331.44665527]
 ..., 
 [ 474.15148926  199.58979797  495.59262085  217.62805176]
 [  43.70663834  128.34368896   68.08647156  150.67329407]
 [ 126.31863403  170.84877014  162.89888     209.52401733]]
433
[[  86.          299.          290.          486.        ]
 [  18.           12.          420.          280.        ]
 [  98.914505      0.          303.59857178  424.71307373]
 ..., 
 [ 140.37852478  335.89569092  223.80123901  397.87347412]
 [ 305.62356567  444.65811157  327.88931274  460.61071777]
 [  29.64713478  269.10720825   49.72740936  293.81542969]]
492
[[ 158.          298.          187.          335.        ]
 [ 191.03565979    0.          491.          490.18167114]
 [ 162.24421692  311.15032959  190.52935791  346.21841431]
 ..., 
 [  54.05020523  232.93322754  230.95500183  267.77706909]
 [ 381.26968384  202.35603333  491.          438.9616394 ]
 [ 384.27697754  224.63084412  491.          381.80569458]]
375
[[  4.00000000e+00   1.14000000e+02   1.74000000e+02   4.83000000e+02]
 [  1.37000000e+02   1.18000000e+02   2.23000000e+02   2.11000000e+02]
 [  0.00000000e+00   4.06423759e+01   2.15028946e+02   4.99375000e+02]
 ..., 
 [  2.18883820e+02   2.39712280e+02   2.40289032e+02   2.58919495e+02]
 [  3.42990756e-01   3.15735779e+02   1.66078014e+01   3.67467560e+02]
 [  1.08442055e+02   3.71377747e+02   2.45471329e+02   4.25012543e+02]]
335
[[  72.           17.          213.          310.        ]
 [ 185.          264.          260.          380.        ]
 [  48.96619797    0.          262.87298584  401.41305542]
 ..., 
 [ 184.59356689   28.40639687  311.09951782  142.06956482]
 [  81.56259155  396.48519897  111.2559967   429.09228516]
 [ 251.79006958  375.68136597  274.21652222  400.11001587]]
335
[[ 142.          141.          215.          291.        ]
 [ 207.          183.          255.          361.        ]
 [ 180.          431.          218.          497.        ]
 ..., 
 [ 109.08348846  442.91360474  147.7381134   478.15505981]
 [ 304.34234619   74.58918762  322.23223877   91.85900879]
 [ 142.03738403  109.2410202   158.51182556  132.6995697 ]]
500
[[  54.          217.          472.          451.        ]
 [  12.7059164     0.          303.9569397   499.16665649]
 [ 134.59230042   72.99819946  411.48077393  499.16665649]
 ..., 
 [ 267.17868042   79.96554565  295.98138428  118.91818237]
 [   0.          349.03399658   69.03965759  499.16665649]
 [   2.28741312  198.23832703   50.06999207  253.46022034]]
500
[[ 319.          200.          478.          235.        ]
 [ 372.          156.          426.          199.        ]
 [ 369.18792725  159.52026367  422.34915161  201.13037109]
 ..., 
 [  35.95106888  284.92269897  261.02813721  374.375     ]
 [   6.56233883  235.08477783   34.67103958  278.73648071]
 [  19.54858208   21.04504013  136.06741333   57.37850952]]
410
[[ 143.           15.          348.          391.        ]
 [ 275.          446.          310.          475.        ]
 [ 220.          440.          262.          477.        ]
 ..., 
 [   0.           98.69094849   70.54718781  402.34326172]
 [ 193.72410583  246.57955933  316.83273315  439.7779541 ]
 [  65.12417603    0.          169.83512878   30.81588936]]
500
[[ 215.          194.          323.          234.        ]
 [ 224.79598999  197.37184143  385.03817749  233.30213928]
 [ 150.98114014  168.02496338  411.37408447  252.31118774]
 ..., 
 [  57.09678268  353.949646    134.1043396   374.375     ]
 [  63.86018753  289.67501831  241.16731262  374.375     ]
 [ 241.84558105  285.82598877  262.53222656  301.3374939 ]]
375
[[  93.          116.          294.          371.        ]
 [ 110.69419861  141.1448822   297.90023804  499.375     ]
 [  55.25320816   32.37657547  314.13653564  499.375     ]
 ..., 
 [  60.04430771  363.28726196  160.96330261  415.25259399]
 [  97.81815338  188.24885559  121.44230652  216.56268311]
 [   0.          409.92443848   68.37644958  462.08377075]]
500
[[  83.          117.          206.          334.        ]
 [ 166.          199.          366.          347.        ]
 [ 204.          126.          246.          194.        ]
 ..., 
 [  29.38494682  190.57063293   59.17829514  220.0479126 ]
 [ 159.52835083  204.52670288  258.54986572  276.23632812]
 [ 306.21551514   46.24817657  348.86764526  125.92021179]]
500
[[ 119.           15.          400.          334.        ]
 [ 399.          115.          485.          270.        ]
 [ 405.           35.          436.          101.        ]
 ..., 
 [ 201.16105652  183.20620728  323.27529907  315.17172241]
 [ 399.33673096  256.28488159  415.06756592  273.56881714]
 [ 123.51661682  167.21029663  142.63015747  187.04016113]]
333
[[  64.          160.          149.          278.        ]
 [  95.          279.          157.          344.        ]
 [ 281.          277.          307.          349.        ]
 ..., 
 [  72.48374939  116.03276825  127.05738068  161.82717896]
 [  82.97101593  378.96359253  101.25177002  400.1854248 ]
 [  76.7292099   203.93881226   96.49539185  223.39283752]]
231
[[  30.           19.          177.          489.        ]
 [   0.            0.          178.401474    499.5       ]
 [  19.45514679   15.44407654  210.33908081  467.93756104]
 ..., 
 [  15.65158081   40.38243103   54.93582916  121.87391663]
 [ 187.02859497  381.03735352  215.10961914  401.54644775]
 [  13.09967613  213.87872314   34.71721649  241.30200195]]
500
[[ 132.           55.          343.          283.        ]
 [ 123.51251984    0.          328.49679565  332.44500732]
 [  68.85116577    2.74585104  280.13729858  332.44500732]
 ..., 
 [ 238.68125916   82.07258606  353.79830933  141.93768311]
 [  30.49459839  144.06182861  145.51245117  262.52404785]
 [ 189.92776489  291.43392944  213.76339722  312.24386597]]
333
[[ 115.          165.          204.          373.        ]
 [  65.25202179    0.          266.10263062  499.5       ]
 [ 168.82524109    0.          332.44500732  499.5       ]
 ..., 
 [ 316.44900513   54.83805084  332.44500732  106.03813934]
 [ 261.09689331  193.18527222  280.97320557  209.93852234]
 [ 181.4513092   149.62863159  212.79747009  202.01126099]]
375
[[  89.            1.          294.          463.        ]
 [  92.98016357    0.          357.12219238  499.375     ]
 [  56.88795853   83.94054413  337.28656006  416.27182007]
 ..., 
 [  50.41664505  167.70074463   91.41088104  189.07197571]
 [   8.47483158  337.33355713   72.07723999  355.11715698]
 [ 218.15036011    0.          300.36712646   77.48708344]]
500
[[ 161.          119.          407.          288.        ]
 [ 159.12992859   38.25780869  430.77346802  374.375     ]
 [ 250.40400696    0.          458.41220093  374.375     ]
 ..., 
 [  87.83630371  278.3722229   104.98418427  302.19515991]
 [ 289.05566406    0.          327.55032349   24.40973663]
 [ 119.87372589  120.03526306  142.87471008  138.46244812]]
500
[[   1.            5.          206.          351.        ]
 [ 208.            2.          498.          170.        ]
 [ 211.          179.          360.          352.        ]
 ..., 
 [ 178.71418762   80.58516693  235.44372559  145.11312866]
 [  71.88196564   50.60508728   95.27347565   91.4222641 ]
 [  99.07328796  283.91903687  161.05673218  352.41168213]]
356
[[   0.           23.          333.          491.        ]
 [   0.            0.          355.40667725  499.58666992]
 [   0.           69.25598907  259.6769104   499.58666992]
 ..., 
 [ 184.6855011   446.25054932  295.95968628  487.59014893]
 [  18.67497253  120.25193787  128.48158264  203.77734375]
 [ 279.05923462   64.03461456  301.32748413   86.25807953]]
416
[[  49.            0.          173.          476.        ]
 [ 252.            0.          404.          498.        ]
 [  28.65196609  142.02163696  177.1182251   499.19998169]
 ..., 
 [ 120.24494171  234.76298523  182.43624878  338.03173828]
 [   0.          414.90396118   36.97580719  445.24551392]
 [ 322.28356934   34.12472534  367.09747314   77.51345062]]
500
[[  80.           52.          434.          299.        ]
 [  33.50877762    0.          252.68511963  332.44500732]
 [  73.09938049    0.          213.6072998   332.44500732]
 ..., 
 [ 455.73803711  267.94851685  480.5065918   292.60223389]
 [   0.          301.32669067   77.75428772  332.44500732]
 [  70.66441345  226.24940491   92.89696503  261.25473022]]
500
[[   0.            0.          499.          333.        ]
 [   0.            0.          268.39120483  333.44332886]
 [   0.           45.47698975  176.95013428  333.44332886]
 ..., 
 [  76.54244995  104.25794983   97.63733673  130.23484802]
 [ 300.91140747  154.60475159  337.61804199  202.1410675 ]
 [ 375.55349731  256.34564209  458.99737549  295.70935059]]
433
[[ 195.          394.          288.          449.        ]
 [ 165.          394.          194.          430.        ]
 [ 119.          384.          182.          437.        ]
 ..., 
 [ 128.70820618    0.          233.60238647  317.63745117]
 [ 225.08589172  479.91940308  295.53042603  499.39334106]
 [  31.35613441  356.02816772  120.76158142  402.95022583]]
500
[[ 197.          245.          260.          371.        ]
 [ 228.          269.          247.          305.        ]
 [ 191.          208.          244.          268.        ]
 ..., 
 [ 113.99868774  277.26123047  133.56298828  292.74407959]
 [ 172.33999634   78.43669891  188.63499451   97.81649017]
 [ 238.41233826  105.60868835  271.45681763  141.81568909]]
500
[[ 231.          186.          250.          216.        ]
 [  57.          218.           80.          241.        ]
 [ 342.           79.          359.          104.        ]
 ..., 
 [  11.20282745  209.33908081   45.14302063  250.33654785]
 [ 204.05555725  114.89531708  263.35952759  173.03807068]
 [ 274.11932373   34.69517517  387.90505981   76.65171051]]
500
[[ 108.           88.          409.          327.        ]
 [  70.14672089  103.34966278  367.81863403  390.34832764]
 [  11.38960934  154.56033325  272.4536438   390.34832764]
 ..., 
 [ 476.31674194  105.39742279  499.17666626  156.55392456]
 [  53.24663925  197.07754517  172.82131958  241.75410461]
 [ 220.96417236  151.42593384  284.03652954  201.54499817]]
500
[[  56.           61.          430.          371.        ]
 [  90.83145142    0.          330.84631348  374.375     ]
 [ 160.87524414    0.          396.54284668  374.375     ]
 ..., 
 [ 336.88894653   36.94218445  363.86703491   64.31816101]
 [ 446.64459229   44.22104263  462.23831177   66.55786896]
 [  65.76894379  354.18234253   82.85926819  373.29046631]]
500
[[  36.          226.          399.          274.        ]
 [ 139.           88.          392.          151.        ]
 [  94.19320679   90.48088074  392.47055054  141.99066162]
 ..., 
 [ 143.34829712  171.79473877  166.94194031  191.83299255]
 [ 144.79707336  216.82714844  214.27682495  251.35153198]
 [  79.64682007    0.          195.07098389  291.45037842]]
500
[[ 124.            1.          443.          338.        ]
 [  35.81475449    0.          315.04443359  340.43167114]
 [  75.7287674    57.18564606  274.787323    340.43167114]
 ..., 
 [ 243.02897644  237.19921875  355.52893066  288.90765381]
 [ 251.77250671    0.          353.13778687  128.83921814]
 [  98.64970398  290.6244812   180.91798401  329.90087891]]
500
[[  2.90000000e+02   1.66000000e+02   4.16000000e+02   2.87000000e+02]
 [  2.59540833e+02   3.70353889e+01   3.53972443e+02   3.74375000e+02]
 [  2.40411987e+02   4.03556824e-01   4.00582031e+02   3.74375000e+02]
 ..., 
 [  1.29033951e+02   5.25424480e+00   2.21574844e+02   5.75594864e+01]
 [  1.88082260e+02   3.45982971e+02   3.27086060e+02   3.74375000e+02]
 [  3.55665802e+02   1.78755325e+02   4.24128418e+02   2.60126770e+02]]
500
[[   0.            6.          216.          247.        ]
 [ 263.           57.          335.          194.        ]
 [ 334.           10.          494.          234.        ]
 ..., 
 [ 145.7938385    76.96096802  228.24687195  182.86209106]
 [  44.48503876   75.94711304  291.38473511  126.71020508]
 [  86.31528473  104.54705811  177.30435181  168.42834473]]
390
[[   6.            1.          126.          230.        ]
 [ 145.            1.          247.          229.        ]
 [ 284.            0.          335.          220.        ]
 ..., 
 [ 129.06887817    0.          220.41113281  186.51477051]
 [ 158.14465332  366.58843994  229.29330444  464.57537842]
 [ 111.3192215    75.53567505  389.3500061   180.99101257]]
500
[[  60.            0.          335.          315.        ]
 [   0.            0.           62.          374.        ]
 [ 335.            1.          465.          373.        ]
 ..., 
 [ 146.57084656   21.99751854  277.28012085  111.72057343]
 [ 463.2722168   132.20399475  481.19891357  156.89131165]
 [   3.552351    109.87081146  115.29467773  189.28727722]]
500
[[ 101.          109.          334.          295.        ]
 [ 320.           91.          497.          248.        ]
 [  33.8911438     0.62503815  341.36169434  374.375     ]
 ..., 
 [ 119.70381927  347.76565552  140.11631775  364.08972168]
 [   0.          142.29159546  166.72015381  340.81118774]
 [ 189.18844604  144.49209595  228.60040283  185.30397034]]
500
[[ 345.           96.          374.          131.        ]
 [ 344.23178101  112.44473267  371.77740479  144.08721924]
 [ 346.66647339   88.18552399  371.82495117  119.686203  ]
 ..., 
 [ 438.47360229   81.38996887  455.85464478   97.44721985]
 [ 151.22001648  141.83320618  191.16601562  173.66815186]
 [  15.12898636   88.31969452  126.04774475  197.39315796]]
500
[[  30.           57.          266.          138.        ]
 [  54.          207.          334.          261.        ]
 [ 332.          177.          446.          403.        ]
 ..., 
 [ 158.06292725  310.42721558  181.5927887   329.13705444]
 [ 389.84646606  250.71600342  492.55007935  300.25512695]
 [ 359.97802734  404.23120117  398.77801514  441.59088135]]
500
[[  14.           38.          334.          450.        ]
 [ 334.           48.          497.          440.        ]
 [   0.           33.00241089  366.234375    455.23999023]
 ..., 
 [   0.          286.09683228   26.97995377  324.19110107]
 [ 301.33596802   65.86514282  358.33380127   89.01258087]
 [ 283.94863892  335.03375244  347.15914917  428.82073975]]
375
[[   0.           35.          334.          497.        ]
 [ 334.           96.          373.          126.        ]
 [ 163.60742188  152.46179199  374.375       499.375     ]
 ..., 
 [ 346.01321411  414.32330322  363.16973877  446.67202759]
 [ 225.85418701  112.48635864  319.73190308  272.33535767]
 [   0.          256.30697632   41.22478485  346.96813965]]
363
[[   4.            1.          334.          493.        ]
 [ 335.           19.          357.          122.        ]
 [  43.74380112    0.          335.24969482  375.00375366]
 ..., 
 [ 266.69284058  163.2923584   362.39498901  271.57144165]
 [ 284.81570435  318.46865845  313.43652344  346.49923706]
 [   0.          138.50344849   53.37345505  199.36631775]]
500
[[  14.           59.          334.          319.        ]
 [ 334.          192.          348.          242.        ]
 [ 364.          238.          449.          314.        ]
 ..., 
 [ 395.81716919    0.          464.29568481  158.16481018]
 [ 256.47445679  170.94299316  295.26754761  293.84133911]
 [ 389.36349487   90.00257111  424.63586426  135.3707428 ]]
500
[[  22.            0.          334.          273.        ]
 [ 320.           96.          457.          358.        ]
 [ 192.58781433    0.          461.10650635  365.39001465]
 ..., 
 [ 240.56614685  309.12948608  256.49612427  326.73278809]
 [  80.00502014  281.78384399   99.11260223  304.94696045]
 [ 418.94039917  174.8447113   437.7492981   215.13285828]]
500
[[   3.           19.          121.          324.        ]
 [ 121.            7.          263.          328.        ]
 [ 263.            6.          402.          310.        ]
 ..., 
 [ 166.75540161    0.          310.14331055   54.78735352]
 [ 137.51626587  246.97988892  203.78436279  300.51217651]
 [ 401.81674194  110.78746033  434.20510864  139.668396  ]]
374
[[ 103.          109.          299.          310.        ]
 [ 243.           30.          350.          142.        ]
 [ 284.          185.          313.          264.        ]
 ..., 
 [ 313.42169189   88.77236176  363.47454834  174.80575562]
 [ 347.51672363  393.49575806  363.65963745  411.73898315]
 [ 155.39479065  108.05554199  218.18811035  184.46733093]]
500
[[   0.            0.          240.          331.        ]
 [ 260.            0.          313.          101.        ]
 [ 279.            0.          440.          166.        ]
 ..., 
 [ 303.62020874   41.46430206  335.62258911   72.36332703]
 [  33.61151123   55.93932724  150.45529175  131.97175598]
 [ 309.14364624  254.60964966  424.40951538  313.9670105 ]]
500
[[ 230.          149.          394.          278.        ]
 [ 194.01567078   20.84173775  472.03820801  338.43499756]
 [ 153.75187683   85.07044983  409.92092896  338.43499756]
 ..., 
 [ 233.90000916   62.33633804  344.38238525  189.0813446 ]
 [ 283.27874756   12.94639015  315.78009033   28.81804657]
 [ 116.92193604  148.71328735  136.30055237  168.85006714]]
500
[[ 285.           96.          413.          292.        ]
 [ 237.61965942    0.          441.82473755  333.44332886]
 [ 205.58557129   49.84127045  466.69107056  333.44332886]
 ..., 
 [ 324.23736572  316.44891357  344.64300537  333.44332886]
 [ 181.17774963   56.82650375  269.83325195   87.8346405 ]
 [ 370.27471924  180.10951233  476.99957275  232.70819092]]
500
[[ 151.          159.          226.          215.        ]
 [ 284.          145.WARNING: Logging before InitGoogleLogging() is written to STDERR
I1026 01:04:47.531169 17254 solver.cpp:48] Initializing solver from parameters: 
train_net: "models/pascal_voc/ZF/faster_rcnn_alt_opt/stage2_fast_rcnn_train.pt"
base_lr: 0.001
display: 20
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 30000
snapshot: 0
snapshot_prefix: "zf_fast_rcnn"
average_loss: 100
I1026 01:04:47.531205 17254 solver.cpp:81] Creating training net from train_net file: models/pascal_voc/ZF/faster_rcnn_alt_opt/stage2_fast_rcnn_train.pt
I1026 01:04:47.538553 17254 net.cpp:49] Initializing net from parameters: 
name: "ZF"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Python"
  top: "data"
  top: "rois"
  top: "labels"
  top: "bbox_targets"
  top: "bbox_inside_weights"
  top: "bbox_outside_weights"
  python_param {
    module: "roi_data_layer.layer"
    layer: "RoIDataLayer"
    param_str: "\'num_classes\': 2"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 3
    kernel_size: 7
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
    engine: CAFFE
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    stride: 2
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
    engine: CAFFE
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "roi_pool_conv5"
  type: "ROIPooling"
  bottom: "conv5"
  bottom: "rois"
  top: "roi_pool_conv5"
  roi_pooling_param {
    pooled_h: 6
    pooled_w: 6
    spatial_scale: 0.0625
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "roi_pool_conv5"
  top: "fc6"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
    scale_train: false
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
    scale_train: false
  }
}
layer {
  name: "cls_score"
  type: "InnerProduct"
  bottom: "fc7"
  top: "cls_score"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bbox_pred"
  type: "InnerProduct"
  bottom: "fc7"
  top: "bbox_pred"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 8
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss_cls"
  type: "SoftmaxWithLoss"
  bottom: "cls_score"
  bottom: "labels"
  top: "cls_loss"
  loss_weight: 1
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: -1
    normalize: true
  }
}
layer {
  name: "loss_bbox"
  type: "SmoothL1Loss"
  bottom: "bbox_pred"
  bottom: "bbox_targets"
  bottom: "bbox_inside_weights"
  bottom: "bbox_outside_weights"
  top: "bbox_loss"
  loss_weight: 1
}
layer {
  name: "rpn_conv1"
  type: "Convolution"
  bottom: "conv5"
  top: "rpn_conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_relu1"
  type: "ReLU"
  bottom: "rpn_conv1"
  top: "rpn_conv1"
}
layer {
  name: "rpn_cls_score"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_cls_score"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 18
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_bbox_pred"
  type: "Convolution"
  bottom: "rpn_conv1"
  top: "rpn_bbox_pred"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 36
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "silence_rpn_cls_score"
  type: "Silence"
  bottom: "rpn_cls_score"
}
layer {
  name: "silence_rpn_bbox_pred"
  type: "Silence"
  bottom: "rpn_bbox_pred"
}
I1026 01:04:47.538661 17254 layer_factory.hpp:77] Creating layer data
I1026 01:04:47.539052 17254 net.cpp:106] Creating Layer data
I1026 01:04:47.539062 17254 net.cpp:411] data -> data
I1026 01:04:47.539069 17254 net.cpp:411] data -> rois
I1026 01:04:47.539073 17254 net.cpp:411] data -> labels
I1026 01:04:47.539077 17254 net.cpp:411] data -> bbox_targets
I1026 01:04:47.539095 17254 net.cpp:411] data -> bbox_inside_weights
I1026 01:04:47.539099 17254 net.cpp:411] data -> bbox_outside_weights
I1026 01:04:47.545655 17254 net.cpp:150] Setting up data
I1026 01:04:47.545676 17254 net.cpp:157] Top shape: 2 3 600 1000 (3600000)
I1026 01:04:47.545680 17254 net.cpp:157] Top shape: 1 5 (5)
I1026 01:04:47.545681 17254 net.cpp:157] Top shape: 1 (1)
I1026 01:04:47.545683 17254 net.cpp:157] Top shape: 1 8 (8)
I1026 01:04:47.545686 17254 net.cpp:157] Top shape: 1 8 (8)
I1026 01:04:47.545687 17254 net.cpp:157] Top shape: 1 8 (8)
I1026 01:04:47.545689 17254 net.cpp:165] Memory required for data: 14400120
I1026 01:04:47.545693 17254 layer_factory.hpp:77] Creating layer conv1
I1026 01:04:47.545707 17254 net.cpp:106] Creating Layer conv1
I1026 01:04:47.545711 17254 net.cpp:454] conv1 <- data
I1026 01:04:47.545717 17254 net.cpp:411] conv1 -> conv1
I1026 01:04:47.649667 17254 net.cpp:150] Setting up conv1
I1026 01:04:47.649689 17254 net.cpp:157] Top shape: 2 96 300 500 (28800000)
I1026 01:04:47.649691 17254 net.cpp:165] Memory required for data: 129600120
I1026 01:04:47.649703 17254 layer_factory.hpp:77] Creating layer relu1
I1026 01:04:47.649713 17254 net.cpp:106] Creating Layer relu1
I1026 01:04:47.649726 17254 net.cpp:454] relu1 <- conv1
I1026 01:04:47.649730 17254 net.cpp:397] relu1 -> conv1 (in-place)
I1026 01:04:47.649946 17254 net.cpp:150] Setting up relu1
I1026 01:04:47.649955 17254 net.cpp:157] Top shape: 2 96 300 500 (28800000)
I1026 01:04:47.649956 17254 net.cpp:165] Memory required for data: 244800120
I1026 01:04:47.649958 17254 layer_factory.hpp:77] Creating layer norm1
I1026 01:04:47.649976 17254 net.cpp:106] Creating Layer norm1
I1026 01:04:47.649979 17254 net.cpp:454] norm1 <- conv1
I1026 01:04:47.649992 17254 net.cpp:411] norm1 -> norm1
I1026 01:04:47.650094 17254 net.cpp:150] Setting up norm1
I1026 01:04:47.650097 17254 net.cpp:157] Top shape: 2 96 300 500 (28800000)
I1026 01:04:47.650099 17254 net.cpp:165] Memory required for data: 360000120
I1026 01:04:47.650101 17254 layer_factory.hpp:77] Creating layer pool1
I1026 01:04:47.650115 17254 net.cpp:106] Creating Layer pool1
I1026 01:04:47.650117 17254 net.cpp:454] pool1 <- norm1
I1026 01:04:47.650120 17254 net.cpp:411] pool1 -> pool1
I1026 01:04:47.650142 17254 net.cpp:150] Setting up pool1
I1026 01:04:47.650156 17254 net.cpp:157] Top shape: 2 96 151 251 (7276992)
I1026 01:04:47.650157 17254 net.cpp:165] Memory required for data: 389108088
I1026 01:04:47.650158 17254 layer_factory.hpp:77] Creating layer conv2
I1026 01:04:47.650174 17254 net.cpp:106] Creating Layer conv2
I1026 01:04:47.650176 17254 net.cpp:454] conv2 <- pool1
I1026 01:04:47.650178 17254 net.cpp:411] conv2 -> conv2
I1026 01:04:47.651594 17254 net.cpp:150] Setting up conv2
I1026 01:04:47.651603 17254 net.cpp:157] Top shape: 2 256 76 126 (4902912)
I1026 01:04:47.651605 17254 net.cpp:165] Memory required for data: 408719736
I1026 01:04:47.651612 17254 layer_factory.hpp:77] Creating layer relu2
I1026 01:04:47.651615 17254 net.cpp:106] Creating Layer relu2
I1026 01:04:47.651618 17254 net.cpp:454] relu2 <- conv2
I1026 01:04:47.651620 17254 net.cpp:397] relu2 -> conv2 (in-place)
I1026 01:04:47.651862 17254 net.cpp:150] Setting up relu2
I1026 01:04:47.651870 17254 net.cpp:157] Top shape: 2 256 76 126 (4902912)
I1026 01:04:47.651883 17254 net.cpp:165] Memory required for data: 428331384
I1026 01:04:47.651885 17254 layer_factory.hpp:77] Creating layer norm2
I1026 01:04:47.651890 17254 net.cpp:106] Creating Layer norm2
I1026 01:04:47.651892 17254 net.cpp:454] norm2 <- conv2
I1026 01:04:47.651904 17254 net.cpp:411] norm2 -> norm2
I1026 01:04:47.651990 17254 net.cpp:150] Setting up norm2
I1026 01:04:47.651994 17254 net.cpp:157] Top shape: 2 256 76 126 (4902912)
I1026 01:04:47.651996 17254 net.cpp:165] Memory required for data: 447943032
I1026 01:04:47.651998 17254 layer_factory.hpp:77] Creating layer pool2
I1026 01:04:47.652014 17254 net.cpp:106] Creating Layer pool2
I1026 01:04:47.652015 17254 net.cpp:454] pool2 <- norm2
I1026 01:04:47.652019 17254 net.cpp:411] pool2 -> pool2
I1026 01:04:47.652050 17254 net.cpp:150] Setting up pool2
I1026 01:04:47.652055 17254 net.cpp:157] Top shape: 2 256 39 64 (1277952)
I1026 01:04:47.652055 17254 net.cpp:165] Memory required for data: 453054840
I1026 01:04:47.652057 17254 layer_factory.hpp:77] Creating layer conv3
I1026 01:04:47.652062 17254 net.cpp:106] Creating Layer conv3
I1026 01:04:47.652063 17254 net.cpp:454] conv3 <- pool2
I1026 01:04:47.652067 17254 net.cpp:411] conv3 -> conv3
I1026 01:04:47.654299 17254 net.cpp:150] Setting up conv3
I1026 01:04:47.654317 17254 net.cpp:157] Top shape: 2 384 39 64 (1916928)
I1026 01:04:47.654320 17254 net.cpp:165] Memory required for data: 460722552
I1026 01:04:47.654328 17254 layer_factory.hpp:77] Creating layer relu3
I1026 01:04:47.654346 17254 net.cpp:106] Creating Layer relu3
I1026 01:04:47.654350 17254 net.cpp:454] relu3 <- conv3
I1026 01:04:47.654355 17254 net.cpp:397] relu3 -> conv3 (in-place)
I1026 01:04:47.654505 17254 net.cpp:150] Setting up relu3
I1026 01:04:47.654511 17254 net.cpp:157] Top shape: 2 384 39 64 (1916928)
I1026 01:04:47.654513 17254 net.cpp:165] Memory required for data: 468390264
I1026 01:04:47.654515 17254 layer_factory.hpp:77] Creating layer conv4
I1026 01:04:47.654532 17254 net.cpp:106] Creating Layer conv4
I1026 01:04:47.654536 17254 net.cpp:454] conv4 <- conv3
I1026 01:04:47.654538 17254 net.cpp:411] conv4 -> conv4
I1026 01:04:47.656824 17254 net.cpp:150] Setting up conv4
I1026 01:04:47.656838 17254 net.cpp:157] Top shape: 2 384 39 64 (1916928)
I1026 01:04:47.656841 17254 net.cpp:165] Memory required for data: 476057976
I1026 01:04:47.656847 17254 layer_factory.hpp:77] Creating layer relu4
I1026 01:04:47.656852 17254 net.cpp:106] Creating Layer relu4
I1026 01:04:47.656853 17254 net.cpp:454] relu4 <- conv4
I1026 01:04:47.656868 17254 net.cpp:397] relu4 -> conv4 (in-place)
I1026 01:04:47.657119 17254 net.cpp:150] Setting up relu4
I1026 01:04:47.657137 17254 net.cpp:157] Top shape: 2 384 39 64 (1916928)
I1026 01:04:47.657141 17254 net.cpp:165] Memory required for data: 483725688
I1026 01:04:47.657155 17254 layer_factory.hpp:77] Creating layer conv5
I1026 01:04:47.657163 17254 net.cpp:106] Creating Layer conv5
I1026 01:04:47.657176 17254 net.cpp:454] conv5 <- conv4
I1026 01:04:47.657183 17254 net.cpp:411] conv5 -> conv5
I1026 01:04:47.658876 17254 net.cpp:150] Setting up conv5
I1026 01:04:47.658887 17254 net.cpp:157] Top shape: 2 256 39 64 (1277952)
I1026 01:04:47.658890 17254 net.cpp:165] Memory required for data: 488837496
I1026 01:04:47.658897 17254 layer_factory.hpp:77] Creating layer relu5
I1026 01:04:47.658901 17254 net.cpp:106] Creating Layer relu5
I1026 01:04:47.658905 17254 net.cpp:454] relu5 <- conv5
I1026 01:04:47.658917 17254 net.cpp:397] relu5 -> conv5 (in-place)
I1026 01:04:47.659121 17254 net.cpp:150] Setting up relu5
I1026 01:04:47.659139 17254 net.cpp:157] Top shape: 2 256 39 64 (1277952)
I1026 01:04:47.659140 17254 net.cpp:165] Memory required for data: 493949304
I1026 01:04:47.659142 17254 layer_factory.hpp:77] Creating layer conv5_relu5_0_split
I1026 01:04:47.659159 17254 net.cpp:106] Creating Layer conv5_relu5_0_split
I1026 01:04:47.659162 17254 net.cpp:454] conv5_relu5_0_split <- conv5
I1026 01:04:47.659165 17254 net.cpp:411] conv5_relu5_0_split -> conv5_relu5_0_split_0
I1026 01:04:47.659170 17254 net.cpp:411] conv5_relu5_0_split -> conv5_relu5_0_split_1
I1026 01:04:47.659202 17254 net.cpp:150] Setting up conv5_relu5_0_split
I1026 01:04:47.659206 17254 net.cpp:157] Top shape: 2 256 39 64 (1277952)
I1026 01:04:47.659210 17254 net.cpp:157] Top shape: 2 256 39 64 (1277952)
I1026 01:04:47.659212 17254 net.cpp:165] Memory required for data: 504172920
I1026 01:04:47.659214 17254 layer_factory.hpp:77] Creating layer roi_pool_conv5
I1026 01:04:47.659224 17254 net.cpp:106] Creating Layer roi_pool_conv5
I1026 01:04:47.659227 17254 net.cpp:454] roi_pool_conv5 <- conv5_relu5_0_split_0
I1026 01:04:47.659229 17254 net.cpp:454] roi_pool_conv5 <- rois
I1026 01:04:47.659232 17254 net.cpp:411] roi_pool_conv5 -> roi_pool_conv5
I1026 01:04:47.659237 17254 roi_pooling_layer.cpp:30] Spatial scale: 0.0625
I1026 01:04:47.659271 17254 net.cpp:150] Setting up roi_pool_conv5
I1026 01:04:47.659276 17254 net.cpp:157] Top shape: 1 256 6 6 (9216)
I1026 01:04:47.659279 17254 net.cpp:165] Memory required for data: 504209784
I1026 01:04:47.659282 17254 layer_factory.hpp:77] Creating layer fc6
I1026 01:04:47.659289 17254 net.cpp:106] Creating Layer fc6
I1026 01:04:47.659293 17254 net.cpp:454] fc6 <- roi_pool_conv5
I1026 01:04:47.659299 17254 net.cpp:411] fc6 -> fc6
I1026 01:04:47.705592 17254 net.cpp:150] Setting up fc6
I1026 01:04:47.705617 17254 net.cpp:157] Top shape: 1 4096 (4096)
I1026 01:04:47.705618 17254 net.cpp:165] Memory required for data: 504226168
I1026 01:04:47.705626 17254 layer_factory.hpp:77] Creating layer relu6
I1026 01:04:47.705634 17254 net.cpp:106] Creating Layer relu6
I1026 01:04:47.705638 17254 net.cpp:454] relu6 <- fc6
I1026 01:04:47.705653 17254 net.cpp:397] relu6 -> fc6 (in-place)
I1026 01:04:47.706044 17254 net.cpp:150] Setting up relu6
I1026 01:04:47.706051 17254 net.cpp:157] Top shape: 1 4096 (4096)
I1026 01:04:47.706053 17254 net.cpp:165] Memory required for data: 504242552
I1026 01:04:47.706054 17254 layer_factory.hpp:77] Creating layer drop6
I1026 01:04:47.706059 17254 net.cpp:106] Creating Layer drop6
I1026 01:04:47.706061 17254 net.cpp:454] drop6 <- fc6
I1026 01:04:47.706066 17254 net.cpp:397] drop6 -> fc6 (in-place)
I1026 01:04:47.706094 17254 net.cpp:150] Setting up drop6
I1026 01:04:47.706099 17254 net.cpp:157] Top shape: 1 4096 (4096)
I1026 01:04:47.706110 17254 net.cpp:165] Memory required for data: 504258936
I1026 01:04:47.706110 17254 layer_factory.hpp:77] Creating layer fc7
I1026 01:04:47.706115 17254 net.cpp:106] Creating Layer fc7
I1026 01:04:47.706128 17254 net.cpp:454] fc7 <- fc6
I1026 01:04:47.706132 17254 net.cpp:411] fc7 -> fc7
I1026 01:04:47.727048 17254 net.cpp:150] Setting up fc7
I1026 01:04:47.727072 17254 net.cpp:157] Top shape: 1 4096 (4096)
I1026 01:04:47.727074 17254 net.cpp:165] Memory required for data: 504275320
I1026 01:04:47.727082 17254 layer_factory.hpp:77] Creating layer relu7
I1026 01:04:47.727090 17254 net.cpp:106] Creating Layer relu7
I1026 01:04:47.727094 17254 net.cpp:454] relu7 <- fc7
I1026 01:04:47.727108 17254 net.cpp:397] relu7 -> fc7 (in-place)
I1026 01:04:47.727313 17254 net.cpp:150] Setting up relu7
I1026 01:04:47.727319 17254 net.cpp:157] Top shape: 1 4096 (4096)
I1026 01:04:47.727320 17254 net.cpp:165] Memory required for data: 504291704
I1026 01:04:47.727322 17254 layer_factory.hpp:77] Creating layer drop7
I1026 01:04:47.727336 17254 net.cpp:106] Creating Layer drop7
I1026 01:04:47.727339 17254 net.cpp:454] drop7 <- fc7
I1026 01:04:47.727344 17254 net.cpp:397] drop7 -> fc7 (in-place)
I1026 01:04:47.727380 17254 net.cpp:150] Setting up drop7
I1026 01:04:47.727385 17254 net.cpp:157] Top shape: 1 4096 (4096)
I1026 01:04:47.727386 17254 net.cpp:165] Memory required for data: 504308088
I1026 01:04:47.727387 17254 layer_factory.hpp:77] Creating layer fc7_drop7_0_split
I1026 01:04:47.727402 17254 net.cpp:106] Creating Layer fc7_drop7_0_split
I1026 01:04:47.727406 17254 net.cpp:454] fc7_drop7_0_split <- fc7
I1026 01:04:47.727417 17254 net.cpp:411] fc7_drop7_0_split -> fc7_drop7_0_split_0
I1026 01:04:47.727422 17254 net.cpp:411] fc7_drop7_0_split -> fc7_drop7_0_split_1
I1026 01:04:47.727484 17254 net.cpp:150] Setting up fc7_drop7_0_split
I1026 01:04:47.727489 17254 net.cpp:157] Top shape: 1 4096 (4096)
I1026 01:04:47.727491 17254 net.cpp:157] Top shape: 1 4096 (4096)
I1026 01:04:47.727493 17254 net.cpp:165] Memory required for data: 504340856
I1026 01:04:47.727494 17254 layer_factory.hpp:77] Creating layer cls_score
I1026 01:04:47.727499 17254 net.cpp:106] Creating Layer cls_score
I1026 01:04:47.727502 17254 net.cpp:454] cls_score <- fc7_drop7_0_split_0
I1026 01:04:47.727506 17254 net.cpp:411] cls_score -> cls_score
I1026 01:04:47.727671 17254 net.cpp:150] Setting up cls_score
I1026 01:04:47.727676 17254 net.cpp:157] Top shape: 1 2 (2)
I1026 01:04:47.727677 17254 net.cpp:165] Memory required for data: 504340864
I1026 01:04:47.727681 17254 layer_factory.hpp:77] Creating layer bbox_pred
I1026 01:04:47.727686 17254 net.cpp:106] Creating Layer bbox_pred
I1026 01:04:47.727689 17254 net.cpp:454] bbox_pred <- fc7_drop7_0_split_1
I1026 01:04:47.727691 17254 net.cpp:411] bbox_pred -> bbox_pred
I1026 01:04:47.728379 17254 net.cpp:150] Setting up bbox_pred
I1026 01:04:47.728385 17254 net.cpp:157] Top shape: 1 8 (8)
I1026 01:04:47.728387 17254 net.cpp:165] Memory required for data: 504340896
I1026 01:04:47.728405 17254 layer_factory.hpp:77] Creating layer loss_cls
I1026 01:04:47.728411 17254 net.cpp:106] Creating Layer loss_cls
I1026 01:04:47.728425 17254 net.cpp:454] loss_cls <- cls_score
I1026 01:04:47.728426 17254 net.cpp:454] loss_cls <- labels
I1026 01:04:47.728430 17254 net.cpp:411] loss_cls -> cls_loss
I1026 01:04:47.728446 17254 layer_factory.hpp:77] Creating layer loss_cls
I1026 01:04:47.728724 17254 net.cpp:150] Setting up loss_cls
I1026 01:04:47.728731 17254 net.cpp:157] Top shape: (1)
I1026 01:04:47.728734 17254 net.cpp:160]     with loss weight 1
I1026 01:04:47.728741 17254 net.cpp:165] Memory required for data: 504340900
I1026 01:04:47.728744 17254 layer_factory.hpp:77] Creating layer loss_bbox
I1026 01:04:47.728749 17254 net.cpp:106] Creating Layer loss_bbox
I1026 01:04:47.728751 17254 net.cpp:454] loss_bbox <- bbox_pred
I1026 01:04:47.728754 17254 net.cpp:454] loss_bbox <- bbox_targets
I1026 01:04:47.728756 17254 net.cpp:454] loss_bbox <- bbox_inside_weights
I1026 01:04:47.728760 17254 net.cpp:454] loss_bbox <- bbox_outside_weights
I1026 01:04:47.728762 17254 net.cpp:411] loss_bbox -> bbox_loss
I1026 01:04:47.728816 17254 net.cpp:150] Setting up loss_bbox
I1026 01:04:47.728819 17254 net.cpp:157] Top shape: (1)
I1026 01:04:47.728821 17254 net.cpp:160]     with loss weight 1
I1026 01:04:47.728823 17254 net.cpp:165] Memory required for data: 504340904
I1026 01:04:47.728826 17254 layer_factory.hpp:77] Creating layer rpn_conv1
I1026 01:04:47.728832 17254 net.cpp:106] Creating Layer rpn_conv1
I1026 01:04:47.728835 17254 net.cpp:454] rpn_conv1 <- conv5_relu5_0_split_1
I1026 01:04:47.728840 17254 net.cpp:411] rpn_conv1 -> rpn_conv1
I1026 01:04:47.734272 17254 net.cpp:150] Setting up rpn_conv1
I1026 01:04:47.734280 17254 net.cpp:157] Top shape: 2 256 39 64 (1277952)
I1026 01:04:47.734293 17254 net.cpp:165] Memory required for data: 509452712
I1026 01:04:47.734297 17254 layer_factory.hpp:77] Creating layer rpn_relu1
I1026 01:04:47.734302 17254 net.cpp:106] Creating Layer rpn_relu1
I1026 01:04:47.734303 17254 net.cpp:454] rpn_relu1 <- rpn_conv1
I1026 01:04:47.734318 17254 net.cpp:397] rpn_relu1 -> rpn_conv1 (in-place)
I1026 01:04:47.734542 17254 net.cpp:150] Setting up rpn_relu1
I1026 01:04:47.734549 17254 net.cpp:157] Top shape: 2 256 39 64 (1277952)
I1026 01:04:47.734561 17254 net.cpp:165] Memory required for data: 514564520
I1026 01:04:47.734565 17254 layer_factory.hpp:77] Creating layer rpn_conv1_rpn_relu1_0_split
I1026 01:04:47.734568 17254 net.cpp:106] Creating Layer rpn_conv1_rpn_relu1_0_split
I1026 01:04:47.734570 17254 net.cpp:454] rpn_conv1_rpn_relu1_0_split <- rpn_conv1
I1026 01:04:47.734575 17254 net.cpp:411] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_0
I1026 01:04:47.734580 17254 net.cpp:411] rpn_conv1_rpn_relu1_0_split -> rpn_conv1_rpn_relu1_0_split_1
I1026 01:04:47.734618 17254 net.cpp:150] Setting up rpn_conv1_rpn_relu1_0_split
I1026 01:04:47.734622 17254 net.cpp:157] Top shape: 2 256 39 64 (1277952)
I1026 01:04:47.734625 17254 net.cpp:157] Top shape: 2 256 39 64 (1277952)
I1026 01:04:47.734627 17254 net.cpp:165] Memory required for data: 524788136
I1026 01:04:47.734628 17254 layer_factory.hpp:77] Creating layer rpn_cls_score
I1026 01:04:47.734645 17254 net.cpp:106] Creating Layer rpn_cls_score
I1026 01:04:47.734659 17254 net.cpp:454] rpn_cls_score <- rpn_conv1_rpn_relu1_0_split_0
I1026 01:04:47.734663 17254 net.cpp:411] rpn_cls_score -> rpn_cls_score
I1026 01:04:47.735462 17254 net.cpp:150] Setting up rpn_cls_score
I1026 01:04:47.735471 17254 net.cpp:157] Top shape: 2 18 39 64 (89856)
I1026 01:04:47.735472 17254 net.cpp:165] Memory required for data: 525147560
I1026 01:04:47.735476 17254 layer_factory.hpp:77] Creating layer rpn_bbox_pred
I1026 01:04:47.735486 17254 net.cpp:106] Creating Layer rpn_bbox_pred
I1026 01:04:47.735489 17254 net.cpp:454] rpn_bbox_pred <- rpn_conv1_rpn_relu1_0_split_1
I1026 01:04:47.735503 17254 net.cpp:411] rpn_bbox_pred -> rpn_bbox_pred
I1026 01:04:47.736277 17254 net.cpp:150] Setting up rpn_bbox_pred
I1026 01:04:47.736284 17254 net.cpp:157] Top shape: 2 36 39 64 (179712)
I1026 01:04:47.736286 17254 net.cpp:165] Memory required for data: 525866408
I1026 01:04:47.736290 17254 layer_factory.hpp:77] Creating layer silence_rpn_cls_score
I1026 01:04:47.736294 17254 net.cpp:106] Creating Layer silence_rpn_cls_score
I1026 01:04:47.736295 17254 net.cpp:454] silence_rpn_cls_score <- rpn_cls_score
I1026 01:04:47.736299 17254 net.cpp:150] Setting up silence_rpn_cls_score
I1026 01:04:47.736301 17254 net.cpp:165] Memory required for data: 525866408
I1026 01:04:47.736302 17254 layer_factory.hpp:77] Creating layer silence_rpn_bbox_pred
I1026 01:04:47.736315 17254 net.cpp:106] Creating Layer silence_rpn_bbox_pred
I1026 01:04:47.736316 17254 net.cpp:454] silence_rpn_bbox_pred <- rpn_bbox_pred
I1026 01:04:47.736318 17254 net.cpp:150] Setting up silence_rpn_bbox_pred
I1026 01:04:47.736320 17254 net.cpp:165] Memory required for data: 525866408
I1026 01:04:47.736321 17254 net.cpp:228] silence_rpn_bbox_pred does not need backward computation.
I1026 01:04:47.736333 17254 net.cpp:228] silence_rpn_cls_score does not need backward computation.
I1026 01:04:47.736335 17254 net.cpp:228] rpn_bbox_pred does not need backward computation.
I1026 01:04:47.736337 17254 net.cpp:228] rpn_cls_score does not need backward computation.
I1026 01:04:47.736338 17254 net.cpp:228] rpn_conv1_rpn_relu1_0_split does not need backward computation.
I1026 01:04:47.736340 17254 net.cpp:228] rpn_relu1 does not need backward computation.
I1026 01:04:47.736342 17254 net.cpp:228] rpn_conv1 does not need backward computation.
I1026 01:04:47.736344 17254 net.cpp:226] loss_bbox needs backward computation.
I1026 01:04:47.736347 17254 net.cpp:226] loss_cls needs backward computation.
I1026 01:04:47.736351 17254 net.cpp:226] bbox_pred needs backward computation.
I1026 01:04:47.736361 17254 net.cpp:226] cls_score needs backward computation.
I1026 01:04:47.736363 17254 net.cpp:226] fc7_drop7_0_split needs backward computation.
I1026 01:04:47.736366 17254 net.cpp:226] drop7 needs backward computation.
I1026 01:04:47.736367 17254 net.cpp:226] relu7 needs backward computation.
I1026 01:04:47.736368 17254 net.cpp:226] fc7 needs backward computation.
I1026 01:04:47.736371 17254 net.cpp:226] drop6 needs backward computation.
I1026 01:04:47.736372 17254 net.cpp:226] relu6 needs backward computation.
I1026 01:04:47.736374 17254 net.cpp:226] fc6 needs backward computation.
I1026 01:04:47.736377 17254 net.cpp:228] roi_pool_conv5 does not need backward computation.
I1026 01:04:47.736379 17254 net.cpp:228] conv5_relu5_0_split does not need backward computation.
I1026 01:04:47.736382 17254 net.cpp:228] relu5 does not need backward computation.
I1026 01:04:47.736398 17254 net.cpp:228] conv5 does not need backward computation.
I1026 01:04:47.736400 17254 net.cpp:228] relu4 does not need backward computation.
I1026 01:04:47.736402 17254 net.cpp:228] conv4 does not need backward computation.
I1026 01:04:47.736404 17254 net.cpp:228] relu3 does not need backward computation.
I1026 01:04:47.736407 17254 net.cpp:228] conv3 does not need backward computation.
I1026 01:04:47.736418 17254 net.cpp:228] pool2 does not need backward computation.
I1026 01:04:47.736420 17254 net.cpp:228] norm2 does not need backward computation.
I1026 01:04:47.736423 17254 net.cpp:228] relu2 does not need backward computation.
I1026 01:04:47.736424 17254 net.cpp:228] conv2 does not need backward computation.
I1026 01:04:47.736436 17254 net.cpp:228] pool1 does not need backward computation.
I1026 01:04:47.736439 17254 net.cpp:228] norm1 does not need backward computation.
I1026 01:04:47.736441 17254 net.cpp:228] relu1 does not need backward computation.
I1026 01:04:47.736443 17254 net.cpp:228] conv1 does not need backward computation.
I1026 01:04:47.736446 17254 net.cpp:228] data does not need backward computation.
I1026 01:04:47.736448 17254 net.cpp:270] This network produces output bbox_loss
I1026 01:04:47.736449 17254 net.cpp:270] This network produces output cls_loss
I1026 01:04:47.736476 17254 net.cpp:283] Network initialization done.
I1026 01:04:47.736560 17254 solver.cpp:60] Solver scaffolding done.
I1026 01:04:47.807054 17254 net.cpp:816] Ignoring source layer input-data
I1026 01:04:47.807075 17254 net.cpp:816] Ignoring source layer data_input-data_0_split
I1026 01:04:47.809680 17254 net.cpp:816] Ignoring source layer rpn_cls_score_rpn_cls_score_0_split
I1026 01:04:47.809715 17254 net.cpp:816] Ignoring source layer rpn_cls_score_reshape
I1026 01:04:47.809717 17254 net.cpp:816] Ignoring source layer rpn-data
I1026 01:04:47.809718 17254 net.cpp:816] Ignoring source layer rpn_loss_cls
I1026 01:04:47.809720 17254 net.cpp:816] Ignoring source layer rpn_loss_bbox
I1026 01:04:47.809721 17254 net.cpp:816] Ignoring source layer dummy_roi_pool_conv5
I1026 01:04:47.841811 17254 net.cpp:816] Ignoring source layer silence_fc7
/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/roi_data_layer/minibatch.py:100: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future
  fg_inds, size=fg_rois_per_this_image, replace=False)
/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/roi_data_layer/minibatch.py:113: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future
  bg_inds, size=bg_rois_per_this_image, replace=False)
/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/roi_data_layer/minibatch.py:120: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future
  labels[fg_rois_per_this_image:] = 0
/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/roi_data_layer/minibatch.py:176: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future
  bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]
/home/cgangee/code/cg-py-faster-rcnn/tools/../lib/roi_data_layer/minibatch.py:177: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future
  bbox_inside_weights[ind, start:end] = cfg.TRAIN.BBOX_INSIDE_WEIGHTS
I1026 01:04:47.953470 17254 solver.cpp:229] Iteration 0, loss = 0.474301
I1026 01:04:47.953510 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.330981 (* 1 = 0.330981 loss)
I1026 01:04:47.953516 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.14332 (* 1 = 0.14332 loss)
I1026 01:04:47.953523 17254 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I1026 01:04:49.033694 17254 solver.cpp:229] Iteration 20, loss = 0.528692
I1026 01:04:49.033726 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.393329 (* 1 = 0.393329 loss)
I1026 01:04:49.033731 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.135363 (* 1 = 0.135363 loss)
I1026 01:04:49.033736 17254 sgd_solver.cpp:106] Iteration 20, lr = 0.001
I1026 01:04:50.103855 17254 solver.cpp:229] Iteration 40, loss = 0.343014
I1026 01:04:50.103886 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.265029 (* 1 = 0.265029 loss)
I1026 01:04:50.103891 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0779847 (* 1 = 0.0779847 loss)
I1026 01:04:50.103895 17254 sgd_solver.cpp:106] Iteration 40, lr = 0.001
I1026 01:04:51.187930 17254 solver.cpp:229] Iteration 60, loss = 0.139668
I1026 01:04:51.187963 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0898481 (* 1 = 0.0898481 loss)
I1026 01:04:51.187968 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0498202 (* 1 = 0.0498202 loss)
I1026 01:04:51.187973 17254 sgd_solver.cpp:106] Iteration 60, lr = 0.001
I1026 01:04:52.269160 17254 solver.cpp:229] Iteration 80, loss = 0.225755
I1026 01:04:52.269191 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.147368 (* 1 = 0.147368 loss)
I1026 01:04:52.269196 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0783872 (* 1 = 0.0783872 loss)
I1026 01:04:52.269201 17254 sgd_solver.cpp:106] Iteration 80, lr = 0.001
I1026 01:04:53.331576 17254 solver.cpp:229] Iteration 100, loss = 0.227049
I1026 01:04:53.331609 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.123454 (* 1 = 0.123454 loss)
I1026 01:04:53.331614 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.103595 (* 1 = 0.103595 loss)
I1026 01:04:53.331619 17254 sgd_solver.cpp:106] Iteration 100, lr = 0.001
I1026 01:04:54.416179 17254 solver.cpp:229] Iteration 120, loss = 0.198259
I1026 01:04:54.416211 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.149239 (* 1 = 0.149239 loss)
I1026 01:04:54.416218 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0490205 (* 1 = 0.0490205 loss)
I1026 01:04:54.416223 17254 sgd_solver.cpp:106] Iteration 120, lr = 0.001
I1026 01:04:55.497215 17254 solver.cpp:229] Iteration 140, loss = 0.200636
I1026 01:04:55.497246 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.116973 (* 1 = 0.116973 loss)
I1026 01:04:55.497251 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0836628 (* 1 = 0.0836628 loss)
I1026 01:04:55.497256 17254 sgd_solver.cpp:106] Iteration 140, lr = 0.001
I1026 01:04:56.570188 17254 solver.cpp:229] Iteration 160, loss = 0.273476
I1026 01:04:56.570220 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.186279 (* 1 = 0.186279 loss)
I1026 01:04:56.570225 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0871969 (* 1 = 0.0871969 loss)
I1026 01:04:56.570230 17254 sgd_solver.cpp:106] Iteration 160, lr = 0.001
I1026 01:04:57.651739 17254 solver.cpp:229] Iteration 180, loss = 0.14592
I1026 01:04:57.651772 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.109031 (* 1 = 0.109031 loss)
I1026 01:04:57.651777 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0368893 (* 1 = 0.0368893 loss)
I1026 01:04:57.651782 17254 sgd_solver.cpp:106] Iteration 180, lr = 0.001
I1026 01:04:58.715688 17254 solver.cpp:229] Iteration 200, loss = 0.117785
I1026 01:04:58.715720 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0601476 (* 1 = 0.0601476 loss)
I1026 01:04:58.715725 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0576376 (* 1 = 0.0576376 loss)
I1026 01:04:58.715730 17254 sgd_solver.cpp:106] Iteration 200, lr = 0.001
I1026 01:04:59.811509 17254 solver.cpp:229] Iteration 220, loss = 0.102484
I1026 01:04:59.811542 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0508401 (* 1 = 0.0508401 loss)
I1026 01:04:59.811547 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0516439 (* 1 = 0.0516439 loss)
I1026 01:04:59.811552 17254 sgd_solver.cpp:106] Iteration 220, lr = 0.001
I1026 01:05:00.891472 17254 solver.cpp:229] Iteration 240, loss = 0.150685
I1026 01:05:00.891505 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0774901 (* 1 = 0.0774901 loss)
I1026 01:05:00.891510 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0731949 (* 1 = 0.0731949 loss)
I1026 01:05:00.891513 17254 sgd_solver.cpp:106] Iteration 240, lr = 0.001
I1026 01:05:01.967294 17254 solver.cpp:229] Iteration 260, loss = 0.17102
I1026 01:05:01.967334 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.115185 (* 1 = 0.115185 loss)
I1026 01:05:01.967339 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0558344 (* 1 = 0.0558344 loss)
I1026 01:05:01.967344 17254 sgd_solver.cpp:106] Iteration 260, lr = 0.001
I1026 01:05:03.029218 17254 solver.cpp:229] Iteration 280, loss = 0.153669
I1026 01:05:03.029252 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0784572 (* 1 = 0.0784572 loss)
I1026 01:05:03.029256 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0752118 (* 1 = 0.0752118 loss)
I1026 01:05:03.029260 17254 sgd_solver.cpp:106] Iteration 280, lr = 0.001
I1026 01:05:04.104337 17254 solver.cpp:229] Iteration 300, loss = 0.154776
I1026 01:05:04.104368 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0655103 (* 1 = 0.0655103 loss)
I1026 01:05:04.104372 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0892659 (* 1 = 0.0892659 loss)
I1026 01:05:04.104377 17254 sgd_solver.cpp:106] Iteration 300, lr = 0.001
I1026 01:05:05.175909 17254 solver.cpp:229] Iteration 320, loss = 0.111401
I1026 01:05:05.175951 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0675451 (* 1 = 0.0675451 loss)
I1026 01:05:05.175956 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0438559 (* 1 = 0.0438559 loss)
I1026 01:05:05.175959 17254 sgd_solver.cpp:106] Iteration 320, lr = 0.001
I1026 01:05:06.247226 17254 solver.cpp:229] Iteration 340, loss = 0.24122
I1026 01:05:06.247258 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.132729 (* 1 = 0.132729 loss)
I1026 01:05:06.247263 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.10849 (* 1 = 0.10849 loss)
I1026 01:05:06.247267 17254 sgd_solver.cpp:106] Iteration 340, lr = 0.001
I1026 01:05:07.319977 17254 solver.cpp:229] Iteration 360, loss = 0.123301
I1026 01:05:07.320008 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0414474 (* 1 = 0.0414474 loss)
I1026 01:05:07.320013 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.081854 (* 1 = 0.081854 loss)
I1026 01:05:07.320019 17254 sgd_solver.cpp:106] Iteration 360, lr = 0.001
I1026 01:05:08.383913 17254 solver.cpp:229] Iteration 380, loss = 0.266738
I1026 01:05:08.383944 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.156774 (* 1 = 0.156774 loss)
I1026 01:05:08.383949 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.109965 (* 1 = 0.109965 loss)
I1026 01:05:08.383954 17254 sgd_solver.cpp:106] Iteration 380, lr = 0.001
I1026 01:05:09.446020 17254 solver.cpp:229] Iteration 400, loss = 0.198468
I1026 01:05:09.446063 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.117469 (* 1 = 0.117469 loss)
I1026 01:05:09.446068 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.080999 (* 1 = 0.080999 loss)
I1026 01:05:09.446072 17254 sgd_solver.cpp:106] Iteration 400, lr = 0.001
I1026 01:05:10.518471 17254 solver.cpp:229] Iteration 420, loss = 0.145932
I1026 01:05:10.518523 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0511611 (* 1 = 0.0511611 loss)
I1026 01:05:10.518528 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0947712 (* 1 = 0.0947712 loss)
I1026 01:05:10.518532 17254 sgd_solver.cpp:106] Iteration 420, lr = 0.001
I1026 01:05:11.583994 17254 solver.cpp:229] Iteration 440, loss = 0.101897
I1026 01:05:11.584028 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0473377 (* 1 = 0.0473377 loss)
I1026 01:05:11.584033 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0545597 (* 1 = 0.0545597 loss)
I1026 01:05:11.584038 17254 sgd_solver.cpp:106] Iteration 440, lr = 0.001
I1026 01:05:12.635877 17254 solver.cpp:229] Iteration 460, loss = 0.175812
I1026 01:05:12.635906 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0741702 (* 1 = 0.0741702 loss)
I1026 01:05:12.635910 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.101642 (* 1 = 0.101642 loss)
I1026 01:05:12.635915 17254 sgd_solver.cpp:106] Iteration 460, lr = 0.001
I1026 01:05:13.707875 17254 solver.cpp:229] Iteration 480, loss = 0.25376
I1026 01:05:13.707916 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.128506 (* 1 = 0.128506 loss)
I1026 01:05:13.707921 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.125254 (* 1 = 0.125254 loss)
I1026 01:05:13.707934 17254 sgd_solver.cpp:106] Iteration 480, lr = 0.001
I1026 01:05:14.767401 17254 solver.cpp:229] Iteration 500, loss = 0.186292
I1026 01:05:14.767436 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.149635 (* 1 = 0.149635 loss)
I1026 01:05:14.767442 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0366565 (* 1 = 0.0366565 loss)
I1026 01:05:14.767446 17254 sgd_solver.cpp:106] Iteration 500, lr = 0.001
I1026 01:05:15.865053 17254 solver.cpp:229] Iteration 520, loss = 0.131042
I1026 01:05:15.865085 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0942932 (* 1 = 0.0942932 loss)
I1026 01:05:15.865090 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0367491 (* 1 = 0.0367491 loss)
I1026 01:05:15.865094 17254 sgd_solver.cpp:106] Iteration 520, lr = 0.001
I1026 01:05:16.946286 17254 solver.cpp:229] Iteration 540, loss = 0.161688
I1026 01:05:16.946329 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0800863 (* 1 = 0.0800863 loss)
I1026 01:05:16.946336 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0816021 (* 1 = 0.0816021 loss)
I1026 01:05:16.946341 17254 sgd_solver.cpp:106] Iteration 540, lr = 0.001
I1026 01:05:18.049896 17254 solver.cpp:229] Iteration 560, loss = 0.207094
I1026 01:05:18.049931 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.121119 (* 1 = 0.121119 loss)
I1026 01:05:18.049937 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0859746 (* 1 = 0.0859746 loss)
I1026 01:05:18.049944 17254 sgd_solver.cpp:106] Iteration 560, lr = 0.001
I1026 01:05:19.180186 17254 solver.cpp:229] Iteration 580, loss = 0.173842
I1026 01:05:19.180219 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.129199 (* 1 = 0.129199 loss)
I1026 01:05:19.180227 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0446433 (* 1 = 0.0446433 loss)
I1026 01:05:19.180232 17254 sgd_solver.cpp:106] Iteration 580, lr = 0.001
I1026 01:05:20.309190 17254 solver.cpp:229] Iteration 600, loss = 0.220917
I1026 01:05:20.309219 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.141559 (* 1 = 0.141559 loss)
I1026 01:05:20.309224 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0793575 (* 1 = 0.0793575 loss)
I1026 01:05:20.309229 17254 sgd_solver.cpp:106] Iteration 600, lr = 0.001
I1026 01:05:21.426892 17254 solver.cpp:229] Iteration 620, loss = 0.0892075
I1026 01:05:21.426934 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0615148 (* 1 = 0.0615148 loss)
I1026 01:05:21.426939 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0276927 (* 1 = 0.0276927 loss)
I1026 01:05:21.426944 17254 sgd_solver.cpp:106] Iteration 620, lr = 0.001
I1026 01:05:22.548355 17254 solver.cpp:229] Iteration 640, loss = 0.165156
I1026 01:05:22.548388 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.100717 (* 1 = 0.100717 loss)
I1026 01:05:22.548391 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.064439 (* 1 = 0.064439 loss)
I1026 01:05:22.548396 17254 sgd_solver.cpp:106] Iteration 640, lr = 0.001
I1026 01:05:23.652521 17254 solver.cpp:229] Iteration 660, loss = 0.112781
I1026 01:05:23.652555 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0586809 (* 1 = 0.0586809 loss)
I1026 01:05:23.652562 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0541004 (* 1 = 0.0541004 loss)
I1026 01:05:23.652568 17254 sgd_solver.cpp:106] Iteration 660, lr = 0.001
I1026 01:05:24.745682 17254 solver.cpp:229] Iteration 680, loss = 0.196918
I1026 01:05:24.745718 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.108478 (* 1 = 0.108478 loss)
I1026 01:05:24.745726 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0884395 (* 1 = 0.0884395 loss)
I1026 01:05:24.745733 17254 sgd_solver.cpp:106] Iteration 680, lr = 0.001
I1026 01:05:25.878548 17254 solver.cpp:229] Iteration 700, loss = 0.214921
I1026 01:05:25.878582 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.127449 (* 1 = 0.127449 loss)
I1026 01:05:25.878588 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0874725 (* 1 = 0.0874725 loss)
I1026 01:05:25.878594 17254 sgd_solver.cpp:106] Iteration 700, lr = 0.001
I1026 01:05:27.004391 17254 solver.cpp:229] Iteration 720, loss = 0.170963
I1026 01:05:27.004422 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0893411 (* 1 = 0.0893411 loss)
I1026 01:05:27.004427 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0816221 (* 1 = 0.0816221 loss)
I1026 01:05:27.004431 17254 sgd_solver.cpp:106] Iteration 720, lr = 0.001
I1026 01:05:28.120491 17254 solver.cpp:229] Iteration 740, loss = 0.185615
I1026 01:05:28.120523 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.121356 (* 1 = 0.121356 loss)
I1026 01:05:28.120528 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0642589 (* 1 = 0.0642589 loss)
I1026 01:05:28.120532 17254 sgd_solver.cpp:106] Iteration 740, lr = 0.001
I1026 01:05:29.231672 17254 solver.cpp:229] Iteration 760, loss = 0.133896
I1026 01:05:29.231703 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0676113 (* 1 = 0.0676113 loss)
I1026 01:05:29.231709 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0662844 (* 1 = 0.0662844 loss)
I1026 01:05:29.231714 17254 sgd_solver.cpp:106] Iteration 760, lr = 0.001
I1026 01:05:30.352138 17254 solver.cpp:229] Iteration 780, loss = 0.178888
I1026 01:05:30.352170 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0783866 (* 1 = 0.0783866 loss)
I1026 01:05:30.352174 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.100502 (* 1 = 0.100502 loss)
I1026 01:05:30.352180 17254 sgd_solver.cpp:106] Iteration 780, lr = 0.001
I1026 01:05:31.466832 17254 solver.cpp:229] Iteration 800, loss = 0.1524
I1026 01:05:31.466864 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0815545 (* 1 = 0.0815545 loss)
I1026 01:05:31.466868 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.070846 (* 1 = 0.070846 loss)
I1026 01:05:31.466873 17254 sgd_solver.cpp:106] Iteration 800, lr = 0.001
I1026 01:05:32.574062 17254 solver.cpp:229] Iteration 820, loss = 0.188869
I1026 01:05:32.574095 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0971339 (* 1 = 0.0971339 loss)
I1026 01:05:32.574100 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0917347 (* 1 = 0.0917347 loss)
I1026 01:05:32.574105 17254 sgd_solver.cpp:106] Iteration 820, lr = 0.001
I1026 01:05:33.672636 17254 solver.cpp:229] Iteration 840, loss = 0.0831403
I1026 01:05:33.672667 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0585961 (* 1 = 0.0585961 loss)
I1026 01:05:33.672672 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0245442 (* 1 = 0.0245442 loss)
I1026 01:05:33.672677 17254 sgd_solver.cpp:106] Iteration 840, lr = 0.001
I1026 01:05:34.804889 17254 solver.cpp:229] Iteration 860, loss = 0.106283
I1026 01:05:34.804922 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0598606 (* 1 = 0.0598606 loss)
I1026 01:05:34.804927 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0464221 (* 1 = 0.0464221 loss)
I1026 01:05:34.804931 17254 sgd_solver.cpp:106] Iteration 860, lr = 0.001
I1026 01:05:35.929126 17254 solver.cpp:229] Iteration 880, loss = 0.117053
I1026 01:05:35.929172 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0707362 (* 1 = 0.0707362 loss)
I1026 01:05:35.929177 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.046317 (* 1 = 0.046317 loss)
I1026 01:05:35.929183 17254 sgd_solver.cpp:106] Iteration 880, lr = 0.001
I1026 01:05:37.038947 17254 solver.cpp:229] Iteration 900, loss = 0.16743
I1026 01:05:37.038980 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.11291 (* 1 = 0.11291 loss)
I1026 01:05:37.038985 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0545199 (* 1 = 0.0545199 loss)
I1026 01:05:37.038990 17254 sgd_solver.cpp:106] Iteration 900, lr = 0.001
I1026 01:05:38.158610 17254 solver.cpp:229] Iteration 920, loss = 0.218095
I1026 01:05:38.158641 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.111984 (* 1 = 0.111984 loss)
I1026 01:05:38.158645 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.106111 (* 1 = 0.106111 loss)
I1026 01:05:38.158649 17254 sgd_solver.cpp:106] Iteration 920, lr = 0.001
I1026 01:05:39.242169 17254 solver.cpp:229] Iteration 940, loss = 0.131061
I1026 01:05:39.242202 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0703116 (* 1 = 0.0703116 loss)
I1026 01:05:39.242207 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0607493 (* 1 = 0.0607493 loss)
I1026 01:05:39.242211 17254 sgd_solver.cpp:106] Iteration 940, lr = 0.001
I1026 01:05:40.360191 17254 solver.cpp:229] Iteration 960, loss = 0.185406
I1026 01:05:40.360224 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0807081 (* 1 = 0.0807081 loss)
I1026 01:05:40.360229 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.104698 (* 1 = 0.104698 loss)
I1026 01:05:40.360234 17254 sgd_solver.cpp:106] Iteration 960, lr = 0.001
I1026 01:05:41.455132 17254 solver.cpp:229] Iteration 980, loss = 0.206815
I1026 01:05:41.455163 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.101833 (* 1 = 0.101833 loss)
I1026 01:05:41.455168 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.104983 (* 1 = 0.104983 loss)
I1026 01:05:41.455173 17254 sgd_solver.cpp:106] Iteration 980, lr = 0.001
I1026 01:05:42.559816 17254 solver.cpp:229] Iteration 1000, loss = 0.174934
I1026 01:05:42.559850 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0746649 (* 1 = 0.0746649 loss)
I1026 01:05:42.559855 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.100269 (* 1 = 0.100269 loss)
I1026 01:05:42.559861 17254 sgd_solver.cpp:106] Iteration 1000, lr = 0.001
I1026 01:05:43.668709 17254 solver.cpp:229] Iteration 1020, loss = 0.0829269
I1026 01:05:43.668740 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0521717 (* 1 = 0.0521717 loss)
I1026 01:05:43.668745 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0307552 (* 1 = 0.0307552 loss)
I1026 01:05:43.668750 17254 sgd_solver.cpp:106] Iteration 1020, lr = 0.001
I1026 01:05:44.772707 17254 solver.cpp:229] Iteration 1040, loss = 0.146263
I1026 01:05:44.772740 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0768339 (* 1 = 0.0768339 loss)
I1026 01:05:44.772744 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0694295 (* 1 = 0.0694295 loss)
I1026 01:05:44.772748 17254 sgd_solver.cpp:106] Iteration 1040, lr = 0.001
I1026 01:05:45.892066 17254 solver.cpp:229] Iteration 1060, loss = 0.191003
I1026 01:05:45.892098 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.110441 (* 1 = 0.110441 loss)
I1026 01:05:45.892103 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0805619 (* 1 = 0.0805619 loss)
I1026 01:05:45.892109 17254 sgd_solver.cpp:106] Iteration 1060, lr = 0.001
I1026 01:05:46.987236 17254 solver.cpp:229] Iteration 1080, loss = 0.204532
I1026 01:05:46.987267 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0866042 (* 1 = 0.0866042 loss)
I1026 01:05:46.987272 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.117928 (* 1 = 0.117928 loss)
I1026 01:05:46.987277 17254 sgd_solver.cpp:106] Iteration 1080, lr = 0.001
I1026 01:05:48.101398 17254 solver.cpp:229] Iteration 1100, loss = 0.184754
I1026 01:05:48.101430 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0946641 (* 1 = 0.0946641 loss)
I1026 01:05:48.101434 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.09009 (* 1 = 0.09009 loss)
I1026 01:05:48.101439 17254 sgd_solver.cpp:106] Iteration 1100, lr = 0.001
I1026 01:05:49.203796 17254 solver.cpp:229] Iteration 1120, loss = 0.0799138
I1026 01:05:49.203829 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0699759 (* 1 = 0.0699759 loss)
I1026 01:05:49.203833 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00993789 (* 1 = 0.00993789 loss)
I1026 01:05:49.203838 17254 sgd_solver.cpp:106] Iteration 1120, lr = 0.001
I1026 01:05:50.278272 17254 solver.cpp:229] Iteration 1140, loss = 0.225941
I1026 01:05:50.278304 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.090188 (* 1 = 0.090188 loss)
I1026 01:05:50.278311 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.135753 (* 1 = 0.135753 loss)
I1026 01:05:50.278316 17254 sgd_solver.cpp:106] Iteration 1140, lr = 0.001
I1026 01:05:51.383657 17254 solver.cpp:229] Iteration 1160, loss = 0.0860696
I1026 01:05:51.383689 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0380514 (* 1 = 0.0380514 loss)
I1026 01:05:51.383694 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0480182 (* 1 = 0.0480182 loss)
I1026 01:05:51.383698 17254 sgd_solver.cpp:106] Iteration 1160, lr = 0.001
I1026 01:05:52.509409 17254 solver.cpp:229] Iteration 1180, loss = 0.106226
I1026 01:05:52.509449 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0432165 (* 1 = 0.0432165 loss)
I1026 01:05:52.509454 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0630093 (* 1 = 0.0630093 loss)
I1026 01:05:52.509459 17254 sgd_solver.cpp:106] Iteration 1180, lr = 0.001
I1026 01:05:53.582195 17254 solver.cpp:229] Iteration 1200, loss = 0.10123
I1026 01:05:53.582240 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0743064 (* 1 = 0.0743064 loss)
I1026 01:05:53.582245 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0269237 (* 1 = 0.0269237 loss)
I1026 01:05:53.582249 17254 sgd_solver.cpp:106] Iteration 1200, lr = 0.001
I1026 01:05:54.690106 17254 solver.cpp:229] Iteration 1220, loss = 0.148891
I1026 01:05:54.690137 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0725819 (* 1 = 0.0725819 loss)
I1026 01:05:54.690142 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0763095 (* 1 = 0.0763095 loss)
I1026 01:05:54.690147 17254 sgd_solver.cpp:106] Iteration 1220, lr = 0.001
I1026 01:05:55.768097 17254 solver.cpp:229] Iteration 1240, loss = 0.126241
I1026 01:05:55.768131 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0493312 (* 1 = 0.0493312 loss)
I1026 01:05:55.768136 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0769098 (* 1 = 0.0769098 loss)
I1026 01:05:55.768141 17254 sgd_solver.cpp:106] Iteration 1240, lr = 0.001
I1026 01:05:56.889075 17254 solver.cpp:229] Iteration 1260, loss = 0.215125
I1026 01:05:56.889108 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0950852 (* 1 = 0.0950852 loss)
I1026 01:05:56.889113 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.120039 (* 1 = 0.120039 loss)
I1026 01:05:56.889117 17254 sgd_solver.cpp:106] Iteration 1260, lr = 0.001
I1026 01:05:58.005766 17254 solver.cpp:229] Iteration 1280, loss = 0.10073
I1026 01:05:58.005798 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0642018 (* 1 = 0.0642018 loss)
I1026 01:05:58.005803 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0365286 (* 1 = 0.0365286 loss)
I1026 01:05:58.005808 17254 sgd_solver.cpp:106] Iteration 1280, lr = 0.001
I1026 01:05:59.100263 17254 solver.cpp:229] Iteration 1300, loss = 0.149498
I1026 01:05:59.100296 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0675356 (* 1 = 0.0675356 loss)
I1026 01:05:59.100301 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0819626 (* 1 = 0.0819626 loss)
I1026 01:05:59.100306 17254 sgd_solver.cpp:106] Iteration 1300, lr = 0.001
I1026 01:06:00.205670 17254 solver.cpp:229] Iteration 1320, loss = 0.239302
I1026 01:06:00.205703 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.117049 (* 1 = 0.117049 loss)
I1026 01:06:00.205708 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.122253 (* 1 = 0.122253 loss)
I1026 01:06:00.205713 17254 sgd_solver.cpp:106] Iteration 1320, lr = 0.001
I1026 01:06:01.318888 17254 solver.cpp:229] Iteration 1340, loss = 0.0956265
I1026 01:06:01.318934 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0646092 (* 1 = 0.0646092 loss)
I1026 01:06:01.318939 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0310173 (* 1 = 0.0310173 loss)
I1026 01:06:01.318943 17254 sgd_solver.cpp:106] Iteration 1340, lr = 0.001
I1026 01:06:02.384483 17254 solver.cpp:229] Iteration 1360, loss = 0.109548
I1026 01:06:02.384516 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0676823 (* 1 = 0.0676823 loss)
I1026 01:06:02.384521 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0418661 (* 1 = 0.0418661 loss)
I1026 01:06:02.384524 17254 sgd_solver.cpp:106] Iteration 1360, lr = 0.001
I1026 01:06:03.493248 17254 solver.cpp:229] Iteration 1380, loss = 0.192746
I1026 01:06:03.493281 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0669895 (* 1 = 0.0669895 loss)
I1026 01:06:03.493300 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.125757 (* 1 = 0.125757 loss)
I1026 01:06:03.493305 17254 sgd_solver.cpp:106] Iteration 1380, lr = 0.001
I1026 01:06:04.592576 17254 solver.cpp:229] Iteration 1400, loss = 0.11033
I1026 01:06:04.592607 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0427189 (* 1 = 0.0427189 loss)
I1026 01:06:04.592612 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0676114 (* 1 = 0.0676114 loss)
I1026 01:06:04.592617 17254 sgd_solver.cpp:106] Iteration 1400, lr = 0.001
I1026 01:06:05.707743 17254 solver.cpp:229] Iteration 1420, loss = 0.118823
I1026 01:06:05.707777 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0494909 (* 1 = 0.0494909 loss)
I1026 01:06:05.707782 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0693318 (* 1 = 0.0693318 loss)
I1026 01:06:05.707787 17254 sgd_solver.cpp:106] Iteration 1420, lr = 0.001
I1026 01:06:06.795383 17254 solver.cpp:229] Iteration 1440, loss = 0.100449
I1026 01:06:06.795416 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0537394 (* 1 = 0.0537394 loss)
I1026 01:06:06.795423 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0467098 (* 1 = 0.0467098 loss)
I1026 01:06:06.795426 17254 sgd_solver.cpp:106] Iteration 1440, lr = 0.001
I1026 01:06:07.896749 17254 solver.cpp:229] Iteration 1460, loss = 0.125129
I1026 01:06:07.896781 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0783604 (* 1 = 0.0783604 loss)
I1026 01:06:07.896786 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.046769 (* 1 = 0.046769 loss)
I1026 01:06:07.896791 17254 sgd_solver.cpp:106] Iteration 1460, lr = 0.001
I1026 01:06:09.015377 17254 solver.cpp:229] Iteration 1480, loss = 0.166432
I1026 01:06:09.015411 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.074401 (* 1 = 0.074401 loss)
I1026 01:06:09.015416 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0920307 (* 1 = 0.0920307 loss)
I1026 01:06:09.015421 17254 sgd_solver.cpp:106] Iteration 1480, lr = 0.001
I1026 01:06:10.118827 17254 solver.cpp:229] Iteration 1500, loss = 0.13708
I1026 01:06:10.118861 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.114248 (* 1 = 0.114248 loss)
I1026 01:06:10.118866 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0228321 (* 1 = 0.0228321 loss)
I1026 01:06:10.118871 17254 sgd_solver.cpp:106] Iteration 1500, lr = 0.001
I1026 01:06:11.226938 17254 solver.cpp:229] Iteration 1520, loss = 0.137627
I1026 01:06:11.226969 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.103194 (* 1 = 0.103194 loss)
I1026 01:06:11.226974 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0344326 (* 1 = 0.0344326 loss)
I1026 01:06:11.226977 17254 sgd_solver.cpp:106] Iteration 1520, lr = 0.001
I1026 01:06:12.344609 17254 solver.cpp:229] Iteration 1540, loss = 0.119725
I1026 01:06:12.344641 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0520215 (* 1 = 0.0520215 loss)
I1026 01:06:12.344646 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0677037 (* 1 = 0.0677037 loss)
I1026 01:06:12.344652 17254 sgd_solver.cpp:106] Iteration 1540, lr = 0.001
I1026 01:06:13.450287 17254 solver.cpp:229] Iteration 1560, loss = 0.109087
I1026 01:06:13.450345 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.054837 (* 1 = 0.054837 loss)
I1026 01:06:13.450358 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0542498 (* 1 = 0.0542498 loss)
I1026 01:06:13.450363 17254 sgd_solver.cpp:106] Iteration 1560, lr = 0.001
I1026 01:06:14.553503 17254 solver.cpp:229] Iteration 1580, loss = 0.129482
I1026 01:06:14.553536 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0893541 (* 1 = 0.0893541 loss)
I1026 01:06:14.553541 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0401276 (* 1 = 0.0401276 loss)
I1026 01:06:14.553546 17254 sgd_solver.cpp:106] Iteration 1580, lr = 0.001
I1026 01:06:15.687669 17254 solver.cpp:229] Iteration 1600, loss = 0.202322
I1026 01:06:15.687701 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.1191 (* 1 = 0.1191 loss)
I1026 01:06:15.687706 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0832227 (* 1 = 0.0832227 loss)
I1026 01:06:15.687711 17254 sgd_solver.cpp:106] Iteration 1600, lr = 0.001
I1026 01:06:16.791488 17254 solver.cpp:229] Iteration 1620, loss = 0.141746
I1026 01:06:16.791520 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0705412 (* 1 = 0.0705412 loss)
I1026 01:06:16.791525 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0712045 (* 1 = 0.0712045 loss)
I1026 01:06:16.791532 17254 sgd_solver.cpp:106] Iteration 1620, lr = 0.001
I1026 01:06:17.894529 17254 solver.cpp:229] Iteration 1640, loss = 0.114424
I1026 01:06:17.894559 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0606926 (* 1 = 0.0606926 loss)
I1026 01:06:17.894563 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0537314 (* 1 = 0.0537314 loss)
I1026 01:06:17.894568 17254 sgd_solver.cpp:106] Iteration 1640, lr = 0.001
I1026 01:06:19.009840 17254 solver.cpp:229] Iteration 1660, loss = 0.242482
I1026 01:06:19.009871 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.161263 (* 1 = 0.161263 loss)
I1026 01:06:19.009876 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0812184 (* 1 = 0.0812184 loss)
I1026 01:06:19.009881 17254 sgd_solver.cpp:106] Iteration 1660, lr = 0.001
I1026 01:06:20.128177 17254 solver.cpp:229] Iteration 1680, loss = 0.249664
I1026 01:06:20.128211 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0805001 (* 1 = 0.0805001 loss)
I1026 01:06:20.128216 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.169164 (* 1 = 0.169164 loss)
I1026 01:06:20.128221 17254 sgd_solver.cpp:106] Iteration 1680, lr = 0.001
I1026 01:06:21.259248 17254 solver.cpp:229] Iteration 1700, loss = 0.121346
I1026 01:06:21.259295 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0556011 (* 1 = 0.0556011 loss)
I1026 01:06:21.259301 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0657451 (* 1 = 0.0657451 loss)
I1026 01:06:21.259306 17254 sgd_solver.cpp:106] Iteration 1700, lr = 0.001
I1026 01:06:22.364382 17254 solver.cpp:229] Iteration 1720, loss = 0.175156
I1026 01:06:22.364414 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.134254 (* 1 = 0.134254 loss)
I1026 01:06:22.364418 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0409024 (* 1 = 0.0409024 loss)
I1026 01:06:22.364423 17254 sgd_solver.cpp:106] Iteration 1720, lr = 0.001
I1026 01:06:23.453805 17254 solver.cpp:229] Iteration 1740, loss = 0.0705242
I1026 01:06:23.453836 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.05026 (* 1 = 0.05026 loss)
I1026 01:06:23.453841 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0202642 (* 1 = 0.0202642 loss)
I1026 01:06:23.453845 17254 sgd_solver.cpp:106] Iteration 1740, lr = 0.001
I1026 01:06:24.571713 17254 solver.cpp:229] Iteration 1760, loss = 0.0830814
I1026 01:06:24.571744 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0496745 (* 1 = 0.0496745 loss)
I1026 01:06:24.571749 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0334069 (* 1 = 0.0334069 loss)
I1026 01:06:24.571753 17254 sgd_solver.cpp:106] Iteration 1760, lr = 0.001
I1026 01:06:25.695240 17254 solver.cpp:229] Iteration 1780, loss = 0.139488
I1026 01:06:25.695272 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0642034 (* 1 = 0.0642034 loss)
I1026 01:06:25.695277 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0752843 (* 1 = 0.0752843 loss)
I1026 01:06:25.695281 17254 sgd_solver.cpp:106] Iteration 1780, lr = 0.001
I1026 01:06:26.814831 17254 solver.cpp:229] Iteration 1800, loss = 0.213362
I1026 01:06:26.814862 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0976989 (* 1 = 0.0976989 loss)
I1026 01:06:26.814867 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.115663 (* 1 = 0.115663 loss)
I1026 01:06:26.814872 17254 sgd_solver.cpp:106] Iteration 1800, lr = 0.001
I1026 01:06:27.905158 17254 solver.cpp:229] Iteration 1820, loss = 0.184198
I1026 01:06:27.905189 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0976643 (* 1 = 0.0976643 loss)
I1026 01:06:27.905194 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0865332 (* 1 = 0.0865332 loss)
I1026 01:06:27.905197 17254 sgd_solver.cpp:106] Iteration 1820, lr = 0.001
I1026 01:06:29.011106 17254 solver.cpp:229] Iteration 1840, loss = 0.108136
I1026 01:06:29.011138 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0501606 (* 1 = 0.0501606 loss)
I1026 01:06:29.011143 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0579749 (* 1 = 0.0579749 loss)
I1026 01:06:29.011148 17254 sgd_solver.cpp:106] Iteration 1840, lr = 0.001
I1026 01:06:30.108572 17254 solver.cpp:229] Iteration 1860, loss = 0.148911
I1026 01:06:30.108603 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0939139 (* 1 = 0.0939139 loss)
I1026 01:06:30.108608 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0549969 (* 1 = 0.0549969 loss)
I1026 01:06:30.108613 17254 sgd_solver.cpp:106] Iteration 1860, lr = 0.001
I1026 01:06:31.200884 17254 solver.cpp:229] Iteration 1880, loss = 0.220788
I1026 01:06:31.200917 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.118034 (* 1 = 0.118034 loss)
I1026 01:06:31.200922 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.102754 (* 1 = 0.102754 loss)
I1026 01:06:31.200925 17254 sgd_solver.cpp:106] Iteration 1880, lr = 0.001
I1026 01:06:32.298106 17254 solver.cpp:229] Iteration 1900, loss = 0.140644
I1026 01:06:32.298138 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.090561 (* 1 = 0.090561 loss)
I1026 01:06:32.298142 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0500829 (* 1 = 0.0500829 loss)
I1026 01:06:32.298147 17254 sgd_solver.cpp:106] Iteration 1900, lr = 0.001
I1026 01:06:33.432384 17254 solver.cpp:229] Iteration 1920, loss = 0.135158
I1026 01:06:33.432417 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0688065 (* 1 = 0.0688065 loss)
I1026 01:06:33.432421 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0663516 (* 1 = 0.0663516 loss)
I1026 01:06:33.432425 17254 sgd_solver.cpp:106] Iteration 1920, lr = 0.001
I1026 01:06:34.542655 17254 solver.cpp:229] Iteration 1940, loss = 0.0844058
I1026 01:06:34.542687 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0708011 (* 1 = 0.0708011 loss)
I1026 01:06:34.542692 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0136047 (* 1 = 0.0136047 loss)
I1026 01:06:34.542697 17254 sgd_solver.cpp:106] Iteration 1940, lr = 0.001
I1026 01:06:35.652858 17254 solver.cpp:229] Iteration 1960, loss = 0.218438
I1026 01:06:35.652892 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0972926 (* 1 = 0.0972926 loss)
I1026 01:06:35.652896 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.121145 (* 1 = 0.121145 loss)
I1026 01:06:35.652901 17254 sgd_solver.cpp:106] Iteration 1960, lr = 0.001
I1026 01:06:36.761790 17254 solver.cpp:229] Iteration 1980, loss = 0.164474
I1026 01:06:36.761821 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.136309 (* 1 = 0.136309 loss)
I1026 01:06:36.761826 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0281643 (* 1 = 0.0281643 loss)
I1026 01:06:36.761831 17254 sgd_solver.cpp:106] Iteration 1980, lr = 0.001
I1026 01:06:37.887354 17254 solver.cpp:229] Iteration 2000, loss = 0.243592
I1026 01:06:37.887398 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.109222 (* 1 = 0.109222 loss)
I1026 01:06:37.887403 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.13437 (* 1 = 0.13437 loss)
I1026 01:06:37.887406 17254 sgd_solver.cpp:106] Iteration 2000, lr = 0.001
I1026 01:06:38.995244 17254 solver.cpp:229] Iteration 2020, loss = 0.127285
I1026 01:06:38.995275 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.077478 (* 1 = 0.077478 loss)
I1026 01:06:38.995280 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0498071 (* 1 = 0.0498071 loss)
I1026 01:06:38.995283 17254 sgd_solver.cpp:106] Iteration 2020, lr = 0.001
I1026 01:06:40.126345 17254 solver.cpp:229] Iteration 2040, loss = 0.16279
I1026 01:06:40.126377 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.104919 (* 1 = 0.104919 loss)
I1026 01:06:40.126382 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0578706 (* 1 = 0.0578706 loss)
I1026 01:06:40.126387 17254 sgd_solver.cpp:106] Iteration 2040, lr = 0.001
I1026 01:06:41.265069 17254 solver.cpp:229] Iteration 2060, loss = 0.0523345
I1026 01:06:41.265101 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0426834 (* 1 = 0.0426834 loss)
I1026 01:06:41.265106 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00965107 (* 1 = 0.00965107 loss)
I1026 01:06:41.265111 17254 sgd_solver.cpp:106] Iteration 2060, lr = 0.001
I1026 01:06:42.371033 17254 solver.cpp:229] Iteration 2080, loss = 0.0966578
I1026 01:06:42.371075 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0725351 (* 1 = 0.0725351 loss)
I1026 01:06:42.371081 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0241226 (* 1 = 0.0241226 loss)
I1026 01:06:42.371086 17254 sgd_solver.cpp:106] Iteration 2080, lr = 0.001
I1026 01:06:43.454653 17254 solver.cpp:229] Iteration 2100, loss = 0.307869
I1026 01:06:43.454685 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.129113 (* 1 = 0.129113 loss)
I1026 01:06:43.454690 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.178755 (* 1 = 0.178755 loss)
I1026 01:06:43.454695 17254 sgd_solver.cpp:106] Iteration 2100, lr = 0.001
I1026 01:06:44.559254 17254 solver.cpp:229] Iteration 2120, loss = 0.157168
I1026 01:06:44.559286 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0772863 (* 1 = 0.0772863 loss)
I1026 01:06:44.559291 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0798813 (* 1 = 0.0798813 loss)
I1026 01:06:44.559296 17254 sgd_solver.cpp:106] Iteration 2120, lr = 0.001
I1026 01:06:45.661298 17254 solver.cpp:229] Iteration 2140, loss = 0.0745677
I1026 01:06:45.661330 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0454659 (* 1 = 0.0454659 loss)
I1026 01:06:45.661335 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0291018 (* 1 = 0.0291018 loss)
I1026 01:06:45.661339 17254 sgd_solver.cpp:106] Iteration 2140, lr = 0.001
I1026 01:06:46.779913 17254 solver.cpp:229] Iteration 2160, loss = 0.186305
I1026 01:06:46.779947 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.135521 (* 1 = 0.135521 loss)
I1026 01:06:46.779956 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0507842 (* 1 = 0.0507842 loss)
I1026 01:06:46.779963 17254 sgd_solver.cpp:106] Iteration 2160, lr = 0.001
I1026 01:06:47.885185 17254 solver.cpp:229] Iteration 2180, loss = 0.24374
I1026 01:06:47.885217 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.16011 (* 1 = 0.16011 loss)
I1026 01:06:47.885222 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0836305 (* 1 = 0.0836305 loss)
I1026 01:06:47.885227 17254 sgd_solver.cpp:106] Iteration 2180, lr = 0.001
I1026 01:06:48.990705 17254 solver.cpp:229] Iteration 2200, loss = 0.10863
I1026 01:06:48.990736 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0547557 (* 1 = 0.0547557 loss)
I1026 01:06:48.990741 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0538743 (* 1 = 0.0538743 loss)
I1026 01:06:48.990746 17254 sgd_solver.cpp:106] Iteration 2200, lr = 0.001
I1026 01:06:50.113100 17254 solver.cpp:229] Iteration 2220, loss = 0.189405
I1026 01:06:50.113142 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.094637 (* 1 = 0.094637 loss)
I1026 01:06:50.113147 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0947681 (* 1 = 0.0947681 loss)
I1026 01:06:50.113152 17254 sgd_solver.cpp:106] Iteration 2220, lr = 0.001
I1026 01:06:51.220480 17254 solver.cpp:229] Iteration 2240, loss = 0.140262
I1026 01:06:51.220512 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0771898 (* 1 = 0.0771898 loss)
I1026 01:06:51.220517 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.063072 (* 1 = 0.063072 loss)
I1026 01:06:51.220521 17254 sgd_solver.cpp:106] Iteration 2240, lr = 0.001
I1026 01:06:52.339659 17254 solver.cpp:229] Iteration 2260, loss = 0.113961
I1026 01:06:52.339707 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.084556 (* 1 = 0.084556 loss)
I1026 01:06:52.339721 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0294054 (* 1 = 0.0294054 loss)
I1026 01:06:52.339727 17254 sgd_solver.cpp:106] Iteration 2260, lr = 0.001
I1026 01:06:53.460084 17254 solver.cpp:229] Iteration 2280, loss = 0.084199
I1026 01:06:53.460116 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0575596 (* 1 = 0.0575596 loss)
I1026 01:06:53.460121 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0266394 (* 1 = 0.0266394 loss)
I1026 01:06:53.460127 17254 sgd_solver.cpp:106] Iteration 2280, lr = 0.001
I1026 01:06:54.581682 17254 solver.cpp:229] Iteration 2300, loss = 0.116816
I1026 01:06:54.581715 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0791668 (* 1 = 0.0791668 loss)
I1026 01:06:54.581720 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0376489 (* 1 = 0.0376489 loss)
I1026 01:06:54.581724 17254 sgd_solver.cpp:106] Iteration 2300, lr = 0.001
I1026 01:06:55.699921 17254 solver.cpp:229] Iteration 2320, loss = 0.0864759
I1026 01:06:55.699954 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0624165 (* 1 = 0.0624165 loss)
I1026 01:06:55.699959 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0240595 (* 1 = 0.0240595 loss)
I1026 01:06:55.699975 17254 sgd_solver.cpp:106] Iteration 2320, lr = 0.001
I1026 01:06:56.808989 17254 solver.cpp:229] Iteration 2340, loss = 0.183828
I1026 01:06:56.809020 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0944246 (* 1 = 0.0944246 loss)
I1026 01:06:56.809026 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.089403 (* 1 = 0.089403 loss)
I1026 01:06:56.809031 17254 sgd_solver.cpp:106] Iteration 2340, lr = 0.001
I1026 01:06:57.912884 17254 solver.cpp:229] Iteration 2360, loss = 0.182204
I1026 01:06:57.912914 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0901806 (* 1 = 0.0901806 loss)
I1026 01:06:57.912919 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0920233 (* 1 = 0.0920233 loss)
I1026 01:06:57.912924 17254 sgd_solver.cpp:106] Iteration 2360, lr = 0.001
I1026 01:06:59.037883 17254 solver.cpp:229] Iteration 2380, loss = 0.24513
I1026 01:06:59.037915 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.145538 (* 1 = 0.145538 loss)
I1026 01:06:59.037921 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0995926 (* 1 = 0.0995926 loss)
I1026 01:06:59.037926 17254 sgd_solver.cpp:106] Iteration 2380, lr = 0.001
I1026 01:07:00.142488 17254 solver.cpp:229] Iteration 2400, loss = 0.169263
I1026 01:07:00.142524 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0799912 (* 1 = 0.0799912 loss)
I1026 01:07:00.142532 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0892718 (* 1 = 0.0892718 loss)
I1026 01:07:00.142539 17254 sgd_solver.cpp:106] Iteration 2400, lr = 0.001
I1026 01:07:01.230515 17254 solver.cpp:229] Iteration 2420, loss = 0.244839
I1026 01:07:01.230552 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.113361 (* 1 = 0.113361 loss)
I1026 01:07:01.230558 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.131478 (* 1 = 0.131478 loss)
I1026 01:07:01.230562 17254 sgd_solver.cpp:106] Iteration 2420, lr = 0.001
I1026 01:07:02.331744 17254 solver.cpp:229] Iteration 2440, loss = 0.118512
I1026 01:07:02.331776 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0722881 (* 1 = 0.0722881 loss)
I1026 01:07:02.331781 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0462238 (* 1 = 0.0462238 loss)
I1026 01:07:02.331786 17254 sgd_solver.cpp:106] Iteration 2440, lr = 0.001
I1026 01:07:03.470710 17254 solver.cpp:229] Iteration 2460, loss = 0.182194
I1026 01:07:03.470752 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0816671 (* 1 = 0.0816671 loss)
I1026 01:07:03.470757 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.100527 (* 1 = 0.100527 loss)
I1026 01:07:03.470762 17254 sgd_solver.cpp:106] Iteration 2460, lr = 0.001
I1026 01:07:04.576210 17254 solver.cpp:229] Iteration 2480, loss = 0.174816
I1026 01:07:04.576243 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0931228 (* 1 = 0.0931228 loss)
I1026 01:07:04.576247 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0816932 (* 1 = 0.0816932 loss)
I1026 01:07:04.576252 17254 sgd_solver.cpp:106] Iteration 2480, lr = 0.001
I1026 01:07:05.681396 17254 solver.cpp:229] Iteration 2500, loss = 0.131914
I1026 01:07:05.681429 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0933827 (* 1 = 0.0933827 loss)
I1026 01:07:05.681434 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0385313 (* 1 = 0.0385313 loss)
I1026 01:07:05.681438 17254 sgd_solver.cpp:106] Iteration 2500, lr = 0.001
I1026 01:07:06.794523 17254 solver.cpp:229] Iteration 2520, loss = 0.196262
I1026 01:07:06.794554 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.105816 (* 1 = 0.105816 loss)
I1026 01:07:06.794559 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0904461 (* 1 = 0.0904461 loss)
I1026 01:07:06.794564 17254 sgd_solver.cpp:106] Iteration 2520, lr = 0.001
I1026 01:07:07.898577 17254 solver.cpp:229] Iteration 2540, loss = 0.149463
I1026 01:07:07.898609 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0754475 (* 1 = 0.0754475 loss)
I1026 01:07:07.898614 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.074015 (* 1 = 0.074015 loss)
I1026 01:07:07.898619 17254 sgd_solver.cpp:106] Iteration 2540, lr = 0.001
I1026 01:07:09.004578 17254 solver.cpp:229] Iteration 2560, loss = 0.0750981
I1026 01:07:09.004611 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.054857 (* 1 = 0.054857 loss)
I1026 01:07:09.004616 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.020241 (* 1 = 0.020241 loss)
I1026 01:07:09.004619 17254 sgd_solver.cpp:106] Iteration 2560, lr = 0.001
I1026 01:07:10.116401 17254 solver.cpp:229] Iteration 2580, loss = 0.229054
I1026 01:07:10.116433 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.131241 (* 1 = 0.131241 loss)
I1026 01:07:10.116438 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0978123 (* 1 = 0.0978123 loss)
I1026 01:07:10.116442 17254 sgd_solver.cpp:106] Iteration 2580, lr = 0.001
I1026 01:07:11.213183 17254 solver.cpp:229] Iteration 2600, loss = 0.198607
I1026 01:07:11.213213 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0861975 (* 1 = 0.0861975 loss)
I1026 01:07:11.213219 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.11241 (* 1 = 0.11241 loss)
I1026 01:07:11.213224 17254 sgd_solver.cpp:106] Iteration 2600, lr = 0.001
I1026 01:07:12.304834 17254 solver.cpp:229] Iteration 2620, loss = 0.140878
I1026 01:07:12.304867 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0603791 (* 1 = 0.0603791 loss)
I1026 01:07:12.304872 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0804993 (* 1 = 0.0804993 loss)
I1026 01:07:12.304877 17254 sgd_solver.cpp:106] Iteration 2620, lr = 0.001
I1026 01:07:13.402978 17254 solver.cpp:229] Iteration 2640, loss = 0.0900583
I1026 01:07:13.403010 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0615416 (* 1 = 0.0615416 loss)
I1026 01:07:13.403014 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0285167 (* 1 = 0.0285167 loss)
I1026 01:07:13.403018 17254 sgd_solver.cpp:106] Iteration 2640, lr = 0.001
I1026 01:07:14.503700 17254 solver.cpp:229] Iteration 2660, loss = 0.184718
I1026 01:07:14.503732 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0945589 (* 1 = 0.0945589 loss)
I1026 01:07:14.503737 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0901594 (* 1 = 0.0901594 loss)
I1026 01:07:14.503742 17254 sgd_solver.cpp:106] Iteration 2660, lr = 0.001
I1026 01:07:15.630121 17254 solver.cpp:229] Iteration 2680, loss = 0.116508
I1026 01:07:15.630153 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0725133 (* 1 = 0.0725133 loss)
I1026 01:07:15.630157 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0439943 (* 1 = 0.0439943 loss)
I1026 01:07:15.630162 17254 sgd_solver.cpp:106] Iteration 2680, lr = 0.001
I1026 01:07:16.733330 17254 solver.cpp:229] Iteration 2700, loss = 0.174567
I1026 01:07:16.733362 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.073127 (* 1 = 0.073127 loss)
I1026 01:07:16.733367 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.10144 (* 1 = 0.10144 loss)
I1026 01:07:16.733372 17254 sgd_solver.cpp:106] Iteration 2700, lr = 0.001
I1026 01:07:17.852249 17254 solver.cpp:229] Iteration 2720, loss = 0.112817
I1026 01:07:17.852283 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0957002 (* 1 = 0.0957002 loss)
I1026 01:07:17.852288 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0171165 (* 1 = 0.0171165 loss)
I1026 01:07:17.852293 17254 sgd_solver.cpp:106] Iteration 2720, lr = 0.001
I1026 01:07:18.950434 17254 solver.cpp:229] Iteration 2740, loss = 0.134164
I1026 01:07:18.950466 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0496559 (* 1 = 0.0496559 loss)
I1026 01:07:18.950471 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0845084 (* 1 = 0.0845084 loss)
I1026 01:07:18.950476 17254 sgd_solver.cpp:106] Iteration 2740, lr = 0.001
I1026 01:07:20.082530 17254 solver.cpp:229] Iteration 2760, loss = 0.186892
I1026 01:07:20.082563 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0747052 (* 1 = 0.0747052 loss)
I1026 01:07:20.082568 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.112187 (* 1 = 0.112187 loss)
I1026 01:07:20.082572 17254 sgd_solver.cpp:106] Iteration 2760, lr = 0.001
I1026 01:07:21.178849 17254 solver.cpp:229] Iteration 2780, loss = 0.0839028
I1026 01:07:21.178882 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0441397 (* 1 = 0.0441397 loss)
I1026 01:07:21.178889 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0397631 (* 1 = 0.0397631 loss)
I1026 01:07:21.178892 17254 sgd_solver.cpp:106] Iteration 2780, lr = 0.001
I1026 01:07:22.290882 17254 solver.cpp:229] Iteration 2800, loss = 0.127898
I1026 01:07:22.290915 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.101155 (* 1 = 0.101155 loss)
I1026 01:07:22.290920 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0267438 (* 1 = 0.0267438 loss)
I1026 01:07:22.290925 17254 sgd_solver.cpp:106] Iteration 2800, lr = 0.001
I1026 01:07:23.406646 17254 solver.cpp:229] Iteration 2820, loss = 0.0727595
I1026 01:07:23.406677 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0439031 (* 1 = 0.0439031 loss)
I1026 01:07:23.406682 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0288564 (* 1 = 0.0288564 loss)
I1026 01:07:23.406697 17254 sgd_solver.cpp:106] Iteration 2820, lr = 0.001
I1026 01:07:24.498394 17254 solver.cpp:229] Iteration 2840, loss = 0.161086
I1026 01:07:24.498426 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0807922 (* 1 = 0.0807922 loss)
I1026 01:07:24.498431 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0802937 (* 1 = 0.0802937 loss)
I1026 01:07:24.498436 17254 sgd_solver.cpp:106] Iteration 2840, lr = 0.001
I1026 01:07:25.605685 17254 solver.cpp:229] Iteration 2860, loss = 0.10132
I1026 01:07:25.605717 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0601051 (* 1 = 0.0601051 loss)
I1026 01:07:25.605722 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0412146 (* 1 = 0.0412146 loss)
I1026 01:07:25.605727 17254 sgd_solver.cpp:106] Iteration 2860, lr = 0.001
I1026 01:07:26.718796 17254 solver.cpp:229] Iteration 2880, loss = 0.0828257
I1026 01:07:26.718829 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0571931 (* 1 = 0.0571931 loss)
I1026 01:07:26.718834 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0256326 (* 1 = 0.0256326 loss)
I1026 01:07:26.718838 17254 sgd_solver.cpp:106] Iteration 2880, lr = 0.001
I1026 01:07:27.811070 17254 solver.cpp:229] Iteration 2900, loss = 0.173127
I1026 01:07:27.811102 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0707736 (* 1 = 0.0707736 loss)
I1026 01:07:27.811107 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.102354 (* 1 = 0.102354 loss)
I1026 01:07:27.811112 17254 sgd_solver.cpp:106] Iteration 2900, lr = 0.001
I1026 01:07:28.917384 17254 solver.cpp:229] Iteration 2920, loss = 0.0989419
I1026 01:07:28.917415 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0581549 (* 1 = 0.0581549 loss)
I1026 01:07:28.917420 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0407869 (* 1 = 0.0407869 loss)
I1026 01:07:28.917424 17254 sgd_solver.cpp:106] Iteration 2920, lr = 0.001
I1026 01:07:29.995720 17254 solver.cpp:229] Iteration 2940, loss = 0.130362
I1026 01:07:29.995753 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0416059 (* 1 = 0.0416059 loss)
I1026 01:07:29.995757 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0887562 (* 1 = 0.0887562 loss)
I1026 01:07:29.995775 17254 sgd_solver.cpp:106] Iteration 2940, lr = 0.001
I1026 01:07:31.096060 17254 solver.cpp:229] Iteration 2960, loss = 0.173469
I1026 01:07:31.096093 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0878077 (* 1 = 0.0878077 loss)
I1026 01:07:31.096098 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.085661 (* 1 = 0.085661 loss)
I1026 01:07:31.096103 17254 sgd_solver.cpp:106] Iteration 2960, lr = 0.001
I1026 01:07:32.238951 17254 solver.cpp:229] Iteration 2980, loss = 0.123445
I1026 01:07:32.238981 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0560117 (* 1 = 0.0560117 loss)
I1026 01:07:32.238986 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0674337 (* 1 = 0.0674337 loss)
I1026 01:07:32.238991 17254 sgd_solver.cpp:106] Iteration 2980, lr = 0.001
I1026 01:07:33.374310 17254 solver.cpp:229] Iteration 3000, loss = 0.194971
I1026 01:07:33.374346 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0909769 (* 1 = 0.0909769 loss)
I1026 01:07:33.374351 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.103994 (* 1 = 0.103994 loss)
I1026 01:07:33.374357 17254 sgd_solver.cpp:106] Iteration 3000, lr = 0.001
I1026 01:07:34.480309 17254 solver.cpp:229] Iteration 3020, loss = 0.177129
I1026 01:07:34.480341 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0719334 (* 1 = 0.0719334 loss)
I1026 01:07:34.480348 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.105196 (* 1 = 0.105196 loss)
I1026 01:07:34.480355 17254 sgd_solver.cpp:106] Iteration 3020, lr = 0.001
I1026 01:07:35.583290 17254 solver.cpp:229] Iteration 3040, loss = 0.0750686
I1026 01:07:35.583325 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0561541 (* 1 = 0.0561541 loss)
I1026 01:07:35.583333 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0189145 (* 1 = 0.0189145 loss)
I1026 01:07:35.583349 17254 sgd_solver.cpp:106] Iteration 3040, lr = 0.001
I1026 01:07:36.698408 17254 solver.cpp:229] Iteration 3060, loss = 0.097572
I1026 01:07:36.698442 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0493673 (* 1 = 0.0493673 loss)
I1026 01:07:36.698448 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0482047 (* 1 = 0.0482047 loss)
I1026 01:07:36.698454 17254 sgd_solver.cpp:106] Iteration 3060, lr = 0.001
I1026 01:07:37.826756 17254 solver.cpp:229] Iteration 3080, loss = 0.0962813
I1026 01:07:37.826792 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0520576 (* 1 = 0.0520576 loss)
I1026 01:07:37.826799 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0442236 (* 1 = 0.0442236 loss)
I1026 01:07:37.826807 17254 sgd_solver.cpp:106] Iteration 3080, lr = 0.001
I1026 01:07:38.939714 17254 solver.cpp:229] Iteration 3100, loss = 0.125224
I1026 01:07:38.939745 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0550455 (* 1 = 0.0550455 loss)
I1026 01:07:38.939750 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0701786 (* 1 = 0.0701786 loss)
I1026 01:07:38.939755 17254 sgd_solver.cpp:106] Iteration 3100, lr = 0.001
I1026 01:07:40.031427 17254 solver.cpp:229] Iteration 3120, loss = 0.0759092
I1026 01:07:40.031474 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0474166 (* 1 = 0.0474166 loss)
I1026 01:07:40.031488 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0284926 (* 1 = 0.0284926 loss)
I1026 01:07:40.031494 17254 sgd_solver.cpp:106] Iteration 3120, lr = 0.001
I1026 01:07:41.124076 17254 solver.cpp:229] Iteration 3140, loss = 0.127112
I1026 01:07:41.124109 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0671271 (* 1 = 0.0671271 loss)
I1026 01:07:41.124114 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0599845 (* 1 = 0.0599845 loss)
I1026 01:07:41.124119 17254 sgd_solver.cpp:106] Iteration 3140, lr = 0.001
I1026 01:07:42.232261 17254 solver.cpp:229] Iteration 3160, loss = 0.121133
I1026 01:07:42.232293 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0741095 (* 1 = 0.0741095 loss)
I1026 01:07:42.232298 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0470231 (* 1 = 0.0470231 loss)
I1026 01:07:42.232305 17254 sgd_solver.cpp:106] Iteration 3160, lr = 0.001
I1026 01:07:43.347260 17254 solver.cpp:229] Iteration 3180, loss = 0.169277
I1026 01:07:43.347292 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0742755 (* 1 = 0.0742755 loss)
I1026 01:07:43.347296 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0950017 (* 1 = 0.0950017 loss)
I1026 01:07:43.347301 17254 sgd_solver.cpp:106] Iteration 3180, lr = 0.001
I1026 01:07:44.449762 17254 solver.cpp:229] Iteration 3200, loss = 0.135258
I1026 01:07:44.449792 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0881351 (* 1 = 0.0881351 loss)
I1026 01:07:44.449797 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0471231 (* 1 = 0.0471231 loss)
I1026 01:07:44.449801 17254 sgd_solver.cpp:106] Iteration 3200, lr = 0.001
I1026 01:07:45.575412 17254 solver.cpp:229] Iteration 3220, loss = 0.136289
I1026 01:07:45.575458 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0476065 (* 1 = 0.0476065 loss)
I1026 01:07:45.575461 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0886827 (* 1 = 0.0886827 loss)
I1026 01:07:45.575466 17254 sgd_solver.cpp:106] Iteration 3220, lr = 0.001
I1026 01:07:46.705451 17254 solver.cpp:229] Iteration 3240, loss = 0.144951
I1026 01:07:46.705483 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0778649 (* 1 = 0.0778649 loss)
I1026 01:07:46.705488 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0670861 (* 1 = 0.0670861 loss)
I1026 01:07:46.705493 17254 sgd_solver.cpp:106] Iteration 3240, lr = 0.001
I1026 01:07:47.809480 17254 solver.cpp:229] Iteration 3260, loss = 0.186141
I1026 01:07:47.809512 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.116359 (* 1 = 0.116359 loss)
I1026 01:07:47.809517 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0697818 (* 1 = 0.0697818 loss)
I1026 01:07:47.809521 17254 sgd_solver.cpp:106] Iteration 3260, lr = 0.001
I1026 01:07:48.932534 17254 solver.cpp:229] Iteration 3280, loss = 0.158351
I1026 01:07:48.932565 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.128796 (* 1 = 0.128796 loss)
I1026 01:07:48.932570 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0295554 (* 1 = 0.0295554 loss)
I1026 01:07:48.932575 17254 sgd_solver.cpp:106] Iteration 3280, lr = 0.001
I1026 01:07:50.039873 17254 solver.cpp:229] Iteration 3300, loss = 0.159461
I1026 01:07:50.039906 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.092848 (* 1 = 0.092848 loss)
I1026 01:07:50.039911 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0666128 (* 1 = 0.0666128 loss)
I1026 01:07:50.039914 17254 sgd_solver.cpp:106] Iteration 3300, lr = 0.001
I1026 01:07:51.161777 17254 solver.cpp:229] Iteration 3320, loss = 0.0942941
I1026 01:07:51.161808 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0467808 (* 1 = 0.0467808 loss)
I1026 01:07:51.161813 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0475133 (* 1 = 0.0475133 loss)
I1026 01:07:51.161818 17254 sgd_solver.cpp:106] Iteration 3320, lr = 0.001
I1026 01:07:52.267014 17254 solver.cpp:229] Iteration 3340, loss = 0.100201
I1026 01:07:52.267047 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0421707 (* 1 = 0.0421707 loss)
I1026 01:07:52.267052 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0580302 (* 1 = 0.0580302 loss)
I1026 01:07:52.267057 17254 sgd_solver.cpp:106] Iteration 3340, lr = 0.001
I1026 01:07:53.371165 17254 solver.cpp:229] Iteration 3360, loss = 0.181469
I1026 01:07:53.371198 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0847738 (* 1 = 0.0847738 loss)
I1026 01:07:53.371203 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0966953 (* 1 = 0.0966953 loss)
I1026 01:07:53.371209 17254 sgd_solver.cpp:106] Iteration 3360, lr = 0.001
I1026 01:07:54.471359 17254 solver.cpp:229] Iteration 3380, loss = 0.164543
I1026 01:07:54.471407 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.109125 (* 1 = 0.109125 loss)
I1026 01:07:54.471412 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0554179 (* 1 = 0.0554179 loss)
I1026 01:07:54.471417 17254 sgd_solver.cpp:106] Iteration 3380, lr = 0.001
I1026 01:07:55.567893 17254 solver.cpp:229] Iteration 3400, loss = 0.176974
I1026 01:07:55.567925 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.071978 (* 1 = 0.071978 loss)
I1026 01:07:55.567931 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.104996 (* 1 = 0.104996 loss)
I1026 01:07:55.567936 17254 sgd_solver.cpp:106] Iteration 3400, lr = 0.001
I1026 01:07:56.661059 17254 solver.cpp:229] Iteration 3420, loss = 0.119865
I1026 01:07:56.661088 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0913329 (* 1 = 0.0913329 loss)
I1026 01:07:56.661093 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0285325 (* 1 = 0.0285325 loss)
I1026 01:07:56.661098 17254 sgd_solver.cpp:106] Iteration 3420, lr = 0.001
I1026 01:07:57.747936 17254 solver.cpp:229] Iteration 3440, loss = 0.153447
I1026 01:07:57.747979 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.114742 (* 1 = 0.114742 loss)
I1026 01:07:57.747985 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0387046 (* 1 = 0.0387046 loss)
I1026 01:07:57.747990 17254 sgd_solver.cpp:106] Iteration 3440, lr = 0.001
I1026 01:07:58.851438 17254 solver.cpp:229] Iteration 3460, loss = 0.106365
I1026 01:07:58.851471 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0831933 (* 1 = 0.0831933 loss)
I1026 01:07:58.851476 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0231719 (* 1 = 0.0231719 loss)
I1026 01:07:58.851481 17254 sgd_solver.cpp:106] Iteration 3460, lr = 0.001
I1026 01:07:59.966769 17254 solver.cpp:229] Iteration 3480, loss = 0.186708
I1026 01:07:59.966801 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0983554 (* 1 = 0.0983554 loss)
I1026 01:07:59.966806 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0883524 (* 1 = 0.0883524 loss)
I1026 01:07:59.966811 17254 sgd_solver.cpp:106] Iteration 3480, lr = 0.001
I1026 01:08:01.070870 17254 solver.cpp:229] Iteration 3500, loss = 0.121836
I1026 01:08:01.070904 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0823474 (* 1 = 0.0823474 loss)
I1026 01:08:01.070909 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0394885 (* 1 = 0.0394885 loss)
I1026 01:08:01.070914 17254 sgd_solver.cpp:106] Iteration 3500, lr = 0.001
I1026 01:08:02.181407 17254 solver.cpp:229] Iteration 3520, loss = 0.159205
I1026 01:08:02.181442 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0491873 (* 1 = 0.0491873 loss)
I1026 01:08:02.181447 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.110017 (* 1 = 0.110017 loss)
I1026 01:08:02.181452 17254 sgd_solver.cpp:106] Iteration 3520, lr = 0.001
I1026 01:08:03.301527 17254 solver.cpp:229] Iteration 3540, loss = 0.0730334
I1026 01:08:03.301569 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0398415 (* 1 = 0.0398415 loss)
I1026 01:08:03.301574 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0331919 (* 1 = 0.0331919 loss)
I1026 01:08:03.301587 17254 sgd_solver.cpp:106] Iteration 3540, lr = 0.001
I1026 01:08:04.403226 17254 solver.cpp:229] Iteration 3560, loss = 0.119044
I1026 01:08:04.403256 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0763868 (* 1 = 0.0763868 loss)
I1026 01:08:04.403260 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0426577 (* 1 = 0.0426577 loss)
I1026 01:08:04.403264 17254 sgd_solver.cpp:106] Iteration 3560, lr = 0.001
I1026 01:08:05.497139 17254 solver.cpp:229] Iteration 3580, loss = 0.0807552
I1026 01:08:05.497180 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0446763 (* 1 = 0.0446763 loss)
I1026 01:08:05.497185 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0360789 (* 1 = 0.0360789 loss)
I1026 01:08:05.497191 17254 sgd_solver.cpp:106] Iteration 3580, lr = 0.001
I1026 01:08:06.586174 17254 solver.cpp:229] Iteration 3600, loss = 0.14298
I1026 01:08:06.586215 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0630168 (* 1 = 0.0630168 loss)
I1026 01:08:06.586220 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0799629 (* 1 = 0.0799629 loss)
I1026 01:08:06.586223 17254 sgd_solver.cpp:106] Iteration 3600, lr = 0.001
I1026 01:08:07.700291 17254 solver.cpp:229] Iteration 3620, loss = 0.136133
I1026 01:08:07.700323 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0862628 (* 1 = 0.0862628 loss)
I1026 01:08:07.700328 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.04987 (* 1 = 0.04987 loss)
I1026 01:08:07.700333 17254 sgd_solver.cpp:106] Iteration 3620, lr = 0.001
I1026 01:08:08.808142 17254 solver.cpp:229] Iteration 3640, loss = 0.227044
I1026 01:08:08.808176 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.122055 (* 1 = 0.122055 loss)
I1026 01:08:08.808179 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.104989 (* 1 = 0.104989 loss)
I1026 01:08:08.808184 17254 sgd_solver.cpp:106] Iteration 3640, lr = 0.001
I1026 01:08:09.902413 17254 solver.cpp:229] Iteration 3660, loss = 0.151595
I1026 01:08:09.902446 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0835726 (* 1 = 0.0835726 loss)
I1026 01:08:09.902451 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0680228 (* 1 = 0.0680228 loss)
I1026 01:08:09.902465 17254 sgd_solver.cpp:106] Iteration 3660, lr = 0.001
I1026 01:08:10.984246 17254 solver.cpp:229] Iteration 3680, loss = 0.0731307
I1026 01:08:10.984288 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0402601 (* 1 = 0.0402601 loss)
I1026 01:08:10.984292 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0328707 (* 1 = 0.0328707 loss)
I1026 01:08:10.984297 17254 sgd_solver.cpp:106] Iteration 3680, lr = 0.001
I1026 01:08:12.091785 17254 solver.cpp:229] Iteration 3700, loss = 0.132676
I1026 01:08:12.091819 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0856289 (* 1 = 0.0856289 loss)
I1026 01:08:12.091822 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0470469 (* 1 = 0.0470469 loss)
I1026 01:08:12.091827 17254 sgd_solver.cpp:106] Iteration 3700, lr = 0.001
I1026 01:08:13.183451 17254 solver.cpp:229] Iteration 3720, loss = 0.178581
I1026 01:08:13.183485 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0958376 (* 1 = 0.0958376 loss)
I1026 01:08:13.183493 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0827435 (* 1 = 0.0827435 loss)
I1026 01:08:13.183501 17254 sgd_solver.cpp:106] Iteration 3720, lr = 0.001
I1026 01:08:14.271275 17254 solver.cpp:229] Iteration 3740, loss = 0.0638664
I1026 01:08:14.271309 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0512595 (* 1 = 0.0512595 loss)
I1026 01:08:14.271316 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0126069 (* 1 = 0.0126069 loss)
I1026 01:08:14.271322 17254 sgd_solver.cpp:106] Iteration 3740, lr = 0.001
I1026 01:08:15.372637 17254 solver.cpp:229] Iteration 3760, loss = 0.0733151
I1026 01:08:15.372671 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0429504 (* 1 = 0.0429504 loss)
I1026 01:08:15.372678 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0303648 (* 1 = 0.0303648 loss)
I1026 01:08:15.372684 17254 sgd_solver.cpp:106] Iteration 3760, lr = 0.001
I1026 01:08:16.472524 17254 solver.cpp:229] Iteration 3780, loss = 0.12262
I1026 01:08:16.472558 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0753101 (* 1 = 0.0753101 loss)
I1026 01:08:16.472563 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0473102 (* 1 = 0.0473102 loss)
I1026 01:08:16.472569 17254 sgd_solver.cpp:106] Iteration 3780, lr = 0.001
I1026 01:08:17.559849 17254 solver.cpp:229] Iteration 3800, loss = 0.20247
I1026 01:08:17.559883 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.120678 (* 1 = 0.120678 loss)
I1026 01:08:17.559890 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0817925 (* 1 = 0.0817925 loss)
I1026 01:08:17.559896 17254 sgd_solver.cpp:106] Iteration 3800, lr = 0.001
I1026 01:08:18.666712 17254 solver.cpp:229] Iteration 3820, loss = 0.146588
I1026 01:08:18.666744 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0622982 (* 1 = 0.0622982 loss)
I1026 01:08:18.666751 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0842896 (* 1 = 0.0842896 loss)
I1026 01:08:18.666757 17254 sgd_solver.cpp:106] Iteration 3820, lr = 0.001
I1026 01:08:19.757666 17254 solver.cpp:229] Iteration 3840, loss = 0.201132
I1026 01:08:19.757699 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.11138 (* 1 = 0.11138 loss)
I1026 01:08:19.757706 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0897514 (* 1 = 0.0897514 loss)
I1026 01:08:19.757712 17254 sgd_solver.cpp:106] Iteration 3840, lr = 0.001
I1026 01:08:20.854535 17254 solver.cpp:229] Iteration 3860, loss = 0.151961
I1026 01:08:20.854569 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.104975 (* 1 = 0.104975 loss)
I1026 01:08:20.854576 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0469859 (* 1 = 0.0469859 loss)
I1026 01:08:20.854583 17254 sgd_solver.cpp:106] Iteration 3860, lr = 0.001
I1026 01:08:21.939050 17254 solver.cpp:229] Iteration 3880, loss = 0.0983085
I1026 01:08:21.939085 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0461347 (* 1 = 0.0461347 loss)
I1026 01:08:21.939091 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0521738 (* 1 = 0.0521738 loss)
I1026 01:08:21.939097 17254 sgd_solver.cpp:106] Iteration 3880, lr = 0.001
I1026 01:08:23.052507 17254 solver.cpp:229] Iteration 3900, loss = 0.212425
I1026 01:08:23.052539 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.161265 (* 1 = 0.161265 loss)
I1026 01:08:23.052547 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0511594 (* 1 = 0.0511594 loss)
I1026 01:08:23.052553 17254 sgd_solver.cpp:106] Iteration 3900, lr = 0.001
I1026 01:08:24.160881 17254 solver.cpp:229] Iteration 3920, loss = 0.0587955
I1026 01:08:24.160915 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0427495 (* 1 = 0.0427495 loss)
I1026 01:08:24.160923 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.016046 (* 1 = 0.016046 loss)
I1026 01:08:24.160928 17254 sgd_solver.cpp:106] Iteration 3920, lr = 0.001
I1026 01:08:25.254026 17254 solver.cpp:229] Iteration 3940, loss = 0.0915932
I1026 01:08:25.254061 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0494825 (* 1 = 0.0494825 loss)
I1026 01:08:25.254070 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0421106 (* 1 = 0.0421106 loss)
I1026 01:08:25.254076 17254 sgd_solver.cpp:106] Iteration 3940, lr = 0.001
I1026 01:08:26.356495 17254 solver.cpp:229] Iteration 3960, loss = 0.136116
I1026 01:08:26.356528 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0755165 (* 1 = 0.0755165 loss)
I1026 01:08:26.356533 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0605999 (* 1 = 0.0605999 loss)
I1026 01:08:26.356536 17254 sgd_solver.cpp:106] Iteration 3960, lr = 0.001
I1026 01:08:27.469606 17254 solver.cpp:229] Iteration 3980, loss = 0.167287
I1026 01:08:27.469638 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.076501 (* 1 = 0.076501 loss)
I1026 01:08:27.469642 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0907859 (* 1 = 0.0907859 loss)
I1026 01:08:27.469647 17254 sgd_solver.cpp:106] Iteration 3980, lr = 0.001
I1026 01:08:28.577481 17254 solver.cpp:229] Iteration 4000, loss = 0.0677904
I1026 01:08:28.577513 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0577591 (* 1 = 0.0577591 loss)
I1026 01:08:28.577518 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0100313 (* 1 = 0.0100313 loss)
I1026 01:08:28.577523 17254 sgd_solver.cpp:106] Iteration 4000, lr = 0.001
I1026 01:08:29.683498 17254 solver.cpp:229] Iteration 4020, loss = 0.0970538
I1026 01:08:29.683531 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0371448 (* 1 = 0.0371448 loss)
I1026 01:08:29.683535 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.059909 (* 1 = 0.059909 loss)
I1026 01:08:29.683540 17254 sgd_solver.cpp:106] Iteration 4020, lr = 0.001
I1026 01:08:30.790894 17254 solver.cpp:229] Iteration 4040, loss = 0.0721231
I1026 01:08:30.790925 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0558412 (* 1 = 0.0558412 loss)
I1026 01:08:30.790930 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0162819 (* 1 = 0.0162819 loss)
I1026 01:08:30.790935 17254 sgd_solver.cpp:106] Iteration 4040, lr = 0.001
I1026 01:08:31.884963 17254 solver.cpp:229] Iteration 4060, loss = 0.140887
I1026 01:08:31.884994 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0944368 (* 1 = 0.0944368 loss)
I1026 01:08:31.884999 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0464501 (* 1 = 0.0464501 loss)
I1026 01:08:31.885004 17254 sgd_solver.cpp:106] Iteration 4060, lr = 0.001
I1026 01:08:32.967277 17254 solver.cpp:229] Iteration 4080, loss = 0.0916455
I1026 01:08:32.967308 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0748102 (* 1 = 0.0748102 loss)
I1026 01:08:32.967313 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0168353 (* 1 = 0.0168353 loss)
I1026 01:08:32.967316 17254 sgd_solver.cpp:106] Iteration 4080, lr = 0.001
I1026 01:08:34.044592 17254 solver.cpp:229] Iteration 4100, loss = 0.20847
I1026 01:08:34.044622 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.106305 (* 1 = 0.106305 loss)
I1026 01:08:34.044626 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.102165 (* 1 = 0.102165 loss)
I1026 01:08:34.044631 17254 sgd_solver.cpp:106] Iteration 4100, lr = 0.001
I1026 01:08:35.150085 17254 solver.cpp:229] Iteration 4120, loss = 0.0965062
I1026 01:08:35.150127 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0463139 (* 1 = 0.0463139 loss)
I1026 01:08:35.150132 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0501923 (* 1 = 0.0501923 loss)
I1026 01:08:35.150136 17254 sgd_solver.cpp:106] Iteration 4120, lr = 0.001
I1026 01:08:36.254042 17254 solver.cpp:229] Iteration 4140, loss = 0.150219
I1026 01:08:36.254075 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0981525 (* 1 = 0.0981525 loss)
I1026 01:08:36.254079 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0520668 (* 1 = 0.0520668 loss)
I1026 01:08:36.254084 17254 sgd_solver.cpp:106] Iteration 4140, lr = 0.001
I1026 01:08:37.332842 17254 solver.cpp:229] Iteration 4160, loss = 0.117057
I1026 01:08:37.332875 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0680782 (* 1 = 0.0680782 loss)
I1026 01:08:37.332880 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0489784 (* 1 = 0.0489784 loss)
I1026 01:08:37.332883 17254 sgd_solver.cpp:106] Iteration 4160, lr = 0.001
I1026 01:08:38.450752 17254 solver.cpp:229] Iteration 4180, loss = 0.171501
I1026 01:08:38.450784 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0808122 (* 1 = 0.0808122 loss)
I1026 01:08:38.450789 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0906886 (* 1 = 0.0906886 loss)
I1026 01:08:38.450793 17254 sgd_solver.cpp:106] Iteration 4180, lr = 0.001
I1026 01:08:39.578106 17254 solver.cpp:229] Iteration 4200, loss = 0.090439
I1026 01:08:39.578140 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0468488 (* 1 = 0.0468488 loss)
I1026 01:08:39.578143 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0435902 (* 1 = 0.0435902 loss)
I1026 01:08:39.578148 17254 sgd_solver.cpp:106] Iteration 4200, lr = 0.001
I1026 01:08:40.654901 17254 solver.cpp:229] Iteration 4220, loss = 0.165078
I1026 01:08:40.654929 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0855849 (* 1 = 0.0855849 loss)
I1026 01:08:40.654933 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0794926 (* 1 = 0.0794926 loss)
I1026 01:08:40.654939 17254 sgd_solver.cpp:106] Iteration 4220, lr = 0.001
I1026 01:08:41.770556 17254 solver.cpp:229] Iteration 4240, loss = 0.148388
I1026 01:08:41.770587 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0614755 (* 1 = 0.0614755 loss)
I1026 01:08:41.770592 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0869121 (* 1 = 0.0869121 loss)
I1026 01:08:41.770597 17254 sgd_solver.cpp:106] Iteration 4240, lr = 0.001
I1026 01:08:42.898190 17254 solver.cpp:229] Iteration 4260, loss = 0.0828609
I1026 01:08:42.898232 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0401044 (* 1 = 0.0401044 loss)
I1026 01:08:42.898238 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0427564 (* 1 = 0.0427564 loss)
I1026 01:08:42.898243 17254 sgd_solver.cpp:106] Iteration 4260, lr = 0.001
I1026 01:08:44.012446 17254 solver.cpp:229] Iteration 4280, loss = 0.110227
I1026 01:08:44.012478 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0642517 (* 1 = 0.0642517 loss)
I1026 01:08:44.012485 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0459748 (* 1 = 0.0459748 loss)
I1026 01:08:44.012488 17254 sgd_solver.cpp:106] Iteration 4280, lr = 0.001
I1026 01:08:45.117619 17254 solver.cpp:229] Iteration 4300, loss = 0.0516749
I1026 01:08:45.117650 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0364353 (* 1 = 0.0364353 loss)
I1026 01:08:45.117657 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0152396 (* 1 = 0.0152396 loss)
I1026 01:08:45.117664 17254 sgd_solver.cpp:106] Iteration 4300, lr = 0.001
I1026 01:08:46.239287 17254 solver.cpp:229] Iteration 4320, loss = 0.157429
I1026 01:08:46.239322 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.111006 (* 1 = 0.111006 loss)
I1026 01:08:46.239329 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0464229 (* 1 = 0.0464229 loss)
I1026 01:08:46.239337 17254 sgd_solver.cpp:106] Iteration 4320, lr = 0.001
I1026 01:08:47.361090 17254 solver.cpp:229] Iteration 4340, loss = 0.153214
I1026 01:08:47.361125 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0601063 (* 1 = 0.0601063 loss)
I1026 01:08:47.361134 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0931076 (* 1 = 0.0931076 loss)
I1026 01:08:47.361140 17254 sgd_solver.cpp:106] Iteration 4340, lr = 0.001
I1026 01:08:48.434303 17254 solver.cpp:229] Iteration 4360, loss = 0.12988
I1026 01:08:48.434336 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0608617 (* 1 = 0.0608617 loss)
I1026 01:08:48.434345 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.069018 (* 1 = 0.069018 loss)
I1026 01:08:48.434351 17254 sgd_solver.cpp:106] Iteration 4360, lr = 0.001
I1026 01:08:49.548050 17254 solver.cpp:229] Iteration 4380, loss = 0.0867742
I1026 01:08:49.548082 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0497194 (* 1 = 0.0497194 loss)
I1026 01:08:49.548087 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0370548 (* 1 = 0.0370548 loss)
I1026 01:08:49.548092 17254 sgd_solver.cpp:106] Iteration 4380, lr = 0.001
I1026 01:08:50.650275 17254 solver.cpp:229] Iteration 4400, loss = 0.221622
I1026 01:08:50.650305 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.101569 (* 1 = 0.101569 loss)
I1026 01:08:50.650310 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.120053 (* 1 = 0.120053 loss)
I1026 01:08:50.650315 17254 sgd_solver.cpp:106] Iteration 4400, lr = 0.001
I1026 01:08:51.739759 17254 solver.cpp:229] Iteration 4420, loss = 0.109848
I1026 01:08:51.739790 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0613058 (* 1 = 0.0613058 loss)
I1026 01:08:51.739795 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0485427 (* 1 = 0.0485427 loss)
I1026 01:08:51.739800 17254 sgd_solver.cpp:106] Iteration 4420, lr = 0.001
I1026 01:08:52.853018 17254 solver.cpp:229] Iteration 4440, loss = 0.0690016
I1026 01:08:52.853050 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.040401 (* 1 = 0.040401 loss)
I1026 01:08:52.853056 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0286006 (* 1 = 0.0286006 loss)
I1026 01:08:52.853061 17254 sgd_solver.cpp:106] Iteration 4440, lr = 0.001
I1026 01:08:53.946560 17254 solver.cpp:229] Iteration 4460, loss = 0.191934
I1026 01:08:53.946591 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.120246 (* 1 = 0.120246 loss)
I1026 01:08:53.946596 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.071688 (* 1 = 0.071688 loss)
I1026 01:08:53.946601 17254 sgd_solver.cpp:106] Iteration 4460, lr = 0.001
I1026 01:08:55.059780 17254 solver.cpp:229] Iteration 4480, loss = 0.0734847
I1026 01:08:55.059813 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0480652 (* 1 = 0.0480652 loss)
I1026 01:08:55.059818 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0254195 (* 1 = 0.0254195 loss)
I1026 01:08:55.059823 17254 sgd_solver.cpp:106] Iteration 4480, lr = 0.001
I1026 01:08:56.168413 17254 solver.cpp:229] Iteration 4500, loss = 0.201716
I1026 01:08:56.168445 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.103063 (* 1 = 0.103063 loss)
I1026 01:08:56.168452 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0986532 (* 1 = 0.0986532 loss)
I1026 01:08:56.168457 17254 sgd_solver.cpp:106] Iteration 4500, lr = 0.001
I1026 01:08:57.277000 17254 solver.cpp:229] Iteration 4520, loss = 0.109125
I1026 01:08:57.277034 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.077231 (* 1 = 0.077231 loss)
I1026 01:08:57.277037 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0318944 (* 1 = 0.0318944 loss)
I1026 01:08:57.277042 17254 sgd_solver.cpp:106] Iteration 4520, lr = 0.001
I1026 01:08:58.390606 17254 solver.cpp:229] Iteration 4540, loss = 0.105519
I1026 01:08:58.390640 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0641431 (* 1 = 0.0641431 loss)
I1026 01:08:58.390645 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0413764 (* 1 = 0.0413764 loss)
I1026 01:08:58.390650 17254 sgd_solver.cpp:106] Iteration 4540, lr = 0.001
I1026 01:08:59.508352 17254 solver.cpp:229] Iteration 4560, loss = 0.124106
I1026 01:08:59.508384 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0470276 (* 1 = 0.0470276 loss)
I1026 01:08:59.508390 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.077078 (* 1 = 0.077078 loss)
I1026 01:08:59.508395 17254 sgd_solver.cpp:106] Iteration 4560, lr = 0.001
I1026 01:09:00.633621 17254 solver.cpp:229] Iteration 4580, loss = 0.156115
I1026 01:09:00.633651 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0537992 (* 1 = 0.0537992 loss)
I1026 01:09:00.633654 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.102316 (* 1 = 0.102316 loss)
I1026 01:09:00.633659 17254 sgd_solver.cpp:106] Iteration 4580, lr = 0.001
I1026 01:09:01.748647 17254 solver.cpp:229] Iteration 4600, loss = 0.132487
I1026 01:09:01.748679 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0427799 (* 1 = 0.0427799 loss)
I1026 01:09:01.748684 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.089707 (* 1 = 0.089707 loss)
I1026 01:09:01.748688 17254 sgd_solver.cpp:106] Iteration 4600, lr = 0.001
I1026 01:09:02.831003 17254 solver.cpp:229] Iteration 4620, loss = 0.185441
I1026 01:09:02.831033 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0725833 (* 1 = 0.0725833 loss)
I1026 01:09:02.831038 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.112857 (* 1 = 0.112857 loss)
I1026 01:09:02.831043 17254 sgd_solver.cpp:106] Iteration 4620, lr = 0.001
I1026 01:09:03.963953 17254 solver.cpp:229] Iteration 4640, loss = 0.107875
I1026 01:09:03.963984 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0544063 (* 1 = 0.0544063 loss)
I1026 01:09:03.963989 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0534682 (* 1 = 0.0534682 loss)
I1026 01:09:03.963994 17254 sgd_solver.cpp:106] Iteration 4640, lr = 0.001
I1026 01:09:05.063930 17254 solver.cpp:229] Iteration 4660, loss = 0.0844993
I1026 01:09:05.063959 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0389738 (* 1 = 0.0389738 loss)
I1026 01:09:05.063963 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0455255 (* 1 = 0.0455255 loss)
I1026 01:09:05.063968 17254 sgd_solver.cpp:106] Iteration 4660, lr = 0.001
I1026 01:09:06.170225 17254 solver.cpp:229] Iteration 4680, loss = 0.171351
I1026 01:09:06.170258 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0997502 (* 1 = 0.0997502 loss)
I1026 01:09:06.170264 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0716006 (* 1 = 0.0716006 loss)
I1026 01:09:06.170269 17254 sgd_solver.cpp:106] Iteration 4680, lr = 0.001
I1026 01:09:07.267745 17254 solver.cpp:229] Iteration 4700, loss = 0.135911
I1026 01:09:07.267777 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0974633 (* 1 = 0.0974633 loss)
I1026 01:09:07.267781 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0384476 (* 1 = 0.0384476 loss)
I1026 01:09:07.267786 17254 sgd_solver.cpp:106] Iteration 4700, lr = 0.001
I1026 01:09:08.379465 17254 solver.cpp:229] Iteration 4720, loss = 0.0646996
I1026 01:09:08.379498 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0492084 (* 1 = 0.0492084 loss)
I1026 01:09:08.379520 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0154912 (* 1 = 0.0154912 loss)
I1026 01:09:08.379525 17254 sgd_solver.cpp:106] Iteration 4720, lr = 0.001
I1026 01:09:09.481570 17254 solver.cpp:229] Iteration 4740, loss = 0.084741
I1026 01:09:09.481603 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.044479 (* 1 = 0.044479 loss)
I1026 01:09:09.481609 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.040262 (* 1 = 0.040262 loss)
I1026 01:09:09.481614 17254 sgd_solver.cpp:106] Iteration 4740, lr = 0.001
I1026 01:09:10.590560 17254 solver.cpp:229] Iteration 4760, loss = 0.156364
I1026 01:09:10.590592 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0837544 (* 1 = 0.0837544 loss)
I1026 01:09:10.590597 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.07261 (* 1 = 0.07261 loss)
I1026 01:09:10.590602 17254 sgd_solver.cpp:106] Iteration 4760, lr = 0.001
I1026 01:09:11.717658 17254 solver.cpp:229] Iteration 4780, loss = 0.149911
I1026 01:09:11.717701 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0751397 (* 1 = 0.0751397 loss)
I1026 01:09:11.717705 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0747714 (* 1 = 0.0747714 loss)
I1026 01:09:11.717710 17254 sgd_solver.cpp:106] Iteration 4780, lr = 0.001
I1026 01:09:12.820410 17254 solver.cpp:229] Iteration 4800, loss = 0.160116
I1026 01:09:12.820442 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0957485 (* 1 = 0.0957485 loss)
I1026 01:09:12.820458 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0643676 (* 1 = 0.0643676 loss)
I1026 01:09:12.820474 17254 sgd_solver.cpp:106] Iteration 4800, lr = 0.001
I1026 01:09:13.930557 17254 solver.cpp:229] Iteration 4820, loss = 0.200349
I1026 01:09:13.930588 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.12748 (* 1 = 0.12748 loss)
I1026 01:09:13.930593 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0728688 (* 1 = 0.0728688 loss)
I1026 01:09:13.930598 17254 sgd_solver.cpp:106] Iteration 4820, lr = 0.001
I1026 01:09:15.045503 17254 solver.cpp:229] Iteration 4840, loss = 0.143714
I1026 01:09:15.045534 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0836968 (* 1 = 0.0836968 loss)
I1026 01:09:15.045538 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0600175 (* 1 = 0.0600175 loss)
I1026 01:09:15.045543 17254 sgd_solver.cpp:106] Iteration 4840, lr = 0.001
I1026 01:09:16.157791 17254 solver.cpp:229] Iteration 4860, loss = 0.080133
I1026 01:09:16.157822 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0387542 (* 1 = 0.0387542 loss)
I1026 01:09:16.157826 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0413788 (* 1 = 0.0413788 loss)
I1026 01:09:16.157831 17254 sgd_solver.cpp:106] Iteration 4860, lr = 0.001
I1026 01:09:17.251492 17254 solver.cpp:229] Iteration 4880, loss = 0.101396
I1026 01:09:17.251524 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0742834 (* 1 = 0.0742834 loss)
I1026 01:09:17.251528 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0271127 (* 1 = 0.0271127 loss)
I1026 01:09:17.251533 17254 sgd_solver.cpp:106] Iteration 4880, lr = 0.001
I1026 01:09:18.356117 17254 solver.cpp:229] Iteration 4900, loss = 0.110189
I1026 01:09:18.356151 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0759857 (* 1 = 0.0759857 loss)
I1026 01:09:18.356156 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0342036 (* 1 = 0.0342036 loss)
I1026 01:09:18.356161 17254 sgd_solver.cpp:106] Iteration 4900, lr = 0.001
I1026 01:09:19.439354 17254 solver.cpp:229] Iteration 4920, loss = 0.117299
I1026 01:09:19.439388 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0679827 (* 1 = 0.0679827 loss)
I1026 01:09:19.439393 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0493161 (* 1 = 0.0493161 loss)
I1026 01:09:19.439398 17254 sgd_solver.cpp:106] Iteration 4920, lr = 0.001
I1026 01:09:20.531276 17254 solver.cpp:229] Iteration 4940, loss = 0.19526
I1026 01:09:20.531309 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.115865 (* 1 = 0.115865 loss)
I1026 01:09:20.531314 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0793949 (* 1 = 0.0793949 loss)
I1026 01:09:20.531319 17254 sgd_solver.cpp:106] Iteration 4940, lr = 0.001
I1026 01:09:21.618855 17254 solver.cpp:229] Iteration 4960, loss = 0.221229
I1026 01:09:21.618886 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.103286 (* 1 = 0.103286 loss)
I1026 01:09:21.618901 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.117943 (* 1 = 0.117943 loss)
I1026 01:09:21.618906 17254 sgd_solver.cpp:106] Iteration 4960, lr = 0.001
I1026 01:09:22.689615 17254 solver.cpp:229] Iteration 4980, loss = 0.14559
I1026 01:09:22.689648 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0909051 (* 1 = 0.0909051 loss)
I1026 01:09:22.689653 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0546845 (* 1 = 0.0546845 loss)
I1026 01:09:22.689658 17254 sgd_solver.cpp:106] Iteration 4980, lr = 0.001
I1026 01:09:23.788270 17254 solver.cpp:229] Iteration 5000, loss = 0.122154
I1026 01:09:23.788303 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0568618 (* 1 = 0.0568618 loss)
I1026 01:09:23.788308 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.065292 (* 1 = 0.065292 loss)
I1026 01:09:23.788314 17254 sgd_solver.cpp:106] Iteration 5000, lr = 0.001
I1026 01:09:24.900705 17254 solver.cpp:229] Iteration 5020, loss = 0.0820496
I1026 01:09:24.900737 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0633102 (* 1 = 0.0633102 loss)
I1026 01:09:24.900741 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0187395 (* 1 = 0.0187395 loss)
I1026 01:09:24.900746 17254 sgd_solver.cpp:106] Iteration 5020, lr = 0.001
I1026 01:09:25.992189 17254 solver.cpp:229] Iteration 5040, loss = 0.103816
I1026 01:09:25.992221 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0535036 (* 1 = 0.0535036 loss)
I1026 01:09:25.992226 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0503122 (* 1 = 0.0503122 loss)
I1026 01:09:25.992230 17254 sgd_solver.cpp:106] Iteration 5040, lr = 0.001
I1026 01:09:27.102859 17254 solver.cpp:229] Iteration 5060, loss = 0.151862
I1026 01:09:27.102890 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0628976 (* 1 = 0.0628976 loss)
I1026 01:09:27.102895 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0889642 (* 1 = 0.0889642 loss)
I1026 01:09:27.102900 17254 sgd_solver.cpp:106] Iteration 5060, lr = 0.001
I1026 01:09:28.212422 17254 solver.cpp:229] Iteration 5080, loss = 0.0925365
I1026 01:09:28.212455 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0664692 (* 1 = 0.0664692 loss)
I1026 01:09:28.212460 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0260673 (* 1 = 0.0260673 loss)
I1026 01:09:28.212465 17254 sgd_solver.cpp:106] Iteration 5080, lr = 0.001
I1026 01:09:29.326381 17254 solver.cpp:229] Iteration 5100, loss = 0.0546118
I1026 01:09:29.326416 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0373665 (* 1 = 0.0373665 loss)
I1026 01:09:29.326421 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0172453 (* 1 = 0.0172453 loss)
I1026 01:09:29.326426 17254 sgd_solver.cpp:106] Iteration 5100, lr = 0.001
I1026 01:09:30.415349 17254 solver.cpp:229] Iteration 5120, loss = 0.18843
I1026 01:09:30.415380 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0775557 (* 1 = 0.0775557 loss)
I1026 01:09:30.415385 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.110874 (* 1 = 0.110874 loss)
I1026 01:09:30.415390 17254 sgd_solver.cpp:106] Iteration 5120, lr = 0.001
I1026 01:09:31.524780 17254 solver.cpp:229] Iteration 5140, loss = 0.0645099
I1026 01:09:31.524812 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0335672 (* 1 = 0.0335672 loss)
I1026 01:09:31.524816 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0309427 (* 1 = 0.0309427 loss)
I1026 01:09:31.524822 17254 sgd_solver.cpp:106] Iteration 5140, lr = 0.001
I1026 01:09:32.609625 17254 solver.cpp:229] Iteration 5160, loss = 0.0988608
I1026 01:09:32.609655 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0464114 (* 1 = 0.0464114 loss)
I1026 01:09:32.609660 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0524493 (* 1 = 0.0524493 loss)
I1026 01:09:32.609664 17254 sgd_solver.cpp:106] Iteration 5160, lr = 0.001
I1026 01:09:33.694988 17254 solver.cpp:229] Iteration 5180, loss = 0.142783
I1026 01:09:33.695020 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0660069 (* 1 = 0.0660069 loss)
I1026 01:09:33.695024 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0767763 (* 1 = 0.0767763 loss)
I1026 01:09:33.695029 17254 sgd_solver.cpp:106] Iteration 5180, lr = 0.001
I1026 01:09:34.796543 17254 solver.cpp:229] Iteration 5200, loss = 0.160003
I1026 01:09:34.796576 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0687039 (* 1 = 0.0687039 loss)
I1026 01:09:34.796581 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0912988 (* 1 = 0.0912988 loss)
I1026 01:09:34.796586 17254 sgd_solver.cpp:106] Iteration 5200, lr = 0.001
I1026 01:09:35.916057 17254 solver.cpp:229] Iteration 5220, loss = 0.17326
I1026 01:09:35.916090 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0842272 (* 1 = 0.0842272 loss)
I1026 01:09:35.916095 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.089033 (* 1 = 0.089033 loss)
I1026 01:09:35.916098 17254 sgd_solver.cpp:106] Iteration 5220, lr = 0.001
I1026 01:09:37.022374 17254 solver.cpp:229] Iteration 5240, loss = 0.201676
I1026 01:09:37.022418 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0745606 (* 1 = 0.0745606 loss)
I1026 01:09:37.022423 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.127115 (* 1 = 0.127115 loss)
I1026 01:09:37.022426 17254 sgd_solver.cpp:106] Iteration 5240, lr = 0.001
I1026 01:09:38.114202 17254 solver.cpp:229] Iteration 5260, loss = 0.103886
I1026 01:09:38.114233 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0699425 (* 1 = 0.0699425 loss)
I1026 01:09:38.114238 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0339439 (* 1 = 0.0339439 loss)
I1026 01:09:38.114241 17254 sgd_solver.cpp:106] Iteration 5260, lr = 0.001
I1026 01:09:39.210474 17254 solver.cpp:229] Iteration 5280, loss = 0.0979417
I1026 01:09:39.210506 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0605072 (* 1 = 0.0605072 loss)
I1026 01:09:39.210511 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0374344 (* 1 = 0.0374344 loss)
I1026 01:09:39.210526 17254 sgd_solver.cpp:106] Iteration 5280, lr = 0.001
I1026 01:09:40.318723 17254 solver.cpp:229] Iteration 5300, loss = 0.153499
I1026 01:09:40.318755 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0981251 (* 1 = 0.0981251 loss)
I1026 01:09:40.318760 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0553742 (* 1 = 0.0553742 loss)
I1026 01:09:40.318768 17254 sgd_solver.cpp:106] Iteration 5300, lr = 0.001
I1026 01:09:41.420508 17254 solver.cpp:229] Iteration 5320, loss = 0.18635
I1026 01:09:41.420542 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0978774 (* 1 = 0.0978774 loss)
I1026 01:09:41.420547 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.088473 (* 1 = 0.088473 loss)
I1026 01:09:41.420552 17254 sgd_solver.cpp:106] Iteration 5320, lr = 0.001
I1026 01:09:42.516690 17254 solver.cpp:229] Iteration 5340, loss = 0.0830268
I1026 01:09:42.516721 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0459193 (* 1 = 0.0459193 loss)
I1026 01:09:42.516726 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0371075 (* 1 = 0.0371075 loss)
I1026 01:09:42.516732 17254 sgd_solver.cpp:106] Iteration 5340, lr = 0.001
I1026 01:09:43.621098 17254 solver.cpp:229] Iteration 5360, loss = 0.0896408
I1026 01:09:43.621129 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0500636 (* 1 = 0.0500636 loss)
I1026 01:09:43.621134 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0395772 (* 1 = 0.0395772 loss)
I1026 01:09:43.621140 17254 sgd_solver.cpp:106] Iteration 5360, lr = 0.001
I1026 01:09:44.736635 17254 solver.cpp:229] Iteration 5380, loss = 0.116138
I1026 01:09:44.736666 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0622422 (* 1 = 0.0622422 loss)
I1026 01:09:44.736671 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0538962 (* 1 = 0.0538962 loss)
I1026 01:09:44.736675 17254 sgd_solver.cpp:106] Iteration 5380, lr = 0.001
I1026 01:09:45.834650 17254 solver.cpp:229] Iteration 5400, loss = 0.237784
I1026 01:09:45.834719 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.114205 (* 1 = 0.114205 loss)
I1026 01:09:45.834744 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.123579 (* 1 = 0.123579 loss)
I1026 01:09:45.834764 17254 sgd_solver.cpp:106] Iteration 5400, lr = 0.001
I1026 01:09:46.932344 17254 solver.cpp:229] Iteration 5420, loss = 0.187164
I1026 01:09:46.932379 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.107701 (* 1 = 0.107701 loss)
I1026 01:09:46.932384 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.079463 (* 1 = 0.079463 loss)
I1026 01:09:46.932390 17254 sgd_solver.cpp:106] Iteration 5420, lr = 0.001
I1026 01:09:48.033624 17254 solver.cpp:229] Iteration 5440, loss = 0.293991
I1026 01:09:48.033659 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.132764 (* 1 = 0.132764 loss)
I1026 01:09:48.033668 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.161227 (* 1 = 0.161227 loss)
I1026 01:09:48.033674 17254 sgd_solver.cpp:106] Iteration 5440, lr = 0.001
I1026 01:09:49.132633 17254 solver.cpp:229] Iteration 5460, loss = 0.204689
I1026 01:09:49.132666 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0953141 (* 1 = 0.0953141 loss)
I1026 01:09:49.132673 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.109375 (* 1 = 0.109375 loss)
I1026 01:09:49.132679 17254 sgd_solver.cpp:106] Iteration 5460, lr = 0.001
I1026 01:09:50.250527 17254 solver.cpp:229] Iteration 5480, loss = 0.122011
I1026 01:09:50.250561 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0646738 (* 1 = 0.0646738 loss)
I1026 01:09:50.250566 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0573369 (* 1 = 0.0573369 loss)
I1026 01:09:50.250571 17254 sgd_solver.cpp:106] Iteration 5480, lr = 0.001
I1026 01:09:51.357029 17254 solver.cpp:229] Iteration 5500, loss = 0.0778068
I1026 01:09:51.357062 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0433596 (* 1 = 0.0433596 loss)
I1026 01:09:51.357067 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0344471 (* 1 = 0.0344471 loss)
I1026 01:09:51.357082 17254 sgd_solver.cpp:106] Iteration 5500, lr = 0.001
I1026 01:09:52.472451 17254 solver.cpp:229] Iteration 5520, loss = 0.0809495
I1026 01:09:52.472484 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0288523 (* 1 = 0.0288523 loss)
I1026 01:09:52.472489 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0520972 (* 1 = 0.0520972 loss)
I1026 01:09:52.472494 17254 sgd_solver.cpp:106] Iteration 5520, lr = 0.001
I1026 01:09:53.571651 17254 solver.cpp:229] Iteration 5540, loss = 0.111826
I1026 01:09:53.571683 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.057363 (* 1 = 0.057363 loss)
I1026 01:09:53.571688 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0544632 (* 1 = 0.0544632 loss)
I1026 01:09:53.571693 17254 sgd_solver.cpp:106] Iteration 5540, lr = 0.001
I1026 01:09:54.666579 17254 solver.cpp:229] Iteration 5560, loss = 0.0615838
I1026 01:09:54.666610 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0367568 (* 1 = 0.0367568 loss)
I1026 01:09:54.666615 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.024827 (* 1 = 0.024827 loss)
I1026 01:09:54.666620 17254 sgd_solver.cpp:106] Iteration 5560, lr = 0.001
I1026 01:09:55.779731 17254 solver.cpp:229] Iteration 5580, loss = 0.10618
I1026 01:09:55.779764 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0709685 (* 1 = 0.0709685 loss)
I1026 01:09:55.779769 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0352113 (* 1 = 0.0352113 loss)
I1026 01:09:55.779783 17254 sgd_solver.cpp:106] Iteration 5580, lr = 0.001
I1026 01:09:56.874126 17254 solver.cpp:229] Iteration 5600, loss = 0.124746
I1026 01:09:56.874160 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0789663 (* 1 = 0.0789663 loss)
I1026 01:09:56.874163 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.04578 (* 1 = 0.04578 loss)
I1026 01:09:56.874168 17254 sgd_solver.cpp:106] Iteration 5600, lr = 0.001
I1026 01:09:57.977906 17254 solver.cpp:229] Iteration 5620, loss = 0.210325
I1026 01:09:57.977938 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.137056 (* 1 = 0.137056 loss)
I1026 01:09:57.977946 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0732685 (* 1 = 0.0732685 loss)
I1026 01:09:57.977960 17254 sgd_solver.cpp:106] Iteration 5620, lr = 0.001
I1026 01:09:59.091815 17254 solver.cpp:229] Iteration 5640, loss = 0.141763
I1026 01:09:59.091848 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.082333 (* 1 = 0.082333 loss)
I1026 01:09:59.091852 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0594301 (* 1 = 0.0594301 loss)
I1026 01:09:59.091857 17254 sgd_solver.cpp:106] Iteration 5640, lr = 0.001
I1026 01:10:00.191964 17254 solver.cpp:229] Iteration 5660, loss = 0.0748642
I1026 01:10:00.191997 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0521174 (* 1 = 0.0521174 loss)
I1026 01:10:00.192001 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0227468 (* 1 = 0.0227468 loss)
I1026 01:10:00.192005 17254 sgd_solver.cpp:106] Iteration 5660, lr = 0.001
I1026 01:10:01.309767 17254 solver.cpp:229] Iteration 5680, loss = 0.104287
I1026 01:10:01.309800 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0661731 (* 1 = 0.0661731 loss)
I1026 01:10:01.309805 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0381135 (* 1 = 0.0381135 loss)
I1026 01:10:01.309810 17254 sgd_solver.cpp:106] Iteration 5680, lr = 0.001
I1026 01:10:02.421802 17254 solver.cpp:229] Iteration 5700, loss = 0.134746
I1026 01:10:02.421836 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0842285 (* 1 = 0.0842285 loss)
I1026 01:10:02.421841 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0505173 (* 1 = 0.0505173 loss)
I1026 01:10:02.421845 17254 sgd_solver.cpp:106] Iteration 5700, lr = 0.001
I1026 01:10:03.514266 17254 solver.cpp:229] Iteration 5720, loss = 0.100494
I1026 01:10:03.514299 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0560425 (* 1 = 0.0560425 loss)
I1026 01:10:03.514303 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0444518 (* 1 = 0.0444518 loss)
I1026 01:10:03.514307 17254 sgd_solver.cpp:106] Iteration 5720, lr = 0.001
I1026 01:10:04.619402 17254 solver.cpp:229] Iteration 5740, loss = 0.103305
I1026 01:10:04.619438 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.063428 (* 1 = 0.063428 loss)
I1026 01:10:04.619444 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.039877 (* 1 = 0.039877 loss)
I1026 01:10:04.619448 17254 sgd_solver.cpp:106] Iteration 5740, lr = 0.001
I1026 01:10:05.723412 17254 solver.cpp:229] Iteration 5760, loss = 0.0941958
I1026 01:10:05.723448 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0737879 (* 1 = 0.0737879 loss)
I1026 01:10:05.723453 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0204078 (* 1 = 0.0204078 loss)
I1026 01:10:05.723459 17254 sgd_solver.cpp:106] Iteration 5760, lr = 0.001
I1026 01:10:06.837079 17254 solver.cpp:229] Iteration 5780, loss = 0.137319
I1026 01:10:06.837110 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0553481 (* 1 = 0.0553481 loss)
I1026 01:10:06.837116 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0819709 (* 1 = 0.0819709 loss)
I1026 01:10:06.837121 17254 sgd_solver.cpp:106] Iteration 5780, lr = 0.001
I1026 01:10:07.936666 17254 solver.cpp:229] Iteration 5800, loss = 0.199349
I1026 01:10:07.936697 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.104619 (* 1 = 0.104619 loss)
I1026 01:10:07.936702 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0947295 (* 1 = 0.0947295 loss)
I1026 01:10:07.936707 17254 sgd_solver.cpp:106] Iteration 5800, lr = 0.001
I1026 01:10:09.043581 17254 solver.cpp:229] Iteration 5820, loss = 0.097993
I1026 01:10:09.043614 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0685833 (* 1 = 0.0685833 loss)
I1026 01:10:09.043619 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0294096 (* 1 = 0.0294096 loss)
I1026 01:10:09.043624 17254 sgd_solver.cpp:106] Iteration 5820, lr = 0.001
I1026 01:10:10.167161 17254 solver.cpp:229] Iteration 5840, loss = 0.106522
I1026 01:10:10.167194 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0598507 (* 1 = 0.0598507 loss)
I1026 01:10:10.167199 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0466712 (* 1 = 0.0466712 loss)
I1026 01:10:10.167204 17254 sgd_solver.cpp:106] Iteration 5840, lr = 0.001
I1026 01:10:11.255257 17254 solver.cpp:229] Iteration 5860, loss = 0.0798823
I1026 01:10:11.255290 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0299192 (* 1 = 0.0299192 loss)
I1026 01:10:11.255295 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.049963 (* 1 = 0.049963 loss)
I1026 01:10:11.255300 17254 sgd_solver.cpp:106] Iteration 5860, lr = 0.001
I1026 01:10:12.355343 17254 solver.cpp:229] Iteration 5880, loss = 0.225361
I1026 01:10:12.355376 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0837663 (* 1 = 0.0837663 loss)
I1026 01:10:12.355382 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.141594 (* 1 = 0.141594 loss)
I1026 01:10:12.355387 17254 sgd_solver.cpp:106] Iteration 5880, lr = 0.001
I1026 01:10:13.453830 17254 solver.cpp:229] Iteration 5900, loss = 0.104955
I1026 01:10:13.453861 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0351439 (* 1 = 0.0351439 loss)
I1026 01:10:13.453866 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0698114 (* 1 = 0.0698114 loss)
I1026 01:10:13.453871 17254 sgd_solver.cpp:106] Iteration 5900, lr = 0.001
I1026 01:10:14.582264 17254 solver.cpp:229] Iteration 5920, loss = 0.211663
I1026 01:10:14.582298 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.115794 (* 1 = 0.115794 loss)
I1026 01:10:14.582303 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0958685 (* 1 = 0.0958685 loss)
I1026 01:10:14.582307 17254 sgd_solver.cpp:106] Iteration 5920, lr = 0.001
I1026 01:10:15.697558 17254 solver.cpp:229] Iteration 5940, loss = 0.126099
I1026 01:10:15.697592 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.10322 (* 1 = 0.10322 loss)
I1026 01:10:15.697598 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0228796 (* 1 = 0.0228796 loss)
I1026 01:10:15.697613 17254 sgd_solver.cpp:106] Iteration 5940, lr = 0.001
I1026 01:10:16.783273 17254 solver.cpp:229] Iteration 5960, loss = 0.0953486
I1026 01:10:16.783306 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.048269 (* 1 = 0.048269 loss)
I1026 01:10:16.783311 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0470796 (* 1 = 0.0470796 loss)
I1026 01:10:16.783315 17254 sgd_solver.cpp:106] Iteration 5960, lr = 0.001
I1026 01:10:17.887046 17254 solver.cpp:229] Iteration 5980, loss = 0.132806
I1026 01:10:17.887089 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0517458 (* 1 = 0.0517458 loss)
I1026 01:10:17.887094 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0810607 (* 1 = 0.0810607 loss)
I1026 01:10:17.887099 17254 sgd_solver.cpp:106] Iteration 5980, lr = 0.001
I1026 01:10:18.997128 17254 solver.cpp:229] Iteration 6000, loss = 0.150548
I1026 01:10:18.997159 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.066259 (* 1 = 0.066259 loss)
I1026 01:10:18.997164 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0842888 (* 1 = 0.0842888 loss)
I1026 01:10:18.997169 17254 sgd_solver.cpp:106] Iteration 6000, lr = 0.001
I1026 01:10:20.084084 17254 solver.cpp:229] Iteration 6020, loss = 0.119628
I1026 01:10:20.084116 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0425021 (* 1 = 0.0425021 loss)
I1026 01:10:20.084122 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0771256 (* 1 = 0.0771256 loss)
I1026 01:10:20.084136 17254 sgd_solver.cpp:106] Iteration 6020, lr = 0.001
I1026 01:10:21.185179 17254 solver.cpp:229] Iteration 6040, loss = 0.123541
I1026 01:10:21.185211 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.106125 (* 1 = 0.106125 loss)
I1026 01:10:21.185226 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0174163 (* 1 = 0.0174163 loss)
I1026 01:10:21.185231 17254 sgd_solver.cpp:106] Iteration 6040, lr = 0.001
I1026 01:10:22.268782 17254 solver.cpp:229] Iteration 6060, loss = 0.168335
I1026 01:10:22.268826 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0786181 (* 1 = 0.0786181 loss)
I1026 01:10:22.268832 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0897165 (* 1 = 0.0897165 loss)
I1026 01:10:22.268837 17254 sgd_solver.cpp:106] Iteration 6060, lr = 0.001
I1026 01:10:23.369146 17254 solver.cpp:229] Iteration 6080, loss = 0.139573
I1026 01:10:23.369179 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0697674 (* 1 = 0.0697674 loss)
I1026 01:10:23.369184 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0698055 (* 1 = 0.0698055 loss)
I1026 01:10:23.369189 17254 sgd_solver.cpp:106] Iteration 6080, lr = 0.001
I1026 01:10:24.438185 17254 solver.cpp:229] Iteration 6100, loss = 0.149627
I1026 01:10:24.438218 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0689653 (* 1 = 0.0689653 loss)
I1026 01:10:24.438222 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0806613 (* 1 = 0.0806613 loss)
I1026 01:10:24.438227 17254 sgd_solver.cpp:106] Iteration 6100, lr = 0.001
I1026 01:10:25.533186 17254 solver.cpp:229] Iteration 6120, loss = 0.0916932
I1026 01:10:25.533219 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0378604 (* 1 = 0.0378604 loss)
I1026 01:10:25.533224 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0538327 (* 1 = 0.0538327 loss)
I1026 01:10:25.533229 17254 sgd_solver.cpp:106] Iteration 6120, lr = 0.001
I1026 01:10:26.632081 17254 solver.cpp:229] Iteration 6140, loss = 0.12787
I1026 01:10:26.632109 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0921946 (* 1 = 0.0921946 loss)
I1026 01:10:26.632114 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.035675 (* 1 = 0.035675 loss)
I1026 01:10:26.632118 17254 sgd_solver.cpp:106] Iteration 6140, lr = 0.001
I1026 01:10:27.713698 17254 solver.cpp:229] Iteration 6160, loss = 0.133068
I1026 01:10:27.713731 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0368012 (* 1 = 0.0368012 loss)
I1026 01:10:27.713735 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0962664 (* 1 = 0.0962664 loss)
I1026 01:10:27.713740 17254 sgd_solver.cpp:106] Iteration 6160, lr = 0.001
I1026 01:10:28.815912 17254 solver.cpp:229] Iteration 6180, loss = 0.0662735
I1026 01:10:28.815943 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0503974 (* 1 = 0.0503974 loss)
I1026 01:10:28.815948 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0158761 (* 1 = 0.0158761 loss)
I1026 01:10:28.815953 17254 sgd_solver.cpp:106] Iteration 6180, lr = 0.001
I1026 01:10:29.896039 17254 solver.cpp:229] Iteration 6200, loss = 0.174104
I1026 01:10:29.896070 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0933895 (* 1 = 0.0933895 loss)
I1026 01:10:29.896075 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0807141 (* 1 = 0.0807141 loss)
I1026 01:10:29.896080 17254 sgd_solver.cpp:106] Iteration 6200, lr = 0.001
I1026 01:10:31.003901 17254 solver.cpp:229] Iteration 6220, loss = 0.186968
I1026 01:10:31.003933 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0584795 (* 1 = 0.0584795 loss)
I1026 01:10:31.003938 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.128489 (* 1 = 0.128489 loss)
I1026 01:10:31.003953 17254 sgd_solver.cpp:106] Iteration 6220, lr = 0.001
I1026 01:10:32.104657 17254 solver.cpp:229] Iteration 6240, loss = 0.0632933
I1026 01:10:32.104691 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0532933 (* 1 = 0.0532933 loss)
I1026 01:10:32.104694 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00999998 (* 1 = 0.00999998 loss)
I1026 01:10:32.104709 17254 sgd_solver.cpp:106] Iteration 6240, lr = 0.001
I1026 01:10:33.206499 17254 solver.cpp:229] Iteration 6260, loss = 0.083969
I1026 01:10:33.206532 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.057973 (* 1 = 0.057973 loss)
I1026 01:10:33.206537 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0259959 (* 1 = 0.0259959 loss)
I1026 01:10:33.206542 17254 sgd_solver.cpp:106] Iteration 6260, lr = 0.001
I1026 01:10:34.321377 17254 solver.cpp:229] Iteration 6280, loss = 0.0571355
I1026 01:10:34.321404 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0375361 (* 1 = 0.0375361 loss)
I1026 01:10:34.321408 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0195994 (* 1 = 0.0195994 loss)
I1026 01:10:34.321413 17254 sgd_solver.cpp:106] Iteration 6280, lr = 0.001
I1026 01:10:35.411727 17254 solver.cpp:229] Iteration 6300, loss = 0.309971
I1026 01:10:35.411761 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.122631 (* 1 = 0.122631 loss)
I1026 01:10:35.411766 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.18734 (* 1 = 0.18734 loss)
I1026 01:10:35.411769 17254 sgd_solver.cpp:106] Iteration 6300, lr = 0.001
I1026 01:10:36.509982 17254 solver.cpp:229] Iteration 6320, loss = 0.0532099
I1026 01:10:36.510015 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0364863 (* 1 = 0.0364863 loss)
I1026 01:10:36.510018 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0167236 (* 1 = 0.0167236 loss)
I1026 01:10:36.510023 17254 sgd_solver.cpp:106] Iteration 6320, lr = 0.001
I1026 01:10:37.626436 17254 solver.cpp:229] Iteration 6340, loss = 0.103231
I1026 01:10:37.626468 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0315305 (* 1 = 0.0315305 loss)
I1026 01:10:37.626472 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0717007 (* 1 = 0.0717007 loss)
I1026 01:10:37.626477 17254 sgd_solver.cpp:106] Iteration 6340, lr = 0.001
I1026 01:10:38.754194 17254 solver.cpp:229] Iteration 6360, loss = 0.0897019
I1026 01:10:38.754227 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.066358 (* 1 = 0.066358 loss)
I1026 01:10:38.754231 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.023344 (* 1 = 0.023344 loss)
I1026 01:10:38.754235 17254 sgd_solver.cpp:106] Iteration 6360, lr = 0.001
I1026 01:10:39.845754 17254 solver.cpp:229] Iteration 6380, loss = 0.144031
I1026 01:10:39.845785 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0556074 (* 1 = 0.0556074 loss)
I1026 01:10:39.845790 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0884233 (* 1 = 0.0884233 loss)
I1026 01:10:39.845795 17254 sgd_solver.cpp:106] Iteration 6380, lr = 0.001
I1026 01:10:40.960350 17254 solver.cpp:229] Iteration 6400, loss = 0.0952581
I1026 01:10:40.960382 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.044547 (* 1 = 0.044547 loss)
I1026 01:10:40.960387 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0507111 (* 1 = 0.0507111 loss)
I1026 01:10:40.960391 17254 sgd_solver.cpp:106] Iteration 6400, lr = 0.001
I1026 01:10:42.065615 17254 solver.cpp:229] Iteration 6420, loss = 0.100681
I1026 01:10:42.065649 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0733812 (* 1 = 0.0733812 loss)
I1026 01:10:42.065652 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0273002 (* 1 = 0.0273002 loss)
I1026 01:10:42.065657 17254 sgd_solver.cpp:106] Iteration 6420, lr = 0.001
I1026 01:10:43.148212 17254 solver.cpp:229] Iteration 6440, loss = 0.101051
I1026 01:10:43.148247 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0510021 (* 1 = 0.0510021 loss)
I1026 01:10:43.148253 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0500491 (* 1 = 0.0500491 loss)
I1026 01:10:43.148259 17254 sgd_solver.cpp:106] Iteration 6440, lr = 0.001
I1026 01:10:44.258518 17254 solver.cpp:229] Iteration 6460, loss = 0.0906878
I1026 01:10:44.258563 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0503058 (* 1 = 0.0503058 loss)
I1026 01:10:44.258568 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.040382 (* 1 = 0.040382 loss)
I1026 01:10:44.258572 17254 sgd_solver.cpp:106] Iteration 6460, lr = 0.001
I1026 01:10:45.322741 17254 solver.cpp:229] Iteration 6480, loss = 0.220348
I1026 01:10:45.322773 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0868831 (* 1 = 0.0868831 loss)
I1026 01:10:45.322777 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.133465 (* 1 = 0.133465 loss)
I1026 01:10:45.322782 17254 sgd_solver.cpp:106] Iteration 6480, lr = 0.001
I1026 01:10:46.435052 17254 solver.cpp:229] Iteration 6500, loss = 0.10053
I1026 01:10:46.435082 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0581288 (* 1 = 0.0581288 loss)
I1026 01:10:46.435086 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0424017 (* 1 = 0.0424017 loss)
I1026 01:10:46.435091 17254 sgd_solver.cpp:106] Iteration 6500, lr = 0.001
I1026 01:10:47.532496 17254 solver.cpp:229] Iteration 6520, loss = 0.0570563
I1026 01:10:47.532528 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0460237 (* 1 = 0.0460237 loss)
I1026 01:10:47.532533 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0110327 (* 1 = 0.0110327 loss)
I1026 01:10:47.532537 17254 sgd_solver.cpp:106] Iteration 6520, lr = 0.001
I1026 01:10:48.634855 17254 solver.cpp:229] Iteration 6540, loss = 0.104471
I1026 01:10:48.634883 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0709591 (* 1 = 0.0709591 loss)
I1026 01:10:48.634888 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0335118 (* 1 = 0.0335118 loss)
I1026 01:10:48.634892 17254 sgd_solver.cpp:106] Iteration 6540, lr = 0.001
I1026 01:10:49.740264 17254 solver.cpp:229] Iteration 6560, loss = 0.15879
I1026 01:10:49.740295 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0645135 (* 1 = 0.0645135 loss)
I1026 01:10:49.740300 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0942766 (* 1 = 0.0942766 loss)
I1026 01:10:49.740304 17254 sgd_solver.cpp:106] Iteration 6560, lr = 0.001
I1026 01:10:50.859220 17254 solver.cpp:229] Iteration 6580, loss = 0.0895986
I1026 01:10:50.859251 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0517577 (* 1 = 0.0517577 loss)
I1026 01:10:50.859256 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0378409 (* 1 = 0.0378409 loss)
I1026 01:10:50.859261 17254 sgd_solver.cpp:106] Iteration 6580, lr = 0.001
I1026 01:10:51.961539 17254 solver.cpp:229] Iteration 6600, loss = 0.0996647
I1026 01:10:51.961571 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0554243 (* 1 = 0.0554243 loss)
I1026 01:10:51.961575 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0442404 (* 1 = 0.0442404 loss)
I1026 01:10:51.961580 17254 sgd_solver.cpp:106] Iteration 6600, lr = 0.001
I1026 01:10:53.085364 17254 solver.cpp:229] Iteration 6620, loss = 0.238965
I1026 01:10:53.085397 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.10583 (* 1 = 0.10583 loss)
I1026 01:10:53.085402 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.133134 (* 1 = 0.133134 loss)
I1026 01:10:53.085407 17254 sgd_solver.cpp:106] Iteration 6620, lr = 0.001
I1026 01:10:54.199419 17254 solver.cpp:229] Iteration 6640, loss = 0.101076
I1026 01:10:54.199462 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0711393 (* 1 = 0.0711393 loss)
I1026 01:10:54.199467 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.029937 (* 1 = 0.029937 loss)
I1026 01:10:54.199471 17254 sgd_solver.cpp:106] Iteration 6640, lr = 0.001
I1026 01:10:55.318459 17254 solver.cpp:229] Iteration 6660, loss = 0.0556703
I1026 01:10:55.318490 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.050304 (* 1 = 0.050304 loss)
I1026 01:10:55.318495 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0053663 (* 1 = 0.0053663 loss)
I1026 01:10:55.318500 17254 sgd_solver.cpp:106] Iteration 6660, lr = 0.001
I1026 01:10:56.448249 17254 solver.cpp:229] Iteration 6680, loss = 0.135985
I1026 01:10:56.448279 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0892417 (* 1 = 0.0892417 loss)
I1026 01:10:56.448284 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0467433 (* 1 = 0.0467433 loss)
I1026 01:10:56.448288 17254 sgd_solver.cpp:106] Iteration 6680, lr = 0.001
I1026 01:10:57.550464 17254 solver.cpp:229] Iteration 6700, loss = 0.120694
I1026 01:10:57.550495 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0680363 (* 1 = 0.0680363 loss)
I1026 01:10:57.550500 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0526578 (* 1 = 0.0526578 loss)
I1026 01:10:57.550505 17254 sgd_solver.cpp:106] Iteration 6700, lr = 0.001
I1026 01:10:58.655179 17254 solver.cpp:229] Iteration 6720, loss = 0.106083
I1026 01:10:58.655208 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0633987 (* 1 = 0.0633987 loss)
I1026 01:10:58.655213 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.042684 (* 1 = 0.042684 loss)
I1026 01:10:58.655217 17254 sgd_solver.cpp:106] Iteration 6720, lr = 0.001
I1026 01:10:59.778442 17254 solver.cpp:229] Iteration 6740, loss = 0.0910043
I1026 01:10:59.778476 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0721898 (* 1 = 0.0721898 loss)
I1026 01:10:59.778483 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0188146 (* 1 = 0.0188146 loss)
I1026 01:10:59.778489 17254 sgd_solver.cpp:106] Iteration 6740, lr = 0.001
I1026 01:11:00.865885 17254 solver.cpp:229] Iteration 6760, loss = 0.109006
I1026 01:11:00.865919 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0458201 (* 1 = 0.0458201 loss)
I1026 01:11:00.865926 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0631861 (* 1 = 0.0631861 loss)
I1026 01:11:00.865931 17254 sgd_solver.cpp:106] Iteration 6760, lr = 0.001
I1026 01:11:01.943184 17254 solver.cpp:229] Iteration 6780, loss = 0.195044
I1026 01:11:01.943217 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.106675 (* 1 = 0.106675 loss)
I1026 01:11:01.943224 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0883689 (* 1 = 0.0883689 loss)
I1026 01:11:01.943230 17254 sgd_solver.cpp:106] Iteration 6780, lr = 0.001
I1026 01:11:03.060727 17254 solver.cpp:229] Iteration 6800, loss = 0.0653697
I1026 01:11:03.060760 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0417134 (* 1 = 0.0417134 loss)
I1026 01:11:03.060765 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0236563 (* 1 = 0.0236563 loss)
I1026 01:11:03.060768 17254 sgd_solver.cpp:106] Iteration 6800, lr = 0.001
I1026 01:11:04.149981 17254 solver.cpp:229] Iteration 6820, loss = 0.172307
I1026 01:11:04.150014 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.130676 (* 1 = 0.130676 loss)
I1026 01:11:04.150019 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0416311 (* 1 = 0.0416311 loss)
I1026 01:11:04.150023 17254 sgd_solver.cpp:106] Iteration 6820, lr = 0.001
I1026 01:11:05.263857 17254 solver.cpp:229] Iteration 6840, loss = 0.106928
I1026 01:11:05.263890 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0608224 (* 1 = 0.0608224 loss)
I1026 01:11:05.263895 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.046106 (* 1 = 0.046106 loss)
I1026 01:11:05.263898 17254 sgd_solver.cpp:106] Iteration 6840, lr = 0.001
I1026 01:11:06.358731 17254 solver.cpp:229] Iteration 6860, loss = 0.0694761
I1026 01:11:06.358759 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0456243 (* 1 = 0.0456243 loss)
I1026 01:11:06.358763 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0238517 (* 1 = 0.0238517 loss)
I1026 01:11:06.358768 17254 sgd_solver.cpp:106] Iteration 6860, lr = 0.001
I1026 01:11:07.451454 17254 solver.cpp:229] Iteration 6880, loss = 0.056771
I1026 01:11:07.451488 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0407706 (* 1 = 0.0407706 loss)
I1026 01:11:07.451491 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0160003 (* 1 = 0.0160003 loss)
I1026 01:11:07.451495 17254 sgd_solver.cpp:106] Iteration 6880, lr = 0.001
I1026 01:11:08.568588 17254 solver.cpp:229] Iteration 6900, loss = 0.179641
I1026 01:11:08.568621 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0825144 (* 1 = 0.0825144 loss)
I1026 01:11:08.568629 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0971271 (* 1 = 0.0971271 loss)
I1026 01:11:08.568634 17254 sgd_solver.cpp:106] Iteration 6900, lr = 0.001
I1026 01:11:09.689898 17254 solver.cpp:229] Iteration 6920, loss = 0.15979
I1026 01:11:09.689929 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0702241 (* 1 = 0.0702241 loss)
I1026 01:11:09.689934 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0895661 (* 1 = 0.0895661 loss)
I1026 01:11:09.689939 17254 sgd_solver.cpp:106] Iteration 6920, lr = 0.001
I1026 01:11:10.809782 17254 solver.cpp:229] Iteration 6940, loss = 0.0975651
I1026 01:11:10.809816 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0791224 (* 1 = 0.0791224 loss)
I1026 01:11:10.809823 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0184427 (* 1 = 0.0184427 loss)
I1026 01:11:10.809829 17254 sgd_solver.cpp:106] Iteration 6940, lr = 0.001
I1026 01:11:11.922911 17254 solver.cpp:229] Iteration 6960, loss = 0.124367
I1026 01:11:11.922945 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0911286 (* 1 = 0.0911286 loss)
I1026 01:11:11.922948 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0332381 (* 1 = 0.0332381 loss)
I1026 01:11:11.922955 17254 sgd_solver.cpp:106] Iteration 6960, lr = 0.001
I1026 01:11:13.057201 17254 solver.cpp:229] Iteration 6980, loss = 0.134862
I1026 01:11:13.057235 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0725244 (* 1 = 0.0725244 loss)
I1026 01:11:13.057241 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0623376 (* 1 = 0.0623376 loss)
I1026 01:11:13.057246 17254 sgd_solver.cpp:106] Iteration 6980, lr = 0.001
I1026 01:11:14.152660 17254 solver.cpp:229] Iteration 7000, loss = 0.155326
I1026 01:11:14.152694 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.072951 (* 1 = 0.072951 loss)
I1026 01:11:14.152698 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.082375 (* 1 = 0.082375 loss)
I1026 01:11:14.152704 17254 sgd_solver.cpp:106] Iteration 7000, lr = 0.001
I1026 01:11:15.249572 17254 solver.cpp:229] Iteration 7020, loss = 0.130536
I1026 01:11:15.249603 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0800733 (* 1 = 0.0800733 loss)
I1026 01:11:15.249625 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0504631 (* 1 = 0.0504631 loss)
I1026 01:11:15.249632 17254 sgd_solver.cpp:106] Iteration 7020, lr = 0.001
I1026 01:11:16.334995 17254 solver.cpp:229] Iteration 7040, loss = 0.129198
I1026 01:11:16.335026 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0476955 (* 1 = 0.0476955 loss)
I1026 01:11:16.335031 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.081503 (* 1 = 0.081503 loss)
I1026 01:11:16.335034 17254 sgd_solver.cpp:106] Iteration 7040, lr = 0.001
I1026 01:11:17.428313 17254 solver.cpp:229] Iteration 7060, loss = 0.134143
I1026 01:11:17.428346 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0829111 (* 1 = 0.0829111 loss)
I1026 01:11:17.428350 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0512314 (* 1 = 0.0512314 loss)
I1026 01:11:17.428355 17254 sgd_solver.cpp:106] Iteration 7060, lr = 0.001
I1026 01:11:18.513038 17254 solver.cpp:229] Iteration 7080, loss = 0.112758
I1026 01:11:18.513072 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0542724 (* 1 = 0.0542724 loss)
I1026 01:11:18.513075 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0584861 (* 1 = 0.0584861 loss)
I1026 01:11:18.513080 17254 sgd_solver.cpp:106] Iteration 7080, lr = 0.001
I1026 01:11:19.606178 17254 solver.cpp:229] Iteration 7100, loss = 0.117925
I1026 01:11:19.606211 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0333159 (* 1 = 0.0333159 loss)
I1026 01:11:19.606215 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0846092 (* 1 = 0.0846092 loss)
I1026 01:11:19.606220 17254 sgd_solver.cpp:106] Iteration 7100, lr = 0.001
I1026 01:11:20.718425 17254 solver.cpp:229] Iteration 7120, loss = 0.145647
I1026 01:11:20.718457 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0770016 (* 1 = 0.0770016 loss)
I1026 01:11:20.718461 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0686457 (* 1 = 0.0686457 loss)
I1026 01:11:20.718466 17254 sgd_solver.cpp:106] Iteration 7120, lr = 0.001
I1026 01:11:21.812667 17254 solver.cpp:229] Iteration 7140, loss = 0.114015
I1026 01:11:21.812700 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0553812 (* 1 = 0.0553812 loss)
I1026 01:11:21.812703 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0586338 (* 1 = 0.0586338 loss)
I1026 01:11:21.812708 17254 sgd_solver.cpp:106] Iteration 7140, lr = 0.001
I1026 01:11:22.907246 17254 solver.cpp:229] Iteration 7160, loss = 0.14996
I1026 01:11:22.907279 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.081013 (* 1 = 0.081013 loss)
I1026 01:11:22.907284 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0689468 (* 1 = 0.0689468 loss)
I1026 01:11:22.907287 17254 sgd_solver.cpp:106] Iteration 7160, lr = 0.001
I1026 01:11:24.000635 17254 solver.cpp:229] Iteration 7180, loss = 0.100541
I1026 01:11:24.000668 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.060966 (* 1 = 0.060966 loss)
I1026 01:11:24.000672 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.039575 (* 1 = 0.039575 loss)
I1026 01:11:24.000676 17254 sgd_solver.cpp:106] Iteration 7180, lr = 0.001
I1026 01:11:25.097754 17254 solver.cpp:229] Iteration 7200, loss = 0.160878
I1026 01:11:25.097785 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0852734 (* 1 = 0.0852734 loss)
I1026 01:11:25.097790 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0756041 (* 1 = 0.0756041 loss)
I1026 01:11:25.097795 17254 sgd_solver.cpp:106] Iteration 7200, lr = 0.001
I1026 01:11:26.206876 17254 solver.cpp:229] Iteration 7220, loss = 0.0755896
I1026 01:11:26.206907 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0498931 (* 1 = 0.0498931 loss)
I1026 01:11:26.206910 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0256965 (* 1 = 0.0256965 loss)
I1026 01:11:26.206914 17254 sgd_solver.cpp:106] Iteration 7220, lr = 0.001
I1026 01:11:27.311074 17254 solver.cpp:229] Iteration 7240, loss = 0.16453
I1026 01:11:27.311106 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.103039 (* 1 = 0.103039 loss)
I1026 01:11:27.311110 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0614914 (* 1 = 0.0614914 loss)
I1026 01:11:27.311115 17254 sgd_solver.cpp:106] Iteration 7240, lr = 0.001
I1026 01:11:28.416935 17254 solver.cpp:229] Iteration 7260, loss = 0.0659682
I1026 01:11:28.416966 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0381587 (* 1 = 0.0381587 loss)
I1026 01:11:28.416971 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0278095 (* 1 = 0.0278095 loss)
I1026 01:11:28.416976 17254 sgd_solver.cpp:106] Iteration 7260, lr = 0.001
I1026 01:11:29.500638 17254 solver.cpp:229] Iteration 7280, loss = 0.196758
I1026 01:11:29.500671 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0904066 (* 1 = 0.0904066 loss)
I1026 01:11:29.500676 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.106351 (* 1 = 0.106351 loss)
I1026 01:11:29.500680 17254 sgd_solver.cpp:106] Iteration 7280, lr = 0.001
I1026 01:11:30.609422 17254 solver.cpp:229] Iteration 7300, loss = 0.153315
I1026 01:11:30.609454 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0452509 (* 1 = 0.0452509 loss)
I1026 01:11:30.609458 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.108064 (* 1 = 0.108064 loss)
I1026 01:11:30.609463 17254 sgd_solver.cpp:106] Iteration 7300, lr = 0.001
I1026 01:11:31.722434 17254 solver.cpp:229] Iteration 7320, loss = 0.119685
I1026 01:11:31.722466 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.067515 (* 1 = 0.067515 loss)
I1026 01:11:31.722471 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0521701 (* 1 = 0.0521701 loss)
I1026 01:11:31.722476 17254 sgd_solver.cpp:106] Iteration 7320, lr = 0.001
I1026 01:11:32.840291 17254 solver.cpp:229] Iteration 7340, loss = 0.259547
I1026 01:11:32.840323 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.134688 (* 1 = 0.134688 loss)
I1026 01:11:32.840328 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.124858 (* 1 = 0.124858 loss)
I1026 01:11:32.840333 17254 sgd_solver.cpp:106] Iteration 7340, lr = 0.001
I1026 01:11:33.921452 17254 solver.cpp:229] Iteration 7360, loss = 0.113871
I1026 01:11:33.921483 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0385173 (* 1 = 0.0385173 loss)
I1026 01:11:33.921488 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0753534 (* 1 = 0.0753534 loss)
I1026 01:11:33.921492 17254 sgd_solver.cpp:106] Iteration 7360, lr = 0.001
I1026 01:11:35.013792 17254 solver.cpp:229] Iteration 7380, loss = 0.0925677
I1026 01:11:35.013825 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0705554 (* 1 = 0.0705554 loss)
I1026 01:11:35.013829 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0220123 (* 1 = 0.0220123 loss)
I1026 01:11:35.013834 17254 sgd_solver.cpp:106] Iteration 7380, lr = 0.001
I1026 01:11:36.102183 17254 solver.cpp:229] Iteration 7400, loss = 0.0858865
I1026 01:11:36.102216 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0574729 (* 1 = 0.0574729 loss)
I1026 01:11:36.102221 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0284136 (* 1 = 0.0284136 loss)
I1026 01:11:36.102226 17254 sgd_solver.cpp:106] Iteration 7400, lr = 0.001
I1026 01:11:37.184110 17254 solver.cpp:229] Iteration 7420, loss = 0.219306
I1026 01:11:37.184142 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.131698 (* 1 = 0.131698 loss)
I1026 01:11:37.184147 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0876082 (* 1 = 0.0876082 loss)
I1026 01:11:37.184152 17254 sgd_solver.cpp:106] Iteration 7420, lr = 0.001
I1026 01:11:38.275657 17254 solver.cpp:229] Iteration 7440, loss = 0.0905492
I1026 01:11:38.275688 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0469494 (* 1 = 0.0469494 loss)
I1026 01:11:38.275693 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0435998 (* 1 = 0.0435998 loss)
I1026 01:11:38.275697 17254 sgd_solver.cpp:106] Iteration 7440, lr = 0.001
I1026 01:11:39.358475 17254 solver.cpp:229] Iteration 7460, loss = 0.115756
I1026 01:11:39.358508 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0814179 (* 1 = 0.0814179 loss)
I1026 01:11:39.358512 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0343385 (* 1 = 0.0343385 loss)
I1026 01:11:39.358517 17254 sgd_solver.cpp:106] Iteration 7460, lr = 0.001
I1026 01:11:40.453305 17254 solver.cpp:229] Iteration 7480, loss = 0.104049
I1026 01:11:40.453336 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0823624 (* 1 = 0.0823624 loss)
I1026 01:11:40.453341 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.021687 (* 1 = 0.021687 loss)
I1026 01:11:40.453346 17254 sgd_solver.cpp:106] Iteration 7480, lr = 0.001
I1026 01:11:41.545570 17254 solver.cpp:229] Iteration 7500, loss = 0.204096
I1026 01:11:41.545603 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.102332 (* 1 = 0.102332 loss)
I1026 01:11:41.545606 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.101764 (* 1 = 0.101764 loss)
I1026 01:11:41.545611 17254 sgd_solver.cpp:106] Iteration 7500, lr = 0.001
I1026 01:11:42.680943 17254 solver.cpp:229] Iteration 7520, loss = 0.109901
I1026 01:11:42.680976 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0605564 (* 1 = 0.0605564 loss)
I1026 01:11:42.680980 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.049345 (* 1 = 0.049345 loss)
I1026 01:11:42.680985 17254 sgd_solver.cpp:106] Iteration 7520, lr = 0.001
I1026 01:11:43.796288 17254 solver.cpp:229] Iteration 7540, loss = 0.114004
I1026 01:11:43.796320 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.06142 (* 1 = 0.06142 loss)
I1026 01:11:43.796324 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0525839 (* 1 = 0.0525839 loss)
I1026 01:11:43.796329 17254 sgd_solver.cpp:106] Iteration 7540, lr = 0.001
I1026 01:11:44.909358 17254 solver.cpp:229] Iteration 7560, loss = 0.139659
I1026 01:11:44.909402 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0904223 (* 1 = 0.0904223 loss)
I1026 01:11:44.909407 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0492365 (* 1 = 0.0492365 loss)
I1026 01:11:44.909412 17254 sgd_solver.cpp:106] Iteration 7560, lr = 0.001
I1026 01:11:45.989949 17254 solver.cpp:229] Iteration 7580, loss = 0.0643229
I1026 01:11:45.989981 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0324005 (* 1 = 0.0324005 loss)
I1026 01:11:45.989985 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0319225 (* 1 = 0.0319225 loss)
I1026 01:11:45.989990 17254 sgd_solver.cpp:106] Iteration 7580, lr = 0.001
I1026 01:11:47.096379 17254 solver.cpp:229] Iteration 7600, loss = 0.153786
I1026 01:11:47.096411 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0960387 (* 1 = 0.0960387 loss)
I1026 01:11:47.096416 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0577471 (* 1 = 0.0577471 loss)
I1026 01:11:47.096421 17254 sgd_solver.cpp:106] Iteration 7600, lr = 0.001
I1026 01:11:48.217846 17254 solver.cpp:229] Iteration 7620, loss = 0.0931826
I1026 01:11:48.217880 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0747629 (* 1 = 0.0747629 loss)
I1026 01:11:48.217883 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0184197 (* 1 = 0.0184197 loss)
I1026 01:11:48.217888 17254 sgd_solver.cpp:106] Iteration 7620, lr = 0.001
I1026 01:11:49.302784 17254 solver.cpp:229] Iteration 7640, loss = 0.15155
I1026 01:11:49.302816 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0800929 (* 1 = 0.0800929 loss)
I1026 01:11:49.302821 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0714573 (* 1 = 0.0714573 loss)
I1026 01:11:49.302826 17254 sgd_solver.cpp:106] Iteration 7640, lr = 0.001
I1026 01:11:50.385933 17254 solver.cpp:229] Iteration 7660, loss = 0.127485
I1026 01:11:50.385967 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0729262 (* 1 = 0.0729262 loss)
I1026 01:11:50.385972 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0545586 (* 1 = 0.0545586 loss)
I1026 01:11:50.385977 17254 sgd_solver.cpp:106] Iteration 7660, lr = 0.001
I1026 01:11:51.482290 17254 solver.cpp:229] Iteration 7680, loss = 0.147292
I1026 01:11:51.482321 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.094821 (* 1 = 0.094821 loss)
I1026 01:11:51.482326 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0524709 (* 1 = 0.0524709 loss)
I1026 01:11:51.482331 17254 sgd_solver.cpp:106] Iteration 7680, lr = 0.001
I1026 01:11:52.587661 17254 solver.cpp:229] Iteration 7700, loss = 0.124524
I1026 01:11:52.587694 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0849457 (* 1 = 0.0849457 loss)
I1026 01:11:52.587698 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.039578 (* 1 = 0.039578 loss)
I1026 01:11:52.587702 17254 sgd_solver.cpp:106] Iteration 7700, lr = 0.001
I1026 01:11:53.711813 17254 solver.cpp:229] Iteration 7720, loss = 0.0878175
I1026 01:11:53.711844 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0300411 (* 1 = 0.0300411 loss)
I1026 01:11:53.711849 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0577764 (* 1 = 0.0577764 loss)
I1026 01:11:53.711853 17254 sgd_solver.cpp:106] Iteration 7720, lr = 0.001
I1026 01:11:54.809525 17254 solver.cpp:229] Iteration 7740, loss = 0.135515
I1026 01:11:54.809558 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0716521 (* 1 = 0.0716521 loss)
I1026 01:11:54.809563 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0638624 (* 1 = 0.0638624 loss)
I1026 01:11:54.809568 17254 sgd_solver.cpp:106] Iteration 7740, lr = 0.001
I1026 01:11:55.925523 17254 solver.cpp:229] Iteration 7760, loss = 0.0635961
I1026 01:11:55.925554 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0394842 (* 1 = 0.0394842 loss)
I1026 01:11:55.925559 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0241119 (* 1 = 0.0241119 loss)
I1026 01:11:55.925562 17254 sgd_solver.cpp:106] Iteration 7760, lr = 0.001
I1026 01:11:57.009901 17254 solver.cpp:229] Iteration 7780, loss = 0.134521
I1026 01:11:57.009932 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0462529 (* 1 = 0.0462529 loss)
I1026 01:11:57.009938 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.088268 (* 1 = 0.088268 loss)
I1026 01:11:57.009943 17254 sgd_solver.cpp:106] Iteration 7780, lr = 0.001
I1026 01:11:58.098731 17254 solver.cpp:229] Iteration 7800, loss = 0.122597
I1026 01:11:58.098762 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0811756 (* 1 = 0.0811756 loss)
I1026 01:11:58.098767 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0414212 (* 1 = 0.0414212 loss)
I1026 01:11:58.098772 17254 sgd_solver.cpp:106] Iteration 7800, lr = 0.001
I1026 01:11:59.167500 17254 solver.cpp:229] Iteration 7820, loss = 0.11979
I1026 01:11:59.167532 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0494215 (* 1 = 0.0494215 loss)
I1026 01:11:59.167536 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0703685 (* 1 = 0.0703685 loss)
I1026 01:11:59.167541 17254 sgd_solver.cpp:106] Iteration 7820, lr = 0.001
I1026 01:12:00.271438 17254 solver.cpp:229] Iteration 7840, loss = 0.124047
I1026 01:12:00.271478 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0836376 (* 1 = 0.0836376 loss)
I1026 01:12:00.271483 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0404093 (* 1 = 0.0404093 loss)
I1026 01:12:00.271497 17254 sgd_solver.cpp:106] Iteration 7840, lr = 0.001
I1026 01:12:01.372632 17254 solver.cpp:229] Iteration 7860, loss = 0.0772319
I1026 01:12:01.372673 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0351916 (* 1 = 0.0351916 loss)
I1026 01:12:01.372678 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0420402 (* 1 = 0.0420402 loss)
I1026 01:12:01.372683 17254 sgd_solver.cpp:106] Iteration 7860, lr = 0.001
I1026 01:12:02.447057 17254 solver.cpp:229] Iteration 7880, loss = 0.162036
I1026 01:12:02.447089 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0862613 (* 1 = 0.0862613 loss)
I1026 01:12:02.447094 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0757748 (* 1 = 0.0757748 loss)
I1026 01:12:02.447099 17254 sgd_solver.cpp:106] Iteration 7880, lr = 0.001
I1026 01:12:03.538039 17254 solver.cpp:229] Iteration 7900, loss = 0.132578
I1026 01:12:03.538074 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0889832 (* 1 = 0.0889832 loss)
I1026 01:12:03.538077 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.043595 (* 1 = 0.043595 loss)
I1026 01:12:03.538082 17254 sgd_solver.cpp:106] Iteration 7900, lr = 0.001
I1026 01:12:04.645035 17254 solver.cpp:229] Iteration 7920, loss = 0.151223
I1026 01:12:04.645088 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0944087 (* 1 = 0.0944087 loss)
I1026 01:12:04.645103 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0568146 (* 1 = 0.0568146 loss)
I1026 01:12:04.645108 17254 sgd_solver.cpp:106] Iteration 7920, lr = 0.001
I1026 01:12:05.727798 17254 solver.cpp:229] Iteration 7940, loss = 0.0888393
I1026 01:12:05.727830 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0509837 (* 1 = 0.0509837 loss)
I1026 01:12:05.727836 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0378556 (* 1 = 0.0378556 loss)
I1026 01:12:05.727840 17254 sgd_solver.cpp:106] Iteration 7940, lr = 0.001
I1026 01:12:06.826270 17254 solver.cpp:229] Iteration 7960, loss = 0.095136
I1026 01:12:06.826303 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0583801 (* 1 = 0.0583801 loss)
I1026 01:12:06.826309 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0367559 (* 1 = 0.0367559 loss)
I1026 01:12:06.826314 17254 sgd_solver.cpp:106] Iteration 7960, lr = 0.001
I1026 01:12:07.909842 17254 solver.cpp:229] Iteration 7980, loss = 0.0848144
I1026 01:12:07.909874 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0314742 (* 1 = 0.0314742 loss)
I1026 01:12:07.909880 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0533401 (* 1 = 0.0533401 loss)
I1026 01:12:07.909884 17254 sgd_solver.cpp:106] Iteration 7980, lr = 0.001
I1026 01:12:09.028736 17254 solver.cpp:229] Iteration 8000, loss = 0.0865353
I1026 01:12:09.028769 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0354031 (* 1 = 0.0354031 loss)
I1026 01:12:09.028774 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0511322 (* 1 = 0.0511322 loss)
I1026 01:12:09.028779 17254 sgd_solver.cpp:106] Iteration 8000, lr = 0.001
I1026 01:12:10.137653 17254 solver.cpp:229] Iteration 8020, loss = 0.122169
I1026 01:12:10.137687 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0552766 (* 1 = 0.0552766 loss)
I1026 01:12:10.137692 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0668923 (* 1 = 0.0668923 loss)
I1026 01:12:10.137696 17254 sgd_solver.cpp:106] Iteration 8020, lr = 0.001
I1026 01:12:11.224679 17254 solver.cpp:229] Iteration 8040, loss = 0.128313
I1026 01:12:11.224712 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0747167 (* 1 = 0.0747167 loss)
I1026 01:12:11.224717 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0535962 (* 1 = 0.0535962 loss)
I1026 01:12:11.224722 17254 sgd_solver.cpp:106] Iteration 8040, lr = 0.001
I1026 01:12:12.301951 17254 solver.cpp:229] Iteration 8060, loss = 0.172316
I1026 01:12:12.301981 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.115106 (* 1 = 0.115106 loss)
I1026 01:12:12.301986 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0572103 (* 1 = 0.0572103 loss)
I1026 01:12:12.301990 17254 sgd_solver.cpp:106] Iteration 8060, lr = 0.001
I1026 01:12:13.392066 17254 solver.cpp:229] Iteration 8080, loss = 0.172502
I1026 01:12:13.392097 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0944443 (* 1 = 0.0944443 loss)
I1026 01:12:13.392102 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0780581 (* 1 = 0.0780581 loss)
I1026 01:12:13.392108 17254 sgd_solver.cpp:106] Iteration 8080, lr = 0.001
I1026 01:12:14.480628 17254 solver.cpp:229] Iteration 8100, loss = 0.0829697
I1026 01:12:14.480660 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.047689 (* 1 = 0.047689 loss)
I1026 01:12:14.480665 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0352807 (* 1 = 0.0352807 loss)
I1026 01:12:14.480670 17254 sgd_solver.cpp:106] Iteration 8100, lr = 0.001
I1026 01:12:15.576408 17254 solver.cpp:229] Iteration 8120, loss = 0.124193
I1026 01:12:15.576441 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0802168 (* 1 = 0.0802168 loss)
I1026 01:12:15.576445 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0439767 (* 1 = 0.0439767 loss)
I1026 01:12:15.576450 17254 sgd_solver.cpp:106] Iteration 8120, lr = 0.001
I1026 01:12:16.655338 17254 solver.cpp:229] Iteration 8140, loss = 0.0296835
I1026 01:12:16.655380 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0183464 (* 1 = 0.0183464 loss)
I1026 01:12:16.655385 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0113371 (* 1 = 0.0113371 loss)
I1026 01:12:16.655390 17254 sgd_solver.cpp:106] Iteration 8140, lr = 0.001
I1026 01:12:17.760398 17254 solver.cpp:229] Iteration 8160, loss = 0.105019
I1026 01:12:17.760433 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0323577 (* 1 = 0.0323577 loss)
I1026 01:12:17.760438 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0726609 (* 1 = 0.0726609 loss)
I1026 01:12:17.760444 17254 sgd_solver.cpp:106] Iteration 8160, lr = 0.001
I1026 01:12:18.880709 17254 solver.cpp:229] Iteration 8180, loss = 0.05408
I1026 01:12:18.880739 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0390859 (* 1 = 0.0390859 loss)
I1026 01:12:18.880743 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0149941 (* 1 = 0.0149941 loss)
I1026 01:12:18.880748 17254 sgd_solver.cpp:106] Iteration 8180, lr = 0.001
I1026 01:12:19.983588 17254 solver.cpp:229] Iteration 8200, loss = 0.0937219
I1026 01:12:19.983621 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0494369 (* 1 = 0.0494369 loss)
I1026 01:12:19.983626 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.044285 (* 1 = 0.044285 loss)
I1026 01:12:19.983631 17254 sgd_solver.cpp:106] Iteration 8200, lr = 0.001
I1026 01:12:21.062959 17254 solver.cpp:229] Iteration 8220, loss = 0.19773
I1026 01:12:21.062993 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0991748 (* 1 = 0.0991748 loss)
I1026 01:12:21.062997 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0985549 (* 1 = 0.0985549 loss)
I1026 01:12:21.063001 17254 sgd_solver.cpp:106] Iteration 8220, lr = 0.001
I1026 01:12:22.173985 17254 solver.cpp:229] Iteration 8240, loss = 0.0746747
I1026 01:12:22.174017 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0398318 (* 1 = 0.0398318 loss)
I1026 01:12:22.174022 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0348429 (* 1 = 0.0348429 loss)
I1026 01:12:22.174027 17254 sgd_solver.cpp:106] Iteration 8240, lr = 0.001
I1026 01:12:23.283120 17254 solver.cpp:229] Iteration 8260, loss = 0.111414
I1026 01:12:23.283154 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0746917 (* 1 = 0.0746917 loss)
I1026 01:12:23.283159 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0367222 (* 1 = 0.0367222 loss)
I1026 01:12:23.283164 17254 sgd_solver.cpp:106] Iteration 8260, lr = 0.001
I1026 01:12:24.382634 17254 solver.cpp:229] Iteration 8280, loss = 0.140621
I1026 01:12:24.382678 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0557665 (* 1 = 0.0557665 loss)
I1026 01:12:24.382683 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0848549 (* 1 = 0.0848549 loss)
I1026 01:12:24.382688 17254 sgd_solver.cpp:106] Iteration 8280, lr = 0.001
I1026 01:12:25.468796 17254 solver.cpp:229] Iteration 8300, loss = 0.0804596
I1026 01:12:25.468829 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0471799 (* 1 = 0.0471799 loss)
I1026 01:12:25.468834 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0332797 (* 1 = 0.0332797 loss)
I1026 01:12:25.468839 17254 sgd_solver.cpp:106] Iteration 8300, lr = 0.001
I1026 01:12:26.555318 17254 solver.cpp:229] Iteration 8320, loss = 0.0668503
I1026 01:12:26.555351 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0333765 (* 1 = 0.0333765 loss)
I1026 01:12:26.555356 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0334738 (* 1 = 0.0334738 loss)
I1026 01:12:26.555359 17254 sgd_solver.cpp:106] Iteration 8320, lr = 0.001
I1026 01:12:27.669612 17254 solver.cpp:229] Iteration 8340, loss = 0.200415
I1026 01:12:27.669646 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.124119 (* 1 = 0.124119 loss)
I1026 01:12:27.669651 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0762959 (* 1 = 0.0762959 loss)
I1026 01:12:27.669667 17254 sgd_solver.cpp:106] Iteration 8340, lr = 0.001
I1026 01:12:28.754035 17254 solver.cpp:229] Iteration 8360, loss = 0.117242
I1026 01:12:28.754067 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0873927 (* 1 = 0.0873927 loss)
I1026 01:12:28.754072 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0298492 (* 1 = 0.0298492 loss)
I1026 01:12:28.754078 17254 sgd_solver.cpp:106] Iteration 8360, lr = 0.001
I1026 01:12:29.861629 17254 solver.cpp:229] Iteration 8380, loss = 0.114453
I1026 01:12:29.861662 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0547092 (* 1 = 0.0547092 loss)
I1026 01:12:29.861667 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0597442 (* 1 = 0.0597442 loss)
I1026 01:12:29.861683 17254 sgd_solver.cpp:106] Iteration 8380, lr = 0.001
I1026 01:12:30.958403 17254 solver.cpp:229] Iteration 8400, loss = 0.131786
I1026 01:12:30.958433 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0496656 (* 1 = 0.0496656 loss)
I1026 01:12:30.958438 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0821201 (* 1 = 0.0821201 loss)
I1026 01:12:30.958443 17254 sgd_solver.cpp:106] Iteration 8400, lr = 0.001
I1026 01:12:32.047113 17254 solver.cpp:229] Iteration 8420, loss = 0.0961039
I1026 01:12:32.047145 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0605003 (* 1 = 0.0605003 loss)
I1026 01:12:32.047149 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0356036 (* 1 = 0.0356036 loss)
I1026 01:12:32.047154 17254 sgd_solver.cpp:106] Iteration 8420, lr = 0.001
I1026 01:12:33.178104 17254 solver.cpp:229] Iteration 8440, loss = 0.11894
I1026 01:12:33.178138 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0864582 (* 1 = 0.0864582 loss)
I1026 01:12:33.178143 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0324816 (* 1 = 0.0324816 loss)
I1026 01:12:33.178149 17254 sgd_solver.cpp:106] Iteration 8440, lr = 0.001
I1026 01:12:34.286955 17254 solver.cpp:229] Iteration 8460, loss = 0.17522
I1026 01:12:34.286988 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.114292 (* 1 = 0.114292 loss)
I1026 01:12:34.286993 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0609282 (* 1 = 0.0609282 loss)
I1026 01:12:34.286998 17254 sgd_solver.cpp:106] Iteration 8460, lr = 0.001
I1026 01:12:35.390527 17254 solver.cpp:229] Iteration 8480, loss = 0.101387
I1026 01:12:35.390558 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0509298 (* 1 = 0.0509298 loss)
I1026 01:12:35.390563 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0504575 (* 1 = 0.0504575 loss)
I1026 01:12:35.390568 17254 sgd_solver.cpp:106] Iteration 8480, lr = 0.001
I1026 01:12:36.504497 17254 solver.cpp:229] Iteration 8500, loss = 0.0827411
I1026 01:12:36.504529 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0557758 (* 1 = 0.0557758 loss)
I1026 01:12:36.504534 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0269654 (* 1 = 0.0269654 loss)
I1026 01:12:36.504539 17254 sgd_solver.cpp:106] Iteration 8500, lr = 0.001
I1026 01:12:37.602489 17254 solver.cpp:229] Iteration 8520, loss = 0.0404533
I1026 01:12:37.602522 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0342703 (* 1 = 0.0342703 loss)
I1026 01:12:37.602529 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00618297 (* 1 = 0.00618297 loss)
I1026 01:12:37.602533 17254 sgd_solver.cpp:106] Iteration 8520, lr = 0.001
I1026 01:12:38.706365 17254 solver.cpp:229] Iteration 8540, loss = 0.127514
I1026 01:12:38.706398 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.101292 (* 1 = 0.101292 loss)
I1026 01:12:38.706403 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0262225 (* 1 = 0.0262225 loss)
I1026 01:12:38.706408 17254 sgd_solver.cpp:106] Iteration 8540, lr = 0.001
I1026 01:12:39.796336 17254 solver.cpp:229] Iteration 8560, loss = 0.111228
I1026 01:12:39.796368 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0761076 (* 1 = 0.0761076 loss)
I1026 01:12:39.796373 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0351203 (* 1 = 0.0351203 loss)
I1026 01:12:39.796378 17254 sgd_solver.cpp:106] Iteration 8560, lr = 0.001
I1026 01:12:40.905422 17254 solver.cpp:229] Iteration 8580, loss = 0.158465
I1026 01:12:40.905455 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0984639 (* 1 = 0.0984639 loss)
I1026 01:12:40.905460 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0600009 (* 1 = 0.0600009 loss)
I1026 01:12:40.905465 17254 sgd_solver.cpp:106] Iteration 8580, lr = 0.001
I1026 01:12:42.003808 17254 solver.cpp:229] Iteration 8600, loss = 0.101749
I1026 01:12:42.003839 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0637591 (* 1 = 0.0637591 loss)
I1026 01:12:42.003844 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0379894 (* 1 = 0.0379894 loss)
I1026 01:12:42.003849 17254 sgd_solver.cpp:106] Iteration 8600, lr = 0.001
I1026 01:12:43.120540 17254 solver.cpp:229] Iteration 8620, loss = 0.0727939
I1026 01:12:43.120573 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0512124 (* 1 = 0.0512124 loss)
I1026 01:12:43.120578 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0215815 (* 1 = 0.0215815 loss)
I1026 01:12:43.120582 17254 sgd_solver.cpp:106] Iteration 8620, lr = 0.001
I1026 01:12:44.212244 17254 solver.cpp:229] Iteration 8640, loss = 0.0794511
I1026 01:12:44.212277 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.048908 (* 1 = 0.048908 loss)
I1026 01:12:44.212285 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0305432 (* 1 = 0.0305432 loss)
I1026 01:12:44.212302 17254 sgd_solver.cpp:106] Iteration 8640, lr = 0.001
I1026 01:12:45.322052 17254 solver.cpp:229] Iteration 8660, loss = 0.07931
I1026 01:12:45.322084 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0541291 (* 1 = 0.0541291 loss)
I1026 01:12:45.322093 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0251808 (* 1 = 0.0251808 loss)
I1026 01:12:45.322109 17254 sgd_solver.cpp:106] Iteration 8660, lr = 0.001
I1026 01:12:46.426365 17254 solver.cpp:229] Iteration 8680, loss = 0.0835284
I1026 01:12:46.426398 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0631296 (* 1 = 0.0631296 loss)
I1026 01:12:46.426405 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0203988 (* 1 = 0.0203988 loss)
I1026 01:12:46.426412 17254 sgd_solver.cpp:106] Iteration 8680, lr = 0.001
I1026 01:12:47.553808 17254 solver.cpp:229] Iteration 8700, loss = 0.0700783
I1026 01:12:47.553843 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.040016 (* 1 = 0.040016 loss)
I1026 01:12:47.553850 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0300623 (* 1 = 0.0300623 loss)
I1026 01:12:47.553856 17254 sgd_solver.cpp:106] Iteration 8700, lr = 0.001
I1026 01:12:48.643633 17254 solver.cpp:229] Iteration 8720, loss = 0.0468621
I1026 01:12:48.643662 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0347483 (* 1 = 0.0347483 loss)
I1026 01:12:48.643666 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0121138 (* 1 = 0.0121138 loss)
I1026 01:12:48.643671 17254 sgd_solver.cpp:106] Iteration 8720, lr = 0.001
I1026 01:12:49.763483 17254 solver.cpp:229] Iteration 8740, loss = 0.0666139
I1026 01:12:49.763515 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0368748 (* 1 = 0.0368748 loss)
I1026 01:12:49.763520 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0297391 (* 1 = 0.0297391 loss)
I1026 01:12:49.763525 17254 sgd_solver.cpp:106] Iteration 8740, lr = 0.001
I1026 01:12:50.860307 17254 solver.cpp:229] Iteration 8760, loss = 0.0919785
I1026 01:12:50.860337 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0666212 (* 1 = 0.0666212 loss)
I1026 01:12:50.860342 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0253574 (* 1 = 0.0253574 loss)
I1026 01:12:50.860347 17254 sgd_solver.cpp:106] Iteration 8760, lr = 0.001
I1026 01:12:51.953480 17254 solver.cpp:229] Iteration 8780, loss = 0.0787452
I1026 01:12:51.953511 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0545896 (* 1 = 0.0545896 loss)
I1026 01:12:51.953516 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0241556 (* 1 = 0.0241556 loss)
I1026 01:12:51.953519 17254 sgd_solver.cpp:106] Iteration 8780, lr = 0.001
I1026 01:12:53.075485 17254 solver.cpp:229] Iteration 8800, loss = 0.0956227
I1026 01:12:53.075518 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.052908 (* 1 = 0.052908 loss)
I1026 01:12:53.075525 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0427148 (* 1 = 0.0427148 loss)
I1026 01:12:53.075528 17254 sgd_solver.cpp:106] Iteration 8800, lr = 0.001
I1026 01:12:54.177482 17254 solver.cpp:229] Iteration 8820, loss = 0.0746127
I1026 01:12:54.177526 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0330808 (* 1 = 0.0330808 loss)
I1026 01:12:54.177532 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0415319 (* 1 = 0.0415319 loss)
I1026 01:12:54.177537 17254 sgd_solver.cpp:106] Iteration 8820, lr = 0.001
I1026 01:12:55.268688 17254 solver.cpp:229] Iteration 8840, loss = 0.0657141
I1026 01:12:55.268721 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0507377 (* 1 = 0.0507377 loss)
I1026 01:12:55.268726 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0149764 (* 1 = 0.0149764 loss)
I1026 01:12:55.268731 17254 sgd_solver.cpp:106] Iteration 8840, lr = 0.001
I1026 01:12:56.395293 17254 solver.cpp:229] Iteration 8860, loss = 0.0734625
I1026 01:12:56.395324 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0325635 (* 1 = 0.0325635 loss)
I1026 01:12:56.395329 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0408989 (* 1 = 0.0408989 loss)
I1026 01:12:56.395334 17254 sgd_solver.cpp:106] Iteration 8860, lr = 0.001
I1026 01:12:57.503681 17254 solver.cpp:229] Iteration 8880, loss = 0.0672665
I1026 01:12:57.503715 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.043648 (* 1 = 0.043648 loss)
I1026 01:12:57.503720 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0236185 (* 1 = 0.0236185 loss)
I1026 01:12:57.503725 17254 sgd_solver.cpp:106] Iteration 8880, lr = 0.001
I1026 01:12:58.616592 17254 solver.cpp:229] Iteration 8900, loss = 0.102308
I1026 01:12:58.616647 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0697228 (* 1 = 0.0697228 loss)
I1026 01:12:58.616652 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0325854 (* 1 = 0.0325854 loss)
I1026 01:12:58.616667 17254 sgd_solver.cpp:106] Iteration 8900, lr = 0.001
I1026 01:12:59.713032 17254 solver.cpp:229] Iteration 8920, loss = 0.180668
I1026 01:12:59.713064 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0967667 (* 1 = 0.0967667 loss)
I1026 01:12:59.713069 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0839016 (* 1 = 0.0839016 loss)
I1026 01:12:59.713074 17254 sgd_solver.cpp:106] Iteration 8920, lr = 0.001
I1026 01:13:00.813163 17254 solver.cpp:229] Iteration 8940, loss = 0.125459
I1026 01:13:00.813194 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0517955 (* 1 = 0.0517955 loss)
I1026 01:13:00.813199 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0736639 (* 1 = 0.0736639 loss)
I1026 01:13:00.813204 17254 sgd_solver.cpp:106] Iteration 8940, lr = 0.001
I1026 01:13:01.917016 17254 solver.cpp:229] Iteration 8960, loss = 0.29296
I1026 01:13:01.917047 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.148497 (* 1 = 0.148497 loss)
I1026 01:13:01.917052 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.144463 (* 1 = 0.144463 loss)
I1026 01:13:01.917055 17254 sgd_solver.cpp:106] Iteration 8960, lr = 0.001
I1026 01:13:02.989563 17254 solver.cpp:229] Iteration 8980, loss = 0.0999689
I1026 01:13:02.989593 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0332572 (* 1 = 0.0332572 loss)
I1026 01:13:02.989598 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0667117 (* 1 = 0.0667117 loss)
I1026 01:13:02.989603 17254 sgd_solver.cpp:106] Iteration 8980, lr = 0.001
I1026 01:13:04.093657 17254 solver.cpp:229] Iteration 9000, loss = 0.0868379
I1026 01:13:04.093690 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0343148 (* 1 = 0.0343148 loss)
I1026 01:13:04.093694 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0525231 (* 1 = 0.0525231 loss)
I1026 01:13:04.093698 17254 sgd_solver.cpp:106] Iteration 9000, lr = 0.001
I1026 01:13:05.183675 17254 solver.cpp:229] Iteration 9020, loss = 0.118881
I1026 01:13:05.183706 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0635836 (* 1 = 0.0635836 loss)
I1026 01:13:05.183712 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0552978 (* 1 = 0.0552978 loss)
I1026 01:13:05.183717 17254 sgd_solver.cpp:106] Iteration 9020, lr = 0.001
I1026 01:13:06.301105 17254 solver.cpp:229] Iteration 9040, loss = 0.0815958
I1026 01:13:06.301137 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0447049 (* 1 = 0.0447049 loss)
I1026 01:13:06.301141 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0368908 (* 1 = 0.0368908 loss)
I1026 01:13:06.301146 17254 sgd_solver.cpp:106] Iteration 9040, lr = 0.001
I1026 01:13:07.406782 17254 solver.cpp:229] Iteration 9060, loss = 0.101255
I1026 01:13:07.406816 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0549111 (* 1 = 0.0549111 loss)
I1026 01:13:07.406824 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0463438 (* 1 = 0.0463438 loss)
I1026 01:13:07.406831 17254 sgd_solver.cpp:106] Iteration 9060, lr = 0.001
I1026 01:13:08.513492 17254 solver.cpp:229] Iteration 9080, loss = 0.081896
I1026 01:13:08.513528 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.037992 (* 1 = 0.037992 loss)
I1026 01:13:08.513536 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0439039 (* 1 = 0.0439039 loss)
I1026 01:13:08.513542 17254 sgd_solver.cpp:106] Iteration 9080, lr = 0.001
I1026 01:13:09.615463 17254 solver.cpp:229] Iteration 9100, loss = 0.0664549
I1026 01:13:09.615496 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0285743 (* 1 = 0.0285743 loss)
I1026 01:13:09.615504 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0378806 (* 1 = 0.0378806 loss)
I1026 01:13:09.615510 17254 sgd_solver.cpp:106] Iteration 9100, lr = 0.001
I1026 01:13:10.722136 17254 solver.cpp:229] Iteration 9120, loss = 0.184292
I1026 01:13:10.722170 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0809203 (* 1 = 0.0809203 loss)
I1026 01:13:10.722179 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.103372 (* 1 = 0.103372 loss)
I1026 01:13:10.722187 17254 sgd_solver.cpp:106] Iteration 9120, lr = 0.001
I1026 01:13:11.815440 17254 solver.cpp:229] Iteration 9140, loss = 0.071124
I1026 01:13:11.815474 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0352428 (* 1 = 0.0352428 loss)
I1026 01:13:11.815482 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0358812 (* 1 = 0.0358812 loss)
I1026 01:13:11.815500 17254 sgd_solver.cpp:106] Iteration 9140, lr = 0.001
I1026 01:13:12.913514 17254 solver.cpp:229] Iteration 9160, loss = 0.149928
I1026 01:13:12.913558 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0623935 (* 1 = 0.0623935 loss)
I1026 01:13:12.913563 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0875348 (* 1 = 0.0875348 loss)
I1026 01:13:12.913568 17254 sgd_solver.cpp:106] Iteration 9160, lr = 0.001
I1026 01:13:13.987526 17254 solver.cpp:229] Iteration 9180, loss = 0.120542
I1026 01:13:13.987571 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0584234 (* 1 = 0.0584234 loss)
I1026 01:13:13.987576 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0621186 (* 1 = 0.0621186 loss)
I1026 01:13:13.987581 17254 sgd_solver.cpp:106] Iteration 9180, lr = 0.001
I1026 01:13:15.094795 17254 solver.cpp:229] Iteration 9200, loss = 0.133197
I1026 01:13:15.094827 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.06716 (* 1 = 0.06716 loss)
I1026 01:13:15.094832 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0660374 (* 1 = 0.0660374 loss)
I1026 01:13:15.094837 17254 sgd_solver.cpp:106] Iteration 9200, lr = 0.001
I1026 01:13:16.207319 17254 solver.cpp:229] Iteration 9220, loss = 0.324116
I1026 01:13:16.207352 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.125536 (* 1 = 0.125536 loss)
I1026 01:13:16.207357 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.19858 (* 1 = 0.19858 loss)
I1026 01:13:16.207362 17254 sgd_solver.cpp:106] Iteration 9220, lr = 0.001
I1026 01:13:17.327616 17254 solver.cpp:229] Iteration 9240, loss = 0.0946467
I1026 01:13:17.327649 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0621472 (* 1 = 0.0621472 loss)
I1026 01:13:17.327654 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0324995 (* 1 = 0.0324995 loss)
I1026 01:13:17.327658 17254 sgd_solver.cpp:106] Iteration 9240, lr = 0.001
I1026 01:13:18.438808 17254 solver.cpp:229] Iteration 9260, loss = 0.0639566
I1026 01:13:18.438843 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0545583 (* 1 = 0.0545583 loss)
I1026 01:13:18.438850 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00939833 (* 1 = 0.00939833 loss)
I1026 01:13:18.438856 17254 sgd_solver.cpp:106] Iteration 9260, lr = 0.001
I1026 01:13:19.544945 17254 solver.cpp:229] Iteration 9280, loss = 0.236864
I1026 01:13:19.544976 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.107599 (* 1 = 0.107599 loss)
I1026 01:13:19.544981 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.129264 (* 1 = 0.129264 loss)
I1026 01:13:19.544986 17254 sgd_solver.cpp:106] Iteration 9280, lr = 0.001
I1026 01:13:20.646009 17254 solver.cpp:229] Iteration 9300, loss = 0.109185
I1026 01:13:20.646039 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0793796 (* 1 = 0.0793796 loss)
I1026 01:13:20.646042 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0298051 (* 1 = 0.0298051 loss)
I1026 01:13:20.646047 17254 sgd_solver.cpp:106] Iteration 9300, lr = 0.001
I1026 01:13:21.764040 17254 solver.cpp:229] Iteration 9320, loss = 0.100697
I1026 01:13:21.764073 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0571712 (* 1 = 0.0571712 loss)
I1026 01:13:21.764078 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.043526 (* 1 = 0.043526 loss)
I1026 01:13:21.764083 17254 sgd_solver.cpp:106] Iteration 9320, lr = 0.001
I1026 01:13:22.870779 17254 solver.cpp:229] Iteration 9340, loss = 0.169791
I1026 01:13:22.870810 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0887556 (* 1 = 0.0887556 loss)
I1026 01:13:22.870815 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0810353 (* 1 = 0.0810353 loss)
I1026 01:13:22.870820 17254 sgd_solver.cpp:106] Iteration 9340, lr = 0.001
I1026 01:13:23.970250 17254 solver.cpp:229] Iteration 9360, loss = 0.0982267
I1026 01:13:23.970283 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0604671 (* 1 = 0.0604671 loss)
I1026 01:13:23.970288 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0377596 (* 1 = 0.0377596 loss)
I1026 01:13:23.970294 17254 sgd_solver.cpp:106] Iteration 9360, lr = 0.001
I1026 01:13:25.077950 17254 solver.cpp:229] Iteration 9380, loss = 0.132085
I1026 01:13:25.077993 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0939378 (* 1 = 0.0939378 loss)
I1026 01:13:25.078009 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0381477 (* 1 = 0.0381477 loss)
I1026 01:13:25.078014 17254 sgd_solver.cpp:106] Iteration 9380, lr = 0.001
I1026 01:13:26.190443 17254 solver.cpp:229] Iteration 9400, loss = 0.0980602
I1026 01:13:26.190476 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0581631 (* 1 = 0.0581631 loss)
I1026 01:13:26.190481 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0398971 (* 1 = 0.0398971 loss)
I1026 01:13:26.190486 17254 sgd_solver.cpp:106] Iteration 9400, lr = 0.001
I1026 01:13:27.329133 17254 solver.cpp:229] Iteration 9420, loss = 0.263419
I1026 01:13:27.329175 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.120694 (* 1 = 0.120694 loss)
I1026 01:13:27.329181 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.142725 (* 1 = 0.142725 loss)
I1026 01:13:27.329186 17254 sgd_solver.cpp:106] Iteration 9420, lr = 0.001
I1026 01:13:28.435753 17254 solver.cpp:229] Iteration 9440, loss = 0.211111
I1026 01:13:28.435786 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0863711 (* 1 = 0.0863711 loss)
I1026 01:13:28.435791 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.12474 (* 1 = 0.12474 loss)
I1026 01:13:28.435797 17254 sgd_solver.cpp:106] Iteration 9440, lr = 0.001
I1026 01:13:29.557737 17254 solver.cpp:229] Iteration 9460, loss = 0.117044
I1026 01:13:29.557770 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0632156 (* 1 = 0.0632156 loss)
I1026 01:13:29.557775 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0538287 (* 1 = 0.0538287 loss)
I1026 01:13:29.557780 17254 sgd_solver.cpp:106] Iteration 9460, lr = 0.001
I1026 01:13:30.659693 17254 solver.cpp:229] Iteration 9480, loss = 0.0927767
I1026 01:13:30.659720 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0474453 (* 1 = 0.0474453 loss)
I1026 01:13:30.659725 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0453313 (* 1 = 0.0453313 loss)
I1026 01:13:30.659730 17254 sgd_solver.cpp:106] Iteration 9480, lr = 0.001
I1026 01:13:31.744601 17254 solver.cpp:229] Iteration 9500, loss = 0.085727
I1026 01:13:31.744633 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0602344 (* 1 = 0.0602344 loss)
I1026 01:13:31.744638 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0254926 (* 1 = 0.0254926 loss)
I1026 01:13:31.744643 17254 sgd_solver.cpp:106] Iteration 9500, lr = 0.001
I1026 01:13:32.850899 17254 solver.cpp:229] Iteration 9520, loss = 0.0497528
I1026 01:13:32.850932 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0404277 (* 1 = 0.0404277 loss)
I1026 01:13:32.850939 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00932505 (* 1 = 0.00932505 loss)
I1026 01:13:32.850942 17254 sgd_solver.cpp:106] Iteration 9520, lr = 0.001
I1026 01:13:33.968374 17254 solver.cpp:229] Iteration 9540, loss = 0.081887
I1026 01:13:33.968418 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0525312 (* 1 = 0.0525312 loss)
I1026 01:13:33.968423 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0293558 (* 1 = 0.0293558 loss)
I1026 01:13:33.968436 17254 sgd_solver.cpp:106] Iteration 9540, lr = 0.001
I1026 01:13:35.058333 17254 solver.cpp:229] Iteration 9560, loss = 0.0753423
I1026 01:13:35.058367 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0523222 (* 1 = 0.0523222 loss)
I1026 01:13:35.058372 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0230201 (* 1 = 0.0230201 loss)
I1026 01:13:35.058377 17254 sgd_solver.cpp:106] Iteration 9560, lr = 0.001
I1026 01:13:36.175926 17254 solver.cpp:229] Iteration 9580, loss = 0.123305
I1026 01:13:36.175958 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0428173 (* 1 = 0.0428173 loss)
I1026 01:13:36.175963 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.080488 (* 1 = 0.080488 loss)
I1026 01:13:36.175968 17254 sgd_solver.cpp:106] Iteration 9580, lr = 0.001
I1026 01:13:37.279798 17254 solver.cpp:229] Iteration 9600, loss = 0.102134
I1026 01:13:37.279830 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0428015 (* 1 = 0.0428015 loss)
I1026 01:13:37.279835 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0593321 (* 1 = 0.0593321 loss)
I1026 01:13:37.279839 17254 sgd_solver.cpp:106] Iteration 9600, lr = 0.001
I1026 01:13:38.375000 17254 solver.cpp:229] Iteration 9620, loss = 0.0999913
I1026 01:13:38.375033 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.070434 (* 1 = 0.070434 loss)
I1026 01:13:38.375038 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0295572 (* 1 = 0.0295572 loss)
I1026 01:13:38.375043 17254 sgd_solver.cpp:106] Iteration 9620, lr = 0.001
I1026 01:13:39.478602 17254 solver.cpp:229] Iteration 9640, loss = 0.230623
I1026 01:13:39.478634 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0894218 (* 1 = 0.0894218 loss)
I1026 01:13:39.478639 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.141201 (* 1 = 0.141201 loss)
I1026 01:13:39.478644 17254 sgd_solver.cpp:106] Iteration 9640, lr = 0.001
I1026 01:13:40.566756 17254 solver.cpp:229] Iteration 9660, loss = 0.0996414
I1026 01:13:40.566788 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0732759 (* 1 = 0.0732759 loss)
I1026 01:13:40.566793 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0263655 (* 1 = 0.0263655 loss)
I1026 01:13:40.566798 17254 sgd_solver.cpp:106] Iteration 9660, lr = 0.001
I1026 01:13:41.679349 17254 solver.cpp:229] Iteration 9680, loss = 0.0451301
I1026 01:13:41.679383 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0269447 (* 1 = 0.0269447 loss)
I1026 01:13:41.679388 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0181854 (* 1 = 0.0181854 loss)
I1026 01:13:41.679394 17254 sgd_solver.cpp:106] Iteration 9680, lr = 0.001
I1026 01:13:42.774204 17254 solver.cpp:229] Iteration 9700, loss = 0.0779555
I1026 01:13:42.774235 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0499553 (* 1 = 0.0499553 loss)
I1026 01:13:42.774241 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0280002 (* 1 = 0.0280002 loss)
I1026 01:13:42.774246 17254 sgd_solver.cpp:106] Iteration 9700, lr = 0.001
I1026 01:13:43.873790 17254 solver.cpp:229] Iteration 9720, loss = 0.070861
I1026 01:13:43.873821 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0309447 (* 1 = 0.0309447 loss)
I1026 01:13:43.873826 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0399163 (* 1 = 0.0399163 loss)
I1026 01:13:43.873831 17254 sgd_solver.cpp:106] Iteration 9720, lr = 0.001
I1026 01:13:44.987206 17254 solver.cpp:229] Iteration 9740, loss = 0.12365
I1026 01:13:44.987233 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0533307 (* 1 = 0.0533307 loss)
I1026 01:13:44.987238 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0703192 (* 1 = 0.0703192 loss)
I1026 01:13:44.987244 17254 sgd_solver.cpp:106] Iteration 9740, lr = 0.001
I1026 01:13:46.104063 17254 solver.cpp:229] Iteration 9760, loss = 0.12953
I1026 01:13:46.104094 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0804154 (* 1 = 0.0804154 loss)
I1026 01:13:46.104099 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0491149 (* 1 = 0.0491149 loss)
I1026 01:13:46.104104 17254 sgd_solver.cpp:106] Iteration 9760, lr = 0.001
I1026 01:13:47.206328 17254 solver.cpp:229] Iteration 9780, loss = 0.102526
I1026 01:13:47.206362 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0326881 (* 1 = 0.0326881 loss)
I1026 01:13:47.206365 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0698377 (* 1 = 0.0698377 loss)
I1026 01:13:47.206369 17254 sgd_solver.cpp:106] Iteration 9780, lr = 0.001
I1026 01:13:48.299901 17254 solver.cpp:229] Iteration 9800, loss = 0.0565834
I1026 01:13:48.299933 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0421935 (* 1 = 0.0421935 loss)
I1026 01:13:48.299937 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0143899 (* 1 = 0.0143899 loss)
I1026 01:13:48.299942 17254 sgd_solver.cpp:106] Iteration 9800, lr = 0.001
I1026 01:13:49.395931 17254 solver.cpp:229] Iteration 9820, loss = 0.16105
I1026 01:13:49.395961 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0778158 (* 1 = 0.0778158 loss)
I1026 01:13:49.395965 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0832346 (* 1 = 0.0832346 loss)
I1026 01:13:49.395969 17254 sgd_solver.cpp:106] Iteration 9820, lr = 0.001
I1026 01:13:50.514410 17254 solver.cpp:229] Iteration 9840, loss = 0.119214
I1026 01:13:50.514441 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0509958 (* 1 = 0.0509958 loss)
I1026 01:13:50.514446 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0682181 (* 1 = 0.0682181 loss)
I1026 01:13:50.514451 17254 sgd_solver.cpp:106] Iteration 9840, lr = 0.001
I1026 01:13:51.597694 17254 solver.cpp:229] Iteration 9860, loss = 0.149564
I1026 01:13:51.597728 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.112954 (* 1 = 0.112954 loss)
I1026 01:13:51.597734 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0366107 (* 1 = 0.0366107 loss)
I1026 01:13:51.597738 17254 sgd_solver.cpp:106] Iteration 9860, lr = 0.001
I1026 01:13:52.705442 17254 solver.cpp:229] Iteration 9880, loss = 0.0779335
I1026 01:13:52.705474 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0362183 (* 1 = 0.0362183 loss)
I1026 01:13:52.705479 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0417153 (* 1 = 0.0417153 loss)
I1026 01:13:52.705484 17254 sgd_solver.cpp:106] Iteration 9880, lr = 0.001
I1026 01:13:53.793246 17254 solver.cpp:229] Iteration 9900, loss = 0.0733192
I1026 01:13:53.793278 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0305938 (* 1 = 0.0305938 loss)
I1026 01:13:53.793283 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0427254 (* 1 = 0.0427254 loss)
I1026 01:13:53.793288 17254 sgd_solver.cpp:106] Iteration 9900, lr = 0.001
I1026 01:13:54.901533 17254 solver.cpp:229] Iteration 9920, loss = 0.150967
I1026 01:13:54.901566 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0693532 (* 1 = 0.0693532 loss)
I1026 01:13:54.901569 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0816143 (* 1 = 0.0816143 loss)
I1026 01:13:54.901574 17254 sgd_solver.cpp:106] Iteration 9920, lr = 0.001
I1026 01:13:55.975217 17254 solver.cpp:229] Iteration 9940, loss = 0.0660844
I1026 01:13:55.975250 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0475433 (* 1 = 0.0475433 loss)
I1026 01:13:55.975255 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.018541 (* 1 = 0.018541 loss)
I1026 01:13:55.975260 17254 sgd_solver.cpp:106] Iteration 9940, lr = 0.001
I1026 01:13:57.065852 17254 solver.cpp:229] Iteration 9960, loss = 0.168497
I1026 01:13:57.065886 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0579668 (* 1 = 0.0579668 loss)
I1026 01:13:57.065891 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.11053 (* 1 = 0.11053 loss)
I1026 01:13:57.065896 17254 sgd_solver.cpp:106] Iteration 9960, lr = 0.001
I1026 01:13:58.174875 17254 solver.cpp:229] Iteration 9980, loss = 0.13267
I1026 01:13:58.174917 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0804146 (* 1 = 0.0804146 loss)
I1026 01:13:58.174923 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0522556 (* 1 = 0.0522556 loss)
I1026 01:13:58.174928 17254 sgd_solver.cpp:106] Iteration 9980, lr = 0.001
I1026 01:13:59.692306 17254 solver.cpp:229] Iteration 10000, loss = 0.118302
I1026 01:13:59.692337 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0714945 (* 1 = 0.0714945 loss)
I1026 01:13:59.692342 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0468074 (* 1 = 0.0468074 loss)
I1026 01:13:59.692348 17254 sgd_solver.cpp:106] Iteration 10000, lr = 0.001
I1026 01:14:00.780107 17254 solver.cpp:229] Iteration 10020, loss = 0.0481258
I1026 01:14:00.780135 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.027017 (* 1 = 0.027017 loss)
I1026 01:14:00.780140 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0211088 (* 1 = 0.0211088 loss)
I1026 01:14:00.780145 17254 sgd_solver.cpp:106] Iteration 10020, lr = 0.001
I1026 01:14:01.858414 17254 solver.cpp:229] Iteration 10040, loss = 0.0500884
I1026 01:14:01.858448 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0396631 (* 1 = 0.0396631 loss)
I1026 01:14:01.858453 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0104253 (* 1 = 0.0104253 loss)
I1026 01:14:01.858458 17254 sgd_solver.cpp:106] Iteration 10040, lr = 0.001
I1026 01:14:02.937860 17254 solver.cpp:229] Iteration 10060, loss = 0.105305
I1026 01:14:02.937891 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.077737 (* 1 = 0.077737 loss)
I1026 01:14:02.937897 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0275676 (* 1 = 0.0275676 loss)
I1026 01:14:02.937902 17254 sgd_solver.cpp:106] Iteration 10060, lr = 0.001
I1026 01:14:04.012501 17254 solver.cpp:229] Iteration 10080, loss = 0.106179
I1026 01:14:04.012534 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0663447 (* 1 = 0.0663447 loss)
I1026 01:14:04.012539 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0398345 (* 1 = 0.0398345 loss)
I1026 01:14:04.012544 17254 sgd_solver.cpp:106] Iteration 10080, lr = 0.001
I1026 01:14:05.112941 17254 solver.cpp:229] Iteration 10100, loss = 0.105167
I1026 01:14:05.112972 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0458166 (* 1 = 0.0458166 loss)
I1026 01:14:05.112977 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0593503 (* 1 = 0.0593503 loss)
I1026 01:14:05.112982 17254 sgd_solver.cpp:106] Iteration 10100, lr = 0.001
I1026 01:14:06.194898 17254 solver.cpp:229] Iteration 10120, loss = 0.120067
I1026 01:14:06.194931 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0696089 (* 1 = 0.0696089 loss)
I1026 01:14:06.194936 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0504581 (* 1 = 0.0504581 loss)
I1026 01:14:06.194941 17254 sgd_solver.cpp:106] Iteration 10120, lr = 0.001
I1026 01:14:07.282023 17254 solver.cpp:229] Iteration 10140, loss = 0.0898148
I1026 01:14:07.282057 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.033115 (* 1 = 0.033115 loss)
I1026 01:14:07.282061 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0566998 (* 1 = 0.0566998 loss)
I1026 01:14:07.282066 17254 sgd_solver.cpp:106] Iteration 10140, lr = 0.001
I1026 01:14:08.349988 17254 solver.cpp:229] Iteration 10160, loss = 0.168617
I1026 01:14:08.350020 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0421031 (* 1 = 0.0421031 loss)
I1026 01:14:08.350025 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.126514 (* 1 = 0.126514 loss)
I1026 01:14:08.350030 17254 sgd_solver.cpp:106] Iteration 10160, lr = 0.001
I1026 01:14:09.431012 17254 solver.cpp:229] Iteration 10180, loss = 0.197915
I1026 01:14:09.431044 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0666173 (* 1 = 0.0666173 loss)
I1026 01:14:09.431049 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.131298 (* 1 = 0.131298 loss)
I1026 01:14:09.431054 17254 sgd_solver.cpp:106] Iteration 10180, lr = 0.001
I1026 01:14:10.490248 17254 solver.cpp:229] Iteration 10200, loss = 0.103236
I1026 01:14:10.490283 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0432584 (* 1 = 0.0432584 loss)
I1026 01:14:10.490288 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0599781 (* 1 = 0.0599781 loss)
I1026 01:14:10.490293 17254 sgd_solver.cpp:106] Iteration 10200, lr = 0.001
I1026 01:14:11.591781 17254 solver.cpp:229] Iteration 10220, loss = 0.112069
I1026 01:14:11.591814 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0380928 (* 1 = 0.0380928 loss)
I1026 01:14:11.591820 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0739759 (* 1 = 0.0739759 loss)
I1026 01:14:11.591823 17254 sgd_solver.cpp:106] Iteration 10220, lr = 0.001
I1026 01:14:12.675549 17254 solver.cpp:229] Iteration 10240, loss = 0.166871
I1026 01:14:12.675580 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0661217 (* 1 = 0.0661217 loss)
I1026 01:14:12.675586 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.100749 (* 1 = 0.100749 loss)
I1026 01:14:12.675591 17254 sgd_solver.cpp:106] Iteration 10240, lr = 0.001
I1026 01:14:13.745795 17254 solver.cpp:229] Iteration 10260, loss = 0.130278
I1026 01:14:13.745826 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0649608 (* 1 = 0.0649608 loss)
I1026 01:14:13.745831 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0653172 (* 1 = 0.0653172 loss)
I1026 01:14:13.745836 17254 sgd_solver.cpp:106] Iteration 10260, lr = 0.001
I1026 01:14:14.836531 17254 solver.cpp:229] Iteration 10280, loss = 0.067165
I1026 01:14:14.836563 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0453718 (* 1 = 0.0453718 loss)
I1026 01:14:14.836570 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0217932 (* 1 = 0.0217932 loss)
I1026 01:14:14.836575 17254 sgd_solver.cpp:106] Iteration 10280, lr = 0.001
I1026 01:14:15.935716 17254 solver.cpp:229] Iteration 10300, loss = 0.110516
I1026 01:14:15.935748 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0777732 (* 1 = 0.0777732 loss)
I1026 01:14:15.935753 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.032743 (* 1 = 0.032743 loss)
I1026 01:14:15.935758 17254 sgd_solver.cpp:106] Iteration 10300, lr = 0.001
I1026 01:14:17.017891 17254 solver.cpp:229] Iteration 10320, loss = 0.116109
I1026 01:14:17.017923 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0460297 (* 1 = 0.0460297 loss)
I1026 01:14:17.017928 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0700796 (* 1 = 0.0700796 loss)
I1026 01:14:17.017933 17254 sgd_solver.cpp:106] Iteration 10320, lr = 0.001
I1026 01:14:18.114253 17254 solver.cpp:229] Iteration 10340, loss = 0.132231
I1026 01:14:18.114289 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0884708 (* 1 = 0.0884708 loss)
I1026 01:14:18.114294 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0437606 (* 1 = 0.0437606 loss)
I1026 01:14:18.114310 17254 sgd_solver.cpp:106] Iteration 10340, lr = 0.001
I1026 01:14:19.201509 17254 solver.cpp:229] Iteration 10360, loss = 0.071244
I1026 01:14:19.201541 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0494826 (* 1 = 0.0494826 loss)
I1026 01:14:19.201547 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0217614 (* 1 = 0.0217614 loss)
I1026 01:14:19.201552 17254 sgd_solver.cpp:106] Iteration 10360, lr = 0.001
I1026 01:14:20.295807 17254 solver.cpp:229] Iteration 10380, loss = 0.079718
I1026 01:14:20.295838 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0535456 (* 1 = 0.0535456 loss)
I1026 01:14:20.295843 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0261724 (* 1 = 0.0261724 loss)
I1026 01:14:20.295850 17254 sgd_solver.cpp:106] Iteration 10380, lr = 0.001
I1026 01:14:21.358832 17254 solver.cpp:229] Iteration 10400, loss = 0.158573
I1026 01:14:21.358865 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0676586 (* 1 = 0.0676586 loss)
I1026 01:14:21.358870 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0909147 (* 1 = 0.0909147 loss)
I1026 01:14:21.358875 17254 sgd_solver.cpp:106] Iteration 10400, lr = 0.001
I1026 01:14:22.444914 17254 solver.cpp:229] Iteration 10420, loss = 0.0839071
I1026 01:14:22.444946 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.049334 (* 1 = 0.049334 loss)
I1026 01:14:22.444950 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0345731 (* 1 = 0.0345731 loss)
I1026 01:14:22.444957 17254 sgd_solver.cpp:106] Iteration 10420, lr = 0.001
I1026 01:14:23.539628 17254 solver.cpp:229] Iteration 10440, loss = 0.0575276
I1026 01:14:23.539660 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0389845 (* 1 = 0.0389845 loss)
I1026 01:14:23.539666 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0185432 (* 1 = 0.0185432 loss)
I1026 01:14:23.539671 17254 sgd_solver.cpp:106] Iteration 10440, lr = 0.001
I1026 01:14:24.638089 17254 solver.cpp:229] Iteration 10460, loss = 0.249328
I1026 01:14:24.638118 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.115331 (* 1 = 0.115331 loss)
I1026 01:14:24.638123 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.133998 (* 1 = 0.133998 loss)
I1026 01:14:24.638128 17254 sgd_solver.cpp:106] Iteration 10460, lr = 0.001
I1026 01:14:25.727383 17254 solver.cpp:229] Iteration 10480, loss = 0.183133
I1026 01:14:25.727416 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0738905 (* 1 = 0.0738905 loss)
I1026 01:14:25.727421 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.109242 (* 1 = 0.109242 loss)
I1026 01:14:25.727452 17254 sgd_solver.cpp:106] Iteration 10480, lr = 0.001
I1026 01:14:26.804420 17254 solver.cpp:229] Iteration 10500, loss = 0.180899
I1026 01:14:26.804452 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.13205 (* 1 = 0.13205 loss)
I1026 01:14:26.804458 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0488487 (* 1 = 0.0488487 loss)
I1026 01:14:26.804462 17254 sgd_solver.cpp:106] Iteration 10500, lr = 0.001
I1026 01:14:27.904508 17254 solver.cpp:229] Iteration 10520, loss = 0.105148
I1026 01:14:27.904541 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0490978 (* 1 = 0.0490978 loss)
I1026 01:14:27.904546 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0560506 (* 1 = 0.0560506 loss)
I1026 01:14:27.904562 17254 sgd_solver.cpp:106] Iteration 10520, lr = 0.001
I1026 01:14:28.997524 17254 solver.cpp:229] Iteration 10540, loss = 0.0569775
I1026 01:14:28.997556 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0397191 (* 1 = 0.0397191 loss)
I1026 01:14:28.997561 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0172584 (* 1 = 0.0172584 loss)
I1026 01:14:28.997567 17254 sgd_solver.cpp:106] Iteration 10540, lr = 0.001
I1026 01:14:30.098978 17254 solver.cpp:229] Iteration 10560, loss = 0.0686674
I1026 01:14:30.099036 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0374442 (* 1 = 0.0374442 loss)
I1026 01:14:30.099051 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0312231 (* 1 = 0.0312231 loss)
I1026 01:14:30.099057 17254 sgd_solver.cpp:106] Iteration 10560, lr = 0.001
I1026 01:14:31.197049 17254 solver.cpp:229] Iteration 10580, loss = 0.141387
I1026 01:14:31.197083 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.081854 (* 1 = 0.081854 loss)
I1026 01:14:31.197088 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0595331 (* 1 = 0.0595331 loss)
I1026 01:14:31.197093 17254 sgd_solver.cpp:106] Iteration 10580, lr = 0.001
I1026 01:14:32.286855 17254 solver.cpp:229] Iteration 10600, loss = 0.257213
I1026 01:14:32.286900 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0667171 (* 1 = 0.0667171 loss)
I1026 01:14:32.286905 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.190496 (* 1 = 0.190496 loss)
I1026 01:14:32.286911 17254 sgd_solver.cpp:106] Iteration 10600, lr = 0.001
I1026 01:14:33.385148 17254 solver.cpp:229] Iteration 10620, loss = 0.0761323
I1026 01:14:33.385190 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0333209 (* 1 = 0.0333209 loss)
I1026 01:14:33.385195 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0428115 (* 1 = 0.0428115 loss)
I1026 01:14:33.385200 17254 sgd_solver.cpp:106] Iteration 10620, lr = 0.001
I1026 01:14:34.457098 17254 solver.cpp:229] Iteration 10640, loss = 0.0919521
I1026 01:14:34.457129 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0626268 (* 1 = 0.0626268 loss)
I1026 01:14:34.457134 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0293253 (* 1 = 0.0293253 loss)
I1026 01:14:34.457139 17254 sgd_solver.cpp:106] Iteration 10640, lr = 0.001
I1026 01:14:35.564018 17254 solver.cpp:229] Iteration 10660, loss = 0.0577636
I1026 01:14:35.564051 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0342125 (* 1 = 0.0342125 loss)
I1026 01:14:35.564056 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0235511 (* 1 = 0.0235511 loss)
I1026 01:14:35.564061 17254 sgd_solver.cpp:106] Iteration 10660, lr = 0.001
I1026 01:14:36.657258 17254 solver.cpp:229] Iteration 10680, loss = 0.132656
I1026 01:14:36.657287 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0877106 (* 1 = 0.0877106 loss)
I1026 01:14:36.657292 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.044945 (* 1 = 0.044945 loss)
I1026 01:14:36.657297 17254 sgd_solver.cpp:106] Iteration 10680, lr = 0.001
I1026 01:14:37.739092 17254 solver.cpp:229] Iteration 10700, loss = 0.0728624
I1026 01:14:37.739125 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0386164 (* 1 = 0.0386164 loss)
I1026 01:14:37.739130 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.034246 (* 1 = 0.034246 loss)
I1026 01:14:37.739135 17254 sgd_solver.cpp:106] Iteration 10700, lr = 0.001
I1026 01:14:38.844025 17254 solver.cpp:229] Iteration 10720, loss = 0.0741376
I1026 01:14:38.844069 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0535448 (* 1 = 0.0535448 loss)
I1026 01:14:38.844075 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0205927 (* 1 = 0.0205927 loss)
I1026 01:14:38.844080 17254 sgd_solver.cpp:106] Iteration 10720, lr = 0.001
I1026 01:14:39.952167 17254 solver.cpp:229] Iteration 10740, loss = 0.177
I1026 01:14:39.952200 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0681438 (* 1 = 0.0681438 loss)
I1026 01:14:39.952206 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.108856 (* 1 = 0.108856 loss)
I1026 01:14:39.952211 17254 sgd_solver.cpp:106] Iteration 10740, lr = 0.001
I1026 01:14:41.029515 17254 solver.cpp:229] Iteration 10760, loss = 0.122958
I1026 01:14:41.029549 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0634305 (* 1 = 0.0634305 loss)
I1026 01:14:41.029554 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.059528 (* 1 = 0.059528 loss)
I1026 01:14:41.029559 17254 sgd_solver.cpp:106] Iteration 10760, lr = 0.001
I1026 01:14:42.115233 17254 solver.cpp:229] Iteration 10780, loss = 0.140332
I1026 01:14:42.115277 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.112402 (* 1 = 0.112402 loss)
I1026 01:14:42.115283 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0279302 (* 1 = 0.0279302 loss)
I1026 01:14:42.115288 17254 sgd_solver.cpp:106] Iteration 10780, lr = 0.001
I1026 01:14:43.194133 17254 solver.cpp:229] Iteration 10800, loss = 0.166293
I1026 01:14:43.194164 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0808999 (* 1 = 0.0808999 loss)
I1026 01:14:43.194169 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0853931 (* 1 = 0.0853931 loss)
I1026 01:14:43.194175 17254 sgd_solver.cpp:106] Iteration 10800, lr = 0.001
I1026 01:14:44.290935 17254 solver.cpp:229] Iteration 10820, loss = 0.0863996
I1026 01:14:44.290967 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0505222 (* 1 = 0.0505222 loss)
I1026 01:14:44.290973 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0358774 (* 1 = 0.0358774 loss)
I1026 01:14:44.290978 17254 sgd_solver.cpp:106] Iteration 10820, lr = 0.001
I1026 01:14:45.399724 17254 solver.cpp:229] Iteration 10840, loss = 0.143031
I1026 01:14:45.399756 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0578418 (* 1 = 0.0578418 loss)
I1026 01:14:45.399762 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0851888 (* 1 = 0.0851888 loss)
I1026 01:14:45.399768 17254 sgd_solver.cpp:106] Iteration 10840, lr = 0.001
I1026 01:14:46.478960 17254 solver.cpp:229] Iteration 10860, loss = 0.1902
I1026 01:14:46.478991 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.110863 (* 1 = 0.110863 loss)
I1026 01:14:46.478996 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0793376 (* 1 = 0.0793376 loss)
I1026 01:14:46.479001 17254 sgd_solver.cpp:106] Iteration 10860, lr = 0.001
I1026 01:14:47.575490 17254 solver.cpp:229] Iteration 10880, loss = 0.0932082
I1026 01:14:47.575533 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0541319 (* 1 = 0.0541319 loss)
I1026 01:14:47.575538 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0390764 (* 1 = 0.0390764 loss)
I1026 01:14:47.575544 17254 sgd_solver.cpp:106] Iteration 10880, lr = 0.001
I1026 01:14:48.666440 17254 solver.cpp:229] Iteration 10900, loss = 0.0757665
I1026 01:14:48.666483 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0480782 (* 1 = 0.0480782 loss)
I1026 01:14:48.666488 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0276883 (* 1 = 0.0276883 loss)
I1026 01:14:48.666493 17254 sgd_solver.cpp:106] Iteration 10900, lr = 0.001
I1026 01:14:49.754561 17254 solver.cpp:229] Iteration 10920, loss = 0.0683977
I1026 01:14:49.754595 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.046459 (* 1 = 0.046459 loss)
I1026 01:14:49.754600 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0219387 (* 1 = 0.0219387 loss)
I1026 01:14:49.754606 17254 sgd_solver.cpp:106] Iteration 10920, lr = 0.001
I1026 01:14:50.843919 17254 solver.cpp:229] Iteration 10940, loss = 0.0550358
I1026 01:14:50.843952 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0347519 (* 1 = 0.0347519 loss)
I1026 01:14:50.843957 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0202839 (* 1 = 0.0202839 loss)
I1026 01:14:50.843963 17254 sgd_solver.cpp:106] Iteration 10940, lr = 0.001
I1026 01:14:51.936975 17254 solver.cpp:229] Iteration 10960, loss = 0.103567
I1026 01:14:51.937007 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0598025 (* 1 = 0.0598025 loss)
I1026 01:14:51.937012 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0437644 (* 1 = 0.0437644 loss)
I1026 01:14:51.937017 17254 sgd_solver.cpp:106] Iteration 10960, lr = 0.001
I1026 01:14:53.012976 17254 solver.cpp:229] Iteration 10980, loss = 0.104908
I1026 01:14:53.013010 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0507288 (* 1 = 0.0507288 loss)
I1026 01:14:53.013015 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0541797 (* 1 = 0.0541797 loss)
I1026 01:14:53.013020 17254 sgd_solver.cpp:106] Iteration 10980, lr = 0.001
I1026 01:14:54.101143 17254 solver.cpp:229] Iteration 11000, loss = 0.102888
I1026 01:14:54.101176 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0441692 (* 1 = 0.0441692 loss)
I1026 01:14:54.101182 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0587192 (* 1 = 0.0587192 loss)
I1026 01:14:54.101187 17254 sgd_solver.cpp:106] Iteration 11000, lr = 0.001
I1026 01:14:55.184878 17254 solver.cpp:229] Iteration 11020, loss = 0.210131
I1026 01:14:55.184911 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.146765 (* 1 = 0.146765 loss)
I1026 01:14:55.184931 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0633665 (* 1 = 0.0633665 loss)
I1026 01:14:55.184937 17254 sgd_solver.cpp:106] Iteration 11020, lr = 0.001
I1026 01:14:56.273561 17254 solver.cpp:229] Iteration 11040, loss = 0.0981606
I1026 01:14:56.273593 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0570014 (* 1 = 0.0570014 loss)
I1026 01:14:56.273598 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0411592 (* 1 = 0.0411592 loss)
I1026 01:14:56.273603 17254 sgd_solver.cpp:106] Iteration 11040, lr = 0.001
I1026 01:14:57.366873 17254 solver.cpp:229] Iteration 11060, loss = 0.111092
I1026 01:14:57.366904 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0344342 (* 1 = 0.0344342 loss)
I1026 01:14:57.366909 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0766577 (* 1 = 0.0766577 loss)
I1026 01:14:57.366914 17254 sgd_solver.cpp:106] Iteration 11060, lr = 0.001
I1026 01:14:58.464272 17254 solver.cpp:229] Iteration 11080, loss = 0.0950149
I1026 01:14:58.464304 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0666404 (* 1 = 0.0666404 loss)
I1026 01:14:58.464309 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0283745 (* 1 = 0.0283745 loss)
I1026 01:14:58.464314 17254 sgd_solver.cpp:106] Iteration 11080, lr = 0.001
I1026 01:14:59.547705 17254 solver.cpp:229] Iteration 11100, loss = 0.0534826
I1026 01:14:59.547737 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0401082 (* 1 = 0.0401082 loss)
I1026 01:14:59.547742 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0133744 (* 1 = 0.0133744 loss)
I1026 01:14:59.547757 17254 sgd_solver.cpp:106] Iteration 11100, lr = 0.001
I1026 01:15:00.626942 17254 solver.cpp:229] Iteration 11120, loss = 0.127242
I1026 01:15:00.626982 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0859039 (* 1 = 0.0859039 loss)
I1026 01:15:00.626987 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0413386 (* 1 = 0.0413386 loss)
I1026 01:15:00.626992 17254 sgd_solver.cpp:106] Iteration 11120, lr = 0.001
I1026 01:15:01.721346 17254 solver.cpp:229] Iteration 11140, loss = 0.132455
I1026 01:15:01.721390 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0778625 (* 1 = 0.0778625 loss)
I1026 01:15:01.721396 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0545927 (* 1 = 0.0545927 loss)
I1026 01:15:01.721401 17254 sgd_solver.cpp:106] Iteration 11140, lr = 0.001
I1026 01:15:02.822729 17254 solver.cpp:229] Iteration 11160, loss = 0.131015
I1026 01:15:02.822762 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0929216 (* 1 = 0.0929216 loss)
I1026 01:15:02.822767 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0380931 (* 1 = 0.0380931 loss)
I1026 01:15:02.822772 17254 sgd_solver.cpp:106] Iteration 11160, lr = 0.001
I1026 01:15:03.899616 17254 solver.cpp:229] Iteration 11180, loss = 0.0727181
I1026 01:15:03.899658 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0577873 (* 1 = 0.0577873 loss)
I1026 01:15:03.899663 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0149308 (* 1 = 0.0149308 loss)
I1026 01:15:03.899669 17254 sgd_solver.cpp:106] Iteration 11180, lr = 0.001
I1026 01:15:04.976081 17254 solver.cpp:229] Iteration 11200, loss = 0.0585539
I1026 01:15:04.976114 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0274609 (* 1 = 0.0274609 loss)
I1026 01:15:04.976119 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.031093 (* 1 = 0.031093 loss)
I1026 01:15:04.976124 17254 sgd_solver.cpp:106] Iteration 11200, lr = 0.001
I1026 01:15:06.053352 17254 solver.cpp:229] Iteration 11220, loss = 0.0989717
I1026 01:15:06.053385 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0620799 (* 1 = 0.0620799 loss)
I1026 01:15:06.053390 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0368918 (* 1 = 0.0368918 loss)
I1026 01:15:06.053395 17254 sgd_solver.cpp:106] Iteration 11220, lr = 0.001
I1026 01:15:07.104982 17254 solver.cpp:229] Iteration 11240, loss = 0.0430526
I1026 01:15:07.105011 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0356734 (* 1 = 0.0356734 loss)
I1026 01:15:07.105016 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00737919 (* 1 = 0.00737919 loss)
I1026 01:15:07.105021 17254 sgd_solver.cpp:106] Iteration 11240, lr = 0.001
I1026 01:15:08.174646 17254 solver.cpp:229] Iteration 11260, loss = 0.098685
I1026 01:15:08.174679 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0474844 (* 1 = 0.0474844 loss)
I1026 01:15:08.174685 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0512006 (* 1 = 0.0512006 loss)
I1026 01:15:08.174690 17254 sgd_solver.cpp:106] Iteration 11260, lr = 0.001
I1026 01:15:09.260936 17254 solver.cpp:229] Iteration 11280, loss = 0.116263
I1026 01:15:09.260968 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0339601 (* 1 = 0.0339601 loss)
I1026 01:15:09.260973 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0823034 (* 1 = 0.0823034 loss)
I1026 01:15:09.260978 17254 sgd_solver.cpp:106] Iteration 11280, lr = 0.001
I1026 01:15:10.347631 17254 solver.cpp:229] Iteration 11300, loss = 0.143477
I1026 01:15:10.347664 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.102369 (* 1 = 0.102369 loss)
I1026 01:15:10.347671 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0411085 (* 1 = 0.0411085 loss)
I1026 01:15:10.347676 17254 sgd_solver.cpp:106] Iteration 11300, lr = 0.001
I1026 01:15:11.427633 17254 solver.cpp:229] Iteration 11320, loss = 0.116262
I1026 01:15:11.427665 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0603847 (* 1 = 0.0603847 loss)
I1026 01:15:11.427670 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0558777 (* 1 = 0.0558777 loss)
I1026 01:15:11.427675 17254 sgd_solver.cpp:106] Iteration 11320, lr = 0.001
I1026 01:15:12.515280 17254 solver.cpp:229] Iteration 11340, loss = 0.0910148
I1026 01:15:12.515312 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0424401 (* 1 = 0.0424401 loss)
I1026 01:15:12.515317 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0485747 (* 1 = 0.0485747 loss)
I1026 01:15:12.515323 17254 sgd_solver.cpp:106] Iteration 11340, lr = 0.001
I1026 01:15:13.587189 17254 solver.cpp:229] Iteration 11360, loss = 0.0678217
I1026 01:15:13.587224 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0503962 (* 1 = 0.0503962 loss)
I1026 01:15:13.587232 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0174256 (* 1 = 0.0174256 loss)
I1026 01:15:13.587239 17254 sgd_solver.cpp:106] Iteration 11360, lr = 0.001
I1026 01:15:14.669098 17254 solver.cpp:229] Iteration 11380, loss = 0.0926592
I1026 01:15:14.669133 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.049496 (* 1 = 0.049496 loss)
I1026 01:15:14.669142 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0431632 (* 1 = 0.0431632 loss)
I1026 01:15:14.669158 17254 sgd_solver.cpp:106] Iteration 11380, lr = 0.001
I1026 01:15:15.747206 17254 solver.cpp:229] Iteration 11400, loss = 0.0770609
I1026 01:15:15.747242 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0330928 (* 1 = 0.0330928 loss)
I1026 01:15:15.747251 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0439682 (* 1 = 0.0439682 loss)
I1026 01:15:15.747257 17254 sgd_solver.cpp:106] Iteration 11400, lr = 0.001
I1026 01:15:16.832222 17254 solver.cpp:229] Iteration 11420, loss = 0.0991157
I1026 01:15:16.832257 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0505952 (* 1 = 0.0505952 loss)
I1026 01:15:16.832262 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0485205 (* 1 = 0.0485205 loss)
I1026 01:15:16.832267 17254 sgd_solver.cpp:106] Iteration 11420, lr = 0.001
I1026 01:15:17.924877 17254 solver.cpp:229] Iteration 11440, loss = 0.148635
I1026 01:15:17.924911 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0846962 (* 1 = 0.0846962 loss)
I1026 01:15:17.924916 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0639388 (* 1 = 0.0639388 loss)
I1026 01:15:17.924921 17254 sgd_solver.cpp:106] Iteration 11440, lr = 0.001
I1026 01:15:19.015028 17254 solver.cpp:229] Iteration 11460, loss = 0.186547
I1026 01:15:19.015059 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0574161 (* 1 = 0.0574161 loss)
I1026 01:15:19.015064 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.129131 (* 1 = 0.129131 loss)
I1026 01:15:19.015069 17254 sgd_solver.cpp:106] Iteration 11460, lr = 0.001
I1026 01:15:20.104110 17254 solver.cpp:229] Iteration 11480, loss = 0.0706879
I1026 01:15:20.104141 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0583184 (* 1 = 0.0583184 loss)
I1026 01:15:20.104146 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0123695 (* 1 = 0.0123695 loss)
I1026 01:15:20.104151 17254 sgd_solver.cpp:106] Iteration 11480, lr = 0.001
I1026 01:15:21.199848 17254 solver.cpp:229] Iteration 11500, loss = 0.0820248
I1026 01:15:21.199880 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0512382 (* 1 = 0.0512382 loss)
I1026 01:15:21.199885 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0307866 (* 1 = 0.0307866 loss)
I1026 01:15:21.199890 17254 sgd_solver.cpp:106] Iteration 11500, lr = 0.001
I1026 01:15:22.284361 17254 solver.cpp:229] Iteration 11520, loss = 0.191787
I1026 01:15:22.284394 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0865786 (* 1 = 0.0865786 loss)
I1026 01:15:22.284399 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.105209 (* 1 = 0.105209 loss)
I1026 01:15:22.284404 17254 sgd_solver.cpp:106] Iteration 11520, lr = 0.001
I1026 01:15:23.390653 17254 solver.cpp:229] Iteration 11540, loss = 0.0508225
I1026 01:15:23.390687 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0414628 (* 1 = 0.0414628 loss)
I1026 01:15:23.390694 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00935964 (* 1 = 0.00935964 loss)
I1026 01:15:23.390702 17254 sgd_solver.cpp:106] Iteration 11540, lr = 0.001
I1026 01:15:24.472307 17254 solver.cpp:229] Iteration 11560, loss = 0.0479807
I1026 01:15:24.472340 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.029435 (* 1 = 0.029435 loss)
I1026 01:15:24.472345 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0185457 (* 1 = 0.0185457 loss)
I1026 01:15:24.472362 17254 sgd_solver.cpp:106] Iteration 11560, lr = 0.001
I1026 01:15:25.568837 17254 solver.cpp:229] Iteration 11580, loss = 0.0746175
I1026 01:15:25.568869 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.040912 (* 1 = 0.040912 loss)
I1026 01:15:25.568876 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0337055 (* 1 = 0.0337055 loss)
I1026 01:15:25.568881 17254 sgd_solver.cpp:106] Iteration 11580, lr = 0.001
I1026 01:15:26.641252 17254 solver.cpp:229] Iteration 11600, loss = 0.140461
I1026 01:15:26.641294 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0758268 (* 1 = 0.0758268 loss)
I1026 01:15:26.641299 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0646341 (* 1 = 0.0646341 loss)
I1026 01:15:26.641305 17254 sgd_solver.cpp:106] Iteration 11600, lr = 0.001
I1026 01:15:27.729338 17254 solver.cpp:229] Iteration 11620, loss = 0.0386248
I1026 01:15:27.729382 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0293359 (* 1 = 0.0293359 loss)
I1026 01:15:27.729387 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00928891 (* 1 = 0.00928891 loss)
I1026 01:15:27.729393 17254 sgd_solver.cpp:106] Iteration 11620, lr = 0.001
I1026 01:15:28.831024 17254 solver.cpp:229] Iteration 11640, loss = 0.0615017
I1026 01:15:28.831071 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0329135 (* 1 = 0.0329135 loss)
I1026 01:15:28.831077 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0285883 (* 1 = 0.0285883 loss)
I1026 01:15:28.831092 17254 sgd_solver.cpp:106] Iteration 11640, lr = 0.001
I1026 01:15:29.926317 17254 solver.cpp:229] Iteration 11660, loss = 0.0611406
I1026 01:15:29.926350 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0312323 (* 1 = 0.0312323 loss)
I1026 01:15:29.926355 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0299083 (* 1 = 0.0299083 loss)
I1026 01:15:29.926360 17254 sgd_solver.cpp:106] Iteration 11660, lr = 0.001
I1026 01:15:31.031659 17254 solver.cpp:229] Iteration 11680, loss = 0.0498579
I1026 01:15:31.031692 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0296297 (* 1 = 0.0296297 loss)
I1026 01:15:31.031697 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0202282 (* 1 = 0.0202282 loss)
I1026 01:15:31.031702 17254 sgd_solver.cpp:106] Iteration 11680, lr = 0.001
I1026 01:15:32.135094 17254 solver.cpp:229] Iteration 11700, loss = 0.0605193
I1026 01:15:32.135128 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0365725 (* 1 = 0.0365725 loss)
I1026 01:15:32.135133 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0239467 (* 1 = 0.0239467 loss)
I1026 01:15:32.135138 17254 sgd_solver.cpp:106] Iteration 11700, lr = 0.001
I1026 01:15:33.247848 17254 solver.cpp:229] Iteration 11720, loss = 0.0760143
I1026 01:15:33.247881 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0488491 (* 1 = 0.0488491 loss)
I1026 01:15:33.247887 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0271653 (* 1 = 0.0271653 loss)
I1026 01:15:33.247892 17254 sgd_solver.cpp:106] Iteration 11720, lr = 0.001
I1026 01:15:34.362164 17254 solver.cpp:229] Iteration 11740, loss = 0.0820593
I1026 01:15:34.362195 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.055173 (* 1 = 0.055173 loss)
I1026 01:15:34.362201 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0268863 (* 1 = 0.0268863 loss)
I1026 01:15:34.362216 17254 sgd_solver.cpp:106] Iteration 11740, lr = 0.001
I1026 01:15:35.450430 17254 solver.cpp:229] Iteration 11760, loss = 0.105539
I1026 01:15:35.450464 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.034691 (* 1 = 0.034691 loss)
I1026 01:15:35.450469 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0708481 (* 1 = 0.0708481 loss)
I1026 01:15:35.450474 17254 sgd_solver.cpp:106] Iteration 11760, lr = 0.001
I1026 01:15:36.544841 17254 solver.cpp:229] Iteration 11780, loss = 0.062383
I1026 01:15:36.544872 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0341972 (* 1 = 0.0341972 loss)
I1026 01:15:36.544877 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0281858 (* 1 = 0.0281858 loss)
I1026 01:15:36.544881 17254 sgd_solver.cpp:106] Iteration 11780, lr = 0.001
I1026 01:15:37.629192 17254 solver.cpp:229] Iteration 11800, loss = 0.0609457
I1026 01:15:37.629225 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0349279 (* 1 = 0.0349279 loss)
I1026 01:15:37.629246 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0260177 (* 1 = 0.0260177 loss)
I1026 01:15:37.629251 17254 sgd_solver.cpp:106] Iteration 11800, lr = 0.001
I1026 01:15:38.715375 17254 solver.cpp:229] Iteration 11820, loss = 0.0800441
I1026 01:15:38.715409 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0518764 (* 1 = 0.0518764 loss)
I1026 01:15:38.715414 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0281677 (* 1 = 0.0281677 loss)
I1026 01:15:38.715418 17254 sgd_solver.cpp:106] Iteration 11820, lr = 0.001
I1026 01:15:39.826692 17254 solver.cpp:229] Iteration 11840, loss = 0.103365
I1026 01:15:39.826725 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0588574 (* 1 = 0.0588574 loss)
I1026 01:15:39.826730 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.044508 (* 1 = 0.044508 loss)
I1026 01:15:39.826735 17254 sgd_solver.cpp:106] Iteration 11840, lr = 0.001
I1026 01:15:40.915906 17254 solver.cpp:229] Iteration 11860, loss = 0.103775
I1026 01:15:40.915941 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0504492 (* 1 = 0.0504492 loss)
I1026 01:15:40.915946 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0533261 (* 1 = 0.0533261 loss)
I1026 01:15:40.915951 17254 sgd_solver.cpp:106] Iteration 11860, lr = 0.001
I1026 01:15:41.994455 17254 solver.cpp:229] Iteration 11880, loss = 0.0424884
I1026 01:15:41.994488 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0322283 (* 1 = 0.0322283 loss)
I1026 01:15:41.994493 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0102601 (* 1 = 0.0102601 loss)
I1026 01:15:41.994498 17254 sgd_solver.cpp:106] Iteration 11880, lr = 0.001
I1026 01:15:43.094425 17254 solver.cpp:229] Iteration 11900, loss = 0.0368333
I1026 01:15:43.094458 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0296179 (* 1 = 0.0296179 loss)
I1026 01:15:43.094465 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00721537 (* 1 = 0.00721537 loss)
I1026 01:15:43.094470 17254 sgd_solver.cpp:106] Iteration 11900, lr = 0.001
I1026 01:15:44.175668 17254 solver.cpp:229] Iteration 11920, loss = 0.0781069
I1026 01:15:44.175700 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0597314 (* 1 = 0.0597314 loss)
I1026 01:15:44.175705 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0183755 (* 1 = 0.0183755 loss)
I1026 01:15:44.175711 17254 sgd_solver.cpp:106] Iteration 11920, lr = 0.001
I1026 01:15:45.253692 17254 solver.cpp:229] Iteration 11940, loss = 0.0866217
I1026 01:15:45.253726 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0637034 (* 1 = 0.0637034 loss)
I1026 01:15:45.253731 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0229184 (* 1 = 0.0229184 loss)
I1026 01:15:45.253736 17254 sgd_solver.cpp:106] Iteration 11940, lr = 0.001
I1026 01:15:46.360565 17254 solver.cpp:229] Iteration 11960, loss = 0.0729756
I1026 01:15:46.360599 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0344718 (* 1 = 0.0344718 loss)
I1026 01:15:46.360604 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0385038 (* 1 = 0.0385038 loss)
I1026 01:15:46.360608 17254 sgd_solver.cpp:106] Iteration 11960, lr = 0.001
I1026 01:15:47.434357 17254 solver.cpp:229] Iteration 11980, loss = 0.116404
I1026 01:15:47.434388 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0700749 (* 1 = 0.0700749 loss)
I1026 01:15:47.434393 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0463293 (* 1 = 0.0463293 loss)
I1026 01:15:47.434399 17254 sgd_solver.cpp:106] Iteration 11980, lr = 0.001
I1026 01:15:48.530655 17254 solver.cpp:229] Iteration 12000, loss = 0.0865502
I1026 01:15:48.530688 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0611259 (* 1 = 0.0611259 loss)
I1026 01:15:48.530692 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0254243 (* 1 = 0.0254243 loss)
I1026 01:15:48.530699 17254 sgd_solver.cpp:106] Iteration 12000, lr = 0.001
I1026 01:15:49.634282 17254 solver.cpp:229] Iteration 12020, loss = 0.0595766
I1026 01:15:49.634315 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0365801 (* 1 = 0.0365801 loss)
I1026 01:15:49.634320 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0229965 (* 1 = 0.0229965 loss)
I1026 01:15:49.634325 17254 sgd_solver.cpp:106] Iteration 12020, lr = 0.001
I1026 01:15:50.729305 17254 solver.cpp:229] Iteration 12040, loss = 0.11978
I1026 01:15:50.729338 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0585358 (* 1 = 0.0585358 loss)
I1026 01:15:50.729343 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0612446 (* 1 = 0.0612446 loss)
I1026 01:15:50.729348 17254 sgd_solver.cpp:106] Iteration 12040, lr = 0.001
I1026 01:15:51.806054 17254 solver.cpp:229] Iteration 12060, loss = 0.102477
I1026 01:15:51.806097 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0461972 (* 1 = 0.0461972 loss)
I1026 01:15:51.806103 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0562793 (* 1 = 0.0562793 loss)
I1026 01:15:51.806108 17254 sgd_solver.cpp:106] Iteration 12060, lr = 0.001
I1026 01:15:52.910533 17254 solver.cpp:229] Iteration 12080, loss = 0.142221
I1026 01:15:52.910567 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0535725 (* 1 = 0.0535725 loss)
I1026 01:15:52.910573 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0886486 (* 1 = 0.0886486 loss)
I1026 01:15:52.910578 17254 sgd_solver.cpp:106] Iteration 12080, lr = 0.001
I1026 01:15:53.997349 17254 solver.cpp:229] Iteration 12100, loss = 0.1818
I1026 01:15:53.997382 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0970412 (* 1 = 0.0970412 loss)
I1026 01:15:53.997388 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0847593 (* 1 = 0.0847593 loss)
I1026 01:15:53.997393 17254 sgd_solver.cpp:106] Iteration 12100, lr = 0.001
I1026 01:15:55.112484 17254 solver.cpp:229] Iteration 12120, loss = 0.141081
I1026 01:15:55.112515 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0486595 (* 1 = 0.0486595 loss)
I1026 01:15:55.112520 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0924214 (* 1 = 0.0924214 loss)
I1026 01:15:55.112525 17254 sgd_solver.cpp:106] Iteration 12120, lr = 0.001
I1026 01:15:56.193581 17254 solver.cpp:229] Iteration 12140, loss = 0.106072
I1026 01:15:56.193614 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.068989 (* 1 = 0.068989 loss)
I1026 01:15:56.193620 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.037083 (* 1 = 0.037083 loss)
I1026 01:15:56.193625 17254 sgd_solver.cpp:106] Iteration 12140, lr = 0.001
I1026 01:15:57.278102 17254 solver.cpp:229] Iteration 12160, loss = 0.0501768
I1026 01:15:57.278148 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0261827 (* 1 = 0.0261827 loss)
I1026 01:15:57.278153 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0239941 (* 1 = 0.0239941 loss)
I1026 01:15:57.278175 17254 sgd_solver.cpp:106] Iteration 12160, lr = 0.001
I1026 01:15:58.364540 17254 solver.cpp:229] Iteration 12180, loss = 0.052987
I1026 01:15:58.364573 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0281835 (* 1 = 0.0281835 loss)
I1026 01:15:58.364578 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0248035 (* 1 = 0.0248035 loss)
I1026 01:15:58.364583 17254 sgd_solver.cpp:106] Iteration 12180, lr = 0.001
I1026 01:15:59.438093 17254 solver.cpp:229] Iteration 12200, loss = 0.121193
I1026 01:15:59.438125 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0739768 (* 1 = 0.0739768 loss)
I1026 01:15:59.438132 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0472164 (* 1 = 0.0472164 loss)
I1026 01:15:59.438135 17254 sgd_solver.cpp:106] Iteration 12200, lr = 0.001
I1026 01:16:00.526088 17254 solver.cpp:229] Iteration 12220, loss = 0.0853565
I1026 01:16:00.526120 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0545162 (* 1 = 0.0545162 loss)
I1026 01:16:00.526125 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0308403 (* 1 = 0.0308403 loss)
I1026 01:16:00.526130 17254 sgd_solver.cpp:106] Iteration 12220, lr = 0.001
I1026 01:16:01.618374 17254 solver.cpp:229] Iteration 12240, loss = 0.068174
I1026 01:16:01.618415 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0619739 (* 1 = 0.0619739 loss)
I1026 01:16:01.618422 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00620004 (* 1 = 0.00620004 loss)
I1026 01:16:01.618427 17254 sgd_solver.cpp:106] Iteration 12240, lr = 0.001
I1026 01:16:02.716531 17254 solver.cpp:229] Iteration 12260, loss = 0.165317
I1026 01:16:02.716563 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.116338 (* 1 = 0.116338 loss)
I1026 01:16:02.716568 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0489788 (* 1 = 0.0489788 loss)
I1026 01:16:02.716573 17254 sgd_solver.cpp:106] Iteration 12260, lr = 0.001
I1026 01:16:03.782465 17254 solver.cpp:229] Iteration 12280, loss = 0.0530695
I1026 01:16:03.782497 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0420323 (* 1 = 0.0420323 loss)
I1026 01:16:03.782502 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0110372 (* 1 = 0.0110372 loss)
I1026 01:16:03.782507 17254 sgd_solver.cpp:106] Iteration 12280, lr = 0.001
I1026 01:16:04.858464 17254 solver.cpp:229] Iteration 12300, loss = 0.124338
I1026 01:16:04.858496 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0647204 (* 1 = 0.0647204 loss)
I1026 01:16:04.858502 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0596174 (* 1 = 0.0596174 loss)
I1026 01:16:04.858507 17254 sgd_solver.cpp:106] Iteration 12300, lr = 0.001
I1026 01:16:05.937978 17254 solver.cpp:229] Iteration 12320, loss = 0.156505
I1026 01:16:05.938011 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0822141 (* 1 = 0.0822141 loss)
I1026 01:16:05.938016 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0742912 (* 1 = 0.0742912 loss)
I1026 01:16:05.938022 17254 sgd_solver.cpp:106] Iteration 12320, lr = 0.001
I1026 01:16:07.037811 17254 solver.cpp:229] Iteration 12340, loss = 0.0394997
I1026 01:16:07.037844 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.030106 (* 1 = 0.030106 loss)
I1026 01:16:07.037849 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00939367 (* 1 = 0.00939367 loss)
I1026 01:16:07.037855 17254 sgd_solver.cpp:106] Iteration 12340, lr = 0.001
I1026 01:16:08.114001 17254 solver.cpp:229] Iteration 12360, loss = 0.0529836
I1026 01:16:08.114032 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.031086 (* 1 = 0.031086 loss)
I1026 01:16:08.114038 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0218977 (* 1 = 0.0218977 loss)
I1026 01:16:08.114043 17254 sgd_solver.cpp:106] Iteration 12360, lr = 0.001
I1026 01:16:09.181669 17254 solver.cpp:229] Iteration 12380, loss = 0.137183
I1026 01:16:09.181701 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.102066 (* 1 = 0.102066 loss)
I1026 01:16:09.181706 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0351173 (* 1 = 0.0351173 loss)
I1026 01:16:09.181711 17254 sgd_solver.cpp:106] Iteration 12380, lr = 0.001
I1026 01:16:10.256331 17254 solver.cpp:229] Iteration 12400, loss = 0.069213
I1026 01:16:10.256364 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0496802 (* 1 = 0.0496802 loss)
I1026 01:16:10.256369 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0195328 (* 1 = 0.0195328 loss)
I1026 01:16:10.256374 17254 sgd_solver.cpp:106] Iteration 12400, lr = 0.001
I1026 01:16:11.349387 17254 solver.cpp:229] Iteration 12420, loss = 0.0554399
I1026 01:16:11.349419 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0370205 (* 1 = 0.0370205 loss)
I1026 01:16:11.349424 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0184194 (* 1 = 0.0184194 loss)
I1026 01:16:11.349439 17254 sgd_solver.cpp:106] Iteration 12420, lr = 0.001
I1026 01:16:12.406922 17254 solver.cpp:229] Iteration 12440, loss = 0.0910497
I1026 01:16:12.406955 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.058134 (* 1 = 0.058134 loss)
I1026 01:16:12.406960 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0329157 (* 1 = 0.0329157 loss)
I1026 01:16:12.406965 17254 sgd_solver.cpp:106] Iteration 12440, lr = 0.001
I1026 01:16:13.485738 17254 solver.cpp:229] Iteration 12460, loss = 0.0653106
I1026 01:16:13.485771 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0369129 (* 1 = 0.0369129 loss)
I1026 01:16:13.485777 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0283977 (* 1 = 0.0283977 loss)
I1026 01:16:13.485782 17254 sgd_solver.cpp:106] Iteration 12460, lr = 0.001
I1026 01:16:14.586493 17254 solver.cpp:229] Iteration 12480, loss = 0.0588214
I1026 01:16:14.586524 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0359086 (* 1 = 0.0359086 loss)
I1026 01:16:14.586529 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0229128 (* 1 = 0.0229128 loss)
I1026 01:16:14.586534 17254 sgd_solver.cpp:106] Iteration 12480, lr = 0.001
I1026 01:16:15.666146 17254 solver.cpp:229] Iteration 12500, loss = 0.0481742
I1026 01:16:15.666177 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0271077 (* 1 = 0.0271077 loss)
I1026 01:16:15.666182 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0210665 (* 1 = 0.0210665 loss)
I1026 01:16:15.666188 17254 sgd_solver.cpp:106] Iteration 12500, lr = 0.001
I1026 01:16:16.737424 17254 solver.cpp:229] Iteration 12520, loss = 0.10169
I1026 01:16:16.737457 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0353034 (* 1 = 0.0353034 loss)
I1026 01:16:16.737462 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.066387 (* 1 = 0.066387 loss)
I1026 01:16:16.737467 17254 sgd_solver.cpp:106] Iteration 12520, lr = 0.001
I1026 01:16:17.814594 17254 solver.cpp:229] Iteration 12540, loss = 0.060509
I1026 01:16:17.814627 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0526077 (* 1 = 0.0526077 loss)
I1026 01:16:17.814633 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00790125 (* 1 = 0.00790125 loss)
I1026 01:16:17.814638 17254 sgd_solver.cpp:106] Iteration 12540, lr = 0.001
I1026 01:16:18.890374 17254 solver.cpp:229] Iteration 12560, loss = 0.119339
I1026 01:16:18.890403 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0596797 (* 1 = 0.0596797 loss)
I1026 01:16:18.890409 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0596597 (* 1 = 0.0596597 loss)
I1026 01:16:18.890414 17254 sgd_solver.cpp:106] Iteration 12560, lr = 0.001
I1026 01:16:19.975836 17254 solver.cpp:229] Iteration 12580, loss = 0.0487614
I1026 01:16:19.975868 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0272935 (* 1 = 0.0272935 loss)
I1026 01:16:19.975873 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0214679 (* 1 = 0.0214679 loss)
I1026 01:16:19.975878 17254 sgd_solver.cpp:106] Iteration 12580, lr = 0.001
I1026 01:16:21.065496 17254 solver.cpp:229] Iteration 12600, loss = 0.0491098
I1026 01:16:21.065529 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0357073 (* 1 = 0.0357073 loss)
I1026 01:16:21.065534 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0134025 (* 1 = 0.0134025 loss)
I1026 01:16:21.065539 17254 sgd_solver.cpp:106] Iteration 12600, lr = 0.001
I1026 01:16:22.148615 17254 solver.cpp:229] Iteration 12620, loss = 0.213587
I1026 01:16:22.148648 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0688356 (* 1 = 0.0688356 loss)
I1026 01:16:22.148653 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.144751 (* 1 = 0.144751 loss)
I1026 01:16:22.148658 17254 sgd_solver.cpp:106] Iteration 12620, lr = 0.001
I1026 01:16:23.238678 17254 solver.cpp:229] Iteration 12640, loss = 0.0883154
I1026 01:16:23.238710 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0358091 (* 1 = 0.0358091 loss)
I1026 01:16:23.238715 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0525063 (* 1 = 0.0525063 loss)
I1026 01:16:23.238721 17254 sgd_solver.cpp:106] Iteration 12640, lr = 0.001
I1026 01:16:24.349846 17254 solver.cpp:229] Iteration 12660, loss = 0.145536
I1026 01:16:24.349879 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0655016 (* 1 = 0.0655016 loss)
I1026 01:16:24.349884 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0800341 (* 1 = 0.0800341 loss)
I1026 01:16:24.349889 17254 sgd_solver.cpp:106] Iteration 12660, lr = 0.001
I1026 01:16:25.427872 17254 solver.cpp:229] Iteration 12680, loss = 0.100313
I1026 01:16:25.427904 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0502889 (* 1 = 0.0502889 loss)
I1026 01:16:25.427909 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0500243 (* 1 = 0.0500243 loss)
I1026 01:16:25.427916 17254 sgd_solver.cpp:106] Iteration 12680, lr = 0.001
I1026 01:16:26.510282 17254 solver.cpp:229] Iteration 12700, loss = 0.0727572
I1026 01:16:26.510315 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0373008 (* 1 = 0.0373008 loss)
I1026 01:16:26.510320 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0354565 (* 1 = 0.0354565 loss)
I1026 01:16:26.510324 17254 sgd_solver.cpp:106] Iteration 12700, lr = 0.001
I1026 01:16:27.585463 17254 solver.cpp:229] Iteration 12720, loss = 0.098259
I1026 01:16:27.585494 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0214406 (* 1 = 0.0214406 loss)
I1026 01:16:27.585499 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0768184 (* 1 = 0.0768184 loss)
I1026 01:16:27.585503 17254 sgd_solver.cpp:106] Iteration 12720, lr = 0.001
I1026 01:16:28.677580 17254 solver.cpp:229] Iteration 12740, loss = 0.0639791
I1026 01:16:28.677613 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0412292 (* 1 = 0.0412292 loss)
I1026 01:16:28.677618 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0227499 (* 1 = 0.0227499 loss)
I1026 01:16:28.677624 17254 sgd_solver.cpp:106] Iteration 12740, lr = 0.001
I1026 01:16:29.769536 17254 solver.cpp:229] Iteration 12760, loss = 0.120005
I1026 01:16:29.769568 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0519993 (* 1 = 0.0519993 loss)
I1026 01:16:29.769574 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0680061 (* 1 = 0.0680061 loss)
I1026 01:16:29.769578 17254 sgd_solver.cpp:106] Iteration 12760, lr = 0.001
I1026 01:16:30.844977 17254 solver.cpp:229] Iteration 12780, loss = 0.0965252
I1026 01:16:30.845010 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0490946 (* 1 = 0.0490946 loss)
I1026 01:16:30.845015 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0474306 (* 1 = 0.0474306 loss)
I1026 01:16:30.845019 17254 sgd_solver.cpp:106] Iteration 12780, lr = 0.001
I1026 01:16:31.937398 17254 solver.cpp:229] Iteration 12800, loss = 0.111206
I1026 01:16:31.937432 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0764341 (* 1 = 0.0764341 loss)
I1026 01:16:31.937436 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.034772 (* 1 = 0.034772 loss)
I1026 01:16:31.937441 17254 sgd_solver.cpp:106] Iteration 12800, lr = 0.001
I1026 01:16:33.045250 17254 solver.cpp:229] Iteration 12820, loss = 0.153643
I1026 01:16:33.045300 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0467665 (* 1 = 0.0467665 loss)
I1026 01:16:33.045305 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.106876 (* 1 = 0.106876 loss)
I1026 01:16:33.045310 17254 sgd_solver.cpp:106] Iteration 12820, lr = 0.001
I1026 01:16:34.148329 17254 solver.cpp:229] Iteration 12840, loss = 0.14241
I1026 01:16:34.148361 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0879038 (* 1 = 0.0879038 loss)
I1026 01:16:34.148366 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0545063 (* 1 = 0.0545063 loss)
I1026 01:16:34.148371 17254 sgd_solver.cpp:106] Iteration 12840, lr = 0.001
I1026 01:16:35.238829 17254 solver.cpp:229] Iteration 12860, loss = 0.0614253
I1026 01:16:35.238862 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0311355 (* 1 = 0.0311355 loss)
I1026 01:16:35.238867 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0302898 (* 1 = 0.0302898 loss)
I1026 01:16:35.238873 17254 sgd_solver.cpp:106] Iteration 12860, lr = 0.001
I1026 01:16:36.336405 17254 solver.cpp:229] Iteration 12880, loss = 0.0468123
I1026 01:16:36.336437 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0187612 (* 1 = 0.0187612 loss)
I1026 01:16:36.336443 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0280511 (* 1 = 0.0280511 loss)
I1026 01:16:36.336447 17254 sgd_solver.cpp:106] Iteration 12880, lr = 0.001
I1026 01:16:37.432020 17254 solver.cpp:229] Iteration 12900, loss = 0.111194
I1026 01:16:37.432052 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.073989 (* 1 = 0.073989 loss)
I1026 01:16:37.432057 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0372045 (* 1 = 0.0372045 loss)
I1026 01:16:37.432062 17254 sgd_solver.cpp:106] Iteration 12900, lr = 0.001
I1026 01:16:38.522974 17254 solver.cpp:229] Iteration 12920, loss = 0.11125
I1026 01:16:38.523006 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0697305 (* 1 = 0.0697305 loss)
I1026 01:16:38.523011 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0415193 (* 1 = 0.0415193 loss)
I1026 01:16:38.523016 17254 sgd_solver.cpp:106] Iteration 12920, lr = 0.001
I1026 01:16:39.604418 17254 solver.cpp:229] Iteration 12940, loss = 0.0477936
I1026 01:16:39.604449 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0418825 (* 1 = 0.0418825 loss)
I1026 01:16:39.604455 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00591104 (* 1 = 0.00591104 loss)
I1026 01:16:39.604461 17254 sgd_solver.cpp:106] Iteration 12940, lr = 0.001
I1026 01:16:40.685650 17254 solver.cpp:229] Iteration 12960, loss = 0.211807
I1026 01:16:40.685683 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.148513 (* 1 = 0.148513 loss)
I1026 01:16:40.685688 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0632945 (* 1 = 0.0632945 loss)
I1026 01:16:40.685693 17254 sgd_solver.cpp:106] Iteration 12960, lr = 0.001
I1026 01:16:41.775774 17254 solver.cpp:229] Iteration 12980, loss = 0.0455901
I1026 01:16:41.775805 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.039479 (* 1 = 0.039479 loss)
I1026 01:16:41.775811 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00611106 (* 1 = 0.00611106 loss)
I1026 01:16:41.775816 17254 sgd_solver.cpp:106] Iteration 12980, lr = 0.001
I1026 01:16:42.860592 17254 solver.cpp:229] Iteration 13000, loss = 0.0596219
I1026 01:16:42.860625 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0445014 (* 1 = 0.0445014 loss)
I1026 01:16:42.860630 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0151205 (* 1 = 0.0151205 loss)
I1026 01:16:42.860635 17254 sgd_solver.cpp:106] Iteration 13000, lr = 0.001
I1026 01:16:43.953251 17254 solver.cpp:229] Iteration 13020, loss = 0.0493382
I1026 01:16:43.953295 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0335203 (* 1 = 0.0335203 loss)
I1026 01:16:43.953300 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0158179 (* 1 = 0.0158179 loss)
I1026 01:16:43.953306 17254 sgd_solver.cpp:106] Iteration 13020, lr = 0.001
I1026 01:16:45.047196 17254 solver.cpp:229] Iteration 13040, loss = 0.0894193
I1026 01:16:45.047230 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0629986 (* 1 = 0.0629986 loss)
I1026 01:16:45.047235 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0264206 (* 1 = 0.0264206 loss)
I1026 01:16:45.047240 17254 sgd_solver.cpp:106] Iteration 13040, lr = 0.001
I1026 01:16:46.128882 17254 solver.cpp:229] Iteration 13060, loss = 0.136673
I1026 01:16:46.128914 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0905183 (* 1 = 0.0905183 loss)
I1026 01:16:46.128921 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0461551 (* 1 = 0.0461551 loss)
I1026 01:16:46.128926 17254 sgd_solver.cpp:106] Iteration 13060, lr = 0.001
I1026 01:16:47.202368 17254 solver.cpp:229] Iteration 13080, loss = 0.0883746
I1026 01:16:47.202396 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0643955 (* 1 = 0.0643955 loss)
I1026 01:16:47.202401 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0239791 (* 1 = 0.0239791 loss)
I1026 01:16:47.202406 17254 sgd_solver.cpp:106] Iteration 13080, lr = 0.001
I1026 01:16:48.282064 17254 solver.cpp:229] Iteration 13100, loss = 0.074624
I1026 01:16:48.282109 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0354728 (* 1 = 0.0354728 loss)
I1026 01:16:48.282114 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0391512 (* 1 = 0.0391512 loss)
I1026 01:16:48.282119 17254 sgd_solver.cpp:106] Iteration 13100, lr = 0.001
I1026 01:16:49.382818 17254 solver.cpp:229] Iteration 13120, loss = 0.0587925
I1026 01:16:49.382858 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0332964 (* 1 = 0.0332964 loss)
I1026 01:16:49.382863 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0254961 (* 1 = 0.0254961 loss)
I1026 01:16:49.382869 17254 sgd_solver.cpp:106] Iteration 13120, lr = 0.001
I1026 01:16:50.466568 17254 solver.cpp:229] Iteration 13140, loss = 0.132358
I1026 01:16:50.466601 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0885207 (* 1 = 0.0885207 loss)
I1026 01:16:50.466606 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0438372 (* 1 = 0.0438372 loss)
I1026 01:16:50.466611 17254 sgd_solver.cpp:106] Iteration 13140, lr = 0.001
I1026 01:16:51.546671 17254 solver.cpp:229] Iteration 13160, loss = 0.135664
I1026 01:16:51.546706 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0613968 (* 1 = 0.0613968 loss)
I1026 01:16:51.546715 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0742677 (* 1 = 0.0742677 loss)
I1026 01:16:51.546721 17254 sgd_solver.cpp:106] Iteration 13160, lr = 0.001
I1026 01:16:52.624624 17254 solver.cpp:229] Iteration 13180, loss = 0.071079
I1026 01:16:52.624656 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0436653 (* 1 = 0.0436653 loss)
I1026 01:16:52.624661 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0274137 (* 1 = 0.0274137 loss)
I1026 01:16:52.624667 17254 sgd_solver.cpp:106] Iteration 13180, lr = 0.001
I1026 01:16:53.700889 17254 solver.cpp:229] Iteration 13200, loss = 0.146869
I1026 01:16:53.700922 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0703772 (* 1 = 0.0703772 loss)
I1026 01:16:53.700927 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.076492 (* 1 = 0.076492 loss)
I1026 01:16:53.700933 17254 sgd_solver.cpp:106] Iteration 13200, lr = 0.001
I1026 01:16:54.778479 17254 solver.cpp:229] Iteration 13220, loss = 0.0750658
I1026 01:16:54.778512 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0463946 (* 1 = 0.0463946 loss)
I1026 01:16:54.778517 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0286712 (* 1 = 0.0286712 loss)
I1026 01:16:54.778522 17254 sgd_solver.cpp:106] Iteration 13220, lr = 0.001
I1026 01:16:55.887341 17254 solver.cpp:229] Iteration 13240, loss = 0.131059
I1026 01:16:55.887373 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.105455 (* 1 = 0.105455 loss)
I1026 01:16:55.887379 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0256037 (* 1 = 0.0256037 loss)
I1026 01:16:55.887384 17254 sgd_solver.cpp:106] Iteration 13240, lr = 0.001
I1026 01:16:56.975373 17254 solver.cpp:229] Iteration 13260, loss = 0.0992346
I1026 01:16:56.975414 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0430281 (* 1 = 0.0430281 loss)
I1026 01:16:56.975419 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0562065 (* 1 = 0.0562065 loss)
I1026 01:16:56.975425 17254 sgd_solver.cpp:106] Iteration 13260, lr = 0.001
I1026 01:16:58.057901 17254 solver.cpp:229] Iteration 13280, loss = 0.0568789
I1026 01:16:58.057934 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0331659 (* 1 = 0.0331659 loss)
I1026 01:16:58.057938 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0237129 (* 1 = 0.0237129 loss)
I1026 01:16:58.057943 17254 sgd_solver.cpp:106] Iteration 13280, lr = 0.001
I1026 01:16:59.136546 17254 solver.cpp:229] Iteration 13300, loss = 0.0551494
I1026 01:16:59.136577 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0394088 (* 1 = 0.0394088 loss)
I1026 01:16:59.136582 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0157406 (* 1 = 0.0157406 loss)
I1026 01:16:59.136589 17254 sgd_solver.cpp:106] Iteration 13300, lr = 0.001
I1026 01:17:00.226578 17254 solver.cpp:229] Iteration 13320, loss = 0.184634
I1026 01:17:00.226611 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0856599 (* 1 = 0.0856599 loss)
I1026 01:17:00.226616 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0989745 (* 1 = 0.0989745 loss)
I1026 01:17:00.226621 17254 sgd_solver.cpp:106] Iteration 13320, lr = 0.001
I1026 01:17:01.293287 17254 solver.cpp:229] Iteration 13340, loss = 0.137194
I1026 01:17:01.293320 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0711218 (* 1 = 0.0711218 loss)
I1026 01:17:01.293325 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0660723 (* 1 = 0.0660723 loss)
I1026 01:17:01.293330 17254 sgd_solver.cpp:106] Iteration 13340, lr = 0.001
I1026 01:17:02.408395 17254 solver.cpp:229] Iteration 13360, loss = 0.0566699
I1026 01:17:02.408427 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0455262 (* 1 = 0.0455262 loss)
I1026 01:17:02.408433 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0111436 (* 1 = 0.0111436 loss)
I1026 01:17:02.408438 17254 sgd_solver.cpp:106] Iteration 13360, lr = 0.001
I1026 01:17:03.504644 17254 solver.cpp:229] Iteration 13380, loss = 0.0742693
I1026 01:17:03.504678 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0568943 (* 1 = 0.0568943 loss)
I1026 01:17:03.504683 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0173751 (* 1 = 0.0173751 loss)
I1026 01:17:03.504688 17254 sgd_solver.cpp:106] Iteration 13380, lr = 0.001
I1026 01:17:04.577657 17254 solver.cpp:229] Iteration 13400, loss = 0.0734393
I1026 01:17:04.577692 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0434002 (* 1 = 0.0434002 loss)
I1026 01:17:04.577697 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0300391 (* 1 = 0.0300391 loss)
I1026 01:17:04.577702 17254 sgd_solver.cpp:106] Iteration 13400, lr = 0.001
I1026 01:17:05.651273 17254 solver.cpp:229] Iteration 13420, loss = 0.234541
I1026 01:17:05.651304 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0878161 (* 1 = 0.0878161 loss)
I1026 01:17:05.651309 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.146725 (* 1 = 0.146725 loss)
I1026 01:17:05.651315 17254 sgd_solver.cpp:106] Iteration 13420, lr = 0.001
I1026 01:17:06.731125 17254 solver.cpp:229] Iteration 13440, loss = 0.0975383
I1026 01:17:06.731158 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0508627 (* 1 = 0.0508627 loss)
I1026 01:17:06.731178 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0466757 (* 1 = 0.0466757 loss)
I1026 01:17:06.731184 17254 sgd_solver.cpp:106] Iteration 13440, lr = 0.001
I1026 01:17:07.819391 17254 solver.cpp:229] Iteration 13460, loss = 0.0790158
I1026 01:17:07.819423 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0327756 (* 1 = 0.0327756 loss)
I1026 01:17:07.819432 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0462401 (* 1 = 0.0462401 loss)
I1026 01:17:07.819437 17254 sgd_solver.cpp:106] Iteration 13460, lr = 0.001
I1026 01:17:08.904022 17254 solver.cpp:229] Iteration 13480, loss = 0.152823
I1026 01:17:08.904055 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0538391 (* 1 = 0.0538391 loss)
I1026 01:17:08.904062 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0989844 (* 1 = 0.0989844 loss)
I1026 01:17:08.904067 17254 sgd_solver.cpp:106] Iteration 13480, lr = 0.001
I1026 01:17:09.999408 17254 solver.cpp:229] Iteration 13500, loss = 0.0964626
I1026 01:17:09.999444 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0516013 (* 1 = 0.0516013 loss)
I1026 01:17:09.999449 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0448613 (* 1 = 0.0448613 loss)
I1026 01:17:09.999454 17254 sgd_solver.cpp:106] Iteration 13500, lr = 0.001
I1026 01:17:11.076059 17254 solver.cpp:229] Iteration 13520, loss = 0.0462834
I1026 01:17:11.076093 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0389023 (* 1 = 0.0389023 loss)
I1026 01:17:11.076098 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00738113 (* 1 = 0.00738113 loss)
I1026 01:17:11.076104 17254 sgd_solver.cpp:106] Iteration 13520, lr = 0.001
I1026 01:17:12.141991 17254 solver.cpp:229] Iteration 13540, loss = 0.090865
I1026 01:17:12.142021 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0478608 (* 1 = 0.0478608 loss)
I1026 01:17:12.142026 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0430042 (* 1 = 0.0430042 loss)
I1026 01:17:12.142030 17254 sgd_solver.cpp:106] Iteration 13540, lr = 0.001
I1026 01:17:13.215322 17254 solver.cpp:229] Iteration 13560, loss = 0.146025
I1026 01:17:13.215365 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0842444 (* 1 = 0.0842444 loss)
I1026 01:17:13.215370 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0617811 (* 1 = 0.0617811 loss)
I1026 01:17:13.215376 17254 sgd_solver.cpp:106] Iteration 13560, lr = 0.001
I1026 01:17:14.295560 17254 solver.cpp:229] Iteration 13580, loss = 0.175472
I1026 01:17:14.295594 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0839572 (* 1 = 0.0839572 loss)
I1026 01:17:14.295599 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0915145 (* 1 = 0.0915145 loss)
I1026 01:17:14.295605 17254 sgd_solver.cpp:106] Iteration 13580, lr = 0.001
I1026 01:17:15.387837 17254 solver.cpp:229] Iteration 13600, loss = 0.112788
I1026 01:17:15.387869 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0562202 (* 1 = 0.0562202 loss)
I1026 01:17:15.387876 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0565673 (* 1 = 0.0565673 loss)
I1026 01:17:15.387881 17254 sgd_solver.cpp:106] Iteration 13600, lr = 0.001
I1026 01:17:16.484091 17254 solver.cpp:229] Iteration 13620, loss = 0.119078
I1026 01:17:16.484124 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0651961 (* 1 = 0.0651961 loss)
I1026 01:17:16.484129 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0538815 (* 1 = 0.0538815 loss)
I1026 01:17:16.484134 17254 sgd_solver.cpp:106] Iteration 13620, lr = 0.001
I1026 01:17:17.585078 17254 solver.cpp:229] Iteration 13640, loss = 0.0820618
I1026 01:17:17.585105 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0530953 (* 1 = 0.0530953 loss)
I1026 01:17:17.585110 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0289666 (* 1 = 0.0289666 loss)
I1026 01:17:17.585115 17254 sgd_solver.cpp:106] Iteration 13640, lr = 0.001
I1026 01:17:18.667114 17254 solver.cpp:229] Iteration 13660, loss = 0.06325
I1026 01:17:18.667146 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.043261 (* 1 = 0.043261 loss)
I1026 01:17:18.667151 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.019989 (* 1 = 0.019989 loss)
I1026 01:17:18.667157 17254 sgd_solver.cpp:106] Iteration 13660, lr = 0.001
I1026 01:17:19.756436 17254 solver.cpp:229] Iteration 13680, loss = 0.037587
I1026 01:17:19.756469 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0299687 (* 1 = 0.0299687 loss)
I1026 01:17:19.756474 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00761828 (* 1 = 0.00761828 loss)
I1026 01:17:19.756479 17254 sgd_solver.cpp:106] Iteration 13680, lr = 0.001
I1026 01:17:20.849882 17254 solver.cpp:229] Iteration 13700, loss = 0.0673705
I1026 01:17:20.849915 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0316288 (* 1 = 0.0316288 loss)
I1026 01:17:20.849920 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0357417 (* 1 = 0.0357417 loss)
I1026 01:17:20.849925 17254 sgd_solver.cpp:106] Iteration 13700, lr = 0.001
I1026 01:17:21.933575 17254 solver.cpp:229] Iteration 13720, loss = 0.0457521
I1026 01:17:21.933609 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0334881 (* 1 = 0.0334881 loss)
I1026 01:17:21.933614 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.012264 (* 1 = 0.012264 loss)
I1026 01:17:21.933619 17254 sgd_solver.cpp:106] Iteration 13720, lr = 0.001
I1026 01:17:23.055692 17254 solver.cpp:229] Iteration 13740, loss = 0.104231
I1026 01:17:23.055723 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.084299 (* 1 = 0.084299 loss)
I1026 01:17:23.055729 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0199315 (* 1 = 0.0199315 loss)
I1026 01:17:23.055734 17254 sgd_solver.cpp:106] Iteration 13740, lr = 0.001
I1026 01:17:24.157637 17254 solver.cpp:229] Iteration 13760, loss = 0.0809775
I1026 01:17:24.157670 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0510703 (* 1 = 0.0510703 loss)
I1026 01:17:24.157675 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0299073 (* 1 = 0.0299073 loss)
I1026 01:17:24.157680 17254 sgd_solver.cpp:106] Iteration 13760, lr = 0.001
I1026 01:17:25.248327 17254 solver.cpp:229] Iteration 13780, loss = 0.0874943
I1026 01:17:25.248370 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0413762 (* 1 = 0.0413762 loss)
I1026 01:17:25.248375 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.046118 (* 1 = 0.046118 loss)
I1026 01:17:25.248380 17254 sgd_solver.cpp:106] Iteration 13780, lr = 0.001
I1026 01:17:26.343768 17254 solver.cpp:229] Iteration 13800, loss = 0.082588
I1026 01:17:26.343801 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0333564 (* 1 = 0.0333564 loss)
I1026 01:17:26.343806 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0492317 (* 1 = 0.0492317 loss)
I1026 01:17:26.343822 17254 sgd_solver.cpp:106] Iteration 13800, lr = 0.001
I1026 01:17:27.454322 17254 solver.cpp:229] Iteration 13820, loss = 0.0507531
I1026 01:17:27.454355 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0293701 (* 1 = 0.0293701 loss)
I1026 01:17:27.454360 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.021383 (* 1 = 0.021383 loss)
I1026 01:17:27.454365 17254 sgd_solver.cpp:106] Iteration 13820, lr = 0.001
I1026 01:17:28.548980 17254 solver.cpp:229] Iteration 13840, loss = 0.127683
I1026 01:17:28.549012 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0645469 (* 1 = 0.0645469 loss)
I1026 01:17:28.549018 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0631365 (* 1 = 0.0631365 loss)
I1026 01:17:28.549023 17254 sgd_solver.cpp:106] Iteration 13840, lr = 0.001
I1026 01:17:29.636277 17254 solver.cpp:229] Iteration 13860, loss = 0.0851139
I1026 01:17:29.636310 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0429075 (* 1 = 0.0429075 loss)
I1026 01:17:29.636317 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0422064 (* 1 = 0.0422064 loss)
I1026 01:17:29.636322 17254 sgd_solver.cpp:106] Iteration 13860, lr = 0.001
I1026 01:17:30.718847 17254 solver.cpp:229] Iteration 13880, loss = 0.133682
I1026 01:17:30.718880 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0631537 (* 1 = 0.0631537 loss)
I1026 01:17:30.718886 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0705279 (* 1 = 0.0705279 loss)
I1026 01:17:30.718891 17254 sgd_solver.cpp:106] Iteration 13880, lr = 0.001
I1026 01:17:31.839701 17254 solver.cpp:229] Iteration 13900, loss = 0.108845
I1026 01:17:31.839735 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0648376 (* 1 = 0.0648376 loss)
I1026 01:17:31.839740 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0440077 (* 1 = 0.0440077 loss)
I1026 01:17:31.839745 17254 sgd_solver.cpp:106] Iteration 13900, lr = 0.001
I1026 01:17:32.931342 17254 solver.cpp:229] Iteration 13920, loss = 0.0852268
I1026 01:17:32.931375 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0356119 (* 1 = 0.0356119 loss)
I1026 01:17:32.931381 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0496148 (* 1 = 0.0496148 loss)
I1026 01:17:32.931386 17254 sgd_solver.cpp:106] Iteration 13920, lr = 0.001
I1026 01:17:34.017853 17254 solver.cpp:229] Iteration 13940, loss = 0.0489163
I1026 01:17:34.017885 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0287003 (* 1 = 0.0287003 loss)
I1026 01:17:34.017891 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.020216 (* 1 = 0.020216 loss)
I1026 01:17:34.017896 17254 sgd_solver.cpp:106] Iteration 13940, lr = 0.001
I1026 01:17:35.099894 17254 solver.cpp:229] Iteration 13960, loss = 0.0858695
I1026 01:17:35.099926 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0501086 (* 1 = 0.0501086 loss)
I1026 01:17:35.099931 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0357608 (* 1 = 0.0357608 loss)
I1026 01:17:35.099937 17254 sgd_solver.cpp:106] Iteration 13960, lr = 0.001
I1026 01:17:36.202095 17254 solver.cpp:229] Iteration 13980, loss = 0.0656358
I1026 01:17:36.202127 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.041874 (* 1 = 0.041874 loss)
I1026 01:17:36.202133 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0237618 (* 1 = 0.0237618 loss)
I1026 01:17:36.202137 17254 sgd_solver.cpp:106] Iteration 13980, lr = 0.001
I1026 01:17:37.301636 17254 solver.cpp:229] Iteration 14000, loss = 0.135313
I1026 01:17:37.301668 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0567532 (* 1 = 0.0567532 loss)
I1026 01:17:37.301673 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0785596 (* 1 = 0.0785596 loss)
I1026 01:17:37.301678 17254 sgd_solver.cpp:106] Iteration 14000, lr = 0.001
I1026 01:17:38.403975 17254 solver.cpp:229] Iteration 14020, loss = 0.12561
I1026 01:17:38.404008 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0797544 (* 1 = 0.0797544 loss)
I1026 01:17:38.404014 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0458556 (* 1 = 0.0458556 loss)
I1026 01:17:38.404019 17254 sgd_solver.cpp:106] Iteration 14020, lr = 0.001
I1026 01:17:39.497678 17254 solver.cpp:229] Iteration 14040, loss = 0.0561665
I1026 01:17:39.497709 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0470965 (* 1 = 0.0470965 loss)
I1026 01:17:39.497733 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00906998 (* 1 = 0.00906998 loss)
I1026 01:17:39.497738 17254 sgd_solver.cpp:106] Iteration 14040, lr = 0.001
I1026 01:17:40.585945 17254 solver.cpp:229] Iteration 14060, loss = 0.127584
I1026 01:17:40.585979 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.065465 (* 1 = 0.065465 loss)
I1026 01:17:40.585984 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0621188 (* 1 = 0.0621188 loss)
I1026 01:17:40.585990 17254 sgd_solver.cpp:106] Iteration 14060, lr = 0.001
I1026 01:17:41.671098 17254 solver.cpp:229] Iteration 14080, loss = 0.0739659
I1026 01:17:41.671130 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0444518 (* 1 = 0.0444518 loss)
I1026 01:17:41.671136 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0295141 (* 1 = 0.0295141 loss)
I1026 01:17:41.671141 17254 sgd_solver.cpp:106] Iteration 14080, lr = 0.001
I1026 01:17:42.749687 17254 solver.cpp:229] Iteration 14100, loss = 0.192986
I1026 01:17:42.749732 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0954117 (* 1 = 0.0954117 loss)
I1026 01:17:42.749737 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0975747 (* 1 = 0.0975747 loss)
I1026 01:17:42.749742 17254 sgd_solver.cpp:106] Iteration 14100, lr = 0.001
I1026 01:17:43.829612 17254 solver.cpp:229] Iteration 14120, loss = 0.17186
I1026 01:17:43.829646 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0797299 (* 1 = 0.0797299 loss)
I1026 01:17:43.829651 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0921299 (* 1 = 0.0921299 loss)
I1026 01:17:43.829658 17254 sgd_solver.cpp:106] Iteration 14120, lr = 0.001
I1026 01:17:44.918768 17254 solver.cpp:229] Iteration 14140, loss = 0.0816517
I1026 01:17:44.918802 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.064591 (* 1 = 0.064591 loss)
I1026 01:17:44.918807 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0170608 (* 1 = 0.0170608 loss)
I1026 01:17:44.918812 17254 sgd_solver.cpp:106] Iteration 14140, lr = 0.001
I1026 01:17:46.008339 17254 solver.cpp:229] Iteration 14160, loss = 0.105175
I1026 01:17:46.008370 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0725054 (* 1 = 0.0725054 loss)
I1026 01:17:46.008375 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0326696 (* 1 = 0.0326696 loss)
I1026 01:17:46.008380 17254 sgd_solver.cpp:106] Iteration 14160, lr = 0.001
I1026 01:17:47.093587 17254 solver.cpp:229] Iteration 14180, loss = 0.0795328
I1026 01:17:47.093621 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0392367 (* 1 = 0.0392367 loss)
I1026 01:17:47.093626 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.040296 (* 1 = 0.040296 loss)
I1026 01:17:47.093631 17254 sgd_solver.cpp:106] Iteration 14180, lr = 0.001
I1026 01:17:48.170424 17254 solver.cpp:229] Iteration 14200, loss = 0.129001
I1026 01:17:48.170457 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0704647 (* 1 = 0.0704647 loss)
I1026 01:17:48.170462 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0585367 (* 1 = 0.0585367 loss)
I1026 01:17:48.170467 17254 sgd_solver.cpp:106] Iteration 14200, lr = 0.001
I1026 01:17:49.269677 17254 solver.cpp:229] Iteration 14220, loss = 0.0943183
I1026 01:17:49.269708 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0577993 (* 1 = 0.0577993 loss)
I1026 01:17:49.269713 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.036519 (* 1 = 0.036519 loss)
I1026 01:17:49.269718 17254 sgd_solver.cpp:106] Iteration 14220, lr = 0.001
I1026 01:17:50.372433 17254 solver.cpp:229] Iteration 14240, loss = 0.0915316
I1026 01:17:50.372476 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0314472 (* 1 = 0.0314472 loss)
I1026 01:17:50.372481 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0600844 (* 1 = 0.0600844 loss)
I1026 01:17:50.372486 17254 sgd_solver.cpp:106] Iteration 14240, lr = 0.001
I1026 01:17:51.460422 17254 solver.cpp:229] Iteration 14260, loss = 0.13512
I1026 01:17:51.460453 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0559471 (* 1 = 0.0559471 loss)
I1026 01:17:51.460458 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0791732 (* 1 = 0.0791732 loss)
I1026 01:17:51.460464 17254 sgd_solver.cpp:106] Iteration 14260, lr = 0.001
I1026 01:17:52.540109 17254 solver.cpp:229] Iteration 14280, loss = 0.157211
I1026 01:17:52.540141 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.111565 (* 1 = 0.111565 loss)
I1026 01:17:52.540146 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0456459 (* 1 = 0.0456459 loss)
I1026 01:17:52.540153 17254 sgd_solver.cpp:106] Iteration 14280, lr = 0.001
I1026 01:17:53.620440 17254 solver.cpp:229] Iteration 14300, loss = 0.0509482
I1026 01:17:53.620472 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0272627 (* 1 = 0.0272627 loss)
I1026 01:17:53.620477 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0236855 (* 1 = 0.0236855 loss)
I1026 01:17:53.620483 17254 sgd_solver.cpp:106] Iteration 14300, lr = 0.001
I1026 01:17:54.708384 17254 solver.cpp:229] Iteration 14320, loss = 0.0792933
I1026 01:17:54.708413 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0248637 (* 1 = 0.0248637 loss)
I1026 01:17:54.708418 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0544295 (* 1 = 0.0544295 loss)
I1026 01:17:54.708425 17254 sgd_solver.cpp:106] Iteration 14320, lr = 0.001
I1026 01:17:55.802939 17254 solver.cpp:229] Iteration 14340, loss = 0.107306
I1026 01:17:55.802971 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0388612 (* 1 = 0.0388612 loss)
I1026 01:17:55.802976 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.068445 (* 1 = 0.068445 loss)
I1026 01:17:55.802981 17254 sgd_solver.cpp:106] Iteration 14340, lr = 0.001
I1026 01:17:56.910946 17254 solver.cpp:229] Iteration 14360, loss = 0.242286
I1026 01:17:56.910989 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0954225 (* 1 = 0.0954225 loss)
I1026 01:17:56.910995 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.146864 (* 1 = 0.146864 loss)
I1026 01:17:56.911000 17254 sgd_solver.cpp:106] Iteration 14360, lr = 0.001
I1026 01:17:57.993836 17254 solver.cpp:229] Iteration 14380, loss = 0.166562
I1026 01:17:57.993880 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0785208 (* 1 = 0.0785208 loss)
I1026 01:17:57.993885 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0880416 (* 1 = 0.0880416 loss)
I1026 01:17:57.993891 17254 sgd_solver.cpp:106] Iteration 14380, lr = 0.001
I1026 01:17:59.081817 17254 solver.cpp:229] Iteration 14400, loss = 0.101617
I1026 01:17:59.081850 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0356868 (* 1 = 0.0356868 loss)
I1026 01:17:59.081854 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.06593 (* 1 = 0.06593 loss)
I1026 01:17:59.081859 17254 sgd_solver.cpp:106] Iteration 14400, lr = 0.001
I1026 01:18:00.156558 17254 solver.cpp:229] Iteration 14420, loss = 0.097946
I1026 01:18:00.156599 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0869086 (* 1 = 0.0869086 loss)
I1026 01:18:00.156605 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0110374 (* 1 = 0.0110374 loss)
I1026 01:18:00.156610 17254 sgd_solver.cpp:106] Iteration 14420, lr = 0.001
I1026 01:18:01.242493 17254 solver.cpp:229] Iteration 14440, loss = 0.18876
I1026 01:18:01.242527 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0777299 (* 1 = 0.0777299 loss)
I1026 01:18:01.242532 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.11103 (* 1 = 0.11103 loss)
I1026 01:18:01.242537 17254 sgd_solver.cpp:106] Iteration 14440, lr = 0.001
I1026 01:18:02.334280 17254 solver.cpp:229] Iteration 14460, loss = 0.0759444
I1026 01:18:02.334313 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0562387 (* 1 = 0.0562387 loss)
I1026 01:18:02.334318 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0197057 (* 1 = 0.0197057 loss)
I1026 01:18:02.334334 17254 sgd_solver.cpp:106] Iteration 14460, lr = 0.001
I1026 01:18:03.402559 17254 solver.cpp:229] Iteration 14480, loss = 0.0735961
I1026 01:18:03.402591 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0375363 (* 1 = 0.0375363 loss)
I1026 01:18:03.402596 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0360597 (* 1 = 0.0360597 loss)
I1026 01:18:03.402602 17254 sgd_solver.cpp:106] Iteration 14480, lr = 0.001
I1026 01:18:04.467335 17254 solver.cpp:229] Iteration 14500, loss = 0.0543134
I1026 01:18:04.467367 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.035483 (* 1 = 0.035483 loss)
I1026 01:18:04.467372 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0188304 (* 1 = 0.0188304 loss)
I1026 01:18:04.467377 17254 sgd_solver.cpp:106] Iteration 14500, lr = 0.001
I1026 01:18:05.566468 17254 solver.cpp:229] Iteration 14520, loss = 0.0735401
I1026 01:18:05.566500 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0479151 (* 1 = 0.0479151 loss)
I1026 01:18:05.566505 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0256249 (* 1 = 0.0256249 loss)
I1026 01:18:05.566510 17254 sgd_solver.cpp:106] Iteration 14520, lr = 0.001
I1026 01:18:06.661993 17254 solver.cpp:229] Iteration 14540, loss = 0.0759351
I1026 01:18:06.662024 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0630066 (* 1 = 0.0630066 loss)
I1026 01:18:06.662029 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0129285 (* 1 = 0.0129285 loss)
I1026 01:18:06.662034 17254 sgd_solver.cpp:106] Iteration 14540, lr = 0.001
I1026 01:18:07.768548 17254 solver.cpp:229] Iteration 14560, loss = 0.223391
I1026 01:18:07.768581 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.110266 (* 1 = 0.110266 loss)
I1026 01:18:07.768586 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.113125 (* 1 = 0.113125 loss)
I1026 01:18:07.768591 17254 sgd_solver.cpp:106] Iteration 14560, lr = 0.001
I1026 01:18:08.848034 17254 solver.cpp:229] Iteration 14580, loss = 0.137643
I1026 01:18:08.848067 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0920363 (* 1 = 0.0920363 loss)
I1026 01:18:08.848073 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.045607 (* 1 = 0.045607 loss)
I1026 01:18:08.848078 17254 sgd_solver.cpp:106] Iteration 14580, lr = 0.001
I1026 01:18:09.970597 17254 solver.cpp:229] Iteration 14600, loss = 0.0814434
I1026 01:18:09.970630 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.059785 (* 1 = 0.059785 loss)
I1026 01:18:09.970635 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0216584 (* 1 = 0.0216584 loss)
I1026 01:18:09.970650 17254 sgd_solver.cpp:106] Iteration 14600, lr = 0.001
I1026 01:18:11.073590 17254 solver.cpp:229] Iteration 14620, loss = 0.202573
I1026 01:18:11.073623 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.131898 (* 1 = 0.131898 loss)
I1026 01:18:11.073629 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.070675 (* 1 = 0.070675 loss)
I1026 01:18:11.073634 17254 sgd_solver.cpp:106] Iteration 14620, lr = 0.001
I1026 01:18:12.162175 17254 solver.cpp:229] Iteration 14640, loss = 0.0856576
I1026 01:18:12.162209 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0457338 (* 1 = 0.0457338 loss)
I1026 01:18:12.162215 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0399238 (* 1 = 0.0399238 loss)
I1026 01:18:12.162220 17254 sgd_solver.cpp:106] Iteration 14640, lr = 0.001
I1026 01:18:13.269642 17254 solver.cpp:229] Iteration 14660, loss = 0.0824611
I1026 01:18:13.269675 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.065696 (* 1 = 0.065696 loss)
I1026 01:18:13.269681 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0167651 (* 1 = 0.0167651 loss)
I1026 01:18:13.269686 17254 sgd_solver.cpp:106] Iteration 14660, lr = 0.001
I1026 01:18:14.355418 17254 solver.cpp:229] Iteration 14680, loss = 0.0689941
I1026 01:18:14.355455 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0397546 (* 1 = 0.0397546 loss)
I1026 01:18:14.355471 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0292396 (* 1 = 0.0292396 loss)
I1026 01:18:14.355476 17254 sgd_solver.cpp:106] Iteration 14680, lr = 0.001
I1026 01:18:15.438901 17254 solver.cpp:229] Iteration 14700, loss = 0.0462315
I1026 01:18:15.438954 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0328752 (* 1 = 0.0328752 loss)
I1026 01:18:15.438959 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0133563 (* 1 = 0.0133563 loss)
I1026 01:18:15.438966 17254 sgd_solver.cpp:106] Iteration 14700, lr = 0.001
I1026 01:18:16.549695 17254 solver.cpp:229] Iteration 14720, loss = 0.0670377
I1026 01:18:16.549728 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0342629 (* 1 = 0.0342629 loss)
I1026 01:18:16.549733 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0327748 (* 1 = 0.0327748 loss)
I1026 01:18:16.549738 17254 sgd_solver.cpp:106] Iteration 14720, lr = 0.001
I1026 01:18:17.624575 17254 solver.cpp:229] Iteration 14740, loss = 0.0865097
I1026 01:18:17.624608 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0522434 (* 1 = 0.0522434 loss)
I1026 01:18:17.624614 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0342663 (* 1 = 0.0342663 loss)
I1026 01:18:17.624619 17254 sgd_solver.cpp:106] Iteration 14740, lr = 0.001
I1026 01:18:18.702479 17254 solver.cpp:229] Iteration 14760, loss = 0.150676
I1026 01:18:18.702513 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0536832 (* 1 = 0.0536832 loss)
I1026 01:18:18.702518 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0969926 (* 1 = 0.0969926 loss)
I1026 01:18:18.702523 17254 sgd_solver.cpp:106] Iteration 14760, lr = 0.001
I1026 01:18:19.788668 17254 solver.cpp:229] Iteration 14780, loss = 0.0855946
I1026 01:18:19.788702 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0558961 (* 1 = 0.0558961 loss)
I1026 01:18:19.788707 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0296984 (* 1 = 0.0296984 loss)
I1026 01:18:19.788712 17254 sgd_solver.cpp:106] Iteration 14780, lr = 0.001
I1026 01:18:20.882740 17254 solver.cpp:229] Iteration 14800, loss = 0.113742
I1026 01:18:20.882772 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0381127 (* 1 = 0.0381127 loss)
I1026 01:18:20.882778 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0756295 (* 1 = 0.0756295 loss)
I1026 01:18:20.882783 17254 sgd_solver.cpp:106] Iteration 14800, lr = 0.001
I1026 01:18:21.979291 17254 solver.cpp:229] Iteration 14820, loss = 0.0919839
I1026 01:18:21.979336 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0269103 (* 1 = 0.0269103 loss)
I1026 01:18:21.979341 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0650736 (* 1 = 0.0650736 loss)
I1026 01:18:21.979346 17254 sgd_solver.cpp:106] Iteration 14820, lr = 0.001
I1026 01:18:23.073923 17254 solver.cpp:229] Iteration 14840, loss = 0.116997
I1026 01:18:23.073956 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0830275 (* 1 = 0.0830275 loss)
I1026 01:18:23.073977 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0339692 (* 1 = 0.0339692 loss)
I1026 01:18:23.073983 17254 sgd_solver.cpp:106] Iteration 14840, lr = 0.001
I1026 01:18:24.170301 17254 solver.cpp:229] Iteration 14860, loss = 0.194987
I1026 01:18:24.170334 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0717397 (* 1 = 0.0717397 loss)
I1026 01:18:24.170339 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.123248 (* 1 = 0.123248 loss)
I1026 01:18:24.170343 17254 sgd_solver.cpp:106] Iteration 14860, lr = 0.001
I1026 01:18:25.254441 17254 solver.cpp:229] Iteration 14880, loss = 0.0472291
I1026 01:18:25.254470 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0370156 (* 1 = 0.0370156 loss)
I1026 01:18:25.254475 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0102135 (* 1 = 0.0102135 loss)
I1026 01:18:25.254482 17254 sgd_solver.cpp:106] Iteration 14880, lr = 0.001
I1026 01:18:26.356554 17254 solver.cpp:229] Iteration 14900, loss = 0.0633831
I1026 01:18:26.356585 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0382249 (* 1 = 0.0382249 loss)
I1026 01:18:26.356590 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0251582 (* 1 = 0.0251582 loss)
I1026 01:18:26.356595 17254 sgd_solver.cpp:106] Iteration 14900, lr = 0.001
I1026 01:18:27.452745 17254 solver.cpp:229] Iteration 14920, loss = 0.111667
I1026 01:18:27.452777 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0576083 (* 1 = 0.0576083 loss)
I1026 01:18:27.452783 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0540583 (* 1 = 0.0540583 loss)
I1026 01:18:27.452788 17254 sgd_solver.cpp:106] Iteration 14920, lr = 0.001
I1026 01:18:28.549706 17254 solver.cpp:229] Iteration 14940, loss = 0.0459214
I1026 01:18:28.549738 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0422601 (* 1 = 0.0422601 loss)
I1026 01:18:28.549743 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0036613 (* 1 = 0.0036613 loss)
I1026 01:18:28.549748 17254 sgd_solver.cpp:106] Iteration 14940, lr = 0.001
I1026 01:18:29.613888 17254 solver.cpp:229] Iteration 14960, loss = 0.0869254
I1026 01:18:29.613921 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0384781 (* 1 = 0.0384781 loss)
I1026 01:18:29.613926 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0484472 (* 1 = 0.0484472 loss)
I1026 01:18:29.613931 17254 sgd_solver.cpp:106] Iteration 14960, lr = 0.001
I1026 01:18:30.712901 17254 solver.cpp:229] Iteration 14980, loss = 0.0452133
I1026 01:18:30.712940 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0274096 (* 1 = 0.0274096 loss)
I1026 01:18:30.712946 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0178037 (* 1 = 0.0178037 loss)
I1026 01:18:30.712951 17254 sgd_solver.cpp:106] Iteration 14980, lr = 0.001
I1026 01:18:31.778678 17254 solver.cpp:229] Iteration 15000, loss = 0.0668935
I1026 01:18:31.778712 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.047315 (* 1 = 0.047315 loss)
I1026 01:18:31.778717 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0195785 (* 1 = 0.0195785 loss)
I1026 01:18:31.778723 17254 sgd_solver.cpp:106] Iteration 15000, lr = 0.001
I1026 01:18:32.846758 17254 solver.cpp:229] Iteration 15020, loss = 0.0304976
I1026 01:18:32.846791 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0180148 (* 1 = 0.0180148 loss)
I1026 01:18:32.846796 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0124827 (* 1 = 0.0124827 loss)
I1026 01:18:32.846801 17254 sgd_solver.cpp:106] Iteration 15020, lr = 0.001
I1026 01:18:33.937381 17254 solver.cpp:229] Iteration 15040, loss = 0.16781
I1026 01:18:33.937413 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.104704 (* 1 = 0.104704 loss)
I1026 01:18:33.937419 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0631055 (* 1 = 0.0631055 loss)
I1026 01:18:33.937424 17254 sgd_solver.cpp:106] Iteration 15040, lr = 0.001
I1026 01:18:35.011421 17254 solver.cpp:229] Iteration 15060, loss = 0.100724
I1026 01:18:35.011467 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0634692 (* 1 = 0.0634692 loss)
I1026 01:18:35.011473 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0372551 (* 1 = 0.0372551 loss)
I1026 01:18:35.011478 17254 sgd_solver.cpp:106] Iteration 15060, lr = 0.001
I1026 01:18:36.108044 17254 solver.cpp:229] Iteration 15080, loss = 0.0789353
I1026 01:18:36.108088 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0342044 (* 1 = 0.0342044 loss)
I1026 01:18:36.108093 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0447308 (* 1 = 0.0447308 loss)
I1026 01:18:36.108098 17254 sgd_solver.cpp:106] Iteration 15080, lr = 0.001
I1026 01:18:37.190701 17254 solver.cpp:229] Iteration 15100, loss = 0.0816501
I1026 01:18:37.190734 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.036702 (* 1 = 0.036702 loss)
I1026 01:18:37.190739 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0449482 (* 1 = 0.0449482 loss)
I1026 01:18:37.190744 17254 sgd_solver.cpp:106] Iteration 15100, lr = 0.001
I1026 01:18:38.276207 17254 solver.cpp:229] Iteration 15120, loss = 0.126595
I1026 01:18:38.276240 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0441715 (* 1 = 0.0441715 loss)
I1026 01:18:38.276245 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0824233 (* 1 = 0.0824233 loss)
I1026 01:18:38.276250 17254 sgd_solver.cpp:106] Iteration 15120, lr = 0.001
I1026 01:18:39.358682 17254 solver.cpp:229] Iteration 15140, loss = 0.0846158
I1026 01:18:39.358714 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0486199 (* 1 = 0.0486199 loss)
I1026 01:18:39.358719 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0359959 (* 1 = 0.0359959 loss)
I1026 01:18:39.358724 17254 sgd_solver.cpp:106] Iteration 15140, lr = 0.001
I1026 01:18:40.438451 17254 solver.cpp:229] Iteration 15160, loss = 0.342572
I1026 01:18:40.438482 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.151006 (* 1 = 0.151006 loss)
I1026 01:18:40.438488 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.191566 (* 1 = 0.191566 loss)
I1026 01:18:40.438493 17254 sgd_solver.cpp:106] Iteration 15160, lr = 0.001
I1026 01:18:41.520799 17254 solver.cpp:229] Iteration 15180, loss = 0.0992833
I1026 01:18:41.520843 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.042545 (* 1 = 0.042545 loss)
I1026 01:18:41.520848 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0567383 (* 1 = 0.0567383 loss)
I1026 01:18:41.520853 17254 sgd_solver.cpp:106] Iteration 15180, lr = 0.001
I1026 01:18:42.623644 17254 solver.cpp:229] Iteration 15200, loss = 0.0993663
I1026 01:18:42.623675 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0686677 (* 1 = 0.0686677 loss)
I1026 01:18:42.623680 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0306986 (* 1 = 0.0306986 loss)
I1026 01:18:42.623685 17254 sgd_solver.cpp:106] Iteration 15200, lr = 0.001
I1026 01:18:43.722043 17254 solver.cpp:229] Iteration 15220, loss = 0.0524209
I1026 01:18:43.722077 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0380854 (* 1 = 0.0380854 loss)
I1026 01:18:43.722082 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0143355 (* 1 = 0.0143355 loss)
I1026 01:18:43.722087 17254 sgd_solver.cpp:106] Iteration 15220, lr = 0.001
I1026 01:18:44.818841 17254 solver.cpp:229] Iteration 15240, loss = 0.0892993
I1026 01:18:44.818874 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0465378 (* 1 = 0.0465378 loss)
I1026 01:18:44.818879 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0427615 (* 1 = 0.0427615 loss)
I1026 01:18:44.818884 17254 sgd_solver.cpp:106] Iteration 15240, lr = 0.001
I1026 01:18:45.927979 17254 solver.cpp:229] Iteration 15260, loss = 0.0462792
I1026 01:18:45.928012 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0360518 (* 1 = 0.0360518 loss)
I1026 01:18:45.928016 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0102274 (* 1 = 0.0102274 loss)
I1026 01:18:45.928022 17254 sgd_solver.cpp:106] Iteration 15260, lr = 0.001
I1026 01:18:47.022598 17254 solver.cpp:229] Iteration 15280, loss = 0.0830667
I1026 01:18:47.022630 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0410921 (* 1 = 0.0410921 loss)
I1026 01:18:47.022635 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0419746 (* 1 = 0.0419746 loss)
I1026 01:18:47.022640 17254 sgd_solver.cpp:106] Iteration 15280, lr = 0.001
I1026 01:18:48.125735 17254 solver.cpp:229] Iteration 15300, loss = 0.0917273
I1026 01:18:48.125768 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0396371 (* 1 = 0.0396371 loss)
I1026 01:18:48.125773 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0520902 (* 1 = 0.0520902 loss)
I1026 01:18:48.125778 17254 sgd_solver.cpp:106] Iteration 15300, lr = 0.001
I1026 01:18:49.215013 17254 solver.cpp:229] Iteration 15320, loss = 0.0727651
I1026 01:18:49.215045 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0498786 (* 1 = 0.0498786 loss)
I1026 01:18:49.215050 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0228866 (* 1 = 0.0228866 loss)
I1026 01:18:49.215055 17254 sgd_solver.cpp:106] Iteration 15320, lr = 0.001
I1026 01:18:50.294381 17254 solver.cpp:229] Iteration 15340, loss = 0.223034
I1026 01:18:50.294414 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.11894 (* 1 = 0.11894 loss)
I1026 01:18:50.294419 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.104094 (* 1 = 0.104094 loss)
I1026 01:18:50.294425 17254 sgd_solver.cpp:106] Iteration 15340, lr = 0.001
I1026 01:18:51.390331 17254 solver.cpp:229] Iteration 15360, loss = 0.0642829
I1026 01:18:51.390363 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0375212 (* 1 = 0.0375212 loss)
I1026 01:18:51.390368 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0267617 (* 1 = 0.0267617 loss)
I1026 01:18:51.390373 17254 sgd_solver.cpp:106] Iteration 15360, lr = 0.001
I1026 01:18:52.457612 17254 solver.cpp:229] Iteration 15380, loss = 0.114992
I1026 01:18:52.457646 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0485974 (* 1 = 0.0485974 loss)
I1026 01:18:52.457651 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0663945 (* 1 = 0.0663945 loss)
I1026 01:18:52.457656 17254 sgd_solver.cpp:106] Iteration 15380, lr = 0.001
I1026 01:18:53.557960 17254 solver.cpp:229] Iteration 15400, loss = 0.121023
I1026 01:18:53.557994 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0669396 (* 1 = 0.0669396 loss)
I1026 01:18:53.558001 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0540832 (* 1 = 0.0540832 loss)
I1026 01:18:53.558006 17254 sgd_solver.cpp:106] Iteration 15400, lr = 0.001
I1026 01:18:54.629333 17254 solver.cpp:229] Iteration 15420, loss = 0.111628
I1026 01:18:54.629362 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0377805 (* 1 = 0.0377805 loss)
I1026 01:18:54.629367 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.073848 (* 1 = 0.073848 loss)
I1026 01:18:54.629372 17254 sgd_solver.cpp:106] Iteration 15420, lr = 0.001
I1026 01:18:55.728132 17254 solver.cpp:229] Iteration 15440, loss = 0.0883209
I1026 01:18:55.728164 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0301407 (* 1 = 0.0301407 loss)
I1026 01:18:55.728169 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0581802 (* 1 = 0.0581802 loss)
I1026 01:18:55.728174 17254 sgd_solver.cpp:106] Iteration 15440, lr = 0.001
I1026 01:18:56.819710 17254 solver.cpp:229] Iteration 15460, loss = 0.0650941
I1026 01:18:56.819742 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0433069 (* 1 = 0.0433069 loss)
I1026 01:18:56.819758 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0217873 (* 1 = 0.0217873 loss)
I1026 01:18:56.819763 17254 sgd_solver.cpp:106] Iteration 15460, lr = 0.001
I1026 01:18:57.901271 17254 solver.cpp:229] Iteration 15480, loss = 0.080534
I1026 01:18:57.901304 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0397444 (* 1 = 0.0397444 loss)
I1026 01:18:57.901310 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0407896 (* 1 = 0.0407896 loss)
I1026 01:18:57.901315 17254 sgd_solver.cpp:106] Iteration 15480, lr = 0.001
I1026 01:18:59.001405 17254 solver.cpp:229] Iteration 15500, loss = 0.0598607
I1026 01:18:59.001438 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0393386 (* 1 = 0.0393386 loss)
I1026 01:18:59.001443 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0205221 (* 1 = 0.0205221 loss)
I1026 01:18:59.001448 17254 sgd_solver.cpp:106] Iteration 15500, lr = 0.001
I1026 01:19:00.090638 17254 solver.cpp:229] Iteration 15520, loss = 0.0721447
I1026 01:19:00.090668 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.026588 (* 1 = 0.026588 loss)
I1026 01:19:00.090673 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0455567 (* 1 = 0.0455567 loss)
I1026 01:19:00.090679 17254 sgd_solver.cpp:106] Iteration 15520, lr = 0.001
I1026 01:19:01.176275 17254 solver.cpp:229] Iteration 15540, loss = 0.05109
I1026 01:19:01.176308 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0314277 (* 1 = 0.0314277 loss)
I1026 01:19:01.176313 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0196623 (* 1 = 0.0196623 loss)
I1026 01:19:01.176319 17254 sgd_solver.cpp:106] Iteration 15540, lr = 0.001
I1026 01:19:02.264236 17254 solver.cpp:229] Iteration 15560, loss = 0.218123
I1026 01:19:02.264267 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.115851 (* 1 = 0.115851 loss)
I1026 01:19:02.264272 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.102272 (* 1 = 0.102272 loss)
I1026 01:19:02.264278 17254 sgd_solver.cpp:106] Iteration 15560, lr = 0.001
I1026 01:19:03.349247 17254 solver.cpp:229] Iteration 15580, loss = 0.076185
I1026 01:19:03.349280 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0458841 (* 1 = 0.0458841 loss)
I1026 01:19:03.349285 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0303009 (* 1 = 0.0303009 loss)
I1026 01:19:03.349290 17254 sgd_solver.cpp:106] Iteration 15580, lr = 0.001
          477.          259.        ]
 [ 463.          289.          479.          330.        ]
 ..., 
 [ 283.7706604   199.01148987  390.73294067  228.44355774]
 [  33.55927277   75.29404449   52.68589783   97.25873566]
 [  97.67021179   79.08891296  119.29088593  109.49904633]]
500
[[  28.            2.          481.          467.        ]
 [  10.37811279  104.37121582  437.89001465  476.20501709]
 [ 191.52952576    6.13465929  461.05517578  476.20501709]
 ..., 
 [   0.          413.52685547   23.31682014  445.6697998 ]
 [ 405.65432739  407.07553101  445.08300781  463.36740112]
 [ 357.38400269  283.4357605   469.70617676  352.98272705]]
500
[[  39.           99.          140.          243.        ]
 [ 364.          107.          469.          233.        ]
 [ 364.14489746   12.79539108  499.375       368.63220215]
 ..., 
 [ 438.79043579  145.5344696   486.89779663  207.71868896]
 [ 123.69529724   66.04401398  146.07595825   86.59928131]
 [ 333.00802612   87.30339813  420.88339233  127.9544754 ]]
500
[[  91.          177.          334.          291.        ]
 [ 334.          175.          368.          196.        ]
 [   0.            0.          330.01593018  374.375     ]
 ..., 
 [ 444.20843506   63.09534836  499.375       247.86399841]
 [ 319.23596191   57.888134    371.53744507  108.61397552]
 [ 395.73660278  328.41503906  488.20028687  370.15170288]]

done
Preparing training data...
done
Output will be saved to `/home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train`
Filtered 0 roidb entries: 1424 -> 1424
Computing bounding-box regression targets...
bbox target means:
[[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]
 [  3.06392405e-10  -1.22458401e-03   1.75835069e-02   5.75202824e-02]]
[  3.06392405e-10  -1.22458401e-03   1.75835069e-02   5.75202824e-02]
bbox target stdevs:
[[ 0.          0.          0.          0.        ]
 [ 0.13086897  0.13064042  0.24408151  0.23814441]]
[ 0.13086897  0.13064042  0.24408151  0.23814441]
Normalizing targets
done
RoiDataLayer: name_to_top: {'bbox_inside_weights': 4, 'labels': 2, 'rois': 1, 'bbox_targets': 3, 'bbox_outside_weights': 5, 'data': 0}
Loading pretrained model weights from /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_rpn_stage2_iter_40000.caffemodel
Solving...
speed: 0.054s / iter
speed: 0.054s / iter
speed: 0.054s / iter
speed: 0.054s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
Wrote snapshot to: /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_fast_rcnn_stage2_iter_10000.caffemodel
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.I1026 01:19:04.447602 17254 solver.cpp:229] Iteration 15600, loss = 0.0797586
I1026 01:19:04.447631 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0349202 (* 1 = 0.0349202 loss)
I1026 01:19:04.447636 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0448384 (* 1 = 0.0448384 loss)
I1026 01:19:04.447641 17254 sgd_solver.cpp:106] Iteration 15600, lr = 0.001
I1026 01:19:05.558763 17254 solver.cpp:229] Iteration 15620, loss = 0.0713391
I1026 01:19:05.558797 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0358017 (* 1 = 0.0358017 loss)
I1026 01:19:05.558802 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0355374 (* 1 = 0.0355374 loss)
I1026 01:19:05.558809 17254 sgd_solver.cpp:106] Iteration 15620, lr = 0.001
I1026 01:19:06.660614 17254 solver.cpp:229] Iteration 15640, loss = 0.138391
I1026 01:19:06.660642 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0745167 (* 1 = 0.0745167 loss)
I1026 01:19:06.660647 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0638739 (* 1 = 0.0638739 loss)
I1026 01:19:06.660652 17254 sgd_solver.cpp:106] Iteration 15640, lr = 0.001
I1026 01:19:07.750540 17254 solver.cpp:229] Iteration 15660, loss = 0.0222793
I1026 01:19:07.750573 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0179688 (* 1 = 0.0179688 loss)
I1026 01:19:07.750579 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00431048 (* 1 = 0.00431048 loss)
I1026 01:19:07.750584 17254 sgd_solver.cpp:106] Iteration 15660, lr = 0.001
I1026 01:19:08.868517 17254 solver.cpp:229] Iteration 15680, loss = 0.141676
I1026 01:19:08.868551 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0934087 (* 1 = 0.0934087 loss)
I1026 01:19:08.868556 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0482677 (* 1 = 0.0482677 loss)
I1026 01:19:08.868561 17254 sgd_solver.cpp:106] Iteration 15680, lr = 0.001
I1026 01:19:09.967730 17254 solver.cpp:229] Iteration 15700, loss = 0.044611
I1026 01:19:09.967764 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0324757 (* 1 = 0.0324757 loss)
I1026 01:19:09.967769 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0121353 (* 1 = 0.0121353 loss)
I1026 01:19:09.967775 17254 sgd_solver.cpp:106] Iteration 15700, lr = 0.001
I1026 01:19:11.065523 17254 solver.cpp:229] Iteration 15720, loss = 0.0771209
I1026 01:19:11.065552 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0602028 (* 1 = 0.0602028 loss)
I1026 01:19:11.065557 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0169181 (* 1 = 0.0169181 loss)
I1026 01:19:11.065563 17254 sgd_solver.cpp:106] Iteration 15720, lr = 0.001
I1026 01:19:12.160778 17254 solver.cpp:229] Iteration 15740, loss = 0.0541591
I1026 01:19:12.160809 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0220005 (* 1 = 0.0220005 loss)
I1026 01:19:12.160814 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0321586 (* 1 = 0.0321586 loss)
I1026 01:19:12.160828 17254 sgd_solver.cpp:106] Iteration 15740, lr = 0.001
I1026 01:19:13.244956 17254 solver.cpp:229] Iteration 15760, loss = 0.105211
I1026 01:19:13.244988 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0715654 (* 1 = 0.0715654 loss)
I1026 01:19:13.244993 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.033646 (* 1 = 0.033646 loss)
I1026 01:19:13.244998 17254 sgd_solver.cpp:106] Iteration 15760, lr = 0.001
I1026 01:19:14.324384 17254 solver.cpp:229] Iteration 15780, loss = 0.258737
I1026 01:19:14.324416 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.161205 (* 1 = 0.161205 loss)
I1026 01:19:14.324422 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0975318 (* 1 = 0.0975318 loss)
I1026 01:19:14.324427 17254 sgd_solver.cpp:106] Iteration 15780, lr = 0.001
I1026 01:19:15.404018 17254 solver.cpp:229] Iteration 15800, loss = 0.0536835
I1026 01:19:15.404052 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0367876 (* 1 = 0.0367876 loss)
I1026 01:19:15.404055 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0168959 (* 1 = 0.0168959 loss)
I1026 01:19:15.404060 17254 sgd_solver.cpp:106] Iteration 15800, lr = 0.001
I1026 01:19:16.493453 17254 solver.cpp:229] Iteration 15820, loss = 0.228493
I1026 01:19:16.493486 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.1696 (* 1 = 0.1696 loss)
I1026 01:19:16.493492 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0588924 (* 1 = 0.0588924 loss)
I1026 01:19:16.493496 17254 sgd_solver.cpp:106] Iteration 15820, lr = 0.001
I1026 01:19:17.582180 17254 solver.cpp:229] Iteration 15840, loss = 0.0540585
I1026 01:19:17.582224 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0373589 (* 1 = 0.0373589 loss)
I1026 01:19:17.582229 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0166997 (* 1 = 0.0166997 loss)
I1026 01:19:17.582234 17254 sgd_solver.cpp:106] Iteration 15840, lr = 0.001
I1026 01:19:18.683148 17254 solver.cpp:229] Iteration 15860, loss = 0.0888693
I1026 01:19:18.683182 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0402001 (* 1 = 0.0402001 loss)
I1026 01:19:18.683187 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0486692 (* 1 = 0.0486692 loss)
I1026 01:19:18.683193 17254 sgd_solver.cpp:106] Iteration 15860, lr = 0.001
I1026 01:19:19.782703 17254 solver.cpp:229] Iteration 15880, loss = 0.091347
I1026 01:19:19.782735 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0311419 (* 1 = 0.0311419 loss)
I1026 01:19:19.782740 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0602052 (* 1 = 0.0602052 loss)
I1026 01:19:19.782747 17254 sgd_solver.cpp:106] Iteration 15880, lr = 0.001
I1026 01:19:20.895915 17254 solver.cpp:229] Iteration 15900, loss = 0.053471
I1026 01:19:20.895957 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0303106 (* 1 = 0.0303106 loss)
I1026 01:19:20.895963 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0231604 (* 1 = 0.0231604 loss)
I1026 01:19:20.895968 17254 sgd_solver.cpp:106] Iteration 15900, lr = 0.001
I1026 01:19:21.990084 17254 solver.cpp:229] Iteration 15920, loss = 0.156299
I1026 01:19:21.990118 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0415603 (* 1 = 0.0415603 loss)
I1026 01:19:21.990123 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.114739 (* 1 = 0.114739 loss)
I1026 01:19:21.990128 17254 sgd_solver.cpp:106] Iteration 15920, lr = 0.001
I1026 01:19:23.083338 17254 solver.cpp:229] Iteration 15940, loss = 0.057561
I1026 01:19:23.083372 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0419802 (* 1 = 0.0419802 loss)
I1026 01:19:23.083379 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0155808 (* 1 = 0.0155808 loss)
I1026 01:19:23.083384 17254 sgd_solver.cpp:106] Iteration 15940, lr = 0.001
I1026 01:19:24.180328 17254 solver.cpp:229] Iteration 15960, loss = 0.039095
I1026 01:19:24.180362 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0328198 (* 1 = 0.0328198 loss)
I1026 01:19:24.180367 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00627515 (* 1 = 0.00627515 loss)
I1026 01:19:24.180373 17254 sgd_solver.cpp:106] Iteration 15960, lr = 0.001
I1026 01:19:25.267017 17254 solver.cpp:229] Iteration 15980, loss = 0.0615873
I1026 01:19:25.267050 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.047123 (* 1 = 0.047123 loss)
I1026 01:19:25.267055 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0144643 (* 1 = 0.0144643 loss)
I1026 01:19:25.267068 17254 sgd_solver.cpp:106] Iteration 15980, lr = 0.001
I1026 01:19:26.365999 17254 solver.cpp:229] Iteration 16000, loss = 0.0612125
I1026 01:19:26.366034 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0549865 (* 1 = 0.0549865 loss)
I1026 01:19:26.366039 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00622605 (* 1 = 0.00622605 loss)
I1026 01:19:26.366044 17254 sgd_solver.cpp:106] Iteration 16000, lr = 0.001
I1026 01:19:27.434000 17254 solver.cpp:229] Iteration 16020, loss = 0.0654121
I1026 01:19:27.434031 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0398936 (* 1 = 0.0398936 loss)
I1026 01:19:27.434036 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0255185 (* 1 = 0.0255185 loss)
I1026 01:19:27.434041 17254 sgd_solver.cpp:106] Iteration 16020, lr = 0.001
I1026 01:19:28.523950 17254 solver.cpp:229] Iteration 16040, loss = 0.0525064
I1026 01:19:28.523983 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0491142 (* 1 = 0.0491142 loss)
I1026 01:19:28.523988 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00339214 (* 1 = 0.00339214 loss)
I1026 01:19:28.523993 17254 sgd_solver.cpp:106] Iteration 16040, lr = 0.001
I1026 01:19:29.609159 17254 solver.cpp:229] Iteration 16060, loss = 0.132128
I1026 01:19:29.609192 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0720211 (* 1 = 0.0720211 loss)
I1026 01:19:29.609197 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0601064 (* 1 = 0.0601064 loss)
I1026 01:19:29.609202 17254 sgd_solver.cpp:106] Iteration 16060, lr = 0.001
I1026 01:19:30.709460 17254 solver.cpp:229] Iteration 16080, loss = 0.117525
I1026 01:19:30.709493 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0579782 (* 1 = 0.0579782 loss)
I1026 01:19:30.709498 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0595468 (* 1 = 0.0595468 loss)
I1026 01:19:30.709504 17254 sgd_solver.cpp:106] Iteration 16080, lr = 0.001
I1026 01:19:31.806694 17254 solver.cpp:229] Iteration 16100, loss = 0.0760058
I1026 01:19:31.806725 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0438815 (* 1 = 0.0438815 loss)
I1026 01:19:31.806730 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0321243 (* 1 = 0.0321243 loss)
I1026 01:19:31.806735 17254 sgd_solver.cpp:106] Iteration 16100, lr = 0.001
I1026 01:19:32.881909 17254 solver.cpp:229] Iteration 16120, loss = 0.201772
I1026 01:19:32.881943 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.107558 (* 1 = 0.107558 loss)
I1026 01:19:32.881948 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0942139 (* 1 = 0.0942139 loss)
I1026 01:19:32.881953 17254 sgd_solver.cpp:106] Iteration 16120, lr = 0.001
I1026 01:19:33.961021 17254 solver.cpp:229] Iteration 16140, loss = 0.0396526
I1026 01:19:33.961053 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0273888 (* 1 = 0.0273888 loss)
I1026 01:19:33.961060 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0122639 (* 1 = 0.0122639 loss)
I1026 01:19:33.961064 17254 sgd_solver.cpp:106] Iteration 16140, lr = 0.001
I1026 01:19:35.041009 17254 solver.cpp:229] Iteration 16160, loss = 0.112721
I1026 01:19:35.041043 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0461151 (* 1 = 0.0461151 loss)
I1026 01:19:35.041049 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0666058 (* 1 = 0.0666058 loss)
I1026 01:19:35.041054 17254 sgd_solver.cpp:106] Iteration 16160, lr = 0.001
I1026 01:19:36.127491 17254 solver.cpp:229] Iteration 16180, loss = 0.0512181
I1026 01:19:36.127534 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0242901 (* 1 = 0.0242901 loss)
I1026 01:19:36.127539 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.026928 (* 1 = 0.026928 loss)
I1026 01:19:36.127544 17254 sgd_solver.cpp:106] Iteration 16180, lr = 0.001
I1026 01:19:37.214901 17254 solver.cpp:229] Iteration 16200, loss = 0.0495816
I1026 01:19:37.214936 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0277094 (* 1 = 0.0277094 loss)
I1026 01:19:37.214941 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0218721 (* 1 = 0.0218721 loss)
I1026 01:19:37.214946 17254 sgd_solver.cpp:106] Iteration 16200, lr = 0.001
I1026 01:19:38.305502 17254 solver.cpp:229] Iteration 16220, loss = 0.130106
I1026 01:19:38.305536 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.081391 (* 1 = 0.081391 loss)
I1026 01:19:38.305541 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0487147 (* 1 = 0.0487147 loss)
I1026 01:19:38.305546 17254 sgd_solver.cpp:106] Iteration 16220, lr = 0.001
I1026 01:19:39.397598 17254 solver.cpp:229] Iteration 16240, loss = 0.140995
I1026 01:19:39.397627 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0967996 (* 1 = 0.0967996 loss)
I1026 01:19:39.397632 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0441958 (* 1 = 0.0441958 loss)
I1026 01:19:39.397637 17254 sgd_solver.cpp:106] Iteration 16240, lr = 0.001
I1026 01:19:40.492414 17254 solver.cpp:229] Iteration 16260, loss = 0.0499499
I1026 01:19:40.492457 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0446529 (* 1 = 0.0446529 loss)
I1026 01:19:40.492463 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00529693 (* 1 = 0.00529693 loss)
I1026 01:19:40.492468 17254 sgd_solver.cpp:106] Iteration 16260, lr = 0.001
I1026 01:19:41.575392 17254 solver.cpp:229] Iteration 16280, loss = 0.0980198
I1026 01:19:41.575424 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0542825 (* 1 = 0.0542825 loss)
I1026 01:19:41.575433 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0437373 (* 1 = 0.0437373 loss)
I1026 01:19:41.575438 17254 sgd_solver.cpp:106] Iteration 16280, lr = 0.001
I1026 01:19:42.657897 17254 solver.cpp:229] Iteration 16300, loss = 0.0633736
I1026 01:19:42.657935 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0487339 (* 1 = 0.0487339 loss)
I1026 01:19:42.657940 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0146397 (* 1 = 0.0146397 loss)
I1026 01:19:42.657945 17254 sgd_solver.cpp:106] Iteration 16300, lr = 0.001
I1026 01:19:43.753267 17254 solver.cpp:229] Iteration 16320, loss = 0.132458
I1026 01:19:43.753310 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0600619 (* 1 = 0.0600619 loss)
I1026 01:19:43.753315 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0723956 (* 1 = 0.0723956 loss)
I1026 01:19:43.753321 17254 sgd_solver.cpp:106] Iteration 16320, lr = 0.001
I1026 01:19:44.849979 17254 solver.cpp:229] Iteration 16340, loss = 0.0917577
I1026 01:19:44.850010 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0463511 (* 1 = 0.0463511 loss)
I1026 01:19:44.850014 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0454065 (* 1 = 0.0454065 loss)
I1026 01:19:44.850019 17254 sgd_solver.cpp:106] Iteration 16340, lr = 0.001
I1026 01:19:45.943495 17254 solver.cpp:229] Iteration 16360, loss = 0.0596669
I1026 01:19:45.943527 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0337555 (* 1 = 0.0337555 loss)
I1026 01:19:45.943531 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0259114 (* 1 = 0.0259114 loss)
I1026 01:19:45.943537 17254 sgd_solver.cpp:106] Iteration 16360, lr = 0.001
I1026 01:19:47.026861 17254 solver.cpp:229] Iteration 16380, loss = 0.0594241
I1026 01:19:47.026895 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0543265 (* 1 = 0.0543265 loss)
I1026 01:19:47.026901 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00509753 (* 1 = 0.00509753 loss)
I1026 01:19:47.026906 17254 sgd_solver.cpp:106] Iteration 16380, lr = 0.001
I1026 01:19:48.119622 17254 solver.cpp:229] Iteration 16400, loss = 0.0516751
I1026 01:19:48.119655 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0392657 (* 1 = 0.0392657 loss)
I1026 01:19:48.119660 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0124094 (* 1 = 0.0124094 loss)
I1026 01:19:48.119665 17254 sgd_solver.cpp:106] Iteration 16400, lr = 0.001
I1026 01:19:49.220674 17254 solver.cpp:229] Iteration 16420, loss = 0.0595881
I1026 01:19:49.220706 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0340899 (* 1 = 0.0340899 loss)
I1026 01:19:49.220712 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0254982 (* 1 = 0.0254982 loss)
I1026 01:19:49.220717 17254 sgd_solver.cpp:106] Iteration 16420, lr = 0.001
I1026 01:19:50.324151 17254 solver.cpp:229] Iteration 16440, loss = 0.0667674
I1026 01:19:50.324182 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0503048 (* 1 = 0.0503048 loss)
I1026 01:19:50.324187 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0164626 (* 1 = 0.0164626 loss)
I1026 01:19:50.324193 17254 sgd_solver.cpp:106] Iteration 16440, lr = 0.001
I1026 01:19:51.403228 17254 solver.cpp:229] Iteration 16460, loss = 0.0660972
I1026 01:19:51.403260 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0434155 (* 1 = 0.0434155 loss)
I1026 01:19:51.403276 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0226818 (* 1 = 0.0226818 loss)
I1026 01:19:51.403281 17254 sgd_solver.cpp:106] Iteration 16460, lr = 0.001
I1026 01:19:52.509979 17254 solver.cpp:229] Iteration 16480, loss = 0.157176
I1026 01:19:52.510012 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.107393 (* 1 = 0.107393 loss)
I1026 01:19:52.510017 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0497826 (* 1 = 0.0497826 loss)
I1026 01:19:52.510022 17254 sgd_solver.cpp:106] Iteration 16480, lr = 0.001
I1026 01:19:53.583602 17254 solver.cpp:229] Iteration 16500, loss = 0.0908117
I1026 01:19:53.583634 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0292624 (* 1 = 0.0292624 loss)
I1026 01:19:53.583641 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0615493 (* 1 = 0.0615493 loss)
I1026 01:19:53.583645 17254 sgd_solver.cpp:106] Iteration 16500, lr = 0.001
I1026 01:19:54.675917 17254 solver.cpp:229] Iteration 16520, loss = 0.124865
I1026 01:19:54.675951 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0664744 (* 1 = 0.0664744 loss)
I1026 01:19:54.675956 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.058391 (* 1 = 0.058391 loss)
I1026 01:19:54.675961 17254 sgd_solver.cpp:106] Iteration 16520, lr = 0.001
I1026 01:19:55.771108 17254 solver.cpp:229] Iteration 16540, loss = 0.0717616
I1026 01:19:55.771142 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0266415 (* 1 = 0.0266415 loss)
I1026 01:19:55.771147 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0451201 (* 1 = 0.0451201 loss)
I1026 01:19:55.771152 17254 sgd_solver.cpp:106] Iteration 16540, lr = 0.001
I1026 01:19:56.858320 17254 solver.cpp:229] Iteration 16560, loss = 0.0544006
I1026 01:19:56.858367 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0279386 (* 1 = 0.0279386 loss)
I1026 01:19:56.858373 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.026462 (* 1 = 0.026462 loss)
I1026 01:19:56.858378 17254 sgd_solver.cpp:106] Iteration 16560, lr = 0.001
I1026 01:19:57.954815 17254 solver.cpp:229] Iteration 16580, loss = 0.0495765
I1026 01:19:57.954849 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0204643 (* 1 = 0.0204643 loss)
I1026 01:19:57.954854 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0291122 (* 1 = 0.0291122 loss)
I1026 01:19:57.954859 17254 sgd_solver.cpp:106] Iteration 16580, lr = 0.001
I1026 01:19:59.044504 17254 solver.cpp:229] Iteration 16600, loss = 0.0789796
I1026 01:19:59.044536 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0648594 (* 1 = 0.0648594 loss)
I1026 01:19:59.044541 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0141202 (* 1 = 0.0141202 loss)
I1026 01:19:59.044548 17254 sgd_solver.cpp:106] Iteration 16600, lr = 0.001
I1026 01:20:00.114387 17254 solver.cpp:229] Iteration 16620, loss = 0.045072
I1026 01:20:00.114418 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0411488 (* 1 = 0.0411488 loss)
I1026 01:20:00.114423 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00392326 (* 1 = 0.00392326 loss)
I1026 01:20:00.114429 17254 sgd_solver.cpp:106] Iteration 16620, lr = 0.001
I1026 01:20:01.202512 17254 solver.cpp:229] Iteration 16640, loss = 0.0784153
I1026 01:20:01.202545 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0415117 (* 1 = 0.0415117 loss)
I1026 01:20:01.202551 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0369036 (* 1 = 0.0369036 loss)
I1026 01:20:01.202558 17254 sgd_solver.cpp:106] Iteration 16640, lr = 0.001
I1026 01:20:02.292225 17254 solver.cpp:229] Iteration 16660, loss = 0.0933954
I1026 01:20:02.292258 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0505837 (* 1 = 0.0505837 loss)
I1026 01:20:02.292263 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0428117 (* 1 = 0.0428117 loss)
I1026 01:20:02.292268 17254 sgd_solver.cpp:106] Iteration 16660, lr = 0.001
I1026 01:20:03.380539 17254 solver.cpp:229] Iteration 16680, loss = 0.0500005
I1026 01:20:03.380571 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0253969 (* 1 = 0.0253969 loss)
I1026 01:20:03.380576 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0246035 (* 1 = 0.0246035 loss)
I1026 01:20:03.380583 17254 sgd_solver.cpp:106] Iteration 16680, lr = 0.001
I1026 01:20:04.446460 17254 solver.cpp:229] Iteration 16700, loss = 0.0481291
I1026 01:20:04.446492 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0306947 (* 1 = 0.0306947 loss)
I1026 01:20:04.446497 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0174344 (* 1 = 0.0174344 loss)
I1026 01:20:04.446502 17254 sgd_solver.cpp:106] Iteration 16700, lr = 0.001
I1026 01:20:05.532064 17254 solver.cpp:229] Iteration 16720, loss = 0.0595104
I1026 01:20:05.532099 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0476817 (* 1 = 0.0476817 loss)
I1026 01:20:05.532104 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0118288 (* 1 = 0.0118288 loss)
I1026 01:20:05.532109 17254 sgd_solver.cpp:106] Iteration 16720, lr = 0.001
I1026 01:20:06.618870 17254 solver.cpp:229] Iteration 16740, loss = 0.0627028
I1026 01:20:06.618902 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0432123 (* 1 = 0.0432123 loss)
I1026 01:20:06.618906 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0194905 (* 1 = 0.0194905 loss)
I1026 01:20:06.618912 17254 sgd_solver.cpp:106] Iteration 16740, lr = 0.001
I1026 01:20:07.709925 17254 solver.cpp:229] Iteration 16760, loss = 0.0695121
I1026 01:20:07.709957 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0463313 (* 1 = 0.0463313 loss)
I1026 01:20:07.709962 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0231808 (* 1 = 0.0231808 loss)
I1026 01:20:07.709967 17254 sgd_solver.cpp:106] Iteration 16760, lr = 0.001
I1026 01:20:08.787899 17254 solver.cpp:229] Iteration 16780, loss = 0.0466442
I1026 01:20:08.787940 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0428341 (* 1 = 0.0428341 loss)
I1026 01:20:08.787945 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0038101 (* 1 = 0.0038101 loss)
I1026 01:20:08.787950 17254 sgd_solver.cpp:106] Iteration 16780, lr = 0.001
I1026 01:20:09.876611 17254 solver.cpp:229] Iteration 16800, loss = 0.162438
I1026 01:20:09.876653 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0825697 (* 1 = 0.0825697 loss)
I1026 01:20:09.876659 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0798686 (* 1 = 0.0798686 loss)
I1026 01:20:09.876664 17254 sgd_solver.cpp:106] Iteration 16800, lr = 0.001
I1026 01:20:10.972892 17254 solver.cpp:229] Iteration 16820, loss = 0.0806485
I1026 01:20:10.972925 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.045659 (* 1 = 0.045659 loss)
I1026 01:20:10.972930 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0349895 (* 1 = 0.0349895 loss)
I1026 01:20:10.972935 17254 sgd_solver.cpp:106] Iteration 16820, lr = 0.001
I1026 01:20:12.053726 17254 solver.cpp:229] Iteration 16840, loss = 0.061012
I1026 01:20:12.053758 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0434959 (* 1 = 0.0434959 loss)
I1026 01:20:12.053763 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0175161 (* 1 = 0.0175161 loss)
I1026 01:20:12.053769 17254 sgd_solver.cpp:106] Iteration 16840, lr = 0.001
I1026 01:20:13.151603 17254 solver.cpp:229] Iteration 16860, loss = 0.0893473
I1026 01:20:13.151645 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0278334 (* 1 = 0.0278334 loss)
I1026 01:20:13.151650 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0615139 (* 1 = 0.0615139 loss)
I1026 01:20:13.151656 17254 sgd_solver.cpp:106] Iteration 16860, lr = 0.001
I1026 01:20:14.235477 17254 solver.cpp:229] Iteration 16880, loss = 0.0737158
I1026 01:20:14.235510 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0271017 (* 1 = 0.0271017 loss)
I1026 01:20:14.235515 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.046614 (* 1 = 0.046614 loss)
I1026 01:20:14.235520 17254 sgd_solver.cpp:106] Iteration 16880, lr = 0.001
I1026 01:20:15.329087 17254 solver.cpp:229] Iteration 16900, loss = 0.0360857
I1026 01:20:15.329113 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0272016 (* 1 = 0.0272016 loss)
I1026 01:20:15.329119 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00888414 (* 1 = 0.00888414 loss)
I1026 01:20:15.329124 17254 sgd_solver.cpp:106] Iteration 16900, lr = 0.001
I1026 01:20:16.425778 17254 solver.cpp:229] Iteration 16920, loss = 0.096102
I1026 01:20:16.425812 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.068564 (* 1 = 0.068564 loss)
I1026 01:20:16.425817 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0275381 (* 1 = 0.0275381 loss)
I1026 01:20:16.425823 17254 sgd_solver.cpp:106] Iteration 16920, lr = 0.001
I1026 01:20:17.516777 17254 solver.cpp:229] Iteration 16940, loss = 0.0816839
I1026 01:20:17.516809 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0308409 (* 1 = 0.0308409 loss)
I1026 01:20:17.516814 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.050843 (* 1 = 0.050843 loss)
I1026 01:20:17.516819 17254 sgd_solver.cpp:106] Iteration 16940, lr = 0.001
I1026 01:20:18.599155 17254 solver.cpp:229] Iteration 16960, loss = 0.108892
I1026 01:20:18.599189 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0451741 (* 1 = 0.0451741 loss)
I1026 01:20:18.599195 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.063718 (* 1 = 0.063718 loss)
I1026 01:20:18.599200 17254 sgd_solver.cpp:106] Iteration 16960, lr = 0.001
I1026 01:20:19.665730 17254 solver.cpp:229] Iteration 16980, loss = 0.21859
I1026 01:20:19.665763 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.122511 (* 1 = 0.122511 loss)
I1026 01:20:19.665768 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0960788 (* 1 = 0.0960788 loss)
I1026 01:20:19.665773 17254 sgd_solver.cpp:106] Iteration 16980, lr = 0.001
I1026 01:20:20.749456 17254 solver.cpp:229] Iteration 17000, loss = 0.116641
I1026 01:20:20.749488 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0596305 (* 1 = 0.0596305 loss)
I1026 01:20:20.749493 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0570103 (* 1 = 0.0570103 loss)
I1026 01:20:20.749498 17254 sgd_solver.cpp:106] Iteration 17000, lr = 0.001
I1026 01:20:21.830304 17254 solver.cpp:229] Iteration 17020, loss = 0.123523
I1026 01:20:21.830337 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0689766 (* 1 = 0.0689766 loss)
I1026 01:20:21.830343 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0545467 (* 1 = 0.0545467 loss)
I1026 01:20:21.830346 17254 sgd_solver.cpp:106] Iteration 17020, lr = 0.001
I1026 01:20:22.919939 17254 solver.cpp:229] Iteration 17040, loss = 0.0526702
I1026 01:20:22.919981 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0481901 (* 1 = 0.0481901 loss)
I1026 01:20:22.919988 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00448007 (* 1 = 0.00448007 loss)
I1026 01:20:22.919993 17254 sgd_solver.cpp:106] Iteration 17040, lr = 0.001
I1026 01:20:24.022830 17254 solver.cpp:229] Iteration 17060, loss = 0.103307
I1026 01:20:24.022863 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0785016 (* 1 = 0.0785016 loss)
I1026 01:20:24.022868 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0248053 (* 1 = 0.0248053 loss)
I1026 01:20:24.022873 17254 sgd_solver.cpp:106] Iteration 17060, lr = 0.001
I1026 01:20:25.105495 17254 solver.cpp:229] Iteration 17080, loss = 0.0829779
I1026 01:20:25.105527 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0365453 (* 1 = 0.0365453 loss)
I1026 01:20:25.105532 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0464325 (* 1 = 0.0464325 loss)
I1026 01:20:25.105537 17254 sgd_solver.cpp:106] Iteration 17080, lr = 0.001
I1026 01:20:26.183460 17254 solver.cpp:229] Iteration 17100, loss = 0.138842
I1026 01:20:26.183503 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.080189 (* 1 = 0.080189 loss)
I1026 01:20:26.183509 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0586531 (* 1 = 0.0586531 loss)
I1026 01:20:26.183514 17254 sgd_solver.cpp:106] Iteration 17100, lr = 0.001
I1026 01:20:27.262071 17254 solver.cpp:229] Iteration 17120, loss = 0.128236
I1026 01:20:27.262104 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0580797 (* 1 = 0.0580797 loss)
I1026 01:20:27.262109 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0701564 (* 1 = 0.0701564 loss)
I1026 01:20:27.262114 17254 sgd_solver.cpp:106] Iteration 17120, lr = 0.001
I1026 01:20:28.348371 17254 solver.cpp:229] Iteration 17140, loss = 0.0848107
I1026 01:20:28.348403 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0507938 (* 1 = 0.0507938 loss)
I1026 01:20:28.348408 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0340169 (* 1 = 0.0340169 loss)
I1026 01:20:28.348414 17254 sgd_solver.cpp:106] Iteration 17140, lr = 0.001
I1026 01:20:29.415123 17254 solver.cpp:229] Iteration 17160, loss = 0.0524761
I1026 01:20:29.415155 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0353522 (* 1 = 0.0353522 loss)
I1026 01:20:29.415160 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0171239 (* 1 = 0.0171239 loss)
I1026 01:20:29.415165 17254 sgd_solver.cpp:106] Iteration 17160, lr = 0.001
I1026 01:20:30.493248 17254 solver.cpp:229] Iteration 17180, loss = 0.119688
I1026 01:20:30.493281 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0670972 (* 1 = 0.0670972 loss)
I1026 01:20:30.493286 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0525913 (* 1 = 0.0525913 loss)
I1026 01:20:30.493291 17254 sgd_solver.cpp:106] Iteration 17180, lr = 0.001
I1026 01:20:31.602764 17254 solver.cpp:229] Iteration 17200, loss = 0.0393865
I1026 01:20:31.602797 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0250429 (* 1 = 0.0250429 loss)
I1026 01:20:31.602802 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0143436 (* 1 = 0.0143436 loss)
I1026 01:20:31.602807 17254 sgd_solver.cpp:106] Iteration 17200, lr = 0.001
I1026 01:20:32.687513 17254 solver.cpp:229] Iteration 17220, loss = 0.126613
I1026 01:20:32.687546 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0749312 (* 1 = 0.0749312 loss)
I1026 01:20:32.687551 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0516813 (* 1 = 0.0516813 loss)
I1026 01:20:32.687556 17254 sgd_solver.cpp:106] Iteration 17220, lr = 0.001
I1026 01:20:33.766415 17254 solver.cpp:229] Iteration 17240, loss = 0.104321
I1026 01:20:33.766448 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0456199 (* 1 = 0.0456199 loss)
I1026 01:20:33.766453 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0587015 (* 1 = 0.0587015 loss)
I1026 01:20:33.766458 17254 sgd_solver.cpp:106] Iteration 17240, lr = 0.001
I1026 01:20:34.863389 17254 solver.cpp:229] Iteration 17260, loss = 0.0405404
I1026 01:20:34.863420 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0378084 (* 1 = 0.0378084 loss)
I1026 01:20:34.863425 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00273198 (* 1 = 0.00273198 loss)
I1026 01:20:34.863435 17254 sgd_solver.cpp:106] Iteration 17260, lr = 0.001
I1026 01:20:35.960713 17254 solver.cpp:229] Iteration 17280, loss = 0.142666
I1026 01:20:35.960747 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0694875 (* 1 = 0.0694875 loss)
I1026 01:20:35.960752 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0731781 (* 1 = 0.0731781 loss)
I1026 01:20:35.960757 17254 sgd_solver.cpp:106] Iteration 17280, lr = 0.001
I1026 01:20:37.058347 17254 solver.cpp:229] Iteration 17300, loss = 0.0787709
I1026 01:20:37.058380 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0401641 (* 1 = 0.0401641 loss)
I1026 01:20:37.058385 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0386069 (* 1 = 0.0386069 loss)
I1026 01:20:37.058390 17254 sgd_solver.cpp:106] Iteration 17300, lr = 0.001
I1026 01:20:38.164036 17254 solver.cpp:229] Iteration 17320, loss = 0.108515
I1026 01:20:38.164068 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.042342 (* 1 = 0.042342 loss)
I1026 01:20:38.164073 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.066173 (* 1 = 0.066173 loss)
I1026 01:20:38.164078 17254 sgd_solver.cpp:106] Iteration 17320, lr = 0.001
I1026 01:20:39.258357 17254 solver.cpp:229] Iteration 17340, loss = 0.0882002
I1026 01:20:39.258391 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0318672 (* 1 = 0.0318672 loss)
I1026 01:20:39.258396 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.056333 (* 1 = 0.056333 loss)
I1026 01:20:39.258402 17254 sgd_solver.cpp:106] Iteration 17340, lr = 0.001
I1026 01:20:40.329001 17254 solver.cpp:229] Iteration 17360, loss = 0.0254293
I1026 01:20:40.329035 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0179784 (* 1 = 0.0179784 loss)
I1026 01:20:40.329041 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00745083 (* 1 = 0.00745083 loss)
I1026 01:20:40.329056 17254 sgd_solver.cpp:106] Iteration 17360, lr = 0.001
I1026 01:20:41.422255 17254 solver.cpp:229] Iteration 17380, loss = 0.0585618
I1026 01:20:41.422287 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0328275 (* 1 = 0.0328275 loss)
I1026 01:20:41.422292 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0257343 (* 1 = 0.0257343 loss)
I1026 01:20:41.422297 17254 sgd_solver.cpp:106] Iteration 17380, lr = 0.001
I1026 01:20:42.506570 17254 solver.cpp:229] Iteration 17400, loss = 0.19879
I1026 01:20:42.506603 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.11812 (* 1 = 0.11812 loss)
I1026 01:20:42.506608 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0806699 (* 1 = 0.0806699 loss)
I1026 01:20:42.506614 17254 sgd_solver.cpp:106] Iteration 17400, lr = 0.001
I1026 01:20:43.593538 17254 solver.cpp:229] Iteration 17420, loss = 0.0748995
I1026 01:20:43.593581 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0417942 (* 1 = 0.0417942 loss)
I1026 01:20:43.593586 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0331054 (* 1 = 0.0331054 loss)
I1026 01:20:43.593591 17254 sgd_solver.cpp:106] Iteration 17420, lr = 0.001
I1026 01:20:44.699278 17254 solver.cpp:229] Iteration 17440, loss = 0.075154
I1026 01:20:44.699323 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0452894 (* 1 = 0.0452894 loss)
I1026 01:20:44.699328 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0298646 (* 1 = 0.0298646 loss)
I1026 01:20:44.699334 17254 sgd_solver.cpp:106] Iteration 17440, lr = 0.001
I1026 01:20:45.768519 17254 solver.cpp:229] Iteration 17460, loss = 0.0407724
I1026 01:20:45.768553 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0362068 (* 1 = 0.0362068 loss)
I1026 01:20:45.768558 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0045656 (* 1 = 0.0045656 loss)
I1026 01:20:45.768563 17254 sgd_solver.cpp:106] Iteration 17460, lr = 0.001
I1026 01:20:46.850517 17254 solver.cpp:229] Iteration 17480, loss = 0.060317
I1026 01:20:46.850561 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.039572 (* 1 = 0.039572 loss)
I1026 01:20:46.850566 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0207449 (* 1 = 0.0207449 loss)
I1026 01:20:46.850571 17254 sgd_solver.cpp:106] Iteration 17480, lr = 0.001
I1026 01:20:47.942237 17254 solver.cpp:229] Iteration 17500, loss = 0.0448113
I1026 01:20:47.942271 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0321565 (* 1 = 0.0321565 loss)
I1026 01:20:47.942276 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0126548 (* 1 = 0.0126548 loss)
I1026 01:20:47.942282 17254 sgd_solver.cpp:106] Iteration 17500, lr = 0.001
I1026 01:20:49.014178 17254 solver.cpp:229] Iteration 17520, loss = 0.07285
I1026 01:20:49.014214 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0507093 (* 1 = 0.0507093 loss)
I1026 01:20:49.014221 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0221407 (* 1 = 0.0221407 loss)
I1026 01:20:49.014228 17254 sgd_solver.cpp:106] Iteration 17520, lr = 0.001
I1026 01:20:50.107357 17254 solver.cpp:229] Iteration 17540, loss = 0.048057
I1026 01:20:50.107390 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.030486 (* 1 = 0.030486 loss)
I1026 01:20:50.107398 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.017571 (* 1 = 0.017571 loss)
I1026 01:20:50.107405 17254 sgd_solver.cpp:106] Iteration 17540, lr = 0.001
I1026 01:20:51.208986 17254 solver.cpp:229] Iteration 17560, loss = 0.0563313
I1026 01:20:51.209020 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0309046 (* 1 = 0.0309046 loss)
I1026 01:20:51.209025 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0254267 (* 1 = 0.0254267 loss)
I1026 01:20:51.209031 17254 sgd_solver.cpp:106] Iteration 17560, lr = 0.001
I1026 01:20:52.297348 17254 solver.cpp:229] Iteration 17580, loss = 0.187204
I1026 01:20:52.297380 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0668061 (* 1 = 0.0668061 loss)
I1026 01:20:52.297385 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.120398 (* 1 = 0.120398 loss)
I1026 01:20:52.297390 17254 sgd_solver.cpp:106] Iteration 17580, lr = 0.001
I1026 01:20:53.390502 17254 solver.cpp:229] Iteration 17600, loss = 0.0418922
I1026 01:20:53.390535 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.026162 (* 1 = 0.026162 loss)
I1026 01:20:53.390540 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0157302 (* 1 = 0.0157302 loss)
I1026 01:20:53.390547 17254 sgd_solver.cpp:106] Iteration 17600, lr = 0.001
I1026 01:20:54.475157 17254 solver.cpp:229] Iteration 17620, loss = 0.0831677
I1026 01:20:54.475188 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0566336 (* 1 = 0.0566336 loss)
I1026 01:20:54.475193 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0265341 (* 1 = 0.0265341 loss)
I1026 01:20:54.475199 17254 sgd_solver.cpp:106] Iteration 17620, lr = 0.001
I1026 01:20:55.565280 17254 solver.cpp:229] Iteration 17640, loss = 0.0503834
I1026 01:20:55.565311 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0345075 (* 1 = 0.0345075 loss)
I1026 01:20:55.565316 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0158759 (* 1 = 0.0158759 loss)
I1026 01:20:55.565322 17254 sgd_solver.cpp:106] Iteration 17640, lr = 0.001
I1026 01:20:56.638967 17254 solver.cpp:229] Iteration 17660, loss = 0.0708127
I1026 01:20:56.638996 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0371795 (* 1 = 0.0371795 loss)
I1026 01:20:56.639001 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0336332 (* 1 = 0.0336332 loss)
I1026 01:20:56.639006 17254 sgd_solver.cpp:106] Iteration 17660, lr = 0.001
I1026 01:20:57.716490 17254 solver.cpp:229] Iteration 17680, loss = 0.0846515
I1026 01:20:57.716522 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0302453 (* 1 = 0.0302453 loss)
I1026 01:20:57.716528 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0544062 (* 1 = 0.0544062 loss)
I1026 01:20:57.716532 17254 sgd_solver.cpp:106] Iteration 17680, lr = 0.001
I1026 01:20:58.801851 17254 solver.cpp:229] Iteration 17700, loss = 0.0867673
I1026 01:20:58.801885 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0411785 (* 1 = 0.0411785 loss)
I1026 01:20:58.801889 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0455888 (* 1 = 0.0455888 loss)
I1026 01:20:58.801894 17254 sgd_solver.cpp:106] Iteration 17700, lr = 0.001
I1026 01:20:59.895393 17254 solver.cpp:229] Iteration 17720, loss = 0.155554
I1026 01:20:59.895426 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0494882 (* 1 = 0.0494882 loss)
I1026 01:20:59.895436 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.106066 (* 1 = 0.106066 loss)
I1026 01:20:59.895440 17254 sgd_solver.cpp:106] Iteration 17720, lr = 0.001
I1026 01:21:00.996714 17254 solver.cpp:229] Iteration 17740, loss = 0.123302
I1026 01:21:00.996747 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.073399 (* 1 = 0.073399 loss)
I1026 01:21:00.996752 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.049903 (* 1 = 0.049903 loss)
I1026 01:21:00.996757 17254 sgd_solver.cpp:106] Iteration 17740, lr = 0.001
I1026 01:21:02.083943 17254 solver.cpp:229] Iteration 17760, loss = 0.162805
I1026 01:21:02.083976 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0928732 (* 1 = 0.0928732 loss)
I1026 01:21:02.083981 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0699319 (* 1 = 0.0699319 loss)
I1026 01:21:02.083986 17254 sgd_solver.cpp:106] Iteration 17760, lr = 0.001
I1026 01:21:03.157642 17254 solver.cpp:229] Iteration 17780, loss = 0.068833
I1026 01:21:03.157675 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0398081 (* 1 = 0.0398081 loss)
I1026 01:21:03.157680 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.029025 (* 1 = 0.029025 loss)
I1026 01:21:03.157685 17254 sgd_solver.cpp:106] Iteration 17780, lr = 0.001
I1026 01:21:04.227674 17254 solver.cpp:229] Iteration 17800, loss = 0.0673739
I1026 01:21:04.227705 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0423499 (* 1 = 0.0423499 loss)
I1026 01:21:04.227710 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0250241 (* 1 = 0.0250241 loss)
I1026 01:21:04.227715 17254 sgd_solver.cpp:106] Iteration 17800, lr = 0.001
I1026 01:21:05.317454 17254 solver.cpp:229] Iteration 17820, loss = 0.0865466
I1026 01:21:05.317488 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0324589 (* 1 = 0.0324589 loss)
I1026 01:21:05.317493 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0540877 (* 1 = 0.0540877 loss)
I1026 01:21:05.317499 17254 sgd_solver.cpp:106] Iteration 17820, lr = 0.001
I1026 01:21:06.411840 17254 solver.cpp:229] Iteration 17840, loss = 0.0421311
I1026 01:21:06.411890 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0351433 (* 1 = 0.0351433 loss)
I1026 01:21:06.411895 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00698789 (* 1 = 0.00698789 loss)
I1026 01:21:06.411901 17254 sgd_solver.cpp:106] Iteration 17840, lr = 0.001
I1026 01:21:07.500699 17254 solver.cpp:229] Iteration 17860, loss = 0.0700473
I1026 01:21:07.500730 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0465576 (* 1 = 0.0465576 loss)
I1026 01:21:07.500735 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0234896 (* 1 = 0.0234896 loss)
I1026 01:21:07.500741 17254 sgd_solver.cpp:106] Iteration 17860, lr = 0.001
I1026 01:21:08.583012 17254 solver.cpp:229] Iteration 17880, loss = 0.131905
I1026 01:21:08.583046 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0834621 (* 1 = 0.0834621 loss)
I1026 01:21:08.583051 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.048443 (* 1 = 0.048443 loss)
I1026 01:21:08.583056 17254 sgd_solver.cpp:106] Iteration 17880, lr = 0.001
I1026 01:21:09.668047 17254 solver.cpp:229] Iteration 17900, loss = 0.109308
I1026 01:21:09.668078 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0655443 (* 1 = 0.0655443 loss)
I1026 01:21:09.668083 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0437638 (* 1 = 0.0437638 loss)
I1026 01:21:09.668088 17254 sgd_solver.cpp:106] Iteration 17900, lr = 0.001
I1026 01:21:10.740260 17254 solver.cpp:229] Iteration 17920, loss = 0.0731186
I1026 01:21:10.740293 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0435016 (* 1 = 0.0435016 loss)
I1026 01:21:10.740298 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0296171 (* 1 = 0.0296171 loss)
I1026 01:21:10.740303 17254 sgd_solver.cpp:106] Iteration 17920, lr = 0.001
I1026 01:21:11.815407 17254 solver.cpp:229] Iteration 17940, loss = 0.0693111
I1026 01:21:11.815441 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0257031 (* 1 = 0.0257031 loss)
I1026 01:21:11.815446 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.043608 (* 1 = 0.043608 loss)
I1026 01:21:11.815451 17254 sgd_solver.cpp:106] Iteration 17940, lr = 0.001
I1026 01:21:12.918243 17254 solver.cpp:229] Iteration 17960, loss = 0.0635162
I1026 01:21:12.918277 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0496377 (* 1 = 0.0496377 loss)
I1026 01:21:12.918282 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0138785 (* 1 = 0.0138785 loss)
I1026 01:21:12.918287 17254 sgd_solver.cpp:106] Iteration 17960, lr = 0.001
I1026 01:21:14.016147 17254 solver.cpp:229] Iteration 17980, loss = 0.0490759
I1026 01:21:14.016180 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0347504 (* 1 = 0.0347504 loss)
I1026 01:21:14.016185 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0143255 (* 1 = 0.0143255 loss)
I1026 01:21:14.016190 17254 sgd_solver.cpp:106] Iteration 17980, lr = 0.001
I1026 01:21:15.107015 17254 solver.cpp:229] Iteration 18000, loss = 0.0805113
I1026 01:21:15.107048 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0344441 (* 1 = 0.0344441 loss)
I1026 01:21:15.107053 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0460672 (* 1 = 0.0460672 loss)
I1026 01:21:15.107059 17254 sgd_solver.cpp:106] Iteration 18000, lr = 0.001
I1026 01:21:16.204144 17254 solver.cpp:229] Iteration 18020, loss = 0.0479119
I1026 01:21:16.204175 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0389722 (* 1 = 0.0389722 loss)
I1026 01:21:16.204180 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00893975 (* 1 = 0.00893975 loss)
I1026 01:21:16.204185 17254 sgd_solver.cpp:106] Iteration 18020, lr = 0.001
I1026 01:21:17.288358 17254 solver.cpp:229] Iteration 18040, loss = 0.0646938
I1026 01:21:17.288408 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0554801 (* 1 = 0.0554801 loss)
I1026 01:21:17.288414 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00921368 (* 1 = 0.00921368 loss)
I1026 01:21:17.288429 17254 sgd_solver.cpp:106] Iteration 18040, lr = 0.001
I1026 01:21:18.401080 17254 solver.cpp:229] Iteration 18060, loss = 0.20199
I1026 01:21:18.401113 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.129754 (* 1 = 0.129754 loss)
I1026 01:21:18.401118 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0722357 (* 1 = 0.0722357 loss)
I1026 01:21:18.401124 17254 sgd_solver.cpp:106] Iteration 18060, lr = 0.001
I1026 01:21:19.507577 17254 solver.cpp:229] Iteration 18080, loss = 0.152158
I1026 01:21:19.507609 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.102419 (* 1 = 0.102419 loss)
I1026 01:21:19.507616 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0497395 (* 1 = 0.0497395 loss)
I1026 01:21:19.507621 17254 sgd_solver.cpp:106] Iteration 18080, lr = 0.001
I1026 01:21:20.594000 17254 solver.cpp:229] Iteration 18100, loss = 0.143037
I1026 01:21:20.594030 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0810041 (* 1 = 0.0810041 loss)
I1026 01:21:20.594035 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0620326 (* 1 = 0.0620326 loss)
I1026 01:21:20.594040 17254 sgd_solver.cpp:106] Iteration 18100, lr = 0.001
I1026 01:21:21.658089 17254 solver.cpp:229] Iteration 18120, loss = 0.104029
I1026 01:21:21.658121 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0619841 (* 1 = 0.0619841 loss)
I1026 01:21:21.658126 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0420452 (* 1 = 0.0420452 loss)
I1026 01:21:21.658133 17254 sgd_solver.cpp:106] Iteration 18120, lr = 0.001
I1026 01:21:22.736639 17254 solver.cpp:229] Iteration 18140, loss = 0.035307
I1026 01:21:22.736672 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.026458 (* 1 = 0.026458 loss)
I1026 01:21:22.736678 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00884905 (* 1 = 0.00884905 loss)
I1026 01:21:22.736683 17254 sgd_solver.cpp:106] Iteration 18140, lr = 0.001
I1026 01:21:23.824059 17254 solver.cpp:229] Iteration 18160, loss = 0.0992295
I1026 01:21:23.824101 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0496762 (* 1 = 0.0496762 loss)
I1026 01:21:23.824106 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0495533 (* 1 = 0.0495533 loss)
I1026 01:21:23.824111 17254 sgd_solver.cpp:106] Iteration 18160, lr = 0.001
I1026 01:21:24.914500 17254 solver.cpp:229] Iteration 18180, loss = 0.126505
I1026 01:21:24.914533 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0639946 (* 1 = 0.0639946 loss)
I1026 01:21:24.914539 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0625106 (* 1 = 0.0625106 loss)
I1026 01:21:24.914544 17254 sgd_solver.cpp:106] Iteration 18180, lr = 0.001
I1026 01:21:25.991791 17254 solver.cpp:229] Iteration 18200, loss = 0.0511262
I1026 01:21:25.991824 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0386952 (* 1 = 0.0386952 loss)
I1026 01:21:25.991829 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.012431 (* 1 = 0.012431 loss)
I1026 01:21:25.991835 17254 sgd_solver.cpp:106] Iteration 18200, lr = 0.001
I1026 01:21:27.096547 17254 solver.cpp:229] Iteration 18220, loss = 0.0782745
I1026 01:21:27.096580 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0489627 (* 1 = 0.0489627 loss)
I1026 01:21:27.096586 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0293118 (* 1 = 0.0293118 loss)
I1026 01:21:27.096591 17254 sgd_solver.cpp:106] Iteration 18220, lr = 0.001
I1026 01:21:28.178340 17254 solver.cpp:229] Iteration 18240, loss = 0.0632466
I1026 01:21:28.178383 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0426419 (* 1 = 0.0426419 loss)
I1026 01:21:28.178390 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0206047 (* 1 = 0.0206047 loss)
I1026 01:21:28.178395 17254 sgd_solver.cpp:106] Iteration 18240, lr = 0.001
I1026 01:21:29.245218 17254 solver.cpp:229] Iteration 18260, loss = 0.063352
I1026 01:21:29.245261 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0293653 (* 1 = 0.0293653 loss)
I1026 01:21:29.245267 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0339866 (* 1 = 0.0339866 loss)
I1026 01:21:29.245273 17254 sgd_solver.cpp:106] Iteration 18260, lr = 0.001
I1026 01:21:30.321800 17254 solver.cpp:229] Iteration 18280, loss = 0.120964
I1026 01:21:30.321833 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0455563 (* 1 = 0.0455563 loss)
I1026 01:21:30.321838 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0754082 (* 1 = 0.0754082 loss)
I1026 01:21:30.321844 17254 sgd_solver.cpp:106] Iteration 18280, lr = 0.001
I1026 01:21:31.411736 17254 solver.cpp:229] Iteration 18300, loss = 0.0445792
I1026 01:21:31.411767 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0273145 (* 1 = 0.0273145 loss)
I1026 01:21:31.411773 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0172646 (* 1 = 0.0172646 loss)
I1026 01:21:31.411778 17254 sgd_solver.cpp:106] Iteration 18300, lr = 0.001
I1026 01:21:32.501978 17254 solver.cpp:229] Iteration 18320, loss = 0.0770843
I1026 01:21:32.502032 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0523632 (* 1 = 0.0523632 loss)
I1026 01:21:32.502046 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0247211 (* 1 = 0.0247211 loss)
I1026 01:21:32.502053 17254 sgd_solver.cpp:106] Iteration 18320, lr = 0.001
I1026 01:21:33.595247 17254 solver.cpp:229] Iteration 18340, loss = 0.108769
I1026 01:21:33.595289 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0259602 (* 1 = 0.0259602 loss)
I1026 01:21:33.595295 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0828088 (* 1 = 0.0828088 loss)
I1026 01:21:33.595300 17254 sgd_solver.cpp:106] Iteration 18340, lr = 0.001
I1026 01:21:34.704265 17254 solver.cpp:229] Iteration 18360, loss = 0.0541414
I1026 01:21:34.704309 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0319929 (* 1 = 0.0319929 loss)
I1026 01:21:34.704314 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0221485 (* 1 = 0.0221485 loss)
I1026 01:21:34.704319 17254 sgd_solver.cpp:106] Iteration 18360, lr = 0.001
I1026 01:21:35.778053 17254 solver.cpp:229] Iteration 18380, loss = 0.0688501
I1026 01:21:35.778085 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0462385 (* 1 = 0.0462385 loss)
I1026 01:21:35.778091 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0226116 (* 1 = 0.0226116 loss)
I1026 01:21:35.778096 17254 sgd_solver.cpp:106] Iteration 18380, lr = 0.001
I1026 01:21:36.870798 17254 solver.cpp:229] Iteration 18400, loss = 0.0931687
I1026 01:21:36.870831 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0576344 (* 1 = 0.0576344 loss)
I1026 01:21:36.870836 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0355343 (* 1 = 0.0355343 loss)
I1026 01:21:36.870842 17254 sgd_solver.cpp:106] Iteration 18400, lr = 0.001
I1026 01:21:37.939595 17254 solver.cpp:229] Iteration 18420, loss = 0.0982045
I1026 01:21:37.939628 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0340153 (* 1 = 0.0340153 loss)
I1026 01:21:37.939635 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0641892 (* 1 = 0.0641892 loss)
I1026 01:21:37.939640 17254 sgd_solver.cpp:106] Iteration 18420, lr = 0.001
I1026 01:21:39.026806 17254 solver.cpp:229] Iteration 18440, loss = 0.173836
I1026 01:21:39.026839 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.110518 (* 1 = 0.110518 loss)
I1026 01:21:39.026844 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0633181 (* 1 = 0.0633181 loss)
I1026 01:21:39.026850 17254 sgd_solver.cpp:106] Iteration 18440, lr = 0.001
I1026 01:21:40.108371 17254 solver.cpp:229] Iteration 18460, loss = 0.0989941
I1026 01:21:40.108403 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0693304 (* 1 = 0.0693304 loss)
I1026 01:21:40.108408 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0296637 (* 1 = 0.0296637 loss)
I1026 01:21:40.108413 17254 sgd_solver.cpp:106] Iteration 18460, lr = 0.001
I1026 01:21:41.187991 17254 solver.cpp:229] Iteration 18480, loss = 0.19662
I1026 01:21:41.188024 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.12491 (* 1 = 0.12491 loss)
I1026 01:21:41.188029 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0717091 (* 1 = 0.0717091 loss)
I1026 01:21:41.188033 17254 sgd_solver.cpp:106] Iteration 18480, lr = 0.001
I1026 01:21:42.256904 17254 solver.cpp:229] Iteration 18500, loss = 0.0515536
I1026 01:21:42.256937 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0282845 (* 1 = 0.0282845 loss)
I1026 01:21:42.256947 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0232691 (* 1 = 0.0232691 loss)
I1026 01:21:42.256963 17254 sgd_solver.cpp:106] Iteration 18500, lr = 0.001
I1026 01:21:43.349318 17254 solver.cpp:229] Iteration 18520, loss = 0.0455157
I1026 01:21:43.349354 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0307547 (* 1 = 0.0307547 loss)
I1026 01:21:43.349361 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.014761 (* 1 = 0.014761 loss)
I1026 01:21:43.349367 17254 sgd_solver.cpp:106] Iteration 18520, lr = 0.001
I1026 01:21:44.405365 17254 solver.cpp:229] Iteration 18540, loss = 0.0705485
I1026 01:21:44.405400 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0290834 (* 1 = 0.0290834 loss)
I1026 01:21:44.405407 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0414651 (* 1 = 0.0414651 loss)
I1026 01:21:44.405424 17254 sgd_solver.cpp:106] Iteration 18540, lr = 0.001
I1026 01:21:45.500277 17254 solver.cpp:229] Iteration 18560, loss = 0.045719
I1026 01:21:45.500319 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0337015 (* 1 = 0.0337015 loss)
I1026 01:21:45.500324 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0120175 (* 1 = 0.0120175 loss)
I1026 01:21:45.500329 17254 sgd_solver.cpp:106] Iteration 18560, lr = 0.001
I1026 01:21:46.568902 17254 solver.cpp:229] Iteration 18580, loss = 0.106113
I1026 01:21:46.568933 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0665428 (* 1 = 0.0665428 loss)
I1026 01:21:46.568955 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0395707 (* 1 = 0.0395707 loss)
I1026 01:21:46.568960 17254 sgd_solver.cpp:106] Iteration 18580, lr = 0.001
I1026 01:21:47.654525 17254 solver.cpp:229] Iteration 18600, loss = 0.0536211
I1026 01:21:47.654557 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0477156 (* 1 = 0.0477156 loss)
I1026 01:21:47.654562 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0059055 (* 1 = 0.0059055 loss)
I1026 01:21:47.654577 17254 sgd_solver.cpp:106] Iteration 18600, lr = 0.001
I1026 01:21:48.750005 17254 solver.cpp:229] Iteration 18620, loss = 0.0528364
I1026 01:21:48.750037 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0401592 (* 1 = 0.0401592 loss)
I1026 01:21:48.750042 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0126772 (* 1 = 0.0126772 loss)
I1026 01:21:48.750047 17254 sgd_solver.cpp:106] Iteration 18620, lr = 0.001
I1026 01:21:49.835769 17254 solver.cpp:229] Iteration 18640, loss = 0.112395
I1026 01:21:49.835801 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0400387 (* 1 = 0.0400387 loss)
I1026 01:21:49.835806 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0723566 (* 1 = 0.0723566 loss)
I1026 01:21:49.835811 17254 sgd_solver.cpp:106] Iteration 18640, lr = 0.001
I1026 01:21:50.928612 17254 solver.cpp:229] Iteration 18660, loss = 0.0437989
I1026 01:21:50.928673 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0315912 (* 1 = 0.0315912 loss)
I1026 01:21:50.928678 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0122077 (* 1 = 0.0122077 loss)
I1026 01:21:50.928692 17254 sgd_solver.cpp:106] Iteration 18660, lr = 0.001
I1026 01:21:52.022680 17254 solver.cpp:229] Iteration 18680, loss = 0.0691479
I1026 01:21:52.022712 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0564864 (* 1 = 0.0564864 loss)
I1026 01:21:52.022717 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0126614 (* 1 = 0.0126614 loss)
I1026 01:21:52.022722 17254 sgd_solver.cpp:106] Iteration 18680, lr = 0.001
I1026 01:21:53.112332 17254 solver.cpp:229] Iteration 18700, loss = 0.158989
I1026 01:21:53.112365 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0797094 (* 1 = 0.0797094 loss)
I1026 01:21:53.112370 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0792793 (* 1 = 0.0792793 loss)
I1026 01:21:53.112375 17254 sgd_solver.cpp:106] Iteration 18700, lr = 0.001
I1026 01:21:54.216480 17254 solver.cpp:229] Iteration 18720, loss = 0.0765549
I1026 01:21:54.216511 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0635465 (* 1 = 0.0635465 loss)
I1026 01:21:54.216516 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0130084 (* 1 = 0.0130084 loss)
I1026 01:21:54.216521 17254 sgd_solver.cpp:106] Iteration 18720, lr = 0.001
I1026 01:21:55.303691 17254 solver.cpp:229] Iteration 18740, loss = 0.0406797
I1026 01:21:55.303725 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.032633 (* 1 = 0.032633 loss)
I1026 01:21:55.303747 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00804669 (* 1 = 0.00804669 loss)
I1026 01:21:55.303752 17254 sgd_solver.cpp:106] Iteration 18740, lr = 0.001
I1026 01:21:56.374447 17254 solver.cpp:229] Iteration 18760, loss = 0.0880038
I1026 01:21:56.374490 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0280634 (* 1 = 0.0280634 loss)
I1026 01:21:56.374495 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0599404 (* 1 = 0.0599404 loss)
I1026 01:21:56.374500 17254 sgd_solver.cpp:106] Iteration 18760, lr = 0.001
I1026 01:21:57.485649 17254 solver.cpp:229] Iteration 18780, loss = 0.146629
I1026 01:21:57.485682 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0609794 (* 1 = 0.0609794 loss)
I1026 01:21:57.485687 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.08565 (* 1 = 0.08565 loss)
I1026 01:21:57.485690 17254 sgd_solver.cpp:106] Iteration 18780, lr = 0.001
I1026 01:21:58.587357 17254 solver.cpp:229] Iteration 18800, loss = 0.0769457
I1026 01:21:58.587389 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0602249 (* 1 = 0.0602249 loss)
I1026 01:21:58.587394 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0167208 (* 1 = 0.0167208 loss)
I1026 01:21:58.587399 17254 sgd_solver.cpp:106] Iteration 18800, lr = 0.001
I1026 01:21:59.657925 17254 solver.cpp:229] Iteration 18820, loss = 0.105459
I1026 01:21:59.657958 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0534855 (* 1 = 0.0534855 loss)
I1026 01:21:59.657963 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0519739 (* 1 = 0.0519739 loss)
I1026 01:21:59.657969 17254 sgd_solver.cpp:106] Iteration 18820, lr = 0.001
I1026 01:22:00.736207 17254 solver.cpp:229] Iteration 18840, loss = 0.0532034
I1026 01:22:00.736239 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0295603 (* 1 = 0.0295603 loss)
I1026 01:22:00.736244 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0236431 (* 1 = 0.0236431 loss)
I1026 01:22:00.736253 17254 sgd_solver.cpp:106] Iteration 18840, lr = 0.001
I1026 01:22:01.818910 17254 solver.cpp:229] Iteration 18860, loss = 0.073332
I1026 01:22:01.818943 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0398761 (* 1 = 0.0398761 loss)
I1026 01:22:01.818948 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0334559 (* 1 = 0.0334559 loss)
I1026 01:22:01.818953 17254 sgd_solver.cpp:106] Iteration 18860, lr = 0.001
I1026 01:22:02.921495 17254 solver.cpp:229] Iteration 18880, loss = 0.155743
I1026 01:22:02.921526 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.107144 (* 1 = 0.107144 loss)
I1026 01:22:02.921532 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0485984 (* 1 = 0.0485984 loss)
I1026 01:22:02.921536 17254 sgd_solver.cpp:106] Iteration 18880, lr = 0.001
I1026 01:22:03.995692 17254 solver.cpp:229] Iteration 18900, loss = 0.0673446
I1026 01:22:03.995723 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0355727 (* 1 = 0.0355727 loss)
I1026 01:22:03.995728 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0317719 (* 1 = 0.0317719 loss)
I1026 01:22:03.995733 17254 sgd_solver.cpp:106] Iteration 18900, lr = 0.001
I1026 01:22:05.067468 17254 solver.cpp:229] Iteration 18920, loss = 0.0810346
I1026 01:22:05.067502 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.047576 (* 1 = 0.047576 loss)
I1026 01:22:05.067507 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0334587 (* 1 = 0.0334587 loss)
I1026 01:22:05.067512 17254 sgd_solver.cpp:106] Iteration 18920, lr = 0.001
I1026 01:22:06.157058 17254 solver.cpp:229] Iteration 18940, loss = 0.117867
I1026 01:22:06.157099 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0820506 (* 1 = 0.0820506 loss)
I1026 01:22:06.157104 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0358165 (* 1 = 0.0358165 loss)
I1026 01:22:06.157109 17254 sgd_solver.cpp:106] Iteration 18940, lr = 0.001
I1026 01:22:07.234683 17254 solver.cpp:229] Iteration 18960, loss = 0.0727936
I1026 01:22:07.234717 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0396212 (* 1 = 0.0396212 loss)
I1026 01:22:07.234722 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0331724 (* 1 = 0.0331724 loss)
I1026 01:22:07.234727 17254 sgd_solver.cpp:106] Iteration 18960, lr = 0.001
I1026 01:22:08.320870 17254 solver.cpp:229] Iteration 18980, loss = 0.0504017
I1026 01:22:08.320902 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0307426 (* 1 = 0.0307426 loss)
I1026 01:22:08.320907 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0196591 (* 1 = 0.0196591 loss)
I1026 01:22:08.320912 17254 sgd_solver.cpp:106] Iteration 18980, lr = 0.001
I1026 01:22:09.420161 17254 solver.cpp:229] Iteration 19000, loss = 0.0626818
I1026 01:22:09.420194 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0452204 (* 1 = 0.0452204 loss)
I1026 01:22:09.420199 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0174614 (* 1 = 0.0174614 loss)
I1026 01:22:09.420204 17254 sgd_solver.cpp:106] Iteration 19000, lr = 0.001
I1026 01:22:10.509802 17254 solver.cpp:229] Iteration 19020, loss = 0.0827819
I1026 01:22:10.509834 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0615488 (* 1 = 0.0615488 loss)
I1026 01:22:10.509838 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.021233 (* 1 = 0.021233 loss)
I1026 01:22:10.509845 17254 sgd_solver.cpp:106] Iteration 19020, lr = 0.001
I1026 01:22:11.585412 17254 solver.cpp:229] Iteration 19040, loss = 0.0841478
I1026 01:22:11.585443 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0643886 (* 1 = 0.0643886 loss)
I1026 01:22:11.585448 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0197592 (* 1 = 0.0197592 loss)
I1026 01:22:11.585454 17254 sgd_solver.cpp:106] Iteration 19040, lr = 0.001
I1026 01:22:12.681073 17254 solver.cpp:229] Iteration 19060, loss = 0.0507671
I1026 01:22:12.681107 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0404191 (* 1 = 0.0404191 loss)
I1026 01:22:12.681112 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.010348 (* 1 = 0.010348 loss)
I1026 01:22:12.681116 17254 sgd_solver.cpp:106] Iteration 19060, lr = 0.001
I1026 01:22:13.780338 17254 solver.cpp:229] Iteration 19080, loss = 0.0490868
I1026 01:22:13.780370 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0387955 (* 1 = 0.0387955 loss)
I1026 01:22:13.780375 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0102913 (* 1 = 0.0102913 loss)
I1026 01:22:13.780381 17254 sgd_solver.cpp:106] Iteration 19080, lr = 0.001
I1026 01:22:14.871656 17254 solver.cpp:229] Iteration 19100, loss = 0.0979087
I1026 01:22:14.871688 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0253208 (* 1 = 0.0253208 loss)
I1026 01:22:14.871693 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0725879 (* 1 = 0.0725879 loss)
I1026 01:22:14.871700 17254 sgd_solver.cpp:106] Iteration 19100, lr = 0.001
I1026 01:22:15.972151 17254 solver.cpp:229] Iteration 19120, loss = 0.0692935
I1026 01:22:15.972182 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0388385 (* 1 = 0.0388385 loss)
I1026 01:22:15.972187 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.030455 (* 1 = 0.030455 loss)
I1026 01:22:15.972193 17254 sgd_solver.cpp:106] Iteration 19120, lr = 0.001
I1026 01:22:17.066675 17254 solver.cpp:229] Iteration 19140, loss = 0.0534357
I1026 01:22:17.066709 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0274221 (* 1 = 0.0274221 loss)
I1026 01:22:17.066714 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0260136 (* 1 = 0.0260136 loss)
I1026 01:22:17.066718 17254 sgd_solver.cpp:106] Iteration 19140, lr = 0.001
I1026 01:22:18.158257 17254 solver.cpp:229] Iteration 19160, loss = 0.0386397
I1026 01:22:18.158298 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0321921 (* 1 = 0.0321921 loss)
I1026 01:22:18.158304 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00644756 (* 1 = 0.00644756 loss)
I1026 01:22:18.158309 17254 sgd_solver.cpp:106] Iteration 19160, lr = 0.001
I1026 01:22:19.254495 17254 solver.cpp:229] Iteration 19180, loss = 0.0658457
I1026 01:22:19.254528 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0398278 (* 1 = 0.0398278 loss)
I1026 01:22:19.254534 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0260179 (* 1 = 0.0260179 loss)
I1026 01:22:19.254539 17254 sgd_solver.cpp:106] Iteration 19180, lr = 0.001
I1026 01:22:20.332124 17254 solver.cpp:229] Iteration 19200, loss = 0.116011
I1026 01:22:20.332157 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0342138 (* 1 = 0.0342138 loss)
I1026 01:22:20.332162 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0817974 (* 1 = 0.0817974 loss)
I1026 01:22:20.332167 17254 sgd_solver.cpp:106] Iteration 19200, lr = 0.001
I1026 01:22:21.425089 17254 solver.cpp:229] Iteration 19220, loss = 0.139546
I1026 01:22:21.425122 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0694734 (* 1 = 0.0694734 loss)
I1026 01:22:21.425127 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0700725 (* 1 = 0.0700725 loss)
I1026 01:22:21.425132 17254 sgd_solver.cpp:106] Iteration 19220, lr = 0.001
I1026 01:22:22.516662 17254 solver.cpp:229] Iteration 19240, loss = 0.122884
I1026 01:22:22.516693 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0702411 (* 1 = 0.0702411 loss)
I1026 01:22:22.516698 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0526431 (* 1 = 0.0526431 loss)
I1026 01:22:22.516703 17254 sgd_solver.cpp:106] Iteration 19240, lr = 0.001
I1026 01:22:23.613422 17254 solver.cpp:229] Iteration 19260, loss = 0.130907
I1026 01:22:23.613456 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.051376 (* 1 = 0.051376 loss)
I1026 01:22:23.613461 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0795306 (* 1 = 0.0795306 loss)
I1026 01:22:23.613466 17254 sgd_solver.cpp:106] Iteration 19260, lr = 0.001
I1026 01:22:24.692162 17254 solver.cpp:229] Iteration 19280, loss = 0.0687162
I1026 01:22:24.692204 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0557336 (* 1 = 0.0557336 loss)
I1026 01:22:24.692209 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0129826 (* 1 = 0.0129826 loss)
I1026 01:22:24.692215 17254 sgd_solver.cpp:106] Iteration 19280, lr = 0.001
I1026 01:22:25.766742 17254 solver.cpp:229] Iteration 19300, loss = 0.102825
I1026 01:22:25.766777 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0723566 (* 1 = 0.0723566 loss)
I1026 01:22:25.766780 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0304682 (* 1 = 0.0304682 loss)
I1026 01:22:25.766785 17254 sgd_solver.cpp:106] Iteration 19300, lr = 0.001
I1026 01:22:26.868118 17254 solver.cpp:229] Iteration 19320, loss = 0.0506141
I1026 01:22:26.868152 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0417735 (* 1 = 0.0417735 loss)
I1026 01:22:26.868161 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0088406 (* 1 = 0.0088406 loss)
I1026 01:22:26.868168 17254 sgd_solver.cpp:106] Iteration 19320, lr = 0.001
I1026 01:22:27.966102 17254 solver.cpp:229] Iteration 19340, loss = 0.0955653
I1026 01:22:27.966135 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0526165 (* 1 = 0.0526165 loss)
I1026 01:22:27.966145 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0429489 (* 1 = 0.0429489 loss)
I1026 01:22:27.966150 17254 sgd_solver.cpp:106] Iteration 19340, lr = 0.001
I1026 01:22:29.062479 17254 solver.cpp:229] Iteration 19360, loss = 0.105886
I1026 01:22:29.062515 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0234357 (* 1 = 0.0234357 loss)
I1026 01:22:29.062521 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0824502 (* 1 = 0.0824502 loss)
I1026 01:22:29.062528 17254 sgd_solver.cpp:106] Iteration 19360, lr = 0.001
I1026 01:22:30.158404 17254 solver.cpp:229] Iteration 19380, loss = 0.122631
I1026 01:22:30.158440 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.067166 (* 1 = 0.067166 loss)
I1026 01:22:30.158449 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0554647 (* 1 = 0.0554647 loss)
I1026 01:22:30.158457 17254 sgd_solver.cpp:106] Iteration 19380, lr = 0.001
I1026 01:22:31.258411 17254 solver.cpp:229] Iteration 19400, loss = 0.0647935
I1026 01:22:31.258446 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0381199 (* 1 = 0.0381199 loss)
I1026 01:22:31.258455 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0266735 (* 1 = 0.0266735 loss)
I1026 01:22:31.258461 17254 sgd_solver.cpp:106] Iteration 19400, lr = 0.001
I1026 01:22:32.337113 17254 solver.cpp:229] Iteration 19420, loss = 0.0880848
I1026 01:22:32.337148 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0532469 (* 1 = 0.0532469 loss)
I1026 01:22:32.337167 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0348379 (* 1 = 0.0348379 loss)
I1026 01:22:32.337173 17254 sgd_solver.cpp:106] Iteration 19420, lr = 0.001
I1026 01:22:33.430826 17254 solver.cpp:229] Iteration 19440, loss = 0.0477014
I1026 01:22:33.430858 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0309533 (* 1 = 0.0309533 loss)
I1026 01:22:33.430863 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0167481 (* 1 = 0.0167481 loss)
I1026 01:22:33.430868 17254 sgd_solver.cpp:106] Iteration 19440, lr = 0.001
I1026 01:22:34.531234 17254 solver.cpp:229] Iteration 19460, loss = 0.106762
I1026 01:22:34.531266 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0622426 (* 1 = 0.0622426 loss)
I1026 01:22:34.531271 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0445197 (* 1 = 0.0445197 loss)
I1026 01:22:34.531276 17254 sgd_solver.cpp:106] Iteration 19460, lr = 0.001
I1026 01:22:35.628764 17254 solver.cpp:229] Iteration 19480, loss = 0.0578835
I1026 01:22:35.628796 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0457799 (* 1 = 0.0457799 loss)
I1026 01:22:35.628803 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0121036 (* 1 = 0.0121036 loss)
I1026 01:22:35.628808 17254 sgd_solver.cpp:106] Iteration 19480, lr = 0.001
I1026 01:22:36.694183 17254 solver.cpp:229] Iteration 19500, loss = 0.049391
I1026 01:22:36.694216 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0317572 (* 1 = 0.0317572 loss)
I1026 01:22:36.694221 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0176338 (* 1 = 0.0176338 loss)
I1026 01:22:36.694236 17254 sgd_solver.cpp:106] Iteration 19500, lr = 0.001
I1026 01:22:37.760103 17254 solver.cpp:229] Iteration 19520, loss = 0.125238
I1026 01:22:37.760136 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0611377 (* 1 = 0.0611377 loss)
I1026 01:22:37.760141 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0640998 (* 1 = 0.0640998 loss)
I1026 01:22:37.760146 17254 sgd_solver.cpp:106] Iteration 19520, lr = 0.001
I1026 01:22:38.846984 17254 solver.cpp:229] Iteration 19540, loss = 0.0483017
I1026 01:22:38.847028 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0343968 (* 1 = 0.0343968 loss)
I1026 01:22:38.847033 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0139049 (* 1 = 0.0139049 loss)
I1026 01:22:38.847038 17254 sgd_solver.cpp:106] Iteration 19540, lr = 0.001
I1026 01:22:39.932696 17254 solver.cpp:229] Iteration 19560, loss = 0.0997304
I1026 01:22:39.932729 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0495883 (* 1 = 0.0495883 loss)
I1026 01:22:39.932734 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0501421 (* 1 = 0.0501421 loss)
I1026 01:22:39.932740 17254 sgd_solver.cpp:106] Iteration 19560, lr = 0.001
I1026 01:22:41.031653 17254 solver.cpp:229] Iteration 19580, loss = 0.242153
I1026 01:22:41.031687 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.105863 (* 1 = 0.105863 loss)
I1026 01:22:41.031692 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.13629 (* 1 = 0.13629 loss)
I1026 01:22:41.031697 17254 sgd_solver.cpp:106] Iteration 19580, lr = 0.001
I1026 01:22:42.114202 17254 solver.cpp:229] Iteration 19600, loss = 0.0748763
I1026 01:22:42.114234 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0652596 (* 1 = 0.0652596 loss)
I1026 01:22:42.114256 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00961674 (* 1 = 0.00961674 loss)
I1026 01:22:42.114262 17254 sgd_solver.cpp:106] Iteration 19600, lr = 0.001
I1026 01:22:43.211751 17254 solver.cpp:229] Iteration 19620, loss = 0.0755429
I1026 01:22:43.211784 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0420095 (* 1 = 0.0420095 loss)
I1026 01:22:43.211804 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0335334 (* 1 = 0.0335334 loss)
I1026 01:22:43.211809 17254 sgd_solver.cpp:106] Iteration 19620, lr = 0.001
I1026 01:22:44.314987 17254 solver.cpp:229] Iteration 19640, loss = 0.0866675
I1026 01:22:44.315022 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0630741 (* 1 = 0.0630741 loss)
I1026 01:22:44.315031 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0235934 (* 1 = 0.0235934 loss)
I1026 01:22:44.315037 17254 sgd_solver.cpp:106] Iteration 19640, lr = 0.001
I1026 01:22:45.429373 17254 solver.cpp:229] Iteration 19660, loss = 0.0791405
I1026 01:22:45.429404 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.063215 (* 1 = 0.063215 loss)
I1026 01:22:45.429412 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0159255 (* 1 = 0.0159255 loss)
I1026 01:22:45.429419 17254 sgd_solver.cpp:106] Iteration 19660, lr = 0.001
I1026 01:22:46.533418 17254 solver.cpp:229] Iteration 19680, loss = 0.0882102
I1026 01:22:46.533453 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0630347 (* 1 = 0.0630347 loss)
I1026 01:22:46.533460 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0251755 (* 1 = 0.0251755 loss)
I1026 01:22:46.533478 17254 sgd_solver.cpp:106] Iteration 19680, lr = 0.001
I1026 01:22:47.638742 17254 solver.cpp:229] Iteration 19700, loss = 0.0343263
I1026 01:22:47.638777 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0279407 (* 1 = 0.0279407 loss)
I1026 01:22:47.638785 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00638561 (* 1 = 0.00638561 loss)
I1026 01:22:47.638803 17254 sgd_solver.cpp:106] Iteration 19700, lr = 0.001
I1026 01:22:48.739225 17254 solver.cpp:229] Iteration 19720, loss = 0.127965
I1026 01:22:48.739259 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0319868 (* 1 = 0.0319868 loss)
I1026 01:22:48.739267 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0959785 (* 1 = 0.0959785 loss)
I1026 01:22:48.739274 17254 sgd_solver.cpp:106] Iteration 19720, lr = 0.001
I1026 01:22:49.817344 17254 solver.cpp:229] Iteration 19740, loss = 0.0797885
I1026 01:22:49.817379 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0518035 (* 1 = 0.0518035 loss)
I1026 01:22:49.817387 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.027985 (* 1 = 0.027985 loss)
I1026 01:22:49.817404 17254 sgd_solver.cpp:106] Iteration 19740, lr = 0.001
I1026 01:22:50.905818 17254 solver.cpp:229] Iteration 19760, loss = 0.0793896
I1026 01:22:50.905853 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0493454 (* 1 = 0.0493454 loss)
I1026 01:22:50.905861 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0300442 (* 1 = 0.0300442 loss)
I1026 01:22:50.905869 17254 sgd_solver.cpp:106] Iteration 19760, lr = 0.001
I1026 01:22:51.980348 17254 solver.cpp:229] Iteration 19780, loss = 0.0378314
I1026 01:22:51.980381 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0274765 (* 1 = 0.0274765 loss)
I1026 01:22:51.980386 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0103549 (* 1 = 0.0103549 loss)
I1026 01:22:51.980392 17254 sgd_solver.cpp:106] Iteration 19780, lr = 0.001
I1026 01:22:53.067956 17254 solver.cpp:229] Iteration 19800, loss = 0.0516825
I1026 01:22:53.068001 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0429157 (* 1 = 0.0429157 loss)
I1026 01:22:53.068006 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00876683 (* 1 = 0.00876683 loss)
I1026 01:22:53.068011 17254 sgd_solver.cpp:106] Iteration 19800, lr = 0.001
I1026 01:22:54.183550 17254 solver.cpp:229] Iteration 19820, loss = 0.117753
I1026 01:22:54.183583 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0718076 (* 1 = 0.0718076 loss)
I1026 01:22:54.183589 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0459457 (* 1 = 0.0459457 loss)
I1026 01:22:54.183594 17254 sgd_solver.cpp:106] Iteration 19820, lr = 0.001
I1026 01:22:55.264621 17254 solver.cpp:229] Iteration 19840, loss = 0.0533522
I1026 01:22:55.264653 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0306319 (* 1 = 0.0306319 loss)
I1026 01:22:55.264659 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0227203 (* 1 = 0.0227203 loss)
I1026 01:22:55.264664 17254 sgd_solver.cpp:106] Iteration 19840, lr = 0.001
I1026 01:22:56.341048 17254 solver.cpp:229] Iteration 19860, loss = 0.132139
I1026 01:22:56.341080 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0912019 (* 1 = 0.0912019 loss)
I1026 01:22:56.341086 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0409369 (* 1 = 0.0409369 loss)
I1026 01:22:56.341091 17254 sgd_solver.cpp:106] Iteration 19860, lr = 0.001
I1026 01:22:57.415092 17254 solver.cpp:229] Iteration 19880, loss = 0.153389
I1026 01:22:57.415127 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0845695 (* 1 = 0.0845695 loss)
I1026 01:22:57.415132 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0688195 (* 1 = 0.0688195 loss)
I1026 01:22:57.415138 17254 sgd_solver.cpp:106] Iteration 19880, lr = 0.001
I1026 01:22:58.523288 17254 solver.cpp:229] Iteration 19900, loss = 0.0562072
I1026 01:22:58.523321 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0383241 (* 1 = 0.0383241 loss)
I1026 01:22:58.523326 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.017883 (* 1 = 0.017883 loss)
I1026 01:22:58.523331 17254 sgd_solver.cpp:106] Iteration 19900, lr = 0.001
I1026 01:22:59.602319 17254 solver.cpp:229] Iteration 19920, loss = 0.0971824
I1026 01:22:59.602352 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0412851 (* 1 = 0.0412851 loss)
I1026 01:22:59.602357 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0558972 (* 1 = 0.0558972 loss)
I1026 01:22:59.602362 17254 sgd_solver.cpp:106] Iteration 19920, lr = 0.001
I1026 01:23:00.712460 17254 solver.cpp:229] Iteration 19940, loss = 0.0660268
I1026 01:23:00.712491 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0621864 (* 1 = 0.0621864 loss)
I1026 01:23:00.712496 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.00384041 (* 1 = 0.00384041 loss)
I1026 01:23:00.712502 17254 sgd_solver.cpp:106] Iteration 19940, lr = 0.001
I1026 01:23:01.802389 17254 solver.cpp:229] Iteration 19960, loss = 0.0636051
I1026 01:23:01.802433 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0283403 (* 1 = 0.0283403 loss)
I1026 01:23:01.802438 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0352648 (* 1 = 0.0352648 loss)
I1026 01:23:01.802443 17254 sgd_solver.cpp:106] Iteration 19960, lr = 0.001
I1026 01:23:02.895017 17254 solver.cpp:229] Iteration 19980, loss = 0.123518
I1026 01:23:02.895051 17254 solver.cpp:245]     Train net output #0: bbox_loss = 0.0512312 (* 1 = 0.0512312 loss)
I1026 01:23:02.895056 17254 solver.cpp:245]     Train net output #1: cls_loss = 0.0722868 (* 1 = 0.0722868 loss)
I1026 01:23:02.895061 17254 sgd_solver.cpp:106] Iteration 19980, lr = 0.001
055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
speed: 0.055s / iter
Wrote snapshot to: /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_fast_rcnn_stage2_iter_20000.caffemodel
done solving
cp /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/zf_fast_rcnn_stage2_iter_20000.caffemodel -> /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/ZF_faster_rcnn_final.caffemodel
Final model: /home/cgangee/code/cg-py-faster-rcnn/output/faster_rcnn_alt_opt/tattoo_train/ZF_faster_rcnn_final.caffemodel
